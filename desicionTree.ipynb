{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a940c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import resample\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, load_wine\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score,precision_score, recall_score\n",
    "import nltk\n",
    "import pickle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Establecer semilla para reproducibilidad\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "609e8eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gini_impurity(y):\n",
    "    \"\"\"Calcular la impureza Gini de un array de etiquetas\"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    # Proporción de cada clase\n",
    "    clases = np.unique(y)\n",
    "    probabilities = [np.mean(y == c) for c in clases]\n",
    "    gini = 1 - sum(p ** 2 for p in probabilities)\n",
    "    return gini\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\"Calcular la entropía de un array de etiquetas\"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    # Proporción de cada clase\n",
    "    clases = np.unique(y)\n",
    "    probabilities = [np.mean(y == c) for c in clases]\n",
    "    # Usamos log2 para calcular la entropía\n",
    "    entropy_value = -sum(p * np.log2(p) for p in probabilities if p > 0)\n",
    "    return entropy_value\n",
    "\n",
    "def information_gain(y, left_indices, right_indices, impurity_function=gini_impurity):\n",
    "    \"\"\"Calcular la ganancia de información de una división utilizando impureza Gini o entropía\"\"\"\n",
    "    parent_impurity = impurity_function(y)\n",
    "\n",
    "    # Subconjuntos izquierdo y derecho\n",
    "    left_impurity = impurity_function(y[left_indices])\n",
    "    right_impurity = impurity_function(y[right_indices])\n",
    "\n",
    "    # Peso de los subconjuntos izquierdo y derecho\n",
    "    left_weight = len(left_indices) / len(y)\n",
    "    right_weight = len(right_indices) / len(y)\n",
    "\n",
    "    # Impureza ponderada de los hijos\n",
    "    weighted_impurity = left_weight * left_impurity + right_weight * right_impurity\n",
    "\n",
    "    # Ganancia de información\n",
    "    info_gain = parent_impurity - weighted_impurity\n",
    "    return info_gain\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, impurity_function=gini_impurity, min_samples_split=2,\n",
    "                 min_samples_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.impurity_function = impurity_function\n",
    "        self.tree = None\n",
    "\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Ajustar el árbol de decisión a los datos\"\"\"\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        if (n_samples < self.min_samples_split\n",
    "            or len(np.unique(y)) == 1\n",
    "            or (self.max_depth and depth == self.max_depth)):\n",
    "            return np.bincount(y).argmax()\n",
    "        # Si ya es un nodo hoja o alcanzamos la profundidad máxima ya no dividimos\n",
    "        if n_labels == 1 or (self.max_depth and depth == self.max_depth):\n",
    "            return np.unique(y)[0]\n",
    "\n",
    "        # Buscamos el atributo que gane más información\n",
    "        best_gain = -1 # Valor de inicio\n",
    "        best_split = None\n",
    "        best_left_indices = None\n",
    "        best_right_indices = None\n",
    "\n",
    "        # Por cada caracterísitca o dimensión\n",
    "        for feature in range(n_features):\n",
    "            # Limitar cantidad de thresholds por feature usando percentiles\n",
    "            # Esto para limitar la cantidad de separaciones\n",
    "            valores = np.percentile(X[:,feature], np.linspace(5, 95, 19))\n",
    "            valores = np.unique(valores)\n",
    "            # Sacamos las ramas\n",
    "            for i in range(1, len(valores)):\n",
    "                # Para que funcione con datos continuos sin la necesidad de discretizar\n",
    "                threshold = (valores[i - 1] + valores[i]) / 2\n",
    "                left_indices = np.where(X[:, feature] <= threshold)[0]\n",
    "                right_indices = np.where(X[:, feature] > threshold)[0]\n",
    "                # Descarta divisiones que dejen muy pocas muestras en una rama\n",
    "                if (len(left_indices) < self.min_samples_leaf\n",
    "                    or len(right_indices) < self.min_samples_leaf):\n",
    "                    continue\n",
    "\n",
    "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                    continue\n",
    "\n",
    "                gain = information_gain(y, left_indices, right_indices, self.impurity_function)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_split = {\n",
    "                        \"feature\": feature,\n",
    "                        \"value\": threshold\n",
    "                    }\n",
    "                    best_left_indices = left_indices\n",
    "                    best_right_indices = right_indices\n",
    "             # Si no se encontró una mejor división, retornamos la clase mayoritaria\n",
    "        if best_gain == -1:\n",
    "            return np.bincount(y).argmax()\n",
    "\n",
    "        # Construir recursivamente las ramas\n",
    "        left_subtree = self._build_tree(X[best_left_indices], y[best_left_indices], depth + 1)\n",
    "        right_subtree = self._build_tree(X[best_right_indices], y[best_right_indices], depth + 1)\n",
    "        return {\n",
    "        \"feature\": best_split[\"feature\"],\n",
    "        \"value\": best_split[\"value\"],\n",
    "        \"left\": left_subtree,\n",
    "        \"right\": right_subtree\n",
    "        }\n",
    "\n",
    "    def predict_one(self, x, node=None):\n",
    "        \"\"\"Predecir una muestra individual\"\"\"\n",
    "        if node is None:\n",
    "            node = self.tree\n",
    "\n",
    "        if not isinstance(node, dict):\n",
    "            return node\n",
    "\n",
    "        feature = node[\"feature\"]\n",
    "        value = node[\"value\"]\n",
    "\n",
    "        if x[feature] <= value:\n",
    "            return self.predict_one(x, node[\"left\"])\n",
    "        else:\n",
    "            return self.predict_one(x, node[\"right\"])\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Obtener los parámetros del modelo\"\"\"\n",
    "        return {\"max_depth\": self.max_depth, \"impurity_function\": self.impurity_function}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Configurar los parámetros del modelo\"\"\"\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predecir múltiples muestras\"\"\"\n",
    "        return np.array([self.predict_one(x) for x in X])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0effb255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album',\n",
      "       'Release Date', 'Key', 'Tempo', 'Loudness (db)', 'Time signature',\n",
      "       'Explicit', 'Popularity', 'Energy', 'Danceability', 'Similar Artist 1'],\n",
      "      dtype='object')\n",
      "<bound method Series.unique of 0        -13.78db\n",
      "1        -10.54db\n",
      "2        -11.63db\n",
      "3        -10.26db\n",
      "4        -10.46db\n",
      "           ...   \n",
      "65102     -8.38db\n",
      "65103     -5.61db\n",
      "65104        -6db\n",
      "65105     -8.44db\n",
      "65106     -12.7db\n",
      "Name: Loudness (db), Length: 65107, dtype: object>\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"./data/spotify_dataset.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "print(df.columns)\n",
    "#print(df[\"Explicit\"].unique)\n",
    "#print(df[\"Danceability\"].unique)\n",
    "print(df[\"Loudness (db)\"].unique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6888d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pioran/nltk_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pioran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/pioran/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/pioran/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Comprueba ruta:\n",
    "print(nltk.data.path[0]) \n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef477ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from text_preprocessing import TextPreprocessor, TFIDFVectorizer\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "# Juntar columnas\n",
    "columns_to_use = ['text', 'song', 'Artist(s)', 'Album', 'Similar Artist 1', 'Genre']\n",
    "df['combined_text'] = df[columns_to_use].fillna('').agg(' '.join, axis=1)\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "df['text_processed'] = df['combined_text'].apply(preprocessor.preprocess)\n",
    "\n",
    "vectorizer = TFIDFVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(df['text_processed'].tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55076c74",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pd.DataFrame(X).to_csv(\"X_tfidf_las_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126ba40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Loudness (db)'] = df['Loudness (db)'].astype(str).str.replace('db', '', regex=False)\n",
    "df['Loudness (db)'] = pd.to_numeric(df['Loudness (db)'], errors='coerce')\n",
    "# Asegurarse que tenga data\n",
    "df[['Danceability', 'Loudness (db)']] = df[['Danceability', 'Loudness (db)']].fillna(0)\n",
    "scaler = StandardScaler()\n",
    "X_num = scaler.fit_transform(df[['Danceability', 'Loudness (db)']])\n",
    "\n",
    "X = pd.read_csv(\"X_tfidf_las_df.csv\").values\n",
    "X = np.concatenate([X, X_num], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a537437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65107, 1002)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8251dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8794\n",
      "Precision: 0.8292\n",
      "F1 Score: 0.7514\n",
      "[[9077  489]\n",
      " [1082 2374]]\n",
      "Accuracy: 0.8846\n",
      "Precision: 0.8245\n",
      "F1 Score: 0.7675\n",
      "[[9038  528]\n",
      " [ 975 2481]]\n",
      "Accuracy: 0.8829\n",
      "Precision: 0.8173\n",
      "F1 Score: 0.7653\n",
      "[[9010  556]\n",
      " [ 969 2487]]\n",
      "Accuracy: 0.8827\n",
      "Precision: 0.8180\n",
      "F1 Score: 0.7647\n",
      "[[9014  552]\n",
      " [ 975 2481]]\n",
      "Accuracy: 0.8820\n",
      "Precision: 0.8166\n",
      "F1 Score: 0.7631\n",
      "[[9010  556]\n",
      " [ 981 2475]]\n",
      "Accuracy: 0.8796\n",
      "Precision: 0.8099\n",
      "F1 Score: 0.7588\n",
      "[[8987  579]\n",
      " [ 989 2467]]\n",
      "Accuracy: 0.8787\n",
      "Precision: 0.7995\n",
      "F1 Score: 0.7602\n",
      "[[8938  628]\n",
      " [ 952 2504]]\n"
     ]
    }
   ],
   "source": [
    "df['Explicit_binary'] = df['Explicit'].map({'Yes': 1, 'No': 0})\n",
    "y = df['Explicit_binary'].values\n",
    "\n",
    "depths = [10]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "for depth in depths:\n",
    "    tree = DecisionTree(max_depth=depth)\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_pred = tree.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Mostrar las métricas\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(cm)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Not Explicit', 'Explicit'],\n",
    "                yticklabels=['Not Explicit', 'Explicit'])\n",
    "\n",
    "    # Etiquetas personalizadas\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'DT C type Confusion Matrix (Depth={depth}) - Explicit Content Detection')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Guardar imagen\n",
    "    plt.savefig( str(depth) + \"confusion_matrix_C_type.png\")\n",
    "    plt.close()\n",
    "\n",
    "    with open(f\"decision_tree_model_depth_{depth}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tree, f)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff1c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8829\n",
      "Precision: 0.8173\n",
      "F1 Score: 0.7653\n",
      "[[9010  556]\n",
      " [ 969 2487]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3809917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
