{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abd89201",
   "metadata": {},
   "source": [
    "We are going to process the lyrics of a song using the batches Api to reduce the cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed34933",
   "metadata": {},
   "source": [
    "## Creación del batch file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a881b8e3",
   "metadata": {},
   "source": [
    "Rate limits\n",
    "Batch API rate limits are separate from existing per-model rate limits. The Batch API has two new types of rate limits:\n",
    "\n",
    "Per-batch limits: A single batch may include up to 50,000 requests, and a batch input file can be up to 200 MB in size. Note that /v1/embeddings batches are also restricted to a maximum of 50,000 embedding inputs across all requests in the batch.\n",
    "Enqueued prompt tokens per model: Each model has a maximum number of enqueued prompt tokens allowed for batch processing. You can find these limits on the Platform Settings page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f4c127",
   "metadata": {},
   "source": [
    "## 1) Creación del archivo jsonl for batch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1b3f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model\tCost\tBatch cost\n",
    "# text-embedding-3-small\t$0.01\t$0.0001\n",
    "# text-embedding-3-large\t$0.065\t$0.00013\n",
    "# text-embedding-ada-002\t$0.05\t$0.0004\n",
    "# https://platform.openai.com/docs/pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "641e894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import json \n",
    "import tiktoken\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989e39d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"/v1/embeddings\"\n",
    "path_data = \"../../../../data\"\n",
    "path_df =os.path.join(path_data, \"spotify_dataset_sin_duplicados_4.csv\")\n",
    "df = pd.read_csv(path_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704faaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comezando en la función\n",
      "Encoding obtenido\n",
      "Conteo listo\n",
      "Archivo generado: batch_embeddings_0_2.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to count the number of tokens\n",
    "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    print(\"Comezando en la función\")\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    print(\"Encoding obtenido\")\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    print(\"Conteo listo\")\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")\n",
    "\n",
    "\n",
    "# Function that take a start and a end to create the jsonl file \n",
    "#1) create bath file\n",
    "def crear_jsonl_embeddings(texts, start, end, dir= \"batch_files\", startfilename=\"batch_embeddings\", model=\"text-embedding-3-small\"):\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    filename = f\"{startfilename}_{start}_{end}.jsonl\"\n",
    "    with open(os.path.join(dir, filename), \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, text in enumerate(texts, start=start):\n",
    "            request = {\n",
    "                \"custom_id\": f\"request-{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/embeddings\",\n",
    "                \"body\": {\n",
    "                    \"model\": model,\n",
    "                    \"input\": text\n",
    "                }\n",
    "            }\n",
    "            f.write(json.dumps(request, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "#2) upload\n",
    "def upload_batch_file_to_openai(jsonl_path, api_key=None):\n",
    "    from openai import OpenAI\n",
    "    if api_key:\n",
    "        client = OpenAI(api_key=api_key)\n",
    "    else:\n",
    "        client = OpenAI()\n",
    "\n",
    "    with open(jsonl_path, \"rb\") as f:\n",
    "        batch_input_file = client.files.create(\n",
    "            file=f,\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "    print(batch_input_file)\n",
    "    return batch_input_file\n",
    "\n",
    "#3 Create the batch  \n",
    "def create_openai_batch(batch_input_file, endpoint=\"/v1/embeddings\", completion_window=\"24h\", metadata=None, api_key=None):\n",
    "    from openai import OpenAI\n",
    "\n",
    "    if api_key:\n",
    "        client = OpenAI(api_key=api_key)\n",
    "    else:\n",
    "        client = OpenAI()\n",
    "\n",
    "    batch_input_file_id = batch_input_file.id\n",
    "    response = client.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=endpoint,\n",
    "        completion_window=completion_window,\n",
    "        metadata=metadata or {\"description\": \"nightly eval job\"}\n",
    "    )\n",
    "    print(response)\n",
    "    return response\n",
    "\n",
    "\n",
    "#4 Check the satus \n",
    "def check_openai_batch(batch_id, api_key=None):\n",
    "    from openai import OpenAI\n",
    "\n",
    "    if api_key:\n",
    "        client = OpenAI(api_key=api_key)\n",
    "    else:\n",
    "        client = OpenAI()\n",
    "\n",
    "    batch = client.batches.retrieve(batch_id)\n",
    "    print(batch)\n",
    "    return batch\n",
    "\n",
    "\n",
    "#5 Retreive or doload the results\n",
    "# Ejemplo de uso\n",
    "texts = [\n",
    "    \"El sol brilla sobre las montañas.\",\n",
    "    \"La inteligencia artificial está transformando el mundo.\",\n",
    "    \"Los datos son el nuevo petróleo.\"\n",
    "]\n",
    "\n",
    "archivo = crear_jsonl_embeddings(texts, 0, 2)\n",
    "print(f\"Archivo generado: {archivo}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f712e9",
   "metadata": {},
   "source": [
    "## 2) Pruebas con textos largos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20ac10a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud de la canción más larga: 10124 caracteres\n",
      "Longitud de la canción más larga: 10008 caracteres\n",
      "Comezando en la función\n",
      "Encoding obtenido\n",
      "Conteo listo\n",
      "La cancion mas larga en words:  13970\n",
      "Comezando en la función\n",
      "Encoding obtenido\n",
      "Conteo listo\n",
      "La cancion mas larga en chars:  12965\n"
     ]
    }
   ],
   "source": [
    "embedding_model = \"text-embedding-3-small\"\n",
    "embedding_encoding = \"cl100k_base\"\n",
    "max_tokens = 8191  # the maximum for text-embedding-3-small is 8191\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "# Cnaciones más largas\n",
    "\n",
    "with open(\"cancion_large.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cancion_mas_larga = f.read()\n",
    "\n",
    "with open(\"cancion_large_in_chars.txt\", \"r\", encoding=\"utf-8\") as f2:\n",
    "    cancion_large_in_chars = f2.read()\n",
    "\n",
    "words = cancion_mas_larga.split(\" \")\n",
    "print(f\"Longitud de la canción más larga: {len(words)} caracteres\")\n",
    "\n",
    "words2 = cancion_large_in_chars.split(\" \")\n",
    "print(f\"Longitud de la canción más larga: {len(words2)} caracteres\")\n",
    "\n",
    "print(\"La cancion mas larga en words: \",num_tokens_from_string(cancion_mas_larga) )\n",
    "print(\"La cancion mas larga en chars: \",num_tokens_from_string(cancion_large_in_chars))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d4b45",
   "metadata": {},
   "source": [
    "we are going to prove if with this song the normal API (without butches) works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316fa072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "826d5fa1",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 13970 tokens (13970 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      2\u001b[39m client = OpenAI(api_key=API_KEY)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mcancion_mas_larga\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding_format\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfloat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Si solo quieres ver el embedding:\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/openai/resources/embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 13970 tokens (13970 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    model=embedding_model,   \n",
    "    input=cancion_mas_larga,\n",
    "    encoding_format=\"float\"\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "# Si solo quieres ver el embedding:\n",
    "print(\"Dimensión del embedding:\", len(response.data[0].embedding))\n",
    "print(\"Primeros 10 valores:\", response.data[0].embedding[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ef5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = 0\n",
    "end = 0\n",
    "\n",
    "# Consider limits of thew official page aaaa (All embbedings are equal) \n",
    "# text-embedding-3-small\n",
    "# Rate limits\n",
    "# - Tokens per minute (TPM): 40,000\n",
    "# - Requests per minute (RPM): 100\n",
    "# - Requests per day (RPD): 2,000\n",
    "\n",
    "# API usage is subject to rate limits applied on tokens per minute (TPM), \n",
    "# requests per minute or day (RPM/RPD), and other model-specific limits.\n",
    "# Your organization's rate limits are listed below.\n",
    "\n",
    "\n",
    "\n",
    "## Recorring by chunks for no saturate the memory unnecesaryly:\n",
    "chunksize = 100\n",
    "chunk_id = 0\n",
    "\n",
    "\n",
    "path_embbedings = \"../../../../data/gpt/embeddings\"\n",
    "path_errors = \"../../../../data/gpt/embeddings_errors\"\n",
    "\n",
    "for chunk in pd.read_csv(path_df, chunksize=chunksize):\n",
    "    embeddings = []\n",
    "    end = start - 1\n",
    "    texts = chunk['text'].fillna(\"\")\n",
    "    for idx, song in enumerate(texts, start=chunk_id*chunksize):\n",
    "        print(\"Procesando la canción: \", idx)\n",
    "        end = idx\n",
    "        try:\n",
    "            print(\"tuki\")\n",
    "            # 1) Crear el archivo jsonl desde start hasta end \n",
    "            # 2) Upload the buth file\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en la canción {idx} (reintento): {e}\")\n",
    "            \n",
    "\n",
    "    if END:\n",
    "        break\n",
    "    chunk_id+=1\n",
    "    # np.save(os.path.join(path_embbedings, f\"embeddings{start}-{end}.npy\"), embeddings)\n",
    "\n",
    "    if embeddings:\n",
    "        if any(e is None for e in embeddings):\n",
    "            save_path = os.path.join(path_errors, f\"embeddings_{start}-{end}.npy\")\n",
    "            print(f\"Contiene None, guardando en carpeta de errores: {save_path}\")\n",
    "        else:\n",
    "            # np.save(\n",
    "            #     os.path.join(path_embbedings, f\"embeddings_{start}-{end}.npy\"),\n",
    "            #     np.array(embeddings, dtype=object)\n",
    "            # )\n",
    "            save_path = os.path.join(path_embbedings, f\"embeddings_{start}-{end}.npy\")\n",
    "            print(f\" Guardando en carpeta normal: {save_path}\")\n",
    "        np.save(save_path, np.array(embeddings, dtype=object))\n",
    "        # print(f\"Guardado embeddings_{start}-{end}.npy con {len(embeddings)} filas\")\n",
    "    start = end + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file_name = os.path.join(path_data, f\"df_{start}_to_{end}.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a532487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391eb9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=\"Your text string goes here\",\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "print(response.data[0].embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
