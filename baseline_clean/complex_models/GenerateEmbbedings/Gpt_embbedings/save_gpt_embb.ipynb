{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62a83fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfd71808",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_first_number(filename):\n",
    "    match = re.search(r\"(\\d+)\", filename)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "def get_second_number(filename: str) -> int:\n",
    "    matches = re.findall(r\"(\\d+)\", filename)\n",
    "    if len(matches) >= 2:\n",
    "        return int(matches[1])  # el segundo número\n",
    "    return float('inf')  # si no lo encuentra\n",
    "\n",
    "\n",
    "def load_embeddings_and_check_dim(file_path):\n",
    "    embeddings = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            j = json.loads(line)\n",
    "            embeddings.append([get_first_number(j[\"custom_id\"]),j[\"embedding\"]])\n",
    "    \n",
    "    if embeddings:\n",
    "        print(f\"Número de embeddings: {len(embeddings)}\")\n",
    "        # print(f\"Dimensión del primer embedding: {len(embeddings[0][1])}\")\n",
    "    else:\n",
    "        print(\"No se encontraron embeddings en el archivo.\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def sort_according_id(embeddings):\n",
    "    return sorted(embeddings, key=lambda x: x[0])\n",
    "\n",
    "def get_faltantes(start, end, embeddings):\n",
    "    faltantes = []\n",
    "    n = len(embeddings)\n",
    "    index = 0   \n",
    "    for i in range(start, end + 1):\n",
    "        if index >= n:  \n",
    "            faltantes.append(i)\n",
    "        elif embeddings[index][0] == i:  \n",
    "            index += 1\n",
    "        else:\n",
    "            faltantes.append(i) \n",
    "    return faltantes\n",
    "\n",
    "def save_embb(embeddings, output_path):\n",
    "    only_embb =[]\n",
    "    for embb in embeddings:\n",
    "        only_embb.append(embb[1])\n",
    "    np.save(output_path, np.array(only_embb))\n",
    "    print(f\"Embeddings saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eca2ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de embeddings: 1301\n",
      "Embeddings saved to ../../../../data/gpt_embd/embeddings_96814_1068124.npy\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../../../../data\"\n",
    "path_embb = \"../../../../data/embeddings_only\"\n",
    "\n",
    "file_embb = os.listdir(path_embb)\n",
    "files_sorted = sorted(file_embb, key=get_first_number)\n",
    "faltantes = []\n",
    "\n",
    "index = 96814\n",
    "for relative_path in files_sorted:\n",
    "    start = get_first_number(relative_path)\n",
    "\n",
    "    if start != index:\n",
    "        continue\n",
    "    path_file_embb = os.path.join(path_embb, relative_path)\n",
    "    embbdings = load_embeddings_and_check_dim(path_file_embb)\n",
    "    embbdings_sorted = sort_according_id(embbdings)\n",
    "    end = get_second_number(relative_path)\n",
    "    faltantes.append(get_faltantes(start, end, embbdings_sorted))\n",
    "    directory = os.path.join(data_dir, \"gpt_embd\")\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    save_embb(embbdings_sorted, f\"{directory}/embeddings_{start}_{end}.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efe5f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        array_ = json.load(f)  \n",
    "    return array_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71f7f1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "irrecuperable = get_array(\"faltantes_according_token.json\")\n",
    "print(len(irrecuperable))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7167c43d",
   "metadata": {},
   "source": [
    "Vamos a verificar que indices tenemos disponibles ne la recuperación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "812ce549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de embeddings: 13699\n",
      "antecesora start: 91812\n",
      "antecesora end: 105511\n",
      "Número de embeddings: 1301\n",
      "start: 105512\n",
      "end: 106812\n"
     ]
    }
   ],
   "source": [
    "path_embb = \"../../../../data/embeddings_only\"\n",
    "\n",
    "# Data antecesora a la data recueperada\n",
    "\n",
    "path_antecesora_data = \"91812_106812_embeddings_only.jsonl\"  \n",
    "path_file_antecesora = os.path.join(path_embb, path_antecesora_data)\n",
    "embbdings_antecesora = load_embeddings_and_check_dim(path_file_antecesora)\n",
    "embbdings_antecesora_sorted = sort_according_id(embbdings_antecesora)\n",
    "print(\"antecesora start:\", embbdings_antecesora_sorted[0][0])\n",
    "print(\"antecesora end:\", embbdings_antecesora_sorted[-1][0])\n",
    "\n",
    "# Data recuperada\n",
    "path_recuperate_data = \"96814_1068124_embeddings_only.jsonl\"\n",
    "path_file_embb = os.path.join(path_embb, path_recuperate_data)\n",
    "embbdings = load_embeddings_and_check_dim(path_file_embb)\n",
    "embbdings_sorted = sort_according_id(embbdings)\n",
    "print(\"start:\", embbdings_sorted[0][0])\n",
    "print(\"end:\", embbdings_sorted[-1][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc0be39",
   "metadata": {},
   "source": [
    "Los índices no presentan problema con la antecesora porque ya tan ordendos, tons vamos a poder fusionar sin problemas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d06744",
   "metadata": {},
   "source": [
    "Fusionar los embbedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1277fbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando: ../../../../data/gpt_embd/embeddings_0_899.npy\n",
      "../../../../data/gpt_embd/embeddings_0_899.npy\n",
      "Cargando: ../../../../data/gpt_embd/embeddings_900_1799.npy\n",
      "../../../../data/gpt_embd/embeddings_900_1799.npy\n",
      "Cargando: ../../../../data/gpt_embd/embeddings_1800_16801.npy\n",
      "../../../../data/gpt_embd/embeddings_1800_16801.npy\n",
      "Cargando: ../../../../data/gpt_embd/embeddings_16802_31802.npy\n",
      "../../../../data/gpt_embd/embeddings_16802_31802.npy\n",
      "Cargando: ../../../../data/gpt_embd/embeddings_31803_46804.npy\n",
      "../../../../data/gpt_embd/embeddings_31803_46804.npy\n",
      "Cargando: ../../../../data/gpt_embd/embeddings_46805_61809.npy\n",
      "../../../../data/gpt_embd/embeddings_46805_61809.npy\n",
      "Cargando: ../../../../data/gpt_embd/embeddings_61810_76811.npy\n",
      "../../../../data/gpt_embd/embeddings_61810_76811.npy\n",
      "Cargando: ../../../../data/gpt_embd/embeddings_76812_91811.npy\n",
      "../../../../data/gpt_embd/embeddings_76812_91811.npy\n",
      "Cargando: ../../../../data/gpt_embd/embeddings_91812_106812.npy\n",
      "../../../../data/gpt_embd/embeddings_91812_106812.npy\n",
      "Cargando: ../../../../data/gpt_embd/embeddings_96814_1068124.npy\n",
      "../../../../data/gpt_embd/embeddings_96814_1068124.npy\n",
      "Cargando: ../../../../data/gpt_embd/embeddings_106813_108137.npy\n",
      "../../../../data/gpt_embd/embeddings_106813_108137.npy\n",
      "Embeddings fusionados guardados en: ../../../../data/gpt_embd/gpt_fussioned/embeddings_fused.npy\n",
      "Shape final: (108125, 1536)\n",
      "Tamaño del archivo: 1,328,640,128 bytes\n",
      "Tamaño aproximado: 1267.09 MB (1.24 GB)\n"
     ]
    }
   ],
   "source": [
    "# A partir de 105512 es que tenemos los embbedings\n",
    "data_dir = \"../../../../data\"\n",
    "directory = os.path.join(data_dir, \"gpt_embd\")\n",
    "\n",
    "files_ = os.listdir(directory)\n",
    "sorted_files = sorted(files_, key=get_first_number)\n",
    "\n",
    "all_embeddings = []\n",
    "dir_fussioned = os.path.join(directory, \"gpt_fussioned\")\n",
    "os.makedirs(dir_fussioned, exist_ok=True)\n",
    "for relatvie_embb_path in sorted_files:\n",
    "    path_embb = os.path.join(directory, relatvie_embb_path)\n",
    "    print(\"Cargando:\", path_embb)\n",
    "    print(path_embb)\n",
    "    arr = np.load(path_embb)\n",
    "    all_embeddings.append(arr)\n",
    "\n",
    "# Fusionar todos\n",
    "fused_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "path_fused = os.path.join(dir_fussioned, \"embeddings_fused.npy\")\n",
    "np.save(path_fused, fused_embeddings)\n",
    "\n",
    "print(\"Embeddings fusionados guardados en:\", path_fused)\n",
    "print(\"Shape final:\", fused_embeddings.shape)\n",
    "\n",
    "#Tamaño del archivo en disco\n",
    "size_bytes = os.path.getsize(path_fused)\n",
    "size_mb = size_bytes / (1024**2)\n",
    "size_gb = size_bytes / (1024**3)\n",
    "\n",
    "print(f\"Tamaño del archivo: {size_bytes:,} bytes\")\n",
    "print(f\"Tamaño aproximado: {size_mb:.2f} MB ({size_gb:.2f} GB)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
