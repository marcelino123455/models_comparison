{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abd89201",
   "metadata": {
    "papermill": {
     "duration": 0.024009,
     "end_time": "2025-10-01T19:32:57.006522",
     "exception": false,
     "start_time": "2025-10-01T19:32:56.982513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We are going to process the lyrics of a song using the batches Api to reduce the cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed34933",
   "metadata": {
    "papermill": {
     "duration": 0.008039,
     "end_time": "2025-10-01T19:32:57.022710",
     "exception": false,
     "start_time": "2025-10-01T19:32:57.014671",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creaci칩n del batch file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a881b8e3",
   "metadata": {
    "papermill": {
     "duration": 0.212587,
     "end_time": "2025-10-01T19:32:57.244117",
     "exception": false,
     "start_time": "2025-10-01T19:32:57.031530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Rate limits\n",
    "Batch API rate limits are separate from existing per-model rate limits. The Batch API has two new types of rate limits:\n",
    "\n",
    "Per-batch limits: A single batch may include up to 50,000 requests, and a batch input file can be up to 200 MB in size. Note that /v1/embeddings batches are also restricted to a maximum of 50,000 embedding inputs across all requests in the batch.\n",
    "Enqueued prompt tokens per model: Each model has a maximum number of enqueued prompt tokens allowed for batch processing. You can find these limits on the Platform Settings page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f4c127",
   "metadata": {
    "papermill": {
     "duration": 0.007117,
     "end_time": "2025-10-01T19:32:57.926214",
     "exception": false,
     "start_time": "2025-10-01T19:32:57.919097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1) Creaci칩n del archivo jsonl for batch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b3f759",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T19:32:57.942515Z",
     "iopub.status.busy": "2025-10-01T19:32:57.941798Z",
     "iopub.status.idle": "2025-10-01T19:32:57.948770Z",
     "shell.execute_reply": "2025-10-01T19:32:57.947761Z"
    },
    "papermill": {
     "duration": 0.017798,
     "end_time": "2025-10-01T19:32:57.950866",
     "exception": false,
     "start_time": "2025-10-01T19:32:57.933068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Model\tCost\tBatch cost\n",
    "# text-embedding-3-small\t$0.01\t$0.0001\n",
    "# text-embedding-3-large\t$0.065\t$0.00013\n",
    "# text-embedding-ada-002\t$0.05\t$0.0004\n",
    "# https://platform.openai.com/docs/pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "641e894f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T19:32:57.963023Z",
     "iopub.status.busy": "2025-10-01T19:32:57.962666Z",
     "iopub.status.idle": "2025-10-01T19:33:01.196627Z",
     "shell.execute_reply": "2025-10-01T19:33:01.195822Z"
    },
    "papermill": {
     "duration": 3.241248,
     "end_time": "2025-10-01T19:33:01.198453",
     "exception": false,
     "start_time": "2025-10-01T19:32:57.957205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import json \n",
    "import tiktoken\n",
    "import json\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "989e39d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T19:33:01.209405Z",
     "iopub.status.busy": "2025-10-01T19:33:01.208989Z",
     "iopub.status.idle": "2025-10-01T19:33:04.742685Z",
     "shell.execute_reply": "2025-10-01T19:33:04.741967Z"
    },
    "papermill": {
     "duration": 3.540369,
     "end_time": "2025-10-01T19:33:04.744226",
     "exception": false,
     "start_time": "2025-10-01T19:33:01.203857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "url = \"/v1/embeddings\"\n",
    "path_data = \"../../../../data\"\n",
    "path_df =os.path.join(path_data, \"spotify_dataset_sin_duplicados_4.csv\")\n",
    "df = pd.read_csv(path_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "704faaf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T19:33:04.753182Z",
     "iopub.status.busy": "2025-10-01T19:33:04.752866Z",
     "iopub.status.idle": "2025-10-01T19:33:04.773288Z",
     "shell.execute_reply": "2025-10-01T19:33:04.772640Z"
    },
    "papermill": {
     "duration": 0.026081,
     "end_time": "2025-10-01T19:33:04.774455",
     "exception": false,
     "start_time": "2025-10-01T19:33:04.748374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# Function to count the number of tokens\n",
    "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    # print(\"Comezando en la funci칩n\")\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    # print(\"Encoding obtenido\")\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    # print(\"Conteo listo\")\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "# Function that see how much the does the file weigh\n",
    "def get_file_size(filepath):\n",
    "    size_bytes = os.path.getsize(filepath)\n",
    "    size_mb = size_bytes / (1024 * 1024)\n",
    "    print(f\"File size: {size_bytes} bytes ({size_mb:.2f} MB)\")\n",
    "    return size_mb\n",
    "\n",
    "\n",
    "# In ram\n",
    "def get_jsons_size(json_list):\n",
    "    total_bytes = 0\n",
    "    for j in json_list:\n",
    "        line = json.dumps(j, ensure_ascii=False) + \"\\n\"  \n",
    "        total_bytes += len(line.encode(\"utf-8\"))         \n",
    "    \n",
    "    return total_bytes / (1024 * 1024)    \n",
    "# 0) Create a single json file \n",
    "\n",
    "def crear_single_json(text, idx, model=\"text-embedding-3-small\"):\n",
    "    request = {\n",
    "        \"custom_id\": f\"request-{idx}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/embeddings\",\n",
    "        \"body\": {\n",
    "            \"model\": model,\n",
    "            \"input\": text\n",
    "        }\n",
    "    }\n",
    "    return request  \n",
    "# Function that take a start and a end to create the jsonl file \n",
    "#1) create bath file\n",
    "def guardar_jsons(json_list, start, end, output_dir=\"./\"):\n",
    "    # Nombre de salida con el rango\n",
    "    filename = f\"{output_dir}/embeddings_{start}_{end}.jsonl\"\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for obj in json_list:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    print(f\"Archivo guardado: {filename} con {len(json_list)} requests\")\n",
    "    return filename\n",
    "\n",
    "#2) upload\n",
    "def upload_batch_file_to_openai(jsonl_path, api_key=None):\n",
    "    print(\"Subiendo el archivo de la ruta:\", jsonl_path)\n",
    "    # Validar ruta\n",
    "    if not os.path.exists(jsonl_path):\n",
    "        raise FileNotFoundError(f\"El archivo no existe: {jsonl_path}\")\n",
    "    if not os.path.isfile(jsonl_path):\n",
    "        raise ValueError(f\"La ruta no es un archivo v치lido: {jsonl_path}\")\n",
    "\n",
    "    # Cliente\n",
    "    if api_key:\n",
    "        client = OpenAI(api_key=api_key)\n",
    "    else:\n",
    "        client = OpenAI()\n",
    "\n",
    "    # Subir archivo\n",
    "    with open(jsonl_path, \"rb\") as f:\n",
    "        batch_input_file = client.files.create(\n",
    "            file=f,\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "    \n",
    "    return batch_input_file\n",
    "#3 Create the batch  \n",
    "def create_openai_batch(batch_input_file, endpoint=\"/v1/embeddings\", completion_window=\"24h\", metadata=None, api_key=None):\n",
    "\n",
    "    if api_key:\n",
    "        client = OpenAI(api_key=api_key)\n",
    "    else:\n",
    "        client = OpenAI()\n",
    "\n",
    "    batch_input_file_id = batch_input_file.id\n",
    "    response = client.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=endpoint,\n",
    "        completion_window=completion_window,\n",
    "        metadata=metadata or {\"description\": \"nightly eval job\"}\n",
    "    )\n",
    "    # print(response)\n",
    "    return response\n",
    "\n",
    "\n",
    "#4 Check the satus \n",
    "def check_openai_batch(batch_id, api_key=None):\n",
    "    from openai import OpenAI\n",
    "\n",
    "    if api_key:\n",
    "        client = OpenAI(api_key=api_key)\n",
    "    else:\n",
    "        client = OpenAI()\n",
    "\n",
    "    batch = client.batches.retrieve(batch_id)\n",
    "    # print(batch)\n",
    "    return batch\n",
    "\n",
    "\n",
    "#5 Retreive or doload the results\n",
    "def download_results(batch_id, api_key=None):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    file_response = client.files.content(batch_id)\n",
    "    print(file_response.text)\n",
    "    return file_response.text\n",
    "\n",
    "\n",
    "# 6 save the results\n",
    "def saveResult(file_response, start, end, output_path=\"../../../../data/gpt_responses\"):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    with open(f\"{output_path}/{start}_{end}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(file_response)\n",
    "    \n",
    "    print(f\"Resultados guardados en: {output_path}\")\n",
    "\n",
    "\n",
    "def save_embeddings_only(file_response_text,  start=0, end=0, output_path=\"../../../../data/gpt_responses\"):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    output_file = f\"{output_path}/{start}_{end}_embeddings_only.jsonl\"\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in file_response_text.splitlines():\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                j = json.loads(line)\n",
    "                custom_id = j[\"custom_id\"]\n",
    "                embedding = j[\"response\"][\"body\"][\"data\"][0][\"embedding\"]\n",
    "                # Guardar como JSON en una sola l칤nea\n",
    "                f.write(json.dumps({\"custom_id\": custom_id, \"embedding\": embedding}, ensure_ascii=False) + \"\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error procesando l칤nea: {e}\")\n",
    "\n",
    "    print(f\"Embeddings guardados solo con custom_id en: {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def load_embeddings_and_check_dim(file_path):\n",
    "    embeddings = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            j = json.loads(line)\n",
    "            embeddings.append(j[\"embedding\"])\n",
    "    \n",
    "    if embeddings:\n",
    "        print(f\"N칰mero de embeddings: {len(embeddings)}\")\n",
    "        print(f\"Dimensi칩n del primer embedding: {len(embeddings[0])}\")\n",
    "    else:\n",
    "        print(\"No se encontraron embeddings en el archivo.\")\n",
    "\n",
    "    return embeddings\n",
    "# Ejemplo de uso\n",
    "texts = [\n",
    "    \"El sol brilla sobre las monta침as.\",\n",
    "    \"La inteligencia artificial est치 transformando el mundo.\",\n",
    "    \"Los datos son el nuevo petr칩leo.\"\n",
    "]\n",
    "\n",
    "def save_batch_metadata(start, end, batch_file_id, batch_id, output_dir=\"../../../../data/gpt_to_check\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    metadata = {\n",
    "        \"start\": start,\n",
    "        \"end\": end,\n",
    "        \"batch_file_id\": batch_file_id,\n",
    "        \"batch_id\": batch_id\n",
    "    }\n",
    "\n",
    "    # Nombre del archivo de metadatos\n",
    "    filename = f\"{output_dir}/batch_metadata_{start}_{end}.json\"\n",
    "    \n",
    "    # Guardar en disco\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "        print(f\"Metadatos del batch guardados en {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error guardando metadatos del batch: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "# Function that clean text\n",
    "\n",
    "def limpiar_letras(texto: str) -> str:\n",
    "    \"\"\"\n",
    "    Elimina anotaciones como [Intro ...], [Chorus ...], [Verse 1: ...], \n",
    "    [Part 2 ...], [Hook ...], [Interlude ...], [Skit ...], [Produced ...], \n",
    "    [Track 1 ...], [Refrain ...], [Pre-Chorus ...], [Company ...], \n",
    "    [Backing vocals ...], [Sample ...], [Segue from ...], [Interview ...], \n",
    "    [2Pac ...], etc.\n",
    "    \"\"\"\n",
    "    patron = r\"\"\"\n",
    "        \\[                                   # abre corchete\n",
    "        (?:                                  # grupo de opciones\n",
    "            Intro\\s*\\d*\n",
    "          | Chorus\\s*\\d*\n",
    "          | Verse\\s*\\d*\n",
    "          | Vers\\s*\\d*\n",
    "          | Hook\\s*\\d*\n",
    "          | Part\\s*\\d*\n",
    "          | Interlude\\s*\\d*\n",
    "          | Skit\\s*\\d*\n",
    "          | Produced\\s*\\d*\n",
    "          | Track\\s*\\d*\n",
    "          | Refrain\\s*\\d*\n",
    "          | Pre-?Chorus\\s*\\d*\n",
    "          | Company\\s*\\d*\n",
    "          | Backing\\s+vocals\n",
    "          | Sample\\s*\\d*\n",
    "          | Segue\\s+from\n",
    "          | Instrumental\n",
    "          | Pre-Hook\n",
    "          | Pre Hook\n",
    "          | Lyrical\n",
    "          | Beat\n",
    "          | Interview\\s*\\d*\n",
    "        )[^\\]]*                              # cualquier cosa hasta ]\n",
    "        \\]                                   # cierra corchete\n",
    "    \"\"\"\n",
    "    return re.sub(patron, \"\", texto, flags=re.IGNORECASE | re.VERBOSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b3dfb1",
   "metadata": {
    "papermill": {
     "duration": 0.00356,
     "end_time": "2025-10-01T19:33:04.781996",
     "exception": false,
     "start_time": "2025-10-01T19:33:04.778436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Clase que se encarga de la limpieza de datos, igual para todos los tipos de embbedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6a061e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T19:33:04.789533Z",
     "iopub.status.busy": "2025-10-01T19:33:04.789257Z",
     "iopub.status.idle": "2025-10-01T19:33:07.215470Z",
     "shell.execute_reply": "2025-10-01T19:33:07.213983Z"
    },
    "papermill": {
     "duration": 2.433239,
     "end_time": "2025-10-01T19:33:07.218446",
     "exception": false,
     "start_time": "2025-10-01T19:33:04.785207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Text preprocessing class for lyrics data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, language: str = 'english'):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor\n",
    "\n",
    "        Args:\n",
    "            language: Language for stopwords (default: english)\n",
    "        \"\"\"\n",
    "        self.language = language\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words(language))\n",
    "        import re\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize text\n",
    "\n",
    "        Args:\n",
    "            text: Input text to clean\n",
    "\n",
    "        Returns:\n",
    "            Cleaned text\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return ''\n",
    "\n",
    "        # Convert to lowercase\n",
    "        text = str(text).lower()\n",
    "\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "        # Remove special characters but keep spaces\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "\n",
    "        return text\n",
    "    \n",
    "\n",
    "\n",
    "    def tokenize_and_process(self, text, remove_stopwords = True,\n",
    "                           apply_stemming = True):\n",
    "        \"\"\"\n",
    "        Tokenize and process text\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "            remove_stopwords: Whether to remove stopwords\n",
    "            apply_stemming: Whether to apply stemming\n",
    "\n",
    "        Returns:\n",
    "            List of processed tokens\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords if requested\n",
    "        if remove_stopwords:\n",
    "            tokens = [token for token in tokens if token not in self.stop_words]\n",
    "\n",
    "        # Apply stemming if requested\n",
    "        if apply_stemming:\n",
    "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "\n",
    "        # Remove very short tokens\n",
    "        tokens = [token for token in tokens if len(token) > 2]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def preprocess(self, text: str, remove_stopwords: bool = True,\n",
    "                   apply_stemming: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Complete preprocessing pipeline\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "            remove_stopwords: Whether to remove stopwords\n",
    "            apply_stemming: Whether to apply stemming\n",
    "\n",
    "        Returns:\n",
    "            Preprocessed text as string\n",
    "        \"\"\"\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        tokens = self.tokenize_and_process(cleaned_text, remove_stopwords, apply_stemming)\n",
    "        return ' '.join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "680ef5dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T19:33:07.234119Z",
     "iopub.status.busy": "2025-10-01T19:33:07.233532Z",
     "iopub.status.idle": "2025-10-01T20:29:57.244189Z",
     "shell.execute_reply": "2025-10-01T20:29:57.242193Z"
    },
    "papermill": {
     "duration": 3410.021309,
     "end_time": "2025-10-01T20:29:57.247246",
     "exception": false,
     "start_time": "2025-10-01T19:33:07.225937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  1800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  3600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  5400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  7200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  10800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  12600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  14400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size en RAM:  4.063328742980957\n",
      "Archivo guardado: ../../../../data/gpt_F_cat/json_files/embeddings_0_14999.jsonl con 15000 requests\n",
      "File size: 4260709 bytes (4.06 MB)\n",
      "Tama침o real al guardar: 4.063328742980957\n",
      "Archivo guardado en ../../../../data/gpt_F_cat/json_files/embeddings_0_14999.jsonl\n",
      "Subiendo el archivo de la ruta: ../../../../data/gpt_F_cat/json_files/embeddings_0_14999.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo subido con 칠xito: file-9ccqg1yiWqMqDxE2wfQW8p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_68dd83aacfbc81909dbcf3df17eb2211', completion_window='24h', created_at=1759347626, endpoint='/v1/embeddings', input_file_id='file-9ccqg1yiWqMqDxE2wfQW8p', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1759434026, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'nightly eval job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0), model=None, usage={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens_details': {'cached_tokens': 0}, 'output_tokens_details': {'reasoning_tokens': 0}})\n",
      "Batch creado con ID: batch_68dd83aacfbc81909dbcf3df17eb2211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo nervioso (nose si me quiere n) tu trankilo, guardaremos la data para hacer la descargation\n",
      "Metadatos del batch guardados en ../../../../data/gpt_F_cat/metadata_butches_files/batch_metadata_0_14999.json\n",
      "Sleeping 1 minute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  16200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  18000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  19800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  21600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  23400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  25200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  27000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  28800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size en RAM:  4.091643333435059\n",
      "Archivo guardado: ../../../../data/gpt_F_cat/json_files/embeddings_15000_29999.jsonl con 15000 requests\n",
      "File size: 4290399 bytes (4.09 MB)\n",
      "Tama침o real al guardar: 4.091643333435059\n",
      "Archivo guardado en ../../../../data/gpt_F_cat/json_files/embeddings_15000_29999.jsonl\n",
      "Subiendo el archivo de la ruta: ../../../../data/gpt_F_cat/json_files/embeddings_15000_29999.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo subido con 칠xito: file-SepQ2WYh3KGnYd5QzvVUej\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_68dd85896f7881909fbf57e29a92880f', completion_window='24h', created_at=1759348105, endpoint='/v1/embeddings', input_file_id='file-SepQ2WYh3KGnYd5QzvVUej', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1759434505, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'nightly eval job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0), model=None, usage={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens_details': {'cached_tokens': 0}, 'output_tokens_details': {'reasoning_tokens': 0}})\n",
      "Batch creado con ID: batch_68dd85896f7881909fbf57e29a92880f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo nervioso (nose si me quiere n) tu trankilo, guardaremos la data para hacer la descargation\n",
      "Metadatos del batch guardados en ../../../../data/gpt_F_cat/metadata_butches_files/batch_metadata_15000_29999.json\n",
      "Sleeping 1 minute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  30600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  32400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  34200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  36000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  37800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  39600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  41400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  43200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  45000\n",
      "Size en RAM:  4.075920104980469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo guardado: ../../../../data/gpt_F_cat/json_files/embeddings_30000_44999.jsonl con 15000 requests\n",
      "File size: 4273912 bytes (4.08 MB)\n",
      "Tama침o real al guardar: 4.075920104980469\n",
      "Archivo guardado en ../../../../data/gpt_F_cat/json_files/embeddings_30000_44999.jsonl\n",
      "Subiendo el archivo de la ruta: ../../../../data/gpt_F_cat/json_files/embeddings_30000_44999.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo subido con 칠xito: file-JeAH9A6mLYvSLHAoELcJYv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_68dd8764dd80819085026ebb4b1223e1', completion_window='24h', created_at=1759348580, endpoint='/v1/embeddings', input_file_id='file-JeAH9A6mLYvSLHAoELcJYv', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1759434980, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'nightly eval job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0), model=None, usage={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens_details': {'cached_tokens': 0}, 'output_tokens_details': {'reasoning_tokens': 0}})\n",
      "Batch creado con ID: batch_68dd8764dd80819085026ebb4b1223e1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo nervioso (nose si me quiere n) tu trankilo, guardaremos la data para hacer la descargation\n",
      "Metadatos del batch guardados en ../../../../data/gpt_F_cat/metadata_butches_files/batch_metadata_30000_44999.json\n",
      "Sleeping 1 minute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  46800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  48600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  50400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  52200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  54000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  55800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  57600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  59400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size en RAM:  3.9688310623168945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo guardado: ../../../../data/gpt_F_cat/json_files/embeddings_45000_59999.jsonl con 15000 requests\n",
      "File size: 4161621 bytes (3.97 MB)\n",
      "Tama침o real al guardar: 3.9688310623168945\n",
      "Archivo guardado en ../../../../data/gpt_F_cat/json_files/embeddings_45000_59999.jsonl\n",
      "Subiendo el archivo de la ruta: ../../../../data/gpt_F_cat/json_files/embeddings_45000_59999.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo subido con 칠xito: file-SVQMTxNhVrU8tGktmLdgue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_68dd8942012881908939a68fc4dfb52e', completion_window='24h', created_at=1759349058, endpoint='/v1/embeddings', input_file_id='file-SVQMTxNhVrU8tGktmLdgue', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1759435458, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'nightly eval job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0), model=None, usage={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens_details': {'cached_tokens': 0}, 'output_tokens_details': {'reasoning_tokens': 0}})\n",
      "Batch creado con ID: batch_68dd8942012881908939a68fc4dfb52e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo nervioso (nose si me quiere n) tu trankilo, guardaremos la data para hacer la descargation\n",
      "Metadatos del batch guardados en ../../../../data/gpt_F_cat/metadata_butches_files/batch_metadata_45000_59999.json\n",
      "Sleeping 1 minute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  61200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  63000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  64800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  66600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  68400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  70200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  72000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  73800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size en RAM:  4.118903160095215\n",
      "Archivo guardado: ../../../../data/gpt_F_cat/json_files/embeddings_60000_74999.jsonl con 15000 requests\n",
      "File size: 4318983 bytes (4.12 MB)\n",
      "Tama침o real al guardar: 4.118903160095215\n",
      "Archivo guardado en ../../../../data/gpt_F_cat/json_files/embeddings_60000_74999.jsonl\n",
      "Subiendo el archivo de la ruta: ../../../../data/gpt_F_cat/json_files/embeddings_60000_74999.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo subido con 칠xito: file-RW9VaE2XvZ54LnVNrjZkfw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_68dd8b29af1081909fda09e0f6c2d8be', completion_window='24h', created_at=1759349545, endpoint='/v1/embeddings', input_file_id='file-RW9VaE2XvZ54LnVNrjZkfw', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1759435945, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'nightly eval job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0), model=None, usage={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens_details': {'cached_tokens': 0}, 'output_tokens_details': {'reasoning_tokens': 0}})\n",
      "Batch creado con ID: batch_68dd8b29af1081909fda09e0f6c2d8be\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo nervioso (nose si me quiere n) tu trankilo, guardaremos la data para hacer la descargation\n",
      "Metadatos del batch guardados en ../../../../data/gpt_F_cat/metadata_butches_files/batch_metadata_60000_74999.json\n",
      "Sleeping 1 minute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  75600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  77400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  79200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  81000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  82800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  84600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  86400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  88200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  90000\n",
      "Size en RAM:  4.090457916259766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo guardado: ../../../../data/gpt_F_cat/json_files/embeddings_75000_89999.jsonl con 15000 requests\n",
      "File size: 4289156 bytes (4.09 MB)\n",
      "Tama침o real al guardar: 4.090457916259766\n",
      "Archivo guardado en ../../../../data/gpt_F_cat/json_files/embeddings_75000_89999.jsonl\n",
      "Subiendo el archivo de la ruta: ../../../../data/gpt_F_cat/json_files/embeddings_75000_89999.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo subido con 칠xito: file-3vkzS88FiJhYZt4CMLcpVj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_68dd8d0c0ff48190af0eef1ff17f4a21', completion_window='24h', created_at=1759350028, endpoint='/v1/embeddings', input_file_id='file-3vkzS88FiJhYZt4CMLcpVj', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1759436428, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'nightly eval job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0), model=None, usage={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens_details': {'cached_tokens': 0}, 'output_tokens_details': {'reasoning_tokens': 0}})\n",
      "Batch creado con ID: batch_68dd8d0c0ff48190af0eef1ff17f4a21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo nervioso (nose si me quiere n) tu trankilo, guardaremos la data para hacer la descargation\n",
      "Metadatos del batch guardados en ../../../../data/gpt_F_cat/metadata_butches_files/batch_metadata_75000_89999.json\n",
      "Sleeping 1 minute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  91800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  93600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  95400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  97200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  99000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  100800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  102600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  104400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size en RAM:  4.069516181945801\n",
      "Archivo guardado: ../../../../data/gpt_F_cat/json_files/embeddings_90000_104999.jsonl con 15000 requests\n",
      "File size: 4267197 bytes (4.07 MB)\n",
      "Tama침o real al guardar: 4.069516181945801\n",
      "Archivo guardado en ../../../../data/gpt_F_cat/json_files/embeddings_90000_104999.jsonl\n",
      "Subiendo el archivo de la ruta: ../../../../data/gpt_F_cat/json_files/embeddings_90000_104999.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo subido con 칠xito: file-5TU6EQRubt6qxoa1vHsmdL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_68dd8ee91e308190916f9377fd407bbb', completion_window='24h', created_at=1759350505, endpoint='/v1/embeddings', input_file_id='file-5TU6EQRubt6qxoa1vHsmdL', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1759436905, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'nightly eval job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0), model=None, usage={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens_details': {'cached_tokens': 0}, 'output_tokens_details': {'reasoning_tokens': 0}})\n",
      "Batch creado con ID: batch_68dd8ee91e308190916f9377fd407bbb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo nervioso (nose si me quiere n) tu trankilo, guardaremos la data para hacer la descargation\n",
      "Metadatos del batch guardados en ../../../../data/gpt_F_cat/metadata_butches_files/batch_metadata_90000_104999.json\n",
      "Sleeping 1 minute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  106200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando la canci칩n:  108000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando el 칰ltimo bloque en RAM...\n",
      "Archivo guardado: ../../../../data/gpt_F_cat/json_files/embeddings_105000_108137.jsonl con 3138 requests\n",
      "File size: 893245 bytes (0.85 MB)\n",
      "Tama침o real al guardar: 0.8518648147583008\n",
      "Archivo guardado en ../../../../data/gpt_F_cat/json_files/embeddings_105000_108137.jsonl\n",
      "Subiendo el archivo de la ruta: ../../../../data/gpt_F_cat/json_files/embeddings_105000_108137.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo subido con 칠xito: file-5cwDrsXtoDg8rNPpf95meY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_68dd8f445e908190a2756a1efcad3066', completion_window='24h', created_at=1759350596, endpoint='/v1/embeddings', input_file_id='file-5cwDrsXtoDg8rNPpf95meY', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1759436996, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'nightly eval job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0), model=None, usage={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens_details': {'cached_tokens': 0}, 'output_tokens_details': {'reasoning_tokens': 0}})\n",
      "Batch creado con ID: batch_68dd8f445e908190a2756a1efcad3066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando metadata para continuar luego...\n",
      "Metadatos del batch guardados en ../../../../data/gpt_F_cat/metadata_butches_files/batch_metadata_105000_108137.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# start = 0\n",
    "# end = 0\n",
    "\n",
    "# Consider limits of thew official page aaaa (All embbedings are equal) \n",
    "# text-embedding-3-small\n",
    "# Rate limits\n",
    "# - Tokens per minute (TPM): 40,000\n",
    "# - Requests per minute (RPM): 100\n",
    "# - Requests per day (RPD): 2,000\n",
    "\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "embedding_encoding = \"cl100k_base\"\n",
    "max_tokens = 8191 \n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv() \n",
    "API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "\n",
    "TESTING = False\n",
    "\n",
    "\n",
    "# CONFIGURATIONS\n",
    "# https://platform.openai.com/docs/guides/batch#rate-limits\n",
    "TPM = 40000\n",
    "RPM = 100\n",
    "RPD = 2000\n",
    "limit_size = 190 # In MB\n",
    "# limit_request_in_file_batch = 50000\n",
    "limit_request_in_file_batch = 15000 # Segun mi regla de 3 simple [900-3MB]\n",
    "chunksize = 100 # For simplicity a multiple of limit_request_in_file_batch \n",
    "chunk_id = 0\n",
    "\n",
    "\n",
    "if TESTING:\n",
    "    limit_request_in_file_batch = 900\n",
    "    # limit_size = 0.5\n",
    "    second = False\n",
    "END = False\n",
    "\n",
    "path_embbedings = \"../../../../data/gpt_F_cat/json_files\"\n",
    "path_msg = \"../../../../data/gpt_F_cat\"\n",
    "output_batch_metadata = \"../../../../data/gpt_F_cat/metadata_butches_files\"\n",
    "# path_errors = \"../../../../data/gpt/embeddings_errors\"\n",
    "\n",
    "os.makedirs(path_embbedings, exist_ok=True)\n",
    "os.makedirs(output_batch_metadata, exist_ok=True)\n",
    "os.makedirs(path_msg, exist_ok=True)\n",
    "# os.makedirs(path_errors, exist_ok=True)\n",
    "\n",
    "it_n = 0\n",
    "start = 0\n",
    "end = None\n",
    "songs_RAM  = []  \n",
    "songs_passed_limit = []\n",
    "failed_songs = []\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "F = ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']\n",
    "\n",
    "COL_GPT = F\n",
    "steaming= True\n",
    "\n",
    "for chunk in pd.read_csv(path_df, chunksize=chunksize):\n",
    "    chunk['combined_text'] = chunk[COL_GPT].fillna('').agg(' '.join, axis=1)\n",
    "\n",
    "\n",
    "    for idx, song in enumerate(chunk['combined_text'], start=chunk_id * chunksize ):\n",
    "        if idx < start:\n",
    "            continue\n",
    "        if idx%1800==0:\n",
    "            print(\"Procesando la canci칩n: \", idx)\n",
    "        # song = limpiar_letras(song) \n",
    "        song = preprocessor.preprocess(song) #Uniformizando misma limpieza de la data para todos los embb\n",
    "\n",
    "        # Ahora con limpieza\n",
    "        end = idx # Solo si se procesra sera este end\n",
    "        if(num_tokens_from_string(song) > max_tokens):\n",
    "            print(f\"La canci칩n {idx} excede el l칤mite de tokens ({num_tokens_from_string(song)} tokens). Se omitir치.\")\n",
    "            songs_passed_limit.append(idx)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if get_jsons_size(songs_RAM) > limit_size or len(songs_RAM) >= limit_request_in_file_batch:\n",
    "                end = idx-1 # Si no se procesa es el anterior\n",
    "                it_n+=1\n",
    "                print(\"Size en RAM: \", get_jsons_size(songs_RAM))\n",
    "                # Limpiar songs RAM y guardarlo \n",
    "                if get_jsons_size(songs_RAM) > limit_size:\n",
    "                    print(\"Song in RAM esta lleno\")\n",
    "                \n",
    "                # Realizamos el guardado del file\n",
    "                saved_in = guardar_jsons(songs_RAM, start, end, output_dir=path_embbedings)\n",
    "                print(\"Tama침o real al guardar:\",get_file_size(saved_in) )\n",
    "                print(f\"Archivo guardado en {saved_in}\")\n",
    "                \n",
    "                \n",
    "                # ---Inicio de los paso de gpt ----\n",
    "                # 1) upload_batch_file_to_openai\n",
    "                batch_input_file = upload_batch_file_to_openai(saved_in, API_KEY)\n",
    "                print(\"Archivo subido con 칠xito:\", batch_input_file.id) # file ID\n",
    "                # 2) Create the batch \n",
    "                response = create_openai_batch(batch_input_file, endpoint=\"/v1/embeddings\", completion_window=\"24h\", metadata=None, api_key=API_KEY)\n",
    "                print(response)\n",
    "                print(f\"Batch creado con ID: {response.id}\") # Batch ID\n",
    "\n",
    "                if (check_openai_batch(response.id, API_KEY).status == \"completed\"):\n",
    "                    print(f\"Estado del batch completed\")\n",
    "                    output_file_id = check_openai_batch(response.id, API_KEY).output_file_id\n",
    "                    pta = download_results(output_file_id, API_KEY)\n",
    "                    saved_file = save_embeddings_only(pta,start,end)\n",
    "                    print(f\"File guardado en: {saved_file}\")\n",
    "                else:\n",
    "                    # Guardar start, end, asociado a batch_input_file.id y response.id\n",
    "                    print(\"Yo nervioso (nose si me quiere n) tu trankilo, guardaremos la data para hacer la descargation\")\n",
    "                    save_batch_metadata(start, end, batch_input_file.id, response.id, output_dir=output_batch_metadata)\n",
    "\n",
    "                # ---Fin de los paso de gpt ----\n",
    "                print(\"Sleeping 1 minute\")\n",
    "                time.sleep(60) # Dormir 1 minuto porciacasito\n",
    "                songs_RAM = []\n",
    "                \n",
    "                start = end + 1\n",
    "                \n",
    "                # Falta guardar la cancion current \n",
    "                json_request = crear_single_json(song, idx)\n",
    "                songs_RAM.append(json_request)\n",
    "\n",
    "                # For testing\n",
    "                if TESTING and it_n == 2:\n",
    "                    END = True\n",
    "                    break\n",
    "            else:\n",
    "                # A침adir la cancion al json file\n",
    "                json_request = crear_single_json(song, idx)\n",
    "                songs_RAM.append(json_request)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error en la canci칩n {idx}: {e}\")\n",
    "            failed_songs.append(idx)\n",
    "\n",
    "    chunk_id+=1\n",
    "    \n",
    "    if END:\n",
    "        print(\"Termino el testing\")\n",
    "        break\n",
    "\n",
    "if songs_RAM:  \n",
    "    print(\"Procesando el 칰ltimo bloque en RAM...\")\n",
    "    end = end if end is not None else start + len(songs_RAM) - 1\n",
    "    saved_in = guardar_jsons(songs_RAM, start, end, output_dir=path_embbedings)\n",
    "    print(\"Tama침o real al guardar:\", get_file_size(saved_in))\n",
    "    print(f\"Archivo guardado en {saved_in}\")\n",
    "    # --- Inicio de los pasos GPT para este 칰ltimo batch ---\n",
    "    try:\n",
    "        batch_input_file = upload_batch_file_to_openai(saved_in, API_KEY)\n",
    "        print(\"Archivo subido con 칠xito:\", batch_input_file.id) # file ID\n",
    "        response = create_openai_batch(batch_input_file, endpoint=\"/v1/embeddings\",\n",
    "                                       completion_window=\"24h\", metadata=None, api_key=API_KEY)\n",
    "        print(response)\n",
    "        print(f\"Batch creado con ID: {response.id}\")\n",
    "\n",
    "        status = check_openai_batch(response.id, API_KEY)\n",
    "        if status.status == \"completed\":\n",
    "            print(\"Estado del batch: completed\")\n",
    "            output_file_id = status.output_file_id\n",
    "            pta = download_results(output_file_id, API_KEY)\n",
    "            saved_file = save_embeddings_only(pta, start, end)\n",
    "            print(f\"File guardado en: {saved_file}\")\n",
    "        else:\n",
    "            print(\"Guardando metadata para continuar luego...\")\n",
    "            save_batch_metadata(start, end, batch_input_file.id, response.id, output_dir=output_batch_metadata)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el 칰ltimo bloque: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8960aacf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T20:29:57.274497Z",
     "iopub.status.busy": "2025-10-01T20:29:57.274204Z",
     "iopub.status.idle": "2025-10-01T20:29:57.279727Z",
     "shell.execute_reply": "2025-10-01T20:29:57.279151Z"
    },
    "papermill": {
     "duration": 0.018266,
     "end_time": "2025-10-01T20:29:57.280837",
     "exception": false,
     "start_time": "2025-10-01T20:29:57.262571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if songs_passed_limit:\n",
    "    path_passed = os.path.join(path_msg, \"a_songs_passed_limit_F.txt\")\n",
    "    with open(path_passed, \"w\") as f:\n",
    "        json.dump(songs_passed_limit, f)\n",
    "    print(f\"Se guardaron {len(songs_passed_limit)} 칤ndices en {path_passed}\")\n",
    "\n",
    "\n",
    "if failed_songs:  \n",
    "    print(\"Terrible y ahora...\")\n",
    "    path_failed = os.path.join(path_msg, \"failed_songs_F.txt\")\n",
    "    with open(path_failed, \"w\") as f:\n",
    "        f.write(\", \".join(map(str, failed_songs))) \n",
    "    print(f\"Se guardaron {len(failed_songs)} 칤ndices en {path_failed}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3423.996196,
   "end_time": "2025-10-01T20:29:58.211683",
   "environment_variables": {},
   "exception": null,
   "input_path": "a_send_gpt_buches.ipynb",
   "output_path": "a_send_gpt_buches_F_20251001_143252.ipynb",
   "parameters": {},
   "start_time": "2025-10-01T19:32:54.215487",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}