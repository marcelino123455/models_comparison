/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/complex_models/models_works/Gpt/main.py:266: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [12:24:27] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
Your are not adding cols numercis
You are executing with [ALL] dataset
--> PaTH:  ../../../../data//lb_npy.npy

##################################################
Running experiment with TFIDF embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5000)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5000)
y shape: (41278,)
Saved on: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf

==============================
Model: Logistic Regression
Accuracy:  0.8250
Precision: 0.6070
Recall:    0.7566
F1-score:  0.6736
              precision    recall  f1-score   support

           0       0.92      0.85      0.88     16465
           1       0.61      0.76      0.67      5160

    accuracy                           0.83     21625
   macro avg       0.76      0.80      0.78     21625
weighted avg       0.84      0.83      0.83     21625

[[13937  2528]
 [ 1256  3904]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/conf_matrix_logistic_regression.png
Modelo guardado como: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.6767
Precision: 0.3854
Recall:    0.5969
F1-score:  0.4684
              precision    recall  f1-score   support

           0       0.85      0.70      0.77     16465
           1       0.39      0.60      0.47      5160

    accuracy                           0.68     21625
   macro avg       0.62      0.65      0.62     21625
weighted avg       0.74      0.68      0.70     21625

[[11554  4911]
 [ 2080  3080]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/conf_matrix_svm.png
Modelo guardado como: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8637
Precision: 0.6782
Recall:    0.8157
F1-score:  0.7406
              precision    recall  f1-score   support

           0       0.94      0.88      0.91     16465
           1       0.68      0.82      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.85      0.82     21625
weighted avg       0.88      0.86      0.87     21625

[[14468  1997]
 [  951  4209]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/conf_matrix_decision_tree.png
Modelo guardado como: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8530
Precision: 0.6721
Recall:    0.7500
F1-score:  0.7089
              precision    recall  f1-score   support

           0       0.92      0.89      0.90     16465
           1       0.67      0.75      0.71      5160

    accuracy                           0.85     21625
   macro avg       0.80      0.82      0.81     21625
weighted avg       0.86      0.85      0.86     21625

[[14577  1888]
 [ 1290  3870]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/conf_matrix_random_forest.png
Modelo guardado como: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8670
Precision: 0.6775
Recall:    0.8444
F1-score:  0.7518
              precision    recall  f1-score   support

           0       0.95      0.87      0.91     16465
           1       0.68      0.84      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.86      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14391  2074]
 [  803  4357]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/conf_matrix_xgboost.png
Modelo guardado como: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8214
Precision: 0.6084
Recall:    0.7054
F1-score:  0.6533
              precision    recall  f1-score   support

           0       0.90      0.86      0.88     16465
           1       0.61      0.71      0.65      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.78      0.77     21625
weighted avg       0.83      0.82      0.83     21625

[[14122  2343]
 [ 1520  3640]]
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/complex_models/models_works/Gpt/main.py:266: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [12:33:26] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Confusion matrix saved as: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/conf_matrix_naive_bayes.png
Modelo guardado como: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.867, 'precision': 0.6775, 'recall': 0.8444, 'f1_score': 0.7518}
Decision Tree: {'accuracy': 0.8637, 'precision': 0.6782, 'recall': 0.8157, 'f1_score': 0.7406}
Random Forest: {'accuracy': 0.853, 'precision': 0.6721, 'recall': 0.75, 'f1_score': 0.7089}
Logistic Regression: {'accuracy': 0.825, 'precision': 0.607, 'recall': 0.7566, 'f1_score': 0.6736}
Naive Bayes: {'accuracy': 0.8214, 'precision': 0.6084, 'recall': 0.7054, 'f1_score': 0.6533}
SVM: {'accuracy': 0.6767, 'precision': 0.3854, 'recall': 0.5969, 'f1_score': 0.4684}

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 300)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 300)
y shape: (41278,)
Saved on: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert

==============================
Model: Logistic Regression
Accuracy:  0.8004
Precision: 0.5601
Recall:    0.7616
F1-score:  0.6455
              precision    recall  f1-score   support

           0       0.92      0.81      0.86     16465
           1       0.56      0.76      0.65      5160

    accuracy                           0.80     21625
   macro avg       0.74      0.79      0.75     21625
weighted avg       0.83      0.80      0.81     21625

[[13378  3087]
 [ 1230  3930]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.3120
Precision: 0.2525
Recall:    0.9609
F1-score:  0.4000
              precision    recall  f1-score   support

           0       0.90      0.11      0.19     16465
           1       0.25      0.96      0.40      5160

    accuracy                           0.31     21625
   macro avg       0.58      0.53      0.30     21625
weighted avg       0.74      0.31      0.24     21625

[[ 1790 14675]
 [  202  4958]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/conf_matrix_svm.png
Modelo guardado como: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6364
Precision: 0.3664
Recall:    0.7178
F1-score:  0.4851
              precision    recall  f1-score   support

           0       0.87      0.61      0.72     16465
           1       0.37      0.72      0.49      5160

    accuracy                           0.64     21625
   macro avg       0.62      0.66      0.60     21625
weighted avg       0.75      0.64      0.66     21625

[[10059  6406]
 [ 1456  3704]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.7719
Precision: 0.5161
Recall:    0.7050
F1-score:  0.5960
              precision    recall  f1-score   support

           0       0.90      0.79      0.84     16465
           1       0.52      0.71      0.60      5160

    accuracy                           0.77     21625
   macro avg       0.71      0.75      0.72     21625
weighted avg       0.81      0.77      0.78     21625

[[13054  3411]
 [ 1522  3638]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/conf_matrix_random_forest.png
Modelo guardado como: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.7984
Precision: 0.5589
Recall:    0.7368
F1-score:  0.6356
              precision    recall  f1-score   support

           0       0.91      0.82      0.86     16465
           1       0.56      0.74      0.64      5160

    accuracy                           0.80     21625
   macro avg       0.73      0.78      0.75     21625
weighted avg       0.82      0.80      0.81     21625

[[13464  3001]
 [ 1358  3802]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/conf_matrix_xgboost.png
Modelo guardado como: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/xgboost_model.pkl

==============================
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/complex_models/models_works/Gpt/main.py:266: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [12:45:40] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Model: Naive Bayes
Accuracy:  0.6441
Precision: 0.3788
Recall:    0.7682
F1-score:  0.5074
              precision    recall  f1-score   support

           0       0.89      0.61      0.72     16465
           1       0.38      0.77      0.51      5160

    accuracy                           0.64     21625
   macro avg       0.64      0.69      0.61     21625
weighted avg       0.77      0.64      0.67     21625

[[9964 6501]
 [1196 3964]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/conf_matrix_naive_bayes.png
Modelo guardado como: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.8004, 'precision': 0.5601, 'recall': 0.7616, 'f1_score': 0.6455}
XGBoost: {'accuracy': 0.7984, 'precision': 0.5589, 'recall': 0.7368, 'f1_score': 0.6356}
Random Forest: {'accuracy': 0.7719, 'precision': 0.5161, 'recall': 0.705, 'f1_score': 0.596}
Naive Bayes: {'accuracy': 0.6441, 'precision': 0.3788, 'recall': 0.7682, 'f1_score': 0.5074}
Decision Tree: {'accuracy': 0.6364, 'precision': 0.3664, 'recall': 0.7178, 'f1_score': 0.4851}
SVM: {'accuracy': 0.312, 'precision': 0.2525, 'recall': 0.9609, 'f1_score': 0.4}

##################################################
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../../../data/spotify_dataset_sin_duplicados_4.csv
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original y: (108138,)
Shape filtrado  y: (108125,)
Label distribution: {0: 82336, 1: 25802}
X shape: (108125, 1536)
y shape: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1536)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1536)
y shape: (41278,)
Saved on: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt

==============================
Model: Logistic Regression
Accuracy:  0.8423
Precision: 0.6289
Recall:    0.8271
F1-score:  0.7145
              precision    recall  f1-score   support

           0       0.94      0.85      0.89     16465
           1       0.63      0.83      0.71      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.84      0.80     21625
weighted avg       0.87      0.84      0.85     21625

[[13947  2518]
 [  892  4268]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.5736
Precision: 0.3249
Recall:    0.7302
F1-score:  0.4497
              precision    recall  f1-score   support

           0       0.86      0.52      0.65     16465
           1       0.32      0.73      0.45      5160

    accuracy                           0.57     21625
   macro avg       0.59      0.63      0.55     21625
weighted avg       0.73      0.57      0.60     21625

[[8637 7828]
 [1392 3768]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/conf_matrix_svm.png
Modelo guardado como: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7598
Precision: 0.4977
Recall:    0.7477
F1-score:  0.5976
              precision    recall  f1-score   support

           0       0.91      0.76      0.83     16465
           1       0.50      0.75      0.60      5160

    accuracy                           0.76     21625
   macro avg       0.70      0.76      0.71     21625
weighted avg       0.81      0.76      0.77     21625

[[12572  3893]
 [ 1302  3858]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8427
Precision: 0.6450
Recall:    0.7583
F1-score:  0.6971
              precision    recall  f1-score   support

           0       0.92      0.87      0.89     16465
           1       0.64      0.76      0.70      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.81      0.80     21625
weighted avg       0.85      0.84      0.85     21625

[[14311  2154]
 [ 1247  3913]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8379
Precision: 0.6240
Recall:    0.8068
F1-score:  0.7037
              precision    recall  f1-score   support

           0       0.93      0.85      0.89     16465
           1       0.62      0.81      0.70      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.83      0.80     21625
weighted avg       0.86      0.84      0.84     21625

[[13957  2508]
 [  997  4163]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8404
Precision: 0.6511
Recall:    0.7132
F1-score:  0.6807
              precision    recall  f1-score   support

           0       0.91      0.88      0.89     16465
           1       0.65      0.71      0.68      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.80      0.79     21625
weighted avg       0.85      0.84      0.84     21625

[[14493  1972]
 [ 1480  3680]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputs/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.8423, 'precision': 0.6289, 'recall': 0.8271, 'f1_score': 0.7145}
XGBoost: {'accuracy': 0.8379, 'precision': 0.624, 'recall': 0.8068, 'f1_score': 0.7037}
Random Forest: {'accuracy': 0.8427, 'precision': 0.645, 'recall': 0.7583, 'f1_score': 0.6971}
Naive Bayes: {'accuracy': 0.8404, 'precision': 0.6511, 'recall': 0.7132, 'f1_score': 0.6807}
Decision Tree: {'accuracy': 0.7598, 'precision': 0.4977, 'recall': 0.7477, 'f1_score': 0.5976}
SVM: {'accuracy': 0.5736, 'precision': 0.3249, 'recall': 0.7302, 'f1_score': 0.4497}

You are executing with this configuration: undersample_True_scaled_True_removestw_True_5000tfidf
