/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/complex_models/models_works/Gpt/main.py:266: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [13:29:17] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
Your are not adding cols numercis

You are executing with [EXAMPLE] of 100 songs
--> PaTH:  ../../../../data//lb_npy.npy

##################################################
Running experiment with TFIDF embeddings
Preprocessing text...
Label distribution: {0: 77, 1: 23}
X shape: (100, 853)
y shape: (100,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 62, 1: 18}
Label distribution en TEST: {0: 15, 1: 5}


==================================================
Data antes del undersampling ...
X: (80, 853)
y: (80,)
Apliying UNDERSAMPLE
18
Label distribution: {0: 18, 1: 18}
X shape: (36, 853)
y shape: (36,)
Saved on: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf

==============================
Model: Logistic Regression
Accuracy:  0.5500
Precision: 0.3333
Recall:    0.8000
F1-score:  0.4706
              precision    recall  f1-score   support

           0       0.88      0.47      0.61        15
           1       0.33      0.80      0.47         5

    accuracy                           0.55        20
   macro avg       0.60      0.63      0.54        20
weighted avg       0.74      0.55      0.57        20

[[7 8]
 [1 4]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/conf_matrix_logistic_regression.png
Modelo guardado como: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.2500
Precision: 0.2500
Recall:    1.0000
F1-score:  0.4000
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        15
           1       0.25      1.00      0.40         5

    accuracy                           0.25        20
   macro avg       0.12      0.50      0.20        20
weighted avg       0.06      0.25      0.10        20

[[ 0 15]
 [ 0  5]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/conf_matrix_svm.png
Modelo guardado como: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7000
Precision: 0.4286
Recall:    0.6000
F1-score:  0.5000
              precision    recall  f1-score   support

           0       0.85      0.73      0.79        15
           1       0.43      0.60      0.50         5

    accuracy                           0.70        20
   macro avg       0.64      0.67      0.64        20
weighted avg       0.74      0.70      0.71        20

[[11  4]
 [ 2  3]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/conf_matrix_decision_tree.png
Modelo guardado como: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.6500
Precision: 0.3750
Recall:    0.6000
F1-score:  0.4615
              precision    recall  f1-score   support

           0       0.83      0.67      0.74        15
           1       0.38      0.60      0.46         5

    accuracy                           0.65        20
   macro avg       0.60      0.63      0.60        20
weighted avg       0.72      0.65      0.67        20

[[10  5]
 [ 2  3]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/conf_matrix_random_forest.png
Modelo guardado como: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.6500
Precision: 0.4000
Recall:    0.8000
F1-score:  0.5333
              precision    recall  f1-score   support

           0       0.90      0.60      0.72        15
           1       0.40      0.80      0.53         5

    accuracy                           0.65        20
   macro avg       0.65      0.70      0.63        20
weighted avg       0.78      0.65      0.67        20

[[9 6]
 [1 4]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/conf_matrix_xgboost.png
Modelo guardado como: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.5000
Precision: 0.3077
Recall:    0.8000
F1-score:  0.4444
              precision    recall  f1-score   support

           0       0.86      0.40      0.55        15
           1       0.31      0.80      0.44         5

    accuracy                           0.50        20
   macro avg       0.58      0.60      0.49        20
weighted avg       0.72      0.50      0.52        20

[[6 9]
 [1 4]]
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}
Confusion matrix saved as: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/conf_matrix_naive_bayes.png
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/complex_models/models_works/Gpt/main.py:266: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [13:29:20] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Modelo guardado como: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/tfidf/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.65, 'precision': 0.4, 'recall': 0.8, 'f1_score': 0.5333}
Decision Tree: {'accuracy': 0.7, 'precision': 0.4286, 'recall': 0.6, 'f1_score': 0.5}
Logistic Regression: {'accuracy': 0.55, 'precision': 0.3333, 'recall': 0.8, 'f1_score': 0.4706}
Random Forest: {'accuracy': 0.65, 'precision': 0.375, 'recall': 0.6, 'f1_score': 0.4615}
Naive Bayes: {'accuracy': 0.5, 'precision': 0.3077, 'recall': 0.8, 'f1_score': 0.4444}
SVM: {'accuracy': 0.25, 'precision': 0.25, 'recall': 1.0, 'f1_score': 0.4}

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 77, 1: 23}
X shape: (100, 300)
y shape: (100,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 62, 1: 18}
Label distribution en TEST: {0: 15, 1: 5}


==================================================
Data antes del undersampling ...
X: (80, 300)
y: (80,)
Apliying UNDERSAMPLE
18
Label distribution: {0: 18, 1: 18}
X shape: (36, 300)
y shape: (36,)
Saved on: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert

==============================
Model: Logistic Regression
Accuracy:  0.5500
Precision: 0.3333
Recall:    0.8000
F1-score:  0.4706
              precision    recall  f1-score   support

           0       0.88      0.47      0.61        15
           1       0.33      0.80      0.47         5

    accuracy                           0.55        20
   macro avg       0.60      0.63      0.54        20
weighted avg       0.74      0.55      0.57        20

[[7 8]
 [1 4]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.5500
Precision: 0.3333
Recall:    0.8000
F1-score:  0.4706
              precision    recall  f1-score   support

           0       0.88      0.47      0.61        15
           1       0.33      0.80      0.47         5

    accuracy                           0.55        20
   macro avg       0.60      0.63      0.54        20
weighted avg       0.74      0.55      0.57        20

[[7 8]
 [1 4]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/conf_matrix_svm.png
Modelo guardado como: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6000
Precision: 0.3333
Recall:    0.6000
F1-score:  0.4286
              precision    recall  f1-score   support

           0       0.82      0.60      0.69        15
           1       0.33      0.60      0.43         5

    accuracy                           0.60        20
   macro avg       0.58      0.60      0.56        20
weighted avg       0.70      0.60      0.63        20

[[9 6]
 [2 3]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.5500
Precision: 0.3333
Recall:    0.8000
F1-score:  0.4706
              precision    recall  f1-score   support

           0       0.88      0.47      0.61        15
           1       0.33      0.80      0.47         5

    accuracy                           0.55        20
   macro avg       0.60      0.63      0.54        20
weighted avg       0.74      0.55      0.57        20

[[7 8]
 [1 4]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/conf_matrix_random_forest.png
Modelo guardado como: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.5000
Precision: 0.2222
Recall:    0.4000
F1-score:  0.2857
              precision    recall  f1-score   support

           0       0.73      0.53      0.62        15
           1       0.22      0.40      0.29         5

    accuracy                           0.50        20
   macro avg       0.47      0.47      0.45        20
weighted avg       0.60      0.50      0.53        20

[[8 7]
 [3 2]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/conf_matrix_xgboost.png
Modelo guardado como: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6500
Precision: 0.3750
Recall:    0.6000
F1-score:  0.4615
              precision    recall  f1-score   support

           0       0.83      0.67      0.74        15
           1       0.38      0.60      0.46         5

    accuracy                           0.65        20
   macro avg       0.60      0.63      0.60        20
weighted avg       0.72      0.65      0.67        20
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/complex_models/models_works/Gpt/main.py:266: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [13:29:24] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

[[10  5]
 [ 2  3]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/conf_matrix_naive_bayes.png
Modelo guardado como: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/lyrics_bert/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.55, 'precision': 0.3333, 'recall': 0.8, 'f1_score': 0.4706}
SVM: {'accuracy': 0.55, 'precision': 0.3333, 'recall': 0.8, 'f1_score': 0.4706}
Random Forest: {'accuracy': 0.55, 'precision': 0.3333, 'recall': 0.8, 'f1_score': 0.4706}
Naive Bayes: {'accuracy': 0.65, 'precision': 0.375, 'recall': 0.6, 'f1_score': 0.4615}
Decision Tree: {'accuracy': 0.6, 'precision': 0.3333, 'recall': 0.6, 'f1_score': 0.4286}
XGBoost: {'accuracy': 0.5, 'precision': 0.2222, 'recall': 0.4, 'f1_score': 0.2857}

##################################################
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../../../data/spotify_dataset_sin_duplicados_4.csv
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Label distribution: {0: 77, 1: 23}
X shape: (100, 1536)
y shape: (100,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 62, 1: 18}
Label distribution en TEST: {0: 15, 1: 5}


==================================================
Data antes del undersampling ...
X: (80, 1536)
y: (80,)
Apliying UNDERSAMPLE
18
Label distribution: {0: 18, 1: 18}
X shape: (36, 1536)
y shape: (36,)
Saved on: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt

==============================
Model: Logistic Regression
Accuracy:  0.7500
Precision: 0.5000
Recall:    1.0000
F1-score:  0.6667
              precision    recall  f1-score   support

           0       1.00      0.67      0.80        15
           1       0.50      1.00      0.67         5

    accuracy                           0.75        20
   macro avg       0.75      0.83      0.73        20
weighted avg       0.88      0.75      0.77        20

[[10  5]
 [ 0  5]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8500
Precision: 0.6667
Recall:    0.8000
F1-score:  0.7273
              precision    recall  f1-score   support

           0       0.93      0.87      0.90        15
           1       0.67      0.80      0.73         5

    accuracy                           0.85        20
   macro avg       0.80      0.83      0.81        20
weighted avg       0.86      0.85      0.85        20

[[13  2]
 [ 1  4]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/conf_matrix_svm.png
Modelo guardado como: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7500
Precision: 0.5000
Recall:    0.8000
F1-score:  0.6154
              precision    recall  f1-score   support

           0       0.92      0.73      0.81        15
           1       0.50      0.80      0.62         5

    accuracy                           0.75        20
   macro avg       0.71      0.77      0.72        20
weighted avg       0.81      0.75      0.76        20

[[11  4]
 [ 1  4]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.6000
Precision: 0.3636
Recall:    0.8000
F1-score:  0.5000
              precision    recall  f1-score   support

           0       0.89      0.53      0.67        15
           1       0.36      0.80      0.50         5

    accuracy                           0.60        20
   macro avg       0.63      0.67      0.58        20
weighted avg       0.76      0.60      0.62        20

[[8 7]
 [1 4]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.7500
Precision: 0.5000
Recall:    0.8000
F1-score:  0.6154
              precision    recall  f1-score   support

           0       0.92      0.73      0.81        15
           1       0.50      0.80      0.62         5

    accuracy                           0.75        20
   macro avg       0.71      0.77      0.72        20
weighted avg       0.81      0.75      0.76        20

[[11  4]
 [ 1  4]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.9000
Precision: 0.8000
Recall:    0.8000
F1-score:  0.8000
              precision    recall  f1-score   support

           0       0.93      0.93      0.93        15
           1       0.80      0.80      0.80         5

    accuracy                           0.90        20
   macro avg       0.87      0.87      0.87        20
weighted avg       0.90      0.90      0.90        20

[[14  1]
 [ 1  4]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputstest/undersample_True_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_False_+_5000_tfidf_T/gpt/naive_bayes_model.pkl


Resumen de métricas:
Naive Bayes: {'accuracy': 0.9, 'precision': 0.8, 'recall': 0.8, 'f1_score': 0.8}
SVM: {'accuracy': 0.85, 'precision': 0.6667, 'recall': 0.8, 'f1_score': 0.7273}
Logistic Regression: {'accuracy': 0.75, 'precision': 0.5, 'recall': 1.0, 'f1_score': 0.6667}
Decision Tree: {'accuracy': 0.75, 'precision': 0.5, 'recall': 0.8, 'f1_score': 0.6154}
XGBoost: {'accuracy': 0.75, 'precision': 0.5, 'recall': 0.8, 'f1_score': 0.6154}
Random Forest: {'accuracy': 0.6, 'precision': 0.3636, 'recall': 0.8, 'f1_score': 0.5}


Resumen GLOBAL de métricas:


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
XGBoost: {'accuracy': 0.65, 'precision': 0.4, 'recall': 0.8, 'f1_score': 0.5333}
Decision Tree: {'accuracy': 0.7, 'precision': 0.4286, 'recall': 0.6, 'f1_score': 0.5}
Logistic Regression: {'accuracy': 0.55, 'precision': 0.3333, 'recall': 0.8, 'f1_score': 0.4706}
Random Forest: {'accuracy': 0.65, 'precision': 0.375, 'recall': 0.6, 'f1_score': 0.4615}
Naive Bayes: {'accuracy': 0.5, 'precision': 0.3077, 'recall': 0.8, 'f1_score': 0.4444}
SVM: {'accuracy': 0.25, 'precision': 0.25, 'recall': 1.0, 'f1_score': 0.4}


EMBEDDINGS TYPE: LYRICS_BERT
Logistic Regression: {'accuracy': 0.55, 'precision': 0.3333, 'recall': 0.8, 'f1_score': 0.4706}
SVM: {'accuracy': 0.55, 'precision': 0.3333, 'recall': 0.8, 'f1_score': 0.4706}
Random Forest: {'accuracy': 0.55, 'precision': 0.3333, 'recall': 0.8, 'f1_score': 0.4706}
Naive Bayes: {'accuracy': 0.65, 'precision': 0.375, 'recall': 0.6, 'f1_score': 0.4615}
Decision Tree: {'accuracy': 0.6, 'precision': 0.3333, 'recall': 0.6, 'f1_score': 0.4286}
XGBoost: {'accuracy': 0.5, 'precision': 0.2222, 'recall': 0.4, 'f1_score': 0.2857}


EMBEDDINGS TYPE: GPT
Naive Bayes: {'accuracy': 0.9, 'precision': 0.8, 'recall': 0.8, 'f1_score': 0.8}
SVM: {'accuracy': 0.85, 'precision': 0.6667, 'recall': 0.8, 'f1_score': 0.7273}
Logistic Regression: {'accuracy': 0.75, 'precision': 0.5, 'recall': 1.0, 'f1_score': 0.6667}
Decision Tree: {'accuracy': 0.75, 'precision': 0.5, 'recall': 0.8, 'f1_score': 0.6154}
XGBoost: {'accuracy': 0.75, 'precision': 0.5, 'recall': 0.8, 'f1_score': 0.6154}
Random Forest: {'accuracy': 0.6, 'precision': 0.3636, 'recall': 0.8, 'f1_score': 0.5}

========== CONFIGURATIONS ==========
TESTING: True
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: False
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: Not used
====================================

