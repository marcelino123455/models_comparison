/home/marcelino.maita/.venv/lib/python3.13/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/main_limit.py:222: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/main_limit.py:222: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
You are executing with [EXAMPLE] of 1000 songs

##################################################
Running experiment with TFIDF embeddings
Preprocessing text...
Label distribution: {0: 763, 1: 237}
X shape: (1000, 5000)
y shape: (1000,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 610, 1: 190}
Label distribution en TEST: {0: 153, 1: 47}
==================================================
Data antes del undersampling ...
X: (800, 2)
y: (800,)
Apliying UNDERSAMPLE
190
Label distribution: {0: 190, 1: 190}
X shape: (380, 2)
y shape: (380,)

==============================
Model: Logistic Regression
Accuracy:  0.6750
Precision: 0.3784
Recall:    0.5957
F1-score:  0.4628
              precision    recall  f1-score   support

           0       0.85      0.70      0.77       153
           1       0.38      0.60      0.46        47

    accuracy                           0.68       200
   macro avg       0.61      0.65      0.61       200
weighted avg       0.74      0.68      0.70       200

[[107  46]
 [ 19  28]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_limi_tes_/undersample_True_scaled_True_steaming_True_removestw_True_numeric_True_5000_tfidf_A/tfidf/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_limi_tes_/undersample_True_scaled_True_steaming_True_removestw_True_numeric_True_5000_tfidf_A/tfidf/logistic_regression_model.pkl
Gráfico guardado en outputs_limi_tes_/undersample_True_scaled_True_steaming_True_removestw_True_numeric_True_5000_tfidf_A/tfidf/decision_boundary_logistic_regression.png

==============================
Model: Decision Tree
Accuracy:  0.7800
Precision: 0.5273
Recall:    0.6170
F1-score:  0.5686
              precision    recall  f1-score   support

           0       0.88      0.83      0.85       153
           1       0.53      0.62      0.57        47

    accuracy                           0.78       200
   macro avg       0.70      0.72      0.71       200
weighted avg       0.79      0.78      0.79       200

[[127  26]
 [ 18  29]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_limi_tes_/undersample_True_scaled_True_steaming_True_removestw_True_numeric_True_5000_tfidf_A/tfidf/conf_matrix_decision_tree.png
Modelo guardado como: outputs_limi_tes_/undersample_True_scaled_True_steaming_True_removestw_True_numeric_True_5000_tfidf_A/tfidf/decision_tree_model.pkl
Gráfico guardado en outputs_limi_tes_/undersample_True_scaled_True_steaming_True_removestw_True_numeric_True_5000_tfidf_A/tfidf/decision_boundary_decision_tree.png


Resumen de métricas:
Decision Tree: {'accuracy': 0.78, 'precision': 0.5273, 'recall': 0.617, 'f1_score': 0.5686}
Logistic Regression: {'accuracy': 0.675, 'precision': 0.3784, 'recall': 0.5957, 'f1_score': 0.4628}

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 763, 1: 237}
X shape: (1000, 300)
y shape: (1000,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 610, 1: 190}
Label distribution en TEST: {0: 153, 1: 47}
==================================================
Data antes del undersampling ...
X: (800, 2)
y: (800,)
Apliying UNDERSAMPLE
190
Label distribution: {0: 190, 1: 190}
X shape: (380, 2)
y shape: (380,)

==============================
Model: Logistic Regression
Accuracy:  0.6250
Precision: 0.3444
Recall:    0.6596
F1-score:  0.4526
              precision    recall  f1-score   support

           0       0.85      0.61      0.71       153
           1       0.34      0.66      0.45        47

    accuracy                           0.62       200
   macro avg       0.60      0.64      0.58       200
weighted avg       0.73      0.62      0.65       200

[[94 59]
 [16 31]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_limi_tes_/undersample_True_scaled_True_steaming_True_removestw_True_numeric_True_5000_tfidf_A/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_limi_tes_/undersample_True_scaled_True_steaming_True_removestw_True_numeric_True_5000_tfidf_A/lyrics_bert/logistic_regression_model.pkl
Gráfico guardado en outputs_limi_tes_/undersample_True_scaled_True_steaming_True_removestw_True_numeric_True_5000_tfidf_A/lyrics_bert/decision_boundary_logistic_regression.png

==============================
Model: Decision Tree
Accuracy:  0.5700
Precision: 0.3069
Recall:    0.6596
F1-score:  0.4189
              precision    recall  f1-score   support

           0       0.84      0.54      0.66       153
           1       0.31      0.66      0.42        47

    accuracy                           0.57       200
   macro avg       0.57      0.60      0.54       200
weighted avg       0.71      0.57      0.60       200

[[83 70]
 [16 31]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_limi_tes_/undersample_True_scaled_True_steaming_True_removestw_True_numeric_True_5000_tfidf_A/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs_limi_tes_/undersample_True_scaled_True_steaming_True_removestw_True_numeric_True_5000_tfidf_A/lyrics_bert/decision_tree_model.pkl
Gráfico guardado en outputs_limi_tes_/undersample_True_scaled_True_steaming_True_removestw_True_numeric_True_5000_tfidf_A/lyrics_bert/decision_boundary_decision_tree.png


Resumen de métricas:
Logistic Regression: {'accuracy': 0.625, 'precision': 0.3444, 'recall': 0.6596, 'f1_score': 0.4526}
Decision Tree: {'accuracy': 0.57, 'precision': 0.3069, 'recall': 0.6596, 'f1_score': 0.4189}
You are executing with this configuration: undersample_True_scaled_True_removestw_True_5000tfidf
