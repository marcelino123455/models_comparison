/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/main.py:220: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/main.py:220: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
You are executing with [EXAMPLE] of 1000 songs

##################################################
Running experiment with TFIDF embeddings
Preprocessing text...
Label distribution: {0: 763, 1: 237}
X shape: (1000, 5000)
y shape: (1000,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 610, 1: 190}
Label distribution en TEST: {0: 153, 1: 47}
==================================================
Data antes del undersampling ...
X: (800, 5000)
y: (800,)
Apliying UNDERSAMPLE
190
Label distribution: {0: 190, 1: 190}
X shape: (380, 5000)
y shape: (380,)

==============================
Model: Logistic Regression
Accuracy:  0.6350
Precision: 0.3556
Recall:    0.6809
F1-score:  0.4672
              precision    recall  f1-score   support

           0       0.86      0.62      0.72       153
           1       0.36      0.68      0.47        47

    accuracy                           0.64       200
   macro avg       0.61      0.65      0.59       200
weighted avg       0.74      0.64      0.66       200

[[95 58]
 [15 32]]
Hiperparámetros: {'C': 100.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_tes_/undersample_True_scaled_True_steaming_True_removestw_False_5000tfidf/tfidf/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_tes_/undersample_True_scaled_True_steaming_True_removestw_False_5000tfidf/tfidf/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.3100
Precision: 0.2541
Recall:    1.0000
F1-score:  0.4052
              precision    recall  f1-score   support

           0       1.00      0.10      0.18       153
           1       0.25      1.00      0.41        47

    accuracy                           0.31       200
   macro avg       0.63      0.55      0.29       200
weighted avg       0.82      0.31      0.23       200

[[ 15 138]
 [  0  47]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_tes_/undersample_True_scaled_True_steaming_True_removestw_False_5000tfidf/tfidf/conf_matrix_svm.png
Modelo guardado como: outputs_tes_/undersample_True_scaled_True_steaming_True_removestw_False_5000tfidf/tfidf/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8200
Precision: 0.6078
Recall:    0.6596
F1-score:  0.6327
              precision    recall  f1-score   support

           0       0.89      0.87      0.88       153
           1       0.61      0.66      0.63        47

    accuracy                           0.82       200
   macro avg       0.75      0.76      0.76       200
weighted avg       0.83      0.82      0.82       200

[[133  20]
 [ 16  31]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'best'}
Confusion matrix saved as: outputs_tes_/undersample_True_scaled_True_steaming_True_removestw_False_5000tfidf/tfidf/conf_matrix_decision_tree.png
Modelo guardado como: outputs_tes_/undersample_True_scaled_True_steaming_True_removestw_False_5000tfidf/tfidf/decision_tree_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6300
Precision: 0.3663
Recall:    0.7872
F1-score:  0.5000
              precision    recall  f1-score   support

           0       0.90      0.58      0.71       153
           1       0.37      0.79      0.50        47

    accuracy                           0.63       200
   macro avg       0.63      0.68      0.60       200
weighted avg       0.77      0.63      0.66       200

[[89 64]
 [10 37]]
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}
Confusion matrix saved as: outputs_tes_/undersample_True_scaled_True_steaming_True_removestw_False_5000tfidf/tfidf/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_tes_/undersample_True_scaled_True_steaming_True_removestw_False_5000tfidf/tfidf/naive_bayes_model.pkl


Resumen final de métricas (ordenado por F1-score):
Decision Tree: {'accuracy': 0.82, 'precision': 0.6078, 'recall': 0.6596, 'f1_score': 0.6327}
Naive Bayes: {'accuracy': 0.63, 'precision': 0.3663, 'recall': 0.7872, 'f1_score': 0.5}
Logistic Regression: {'accuracy': 0.635, 'precision': 0.3556, 'recall': 0.6809, 'f1_score': 0.4672}
SVM: {'accuracy': 0.31, 'precision': 0.2541, 'recall': 1.0, 'f1_score': 0.4052}

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 763, 1: 237}
X shape: (1000, 300)
y shape: (1000,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 610, 1: 190}
Label distribution en TEST: {0: 153, 1: 47}
==================================================
Data antes del undersampling ...
X: (800, 300)
y: (800,)
Apliying UNDERSAMPLE
190
Label distribution: {0: 190, 1: 190}
X shape: (380, 300)
y shape: (380,)

==============================
Model: Logistic Regression
Accuracy:  0.6250
Precision: 0.3409
Recall:    0.6383
F1-score:  0.4444
              precision    recall  f1-score   support

           0       0.85      0.62      0.72       153
           1       0.34      0.64      0.44        47

    accuracy                           0.62       200
   macro avg       0.59      0.63      0.58       200
weighted avg       0.73      0.62      0.65       200

[[95 58]
 [17 30]]
Hiperparámetros: {'C': 100.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_tes_/undersample_True_scaled_True_steaming_True_removestw_False_5000tfidf/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_tes_/undersample_True_scaled_True_steaming_True_removestw_False_5000tfidf/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7300
Precision: 0.4507
Recall:    0.6809
F1-score:  0.5424
              precision    recall  f1-score   support

           0       0.88      0.75      0.81       153
           1       0.45      0.68      0.54        47

    accuracy                           0.73       200
   macro avg       0.67      0.71      0.68       200
weighted avg       0.78      0.73      0.75       200

[[114  39]
 [ 15  32]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_tes_/undersample_True_scaled_True_steaming_True_removestw_False_5000tfidf/lyrics_bert/conf_matrix_svm.png
Modelo guardado como: outputs_tes_/undersample_True_scaled_True_steaming_True_removestw_False_5000tfidf/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.5950
Precision: 0.3023
Recall:    0.5532
F1-score:  0.3910
              precision    recall  f1-score   support

           0       0.82      0.61      0.70       153
           1       0.30      0.55      0.39        47

    accuracy                           0.59       200
   macro avg       0.56      0.58      0.54       200
weighted avg       0.70      0.59      0.62       200

[[93 60]
 [21 26]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'best'}
Confusion matrix saved as: outputs_tes_/undersample_True_scaled_True_steaming_True_removestw_False_5000tfidf/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs_tes_/undersample_True_scaled_True_steaming_True_removestw_False_5000tfidf/lyrics_bert/decision_tree_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7000
Precision: 0.4270
Recall:    0.8085
F1-score:  0.5588
              precision    recall  f1-score   support

           0       0.92      0.67      0.77       153
           1       0.43      0.81      0.56        47

    accuracy                           0.70       200
   macro avg       0.67      0.74      0.67       200
weighted avg       0.80      0.70      0.72       200

[[102  51]
 [  9  38]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_tes_/undersample_True_scaled_True_steaming_True_removestw_False_5000tfidf/lyrics_bert/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_tes_/undersample_True_scaled_True_steaming_True_removestw_False_5000tfidf/lyrics_bert/naive_bayes_model.pkl


Resumen final de métricas (ordenado por F1-score):
Naive Bayes: {'accuracy': 0.7, 'precision': 0.427, 'recall': 0.8085, 'f1_score': 0.5588}
SVM: {'accuracy': 0.73, 'precision': 0.4507, 'recall': 0.6809, 'f1_score': 0.5424}
Logistic Regression: {'accuracy': 0.625, 'precision': 0.3409, 'recall': 0.6383, 'f1_score': 0.4444}
Decision Tree: {'accuracy': 0.595, 'precision': 0.3023, 'recall': 0.5532, 'f1_score': 0.391}
You are executing with this configuration: undersample_True_scaled_True_removestw_False_5000tfidf
