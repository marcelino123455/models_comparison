/home/marcelino.maita/.venv/lib/python3.13/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
For TF-IDF embbedings you are selecteing this columns:
--> ['Artist(s)', 'song', 'emotion', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
Your are not adding cols numercis
You are executing with [ALL] dataset
--> PaTH:  ../../data/embbedings_khipu/LB_fuss/lb_khipu_B.npy

##################################################
Running experiment with TFIDF embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65868, 1: 20642}
Label distribution en TEST: {0: 16468, 1: 5160}




Aplicando SMOTE oversampling...
Nueva distribución de clases: {0: 65868, 1: 65868}

==============================
Model: Logistic Regression
Accuracy:  0.6259
Precision: 0.3465
Recall:    0.6411
F1-score:  0.4499
              precision    recall  f1-score   support

           0       0.85      0.62      0.72     16468
           1       0.35      0.64      0.45      5160

    accuracy                           0.63     21628
   macro avg       0.60      0.63      0.58     21628
weighted avg       0.73      0.63      0.65     21628

[[10229  6239]
 [ 1852  3308]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_limi/undersample_False_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_True_+_5000_tfidf_B/tfidf/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_limi/undersample_False_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_True_+_5000_tfidf_B/tfidf/logistic_regression_model.pkl
Gráfico guardado en outputs_limi/undersample_False_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_True_+_5000_tfidf_B/tfidf/decision_boundary_logistic_regression.png

==============================
Model: Decision Tree
Accuracy:  0.8533
Precision: 0.6819
Recall:    0.7217
F1-score:  0.7013
              precision    recall  f1-score   support

           0       0.91      0.89      0.90     16468
           1       0.68      0.72      0.70      5160

    accuracy                           0.85     21628
   macro avg       0.80      0.81      0.80     21628
weighted avg       0.86      0.85      0.85     21628

[[14731  1737]
 [ 1436  3724]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_limi/undersample_False_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_True_+_5000_tfidf_B/tfidf/conf_matrix_decision_tree.png
Modelo guardado como: outputs_limi/undersample_False_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_True_+_5000_tfidf_B/tfidf/decision_tree_model.pkl
Gráfico guardado en outputs_limi/undersample_False_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_True_+_5000_tfidf_B/tfidf/decision_boundary_decision_tree.png


Resumen de métricas:
Decision Tree: {'accuracy': 0.8533, 'precision': 0.6819, 'recall': 0.7217, 'f1_score': 0.7013}
Logistic Regression: {'accuracy': 0.6259, 'precision': 0.3465, 'recall': 0.6411, 'f1_score': 0.4499}

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65868, 1: 20642}
Label distribution en TEST: {0: 16468, 1: 5160}




Aplicando SMOTE oversampling...
Nueva distribución de clases: {0: 65868, 1: 65868}

==============================
Model: Logistic Regression
Accuracy:  0.7072
Precision: 0.4319
Recall:    0.7209
F1-score:  0.5402
              precision    recall  f1-score   support

           0       0.89      0.70      0.79     16468
           1       0.43      0.72      0.54      5160

    accuracy                           0.71     21628
   macro avg       0.66      0.71      0.66     21628
weighted avg       0.78      0.71      0.73     21628

[[11575  4893]
 [ 1440  3720]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_limi/undersample_False_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_True_+_5000_tfidf_B/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_limi/undersample_False_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_True_+_5000_tfidf_B/lyrics_bert/logistic_regression_model.pkl
Gráfico guardado en outputs_limi/undersample_False_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_True_+_5000_tfidf_B/lyrics_bert/decision_boundary_logistic_regression.png

==============================
Model: Decision Tree
Accuracy:  0.8511
Precision: 0.6767
Recall:    0.7198
F1-score:  0.6976
              precision    recall  f1-score   support

           0       0.91      0.89      0.90     16468
           1       0.68      0.72      0.70      5160

    accuracy                           0.85     21628
   macro avg       0.79      0.81      0.80     21628
weighted avg       0.85      0.85      0.85     21628

[[14694  1774]
 [ 1446  3714]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_limi/undersample_False_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_True_+_5000_tfidf_B/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs_limi/undersample_False_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_True_+_5000_tfidf_B/lyrics_bert/decision_tree_model.pkl
Gráfico guardado en outputs_limi/undersample_False_scaled_True_steaming_True_removestw_True_numeric_False_useSmote_True_+_5000_tfidf_B/lyrics_bert/decision_boundary_decision_tree.png


Resumen de métricas:
Decision Tree: {'accuracy': 0.8511, 'precision': 0.6767, 'recall': 0.7198, 'f1_score': 0.6976}
Logistic Regression: {'accuracy': 0.7072, 'precision': 0.4319, 'recall': 0.7209, 'f1_score': 0.5402}

You are executing with this configuration: undersample_False_scaled_True_removestw_True_5000tfidf
