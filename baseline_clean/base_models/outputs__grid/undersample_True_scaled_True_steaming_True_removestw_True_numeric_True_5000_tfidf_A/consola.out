/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/main.py:220: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/main.py:220: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
You are executing with [ALL] dataset

##################################################
Running experiment with TFIDF embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65868, 1: 20642}
Label distribution en TEST: {0: 16468, 1: 5160}
==================================================
Data antes del undersampling ...
X: (86510, 5002)
y: (86510,)
Apliying UNDERSAMPLE
20642
Label distribution: {0: 20642, 1: 20642}
X shape: (41284, 5002)
y shape: (41284,)

==============================
Optimizando Naive Bayes...
Mejores parámetros: {'alpha': 0.5}
Accuracy:  0.8448
Precision: 0.6536
Recall:    0.7438
F1-score:  0.6958
              precision    recall  f1-score   support

           0       0.92      0.88      0.90     16468
           1       0.65      0.74      0.70      5160

    accuracy                           0.84     21628
   macro avg       0.78      0.81      0.80     21628
weighted avg       0.85      0.84      0.85     21628

Confusion matrix:
[[14434  2034]
 [ 1322  3838]]

==============================
Optimizando Logistic Regression...
Mejores parámetros: {'C': 1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}
Accuracy:  0.8603
Precision: 0.6697
Recall:    0.8174
F1-score:  0.7363
              precision    recall  f1-score   support

           0       0.94      0.87      0.90     16468
           1       0.67      0.82      0.74      5160

    accuracy                           0.86     21628
   macro avg       0.80      0.85      0.82     21628
weighted avg       0.87      0.86      0.86     21628

Confusion matrix:
[[14388  2080]
 [  942  4218]]

==============================
Optimizando SVM...
Mejores parámetros: {'C': 1, 'kernel': 'rbf', 'max_iter': 1000}
Accuracy:  0.6308
Precision: 0.3734
Recall:    0.8074
F1-score:  0.5106
              precision    recall  f1-score   support

           0       0.91      0.58      0.70     16468
           1       0.37      0.81      0.51      5160

    accuracy                           0.63     21628
   macro avg       0.64      0.69      0.61     21628
weighted avg       0.78      0.63      0.66     21628

Confusion matrix:
[[9477 6991]
 [ 994 4166]]

==============================
Optimizando Decision Tree...
Mejores parámetros: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}
Accuracy:  0.8905
Precision: 0.7304
Recall:    0.8578
F1-score:  0.7889
              precision    recall  f1-score   support

           0       0.95      0.90      0.93     16468
           1       0.73      0.86      0.79      5160

    accuracy                           0.89     21628
   macro avg       0.84      0.88      0.86     21628
weighted avg       0.90      0.89      0.89     21628

Confusion matrix:
[[14834  1634]
 [  734  4426]]


Resumen final de métricas:
Decision Tree: {'accuracy': 0.8905, 'precision': 0.7304, 'recall': 0.8578, 'f1_score': 0.7889, 'best_params': {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}}
Logistic Regression: {'accuracy': 0.8603, 'precision': 0.6697, 'recall': 0.8174, 'f1_score': 0.7363, 'best_params': {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}}
Naive Bayes: {'accuracy': 0.8448, 'precision': 0.6536, 'recall': 0.7438, 'f1_score': 0.6958, 'best_params': {'alpha': 0.5, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}}
SVM: {'accuracy': 0.6308, 'precision': 0.3734, 'recall': 0.8074, 'f1_score': 0.5106, 'best_params': {'C': 1, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}}

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65868, 1: 20642}
Label distribution en TEST: {0: 16468, 1: 5160}
==================================================
Data antes del undersampling ...
X: (86510, 302)
y: (86510,)
Apliying UNDERSAMPLE
20642
Label distribution: {0: 20642, 1: 20642}
X shape: (41284, 302)
y shape: (41284,)

==============================
Optimizando Naive Bayes...
Mejores parámetros: {'var_smoothing': 1e-09}
Accuracy:  0.6603
Precision: 0.3908
Recall:    0.7583
F1-score:  0.5158
              precision    recall  f1-score   support

           0       0.89      0.63      0.74     16468
           1       0.39      0.76      0.52      5160

    accuracy                           0.66     21628
   macro avg       0.64      0.69      0.63     21628
weighted avg       0.77      0.66      0.69     21628

Confusion matrix:
[[10368  6100]
 [ 1247  3913]]

==============================
Optimizando Logistic Regression...
Mejores parámetros: {'C': 1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}
Accuracy:  0.8045
Precision: 0.5656
Recall:    0.7789
F1-score:  0.6553
              precision    recall  f1-score   support

           0       0.92      0.81      0.86     16468
           1       0.57      0.78      0.66      5160

    accuracy                           0.80     21628
   macro avg       0.74      0.80      0.76     21628
weighted avg       0.84      0.80      0.81     21628

Confusion matrix:
[[13381  3087]
 [ 1141  4019]]

==============================
Optimizando SVM...
Mejores parámetros: {'C': 1, 'kernel': 'rbf', 'max_iter': 1000}
Accuracy:  0.4780
Precision: 0.3019
Recall:    0.9050
F1-score:  0.4528
              precision    recall  f1-score   support

           0       0.92      0.34      0.50     16468
           1       0.30      0.91      0.45      5160

    accuracy                           0.48     21628
   macro avg       0.61      0.62      0.48     21628
weighted avg       0.77      0.48      0.49     21628

Confusion matrix:
[[ 5669 10799]
 [  490  4670]]

==============================
Optimizando Decision Tree...
Mejores parámetros: {'criterion': 'gini', 'max_depth': 7, 'min_samples_leaf': 1, 'min_samples_split': 2}
Accuracy:  0.6813
Precision: 0.4018
Recall:    0.6866
F1-score:  0.5069
              precision    recall  f1-score   support

           0       0.87      0.68      0.76     16468
           1       0.40      0.69      0.51      5160

    accuracy                           0.68     21628
   macro avg       0.64      0.68      0.64     21628
weighted avg       0.76      0.68      0.70     21628

Confusion matrix:
[[11193  5275]
 [ 1617  3543]]


Resumen final de métricas:
Logistic Regression: {'accuracy': 0.8045, 'precision': 0.5656, 'recall': 0.7789, 'f1_score': 0.6553, 'best_params': {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}}
Naive Bayes: {'accuracy': 0.6603, 'precision': 0.3908, 'recall': 0.7583, 'f1_score': 0.5158, 'best_params': {'priors': None, 'var_smoothing': 1e-09}}
Decision Tree: {'accuracy': 0.6813, 'precision': 0.4018, 'recall': 0.6866, 'f1_score': 0.5069, 'best_params': {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 7, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}}
SVM: {'accuracy': 0.478, 'precision': 0.3019, 'recall': 0.905, 'f1_score': 0.4528, 'best_params': {'C': 1, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}}
You are executing with this configuration: undersample_True_scaled_True_removestw_True_5000tfidf
