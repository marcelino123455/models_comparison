/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/main_grid.py:220: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/main_grid.py:220: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
You are executing with [EXAMPLE] of 1000 songs

##################################################
Running experiment with TFIDF embeddings
Preprocessing text...
Label distribution: {0: 763, 1: 237}
X shape: (1000, 5000)
y shape: (1000,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 610, 1: 190}
Label distribution en TEST: {0: 153, 1: 47}
==================================================
Data antes del undersampling ...
X: (800, 5000)
y: (800,)
Apliying UNDERSAMPLE
190
Label distribution: {0: 190, 1: 190}
X shape: (380, 5000)
y shape: (380,)

==============================
Optimizando Naive Bayes...
Mejores parámetros: {'alpha': 2.0}
Accuracy:  0.6450
Precision: 0.3776
Recall:    0.7872
F1-score:  0.5103
              precision    recall  f1-score   support

           0       0.90      0.60      0.72       153
           1       0.38      0.79      0.51        47

    accuracy                           0.65       200
   macro avg       0.64      0.69      0.62       200
weighted avg       0.78      0.65      0.67       200


==============================
Optimizando Logistic Regression...
Mejores parámetros: {'C': 1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}
Accuracy:  0.6600
Precision: 0.3735
Recall:    0.6596
F1-score:  0.4769
              precision    recall  f1-score   support

           0       0.86      0.66      0.75       153
           1       0.37      0.66      0.48        47

    accuracy                           0.66       200
   macro avg       0.62      0.66      0.61       200
weighted avg       0.75      0.66      0.68       200


==============================
Optimizando SVM...
Mejores parámetros: {'C': 10, 'kernel': 'rbf', 'max_iter': 1000}
Accuracy:  0.3000
Precision: 0.2513
Recall:    1.0000
F1-score:  0.4017
              precision    recall  f1-score   support

           0       1.00      0.08      0.16       153
           1       0.25      1.00      0.40        47

    accuracy                           0.30       200
   macro avg       0.63      0.54      0.28       200
weighted avg       0.82      0.30      0.21       200


==============================
Optimizando Decision Tree...
Mejores parámetros: {'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2}
Accuracy:  0.7700
Precision: 0.5082
Recall:    0.6596
F1-score:  0.5741
              precision    recall  f1-score   support

           0       0.88      0.80      0.84       153
           1       0.51      0.66      0.57        47

    accuracy                           0.77       200
   macro avg       0.70      0.73      0.71       200
weighted avg       0.80      0.77      0.78       200



Resumen final de métricas:
Decision Tree: {'accuracy': 0.77, 'precision': 0.5082, 'recall': 0.6596, 'f1_score': 0.5741, 'best_params': {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 20, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}}
Naive Bayes: {'accuracy': 0.645, 'precision': 0.3776, 'recall': 0.7872, 'f1_score': 0.5103, 'best_params': {'alpha': 2.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}}
Logistic Regression: {'accuracy': 0.66, 'precision': 0.3735, 'recall': 0.6596, 'f1_score': 0.4769, 'best_params': {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}}
SVM: {'accuracy': 0.3, 'precision': 0.2513, 'recall': 1.0, 'f1_score': 0.4017, 'best_params': {'C': 10, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}}

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 763, 1: 237}
X shape: (1000, 300)
y shape: (1000,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 610, 1: 190}
Label distribution en TEST: {0: 153, 1: 47}
==================================================
Data antes del undersampling ...
X: (800, 300)
y: (800,)
Apliying UNDERSAMPLE
190
Label distribution: {0: 190, 1: 190}
X shape: (380, 300)
y shape: (380,)

==============================
Optimizando Naive Bayes...
Sin parámetros a optimizar para este modelo.
Accuracy:  0.7000
Precision: 0.4270
Recall:    0.8085
F1-score:  0.5588
              precision    recall  f1-score   support

           0       0.92      0.67      0.77       153
           1       0.43      0.81      0.56        47

    accuracy                           0.70       200
   macro avg       0.67      0.74      0.67       200
weighted avg       0.80      0.70      0.72       200


==============================
Optimizando Logistic Regression...
Mejores parámetros: {'C': 0.1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}
Accuracy:  0.6950
Precision: 0.4125
Recall:    0.7021
F1-score:  0.5197
              precision    recall  f1-score   support

           0       0.88      0.69      0.78       153
           1       0.41      0.70      0.52        47

    accuracy                           0.69       200
   macro avg       0.65      0.70      0.65       200
weighted avg       0.77      0.69      0.72       200


==============================
Optimizando SVM...
Mejores parámetros: {'C': 0.1, 'kernel': 'linear', 'max_iter': 1000}
Accuracy:  0.6900
Precision: 0.4118
Recall:    0.7447
F1-score:  0.5303
              precision    recall  f1-score   support

           0       0.90      0.67      0.77       153
           1       0.41      0.74      0.53        47

    accuracy                           0.69       200
   macro avg       0.65      0.71      0.65       200
weighted avg       0.78      0.69      0.71       200


==============================
Optimizando Decision Tree...
Mejores parámetros: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}
Accuracy:  0.5650
Precision: 0.3148
Recall:    0.7234
F1-score:  0.4387
              precision    recall  f1-score   support

           0       0.86      0.52      0.64       153
           1       0.31      0.72      0.44        47

    accuracy                           0.56       200
   macro avg       0.59      0.62      0.54       200
weighted avg       0.73      0.57      0.60       200



Resumen final de métricas:
Naive Bayes: {'accuracy': 0.7, 'precision': 0.427, 'recall': 0.8085, 'f1_score': 0.5588, 'best_params': {'priors': None, 'var_smoothing': 1e-09}}
SVM: {'accuracy': 0.69, 'precision': 0.4118, 'recall': 0.7447, 'f1_score': 0.5303, 'best_params': {'C': 0.1, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'linear', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}}
Logistic Regression: {'accuracy': 0.695, 'precision': 0.4125, 'recall': 0.7021, 'f1_score': 0.5197, 'best_params': {'C': 0.1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}}
Decision Tree: {'accuracy': 0.565, 'precision': 0.3148, 'recall': 0.7234, 'f1_score': 0.4387, 'best_params': {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 5, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}}
You are executing with this configuration: undersample_True_scaled_True_removestw_False_5000tfidf
