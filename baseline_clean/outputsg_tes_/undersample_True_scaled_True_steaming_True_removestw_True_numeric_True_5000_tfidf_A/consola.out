/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/main.py:220: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/main.py:220: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
You are executing with [EXAMPLE] of 1000 songs

##################################################
Running experiment with TFIDF embeddings
Preprocessing text...
Label distribution: {0: 763, 1: 237}
X shape: (1000, 5000)
y shape: (1000,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 610, 1: 190}
Label distribution en TEST: {0: 153, 1: 47}
==================================================
Data antes del undersampling ...
X: (800, 5000)
y: (800,)
Apliying UNDERSAMPLE
190
Label distribution: {0: 190, 1: 190}
X shape: (380, 5000)
y shape: (380,)

==============================
Optimizando Naive Bayes...
Mejores parámetros: {'alpha': 2.0}
Accuracy:  0.6850
Precision: 0.4091
Recall:    0.7660
F1-score:  0.5333
              precision    recall  f1-score   support

           0       0.90      0.66      0.76       153
           1       0.41      0.77      0.53        47

    accuracy                           0.69       200
   macro avg       0.66      0.71      0.65       200
weighted avg       0.79      0.69      0.71       200

Confusion matrix:
[[101  52]
 [ 11  36]]

==============================
Optimizando Logistic Regression...
Mejores parámetros: {'C': 1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}
Accuracy:  0.7000
Precision: 0.4133
Recall:    0.6596
F1-score:  0.5082
              precision    recall  f1-score   support

           0       0.87      0.71      0.78       153
           1       0.41      0.66      0.51        47

    accuracy                           0.70       200
   macro avg       0.64      0.69      0.65       200
weighted avg       0.76      0.70      0.72       200

Confusion matrix:
[[109  44]
 [ 16  31]]

==============================
Optimizando SVM...
Mejores parámetros: {'C': 1, 'kernel': 'rbf', 'max_iter': 1000}
Accuracy:  0.3000
Precision: 0.2513
Recall:    1.0000
F1-score:  0.4017
              precision    recall  f1-score   support

           0       1.00      0.08      0.16       153
           1       0.25      1.00      0.40        47

    accuracy                           0.30       200
   macro avg       0.63      0.54      0.28       200
weighted avg       0.82      0.30      0.21       200

Confusion matrix:
[[ 13 140]
 [  0  47]]

==============================
Optimizando Decision Tree...
Mejores parámetros: {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 1, 'min_samples_split': 2}
Accuracy:  0.8800
Precision: 0.7347
Recall:    0.7660
F1-score:  0.7500
              precision    recall  f1-score   support

           0       0.93      0.92      0.92       153
           1       0.73      0.77      0.75        47

    accuracy                           0.88       200
   macro avg       0.83      0.84      0.84       200
weighted avg       0.88      0.88      0.88       200

Confusion matrix:
[[140  13]
 [ 11  36]]


Resumen final de métricas:
Decision Tree: {'accuracy': 0.88, 'precision': 0.7347, 'recall': 0.766, 'f1_score': 0.75, 'best_params': {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 4, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}}
Naive Bayes: {'accuracy': 0.685, 'precision': 0.4091, 'recall': 0.766, 'f1_score': 0.5333, 'best_params': {'alpha': 2.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}}
Logistic Regression: {'accuracy': 0.7, 'precision': 0.4133, 'recall': 0.6596, 'f1_score': 0.5082, 'best_params': {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}}
SVM: {'accuracy': 0.3, 'precision': 0.2513, 'recall': 1.0, 'f1_score': 0.4017, 'best_params': {'C': 1, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}}

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 763, 1: 237}
X shape: (1000, 300)
y shape: (1000,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 610, 1: 190}
Label distribution en TEST: {0: 153, 1: 47}
==================================================
Data antes del undersampling ...
X: (800, 300)
y: (800,)
Apliying UNDERSAMPLE
190
Label distribution: {0: 190, 1: 190}
X shape: (380, 300)
y shape: (380,)

==============================
Optimizando Naive Bayes...
Mejores parámetros: {'var_smoothing': 1e-09}
Accuracy:  0.7100
Precision: 0.4368
Recall:    0.8085
F1-score:  0.5672
              precision    recall  f1-score   support

           0       0.92      0.68      0.78       153
           1       0.44      0.81      0.57        47

    accuracy                           0.71       200
   macro avg       0.68      0.74      0.67       200
weighted avg       0.81      0.71      0.73       200

Confusion matrix:
[[104  49]
 [  9  38]]

==============================
Optimizando Logistic Regression...
Mejores parámetros: {'C': 0.1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}
Accuracy:  0.6950
Precision: 0.4146
Recall:    0.7234
F1-score:  0.5271
              precision    recall  f1-score   support

           0       0.89      0.69      0.77       153
           1       0.41      0.72      0.53        47

    accuracy                           0.69       200
   macro avg       0.65      0.70      0.65       200
weighted avg       0.78      0.69      0.72       200

Confusion matrix:
[[105  48]
 [ 13  34]]

==============================
Optimizando SVM...
Mejores parámetros: {'C': 1, 'kernel': 'rbf', 'max_iter': 1000}
Accuracy:  0.7400
Precision: 0.4648
Recall:    0.7021
F1-score:  0.5593
              precision    recall  f1-score   support

           0       0.89      0.75      0.82       153
           1       0.46      0.70      0.56        47

    accuracy                           0.74       200
   macro avg       0.68      0.73      0.69       200
weighted avg       0.79      0.74      0.76       200

Confusion matrix:
[[115  38]
 [ 14  33]]

==============================
Optimizando Decision Tree...
Mejores parámetros: {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5}
Accuracy:  0.6150
Precision: 0.3256
Recall:    0.5957
F1-score:  0.4211
              precision    recall  f1-score   support

           0       0.83      0.62      0.71       153
           1       0.33      0.60      0.42        47

    accuracy                           0.61       200
   macro avg       0.58      0.61      0.57       200
weighted avg       0.71      0.61      0.64       200

Confusion matrix:
[[95 58]
 [19 28]]


Resumen final de métricas:
Naive Bayes: {'accuracy': 0.71, 'precision': 0.4368, 'recall': 0.8085, 'f1_score': 0.5672, 'best_params': {'priors': None, 'var_smoothing': 1e-09}}
SVM: {'accuracy': 0.74, 'precision': 0.4648, 'recall': 0.7021, 'f1_score': 0.5593, 'best_params': {'C': 1, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}}
Logistic Regression: {'accuracy': 0.695, 'precision': 0.4146, 'recall': 0.7234, 'f1_score': 0.5271, 'best_params': {'C': 0.1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}}
Decision Tree: {'accuracy': 0.615, 'precision': 0.3256, 'recall': 0.5957, 'f1_score': 0.4211, 'best_params': {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 5, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 5, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}}
You are executing with this configuration: undersample_True_scaled_True_removestw_True_5000tfidf
