2025-09-15 20:04:40.971526: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-09-15 20:04:41.033568: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-09-15 20:04:45.564644: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_without_artist.py:278: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
For TF-IDF embbedings you are selecteing this columns:
--> ['emotion', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Release Date', 'Key', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
After removing some columns that doesn't love me, for TF-IDF embbedings you are selecteing this columns:
--> ['emotion', 'Time signature', 'song', 'Genre', 'Album', 'Release Date', 'Key', 'Similar Song 1', 'Similar Song 2', 'Similar Song 3', 'song_normalized', 'artist_normalized']
After removing some columns that doesn't love me, for numeric cols you are selecteing this columns:
--> ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
--> PaTH:  ../../data/embbedings_khipu/LB_fuss/lb_khipu_B.npy

##################################################
Running experiment with TFIDF embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [5020, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4628, Test Loss: 0.4049, F1: 0.8029, AUC: 0.9017
Epoch [10/30] Train Loss: 0.2215, Test Loss: 0.3061, F1: 0.8640, AUC: 0.9534
Epoch [20/30] Train Loss: 0.1811, Test Loss: 0.2916, F1: 0.8804, AUC: 0.9525
Mejores resultados en la época:  11
f1-score 0.8850491960643149
AUC según el mejor F1-score 0.953903092794153

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4589, Test Loss: 0.4062, F1: 0.8312, AUC: 0.9008
Epoch [10/30] Train Loss: 0.2219, Test Loss: 0.2931, F1: 0.8658, AUC: 0.9565
Epoch [20/30] Train Loss: 0.1742, Test Loss: 0.3130, F1: 0.8670, AUC: 0.9562
Mejores resultados en la época:  29
f1-score 0.8931804465902233
AUC según el mejor F1-score 0.9591350464872154

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4656, Test Loss: 0.4357, F1: 0.7759, AUC: 0.8939
Epoch [10/30] Train Loss: 0.2289, Test Loss: 0.3500, F1: 0.8364, AUC: 0.9532
Epoch [20/30] Train Loss: 0.1864, Test Loss: 0.2812, F1: 0.8838, AUC: 0.9555
Mejores resultados en la época:  20
f1-score 0.8838479809976247
AUC según el mejor F1-score 0.9555147935633376

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4632, Test Loss: 0.4237, F1: 0.7909, AUC: 0.8934
Epoch [10/30] Train Loss: 0.2264, Test Loss: 0.3089, F1: 0.8757, AUC: 0.9549
Epoch [20/30] Train Loss: 0.1868, Test Loss: 0.2847, F1: 0.8813, AUC: 0.9565
Mejores resultados en la época:  28
f1-score 0.8871224165341812
AUC según el mejor F1-score 0.9576220855098678

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4754, Test Loss: 0.4260, F1: 0.7930, AUC: 0.8873
Epoch [10/30] Train Loss: 0.2378, Test Loss: 0.2815, F1: 0.8749, AUC: 0.9525
Epoch [20/30] Train Loss: 0.1919, Test Loss: 0.2825, F1: 0.8809, AUC: 0.9553
Mejores resultados en la época:  26
f1-score 0.8873508353221957
AUC según el mejor F1-score 0.9562461963473664
Epoch [0/30] Train Loss: 0.4532, Test Loss: 0.3670, F1: 0.7226, AUC: 0.9067
Epoch [10/30] Train Loss: 0.2250, Test Loss: 0.2491, F1: 0.8015, AUC: 0.9575
Epoch [20/30] Train Loss: 0.1843, Test Loss: 0.2927, F1: 0.7792, AUC: 0.9600
Mejores resultados en la época:  29
f1-score 0.8251527695643603
AUC según el mejor F1-score 0.9599950564622631
Confusion matrix Test saved: outputs_without_artist/0/tfidf/cm_mlp_1.png

========================================
Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4560, Test Loss: 0.3910, F1: 0.8424, AUC: 0.9161
Epoch [10/30] Train Loss: 0.2155, Test Loss: 0.2749, F1: 0.8901, AUC: 0.9551
Epoch [20/30] Train Loss: 0.1402, Test Loss: 0.3065, F1: 0.8851, AUC: 0.9587
Mejores resultados en la época:  18
f1-score 0.8931506849315068
AUC según el mejor F1-score 0.9592249799065561

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4676, Test Loss: 0.3992, F1: 0.8342, AUC: 0.9065
Epoch [10/30] Train Loss: 0.2090, Test Loss: 0.2985, F1: 0.8716, AUC: 0.9572
Epoch [20/30] Train Loss: 0.1434, Test Loss: 0.3216, F1: 0.8840, AUC: 0.9599
Mejores resultados en la época:  25
f1-score 0.8961431507677428
AUC según el mejor F1-score 0.9607762653705457

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4545, Test Loss: 0.3660, F1: 0.8352, AUC: 0.9195
Epoch [10/30] Train Loss: 0.2122, Test Loss: 0.2913, F1: 0.8846, AUC: 0.9568
Epoch [20/30] Train Loss: 0.1348, Test Loss: 0.3134, F1: 0.8877, AUC: 0.9590
Mejores resultados en la época:  24
f1-score 0.8965265970366771
AUC según el mejor F1-score 0.9613396036145663

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4559, Test Loss: 0.3980, F1: 0.8325, AUC: 0.9113
Epoch [10/30] Train Loss: 0.2122, Test Loss: 0.3402, F1: 0.8644, AUC: 0.9520
Epoch [20/30] Train Loss: 0.1357, Test Loss: 0.3098, F1: 0.8849, AUC: 0.9586
Mejores resultados en la época:  15
f1-score 0.8946737044145874
AUC según el mejor F1-score 0.959444962555153

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4563, Test Loss: 0.3830, F1: 0.8257, AUC: 0.9099
Epoch [10/30] Train Loss: 0.2204, Test Loss: 0.3451, F1: 0.8580, AUC: 0.9553
Epoch [20/30] Train Loss: 0.1416, Test Loss: 0.3916, F1: 0.8619, AUC: 0.9566
Mejores resultados en la época:  21
f1-score 0.8979342378682312
AUC según el mejor F1-score 0.9638076288592988
Epoch [0/30] Train Loss: 0.4558, Test Loss: 0.3800, F1: 0.7231, AUC: 0.9168
Epoch [10/30] Train Loss: 0.2069, Test Loss: 0.4497, F1: 0.7122, AUC: 0.9602
Epoch [20/30] Train Loss: 0.1405, Test Loss: 0.2559, F1: 0.7994, AUC: 0.9651
Mejores resultados en la época:  23
f1-score 0.8349096357336829
AUC según el mejor F1-score 0.9673434781789891
Confusion matrix Test saved: outputs_without_artist/0/tfidf/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160705': {'f1_cv_mean': 0.8873, 'f1_cv_std': 0.0032, 'params': 160705, 'f1_score_test': 0.8252}, 'MLP_5840897': {'f1_cv_mean': 0.8957, 'f1_cv_std': 0.0016, 'params': 5840897, 'f1_score_test': 0.8349}}}
Saved on: outputs_without_artist/0/tfidf

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv': 0.8786, 'precision_cv': 0.8929, 'recall_cv': 0.8605, 'f1_score_cv': 0.8764}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.90      0.93     16465
           1       0.73      0.87      0.79      5160

    accuracy                           0.89     21625
   macro avg       0.84      0.88      0.86     21625
weighted avg       0.90      0.89      0.89     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [20:45:52] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [20:49:10] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [20:52:26] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [20:55:43] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [20:59:01] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [21:02:26] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Confusion matrix Test saved as: outputs_without_artist/0/tfidf/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_without_artist/0/tfidf/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv': 0.734, 'precision_cv': 0.6924, 'recall_cv': 0.85, 'f1_score_cv': 0.7618}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.72      0.80     16465
           1       0.47      0.79      0.59      5160

    accuracy                           0.73     21625
   macro avg       0.69      0.75      0.69     21625
weighted avg       0.81      0.73      0.75     21625

Confusion matrix Test saved as: outputs_without_artist/0/tfidf/conf_matrix_test_svm.png
Modelo guardado como: outputs_without_artist/0/tfidf/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv': 0.8254, 'precision_cv': 0.883, 'recall_cv': 0.7506, 'f1_score_cv': 0.8113}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.83     21625
weighted avg       0.88      0.87      0.87     21625

Confusion matrix Test saved as: outputs_without_artist/0/tfidf/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_without_artist/0/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv': 0.8204, 'precision_cv': 0.8566, 'recall_cv': 0.7698, 'f1_score_cv': 0.8108}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.87      0.90     16465
           1       0.66      0.77      0.71      5160

    accuracy                           0.85     21625
   macro avg       0.79      0.82      0.80     21625
weighted avg       0.86      0.85      0.85     21625

Confusion matrix Test saved as: outputs_without_artist/0/tfidf/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_without_artist/0/tfidf/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv': 0.8785, 'precision_cv': 0.9036, 'recall_cv': 0.8474, 'f1_score_cv': 0.8746}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.91      0.93     16465
           1       0.75      0.85      0.80      5160

    accuracy                           0.90     21625
   macro avg       0.85      0.88      0.86     21625
weighted avg       0.90      0.90      0.90     21625

Confusion matrix Test saved as: outputs_without_artist/0/tfidf/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_without_artist/0/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv': 0.8467, 'precision_cv': 0.8729, 'recall_cv': 0.8116, 'f1_score_cv': 0.8411}
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.88      0.91     16465
           1       0.68      0.81      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.85      0.83     21625
weighted avg       0.88      0.87      0.87     21625

Confusion matrix Test saved as: outputs_without_artist/0/tfidf/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_without_artist/0/tfidf/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv': 0.8785, 'precision_cv': 0.9036, 'recall_cv': 0.8474, 'f1_score_cv': 0.8746, 'accuracy_test': 0.8962, 'precision_test': 0.7486, 'recall_test': 0.8506, 'f1_score_test': 0.7963}
Logistic Regression: {'accuracy_cv': 0.8786, 'precision_cv': 0.8929, 'recall_cv': 0.8605, 'f1_score_cv': 0.8764, 'accuracy_test': 0.8908, 'precision_test': 0.7281, 'recall_test': 0.8657, 'f1_score_test': 0.791}
Naive Bayes: {'accuracy_cv': 0.8467, 'precision_cv': 0.8729, 'recall_cv': 0.8116, 'f1_score_cv': 0.8411, 'accuracy_test': 0.866, 'precision_test': 0.6847, 'recall_test': 0.813, 'f1_score_test': 0.7433}
Decision Tree: {'accuracy_cv': 0.8254, 'precision_cv': 0.883, 'recall_cv': 0.7506, 'f1_score_cv': 0.8113, 'accuracy_test': 0.8732, 'precision_test': 0.7232, 'recall_test': 0.7589, 'f1_score_test': 0.7406}
Random Forest: {'accuracy_cv': 0.8204, 'precision_cv': 0.8566, 'recall_cv': 0.7698, 'f1_score_cv': 0.8108, 'accuracy_test': 0.8491, 'precision_test': 0.6573, 'recall_test': 0.7678, 'f1_score_test': 0.7083}
SVM: {'accuracy_cv': 0.734, 'precision_cv': 0.6924, 'recall_cv': 0.85, 'f1_score_cv': 0.7618, 'accuracy_test': 0.7332, 'precision_test': 0.4652, 'recall_test': 0.7882, 'f1_score_test': 0.5851}
resultados_globales con ML {'tfidf': {'MLP_160705': {'f1_cv_mean': 0.8873, 'f1_cv_std': 0.0032, 'params': 160705, 'f1_score_test': 0.8252}, 'MLP_5840897': {'f1_cv_mean': 0.8957, 'f1_cv_std': 0.0016, 'params': 5840897, 'f1_score_test': 0.8349}, 'Logistic Regression': {'accuracy_cv': 0.8786, 'precision_cv': 0.8929, 'recall_cv': 0.8605, 'f1_score_cv': 0.8764, 'accuracy_test': 0.8908, 'precision_test': 0.7281, 'recall_test': 0.8657, 'f1_score_test': 0.791}, 'SVM': {'accuracy_cv': 0.734, 'precision_cv': 0.6924, 'recall_cv': 0.85, 'f1_score_cv': 0.7618, 'accuracy_test': 0.7332, 'precision_test': 0.4652, 'recall_test': 0.7882, 'f1_score_test': 0.5851}, 'Decision Tree': {'accuracy_cv': 0.8254, 'precision_cv': 0.883, 'recall_cv': 0.7506, 'f1_score_cv': 0.8113, 'accuracy_test': 0.8732, 'precision_test': 0.7232, 'recall_test': 0.7589, 'f1_score_test': 0.7406}, 'Random Forest': {'accuracy_cv': 0.8204, 'precision_cv': 0.8566, 'recall_cv': 0.7698, 'f1_score_cv': 0.8108, 'accuracy_test': 0.8491, 'precision_test': 0.6573, 'r/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_without_artist.py:278: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
ecall_test': 0.7678, 'f1_score_test': 0.7083}, 'XGBoost': {'accuracy_cv': 0.8785, 'precision_cv': 0.9036, 'recall_cv': 0.8474, 'f1_score_cv': 0.8746, 'accuracy_test': 0.8962, 'precision_test': 0.7486, 'recall_test': 0.8506, 'f1_score_test': 0.7963}, 'Naive Bayes': {'accuracy_cv': 0.8467, 'precision_cv': 0.8729, 'recall_cv': 0.8116, 'f1_score_cv': 0.8411, 'accuracy_test': 0.866, 'precision_test': 0.6847, 'recall_test': 0.813, 'f1_score_test': 0.7433}}}

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 320)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 320)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [320, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4472, Test Loss: 0.4027, F1: 0.8086, AUC: 0.9059
Epoch [10/30] Train Loss: 0.3215, Test Loss: 0.3400, F1: 0.8580, AUC: 0.9310
Epoch [20/30] Train Loss: 0.2926, Test Loss: 0.3564, F1: 0.8557, AUC: 0.9402
Mejores resultados en la época:  18
f1-score 0.8671311575602004
AUC según el mejor F1-score 0.9396809954517457

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4840, Test Loss: 0.4076, F1: 0.8074, AUC: 0.8970
Epoch [10/30] Train Loss: 0.3258, Test Loss: 0.3394, F1: 0.8533, AUC: 0.9317
Epoch [20/30] Train Loss: 0.3049, Test Loss: 0.3166, F1: 0.8586, AUC: 0.9385
Mejores resultados en la época:  22
f1-score 0.8693933028543186
AUC según el mejor F1-score 0.93920964457139

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4562, Test Loss: 0.4065, F1: 0.8095, AUC: 0.8989
Epoch [10/30] Train Loss: 0.3275, Test Loss: 0.3410, F1: 0.8561, AUC: 0.9323
Epoch [20/30] Train Loss: 0.2956, Test Loss: 0.3317, F1: 0.8445, AUC: 0.9406
Mejores resultados en la época:  28
f1-score 0.8657029610631849
AUC según el mejor F1-score 0.9425635009164112

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4628, Test Loss: 0.3968, F1: 0.8314, AUC: 0.9025
Epoch [10/30] Train Loss: 0.3289, Test Loss: 0.3302, F1: 0.8571, AUC: 0.9323
Epoch [20/30] Train Loss: 0.3019, Test Loss: 0.3179, F1: 0.8645, AUC: 0.9383
Mejores resultados en la época:  21
f1-score 0.8693333333333333
AUC según el mejor F1-score 0.9409366412432403

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4576, Test Loss: 0.4048, F1: 0.8187, AUC: 0.8977
Epoch [10/30] Train Loss: 0.3207, Test Loss: 0.3302, F1: 0.8513, AUC: 0.9328
Epoch [20/30] Train Loss: 0.2945, Test Loss: 0.3246, F1: 0.8538, AUC: 0.9411
Mejores resultados en la época:  24
f1-score 0.8732394366197183
AUC según el mejor F1-score 0.9429662245037879
Epoch [0/30] Train Loss: 0.4302, Test Loss: 0.3742, F1: 0.7208, AUC: 0.9066
Epoch [10/30] Train Loss: 0.3256, Test Loss: 0.3324, F1: 0.7428, AUC: 0.9359
Epoch [20/30] Train Loss: 0.3034, Test Loss: 0.2831, F1: 0.7762, AUC: 0.9398
Mejores resultados en la época:  28
f1-score 0.7871634802965275
AUC según el mejor F1-score 0.9427576524787133
Confusion matrix Test saved: outputs_without_artist/0/lyrics_bert/cm_mlp_1.png

========================================
Entrenando red 6 con capas [320, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4451, Test Loss: 0.3807, F1: 0.8350, AUC: 0.9131
Epoch [10/30] Train Loss: 0.2890, Test Loss: 0.3067, F1: 0.8683, AUC: 0.9427
Epoch [20/30] Train Loss: 0.2362, Test Loss: 0.2938, F1: 0.8790, AUC: 0.9485
Mejores resultados en la época:  28
f1-score 0.8849644346333088
AUC según el mejor F1-score 0.9509940914477044

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4448, Test Loss: 0.4310, F1: 0.8356, AUC: 0.9097
Epoch [10/30] Train Loss: 0.2921, Test Loss: 0.3541, F1: 0.8518, AUC: 0.9397
Epoch [20/30] Train Loss: 0.2388, Test Loss: 0.3182, F1: 0.8740, AUC: 0.9464
Mejores resultados en la época:  19
f1-score 0.8754106053496011
AUC según el mejor F1-score 0.9489564906518538

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4437, Test Loss: 0.3987, F1: 0.8110, AUC: 0.9074
Epoch [10/30] Train Loss: 0.2935, Test Loss: 0.3109, F1: 0.8638, AUC: 0.9416
Epoch [20/30] Train Loss: 0.2398, Test Loss: 0.3187, F1: 0.8653, AUC: 0.9452
Mejores resultados en la época:  24
f1-score 0.8808056872037915
AUC según el mejor F1-score 0.9511317644015533

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4413, Test Loss: 0.4577, F1: 0.8011, AUC: 0.9070
Epoch [10/30] Train Loss: 0.2968, Test Loss: 0.3191, F1: 0.8509, AUC: 0.9425
Epoch [20/30] Train Loss: 0.2436, Test Loss: 0.2977, F1: 0.8714, AUC: 0.9484
Mejores resultados en la época:  26
f1-score 0.8822182037371911
AUC según el mejor F1-score 0.9494566176981609

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4437, Test Loss: 0.3949, F1: 0.8149, AUC: 0.9042
Epoch [10/30] Train Loss: 0.2926, Test Loss: 0.3250, F1: 0.8589, AUC: 0.9352
Epoch [20/30] Train Loss: 0.2435, Test Loss: 0.3391, F1: 0.8635, AUC: 0.9448
Mejores resultados en la época:  29
f1-score 0.8794159208666981
AUC según el mejor F1-score 0.9504112875505041
Epoch [0/30] Train Loss: 0.4359, Test Loss: 0.3533, F1: 0.7397, AUC: 0.9111
Epoch [10/30] Train Loss: 0.2878, Test Loss: 0.2673, F1: 0.7910, AUC: 0.9466
Epoch [20/30] Train Loss: 0.2348, Test Loss: 0.2918, F1: 0.7883, AUC: 0.9486
Mejores resultados en la época:  23
f1-score 0.806872037914692
AUC según el mejor F1-score 0.9511932993877075
Confusion matrix Test saved: outputs_without_artist/0/lyrics_bert/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160705': {'f1_cv_mean': 0.8873, 'f1_cv_std': 0.0032, 'params': 160705, 'f1_score_test': 0.8252}, 'MLP_5840897': {'f1_cv_mean': 0.8957, 'f1_cv_std': 0.0016, 'params': 5840897, 'f1_score_test': 0.8349}, 'Logistic Regression': {'accuracy_cv': 0.8786, 'precision_cv': 0.8929, 'recall_cv': 0.8605, 'f1_score_cv': 0.8764, 'accuracy_test': 0.8908, 'precision_test': 0.7281, 'recall_test': 0.8657, 'f1_score_test': 0.791}, 'SVM': {'accuracy_cv': 0.734, 'precision_cv': 0.6924, 'recall_cv': 0.85, 'f1_score_cv': 0.7618, 'accuracy_test': 0.7332, 'precision_test': 0.4652, 'recall_test': 0.7882, 'f1_score_test': 0.5851}, 'Decision Tree': {'accuracy_cv': 0.8254, 'precision_cv': 0.883, 'recall_cv': 0.7506, 'f1_score_cv': 0.8113, 'accuracy_test': 0.8732, 'precision_test': 0.7232, 'recall_test': 0.7589, 'f1_score_test': 0.7406}, 'Random Forest': {'accuracy_cv': 0.8204, 'precision_cv': 0.8566, 'recall_cv': 0.7698, 'f1_score_cv': 0.8108, 'accuracy_test': 0.8491, 'precision_test': 0.6573, 'recall_test': 0.7678, 'f1_score_test': 0.7083}, 'XGBoost': {'accuracy_cv': 0.8785, 'precision_cv': 0.9036, 'recall_cv': 0.8474, 'f1_score_cv': 0.8746, 'accuracy_test': 0.8962, 'precision_test': 0.7486, 'recall_test': 0.8506, 'f1_score_test': 0.7963}, 'Naive Bayes': {'accuracy_cv': 0.8467, 'precision_cv': 0.8729, 'recall_cv': 0.8116, 'f1_score_cv': 0.8411, 'accuracy_test': 0.866, 'precision_test': 0.6847, 'recall_test': 0.813, 'f1_score_test': 0.7433}}, 'lyrics_bert': {'MLP_10305': {'f1_cv_mean': 0.869, 'f1_cv_std': 0.0026, 'params': 10305, 'f1_score_test': 0.7872}, 'MLP_1028097': {'f1_cv_mean': 0.8806, 'f1_cv_std': 0.0032, 'params': 1028097, 'f1_score_test': 0.8069}}}
Saved on: outputs_without_artist/0/lyrics_bert

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv': 0.8484, 'precision_cv': 0.863, 'recall_cv': 0.8282, 'f1_score_cv': 0.8453}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.87      0.91     16465
           1       0.67      0.83      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.85      0.82     21625
weighted avg       0.88      0.86      0.87     21625

/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [21:30:09] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [21:30:47] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [21:31:25] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [21:32:03] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [21:32:41] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [21:33:20] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Confusion matrix Test saved as: outputs_without_artist/0/lyrics_bert/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_without_artist/0/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv': 0.6485, 'precision_cv': 0.6268, 'recall_cv': 0.757, 'f1_score_cv': 0.6827}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.88      0.65      0.75     16465
           1       0.39      0.71      0.50      5160

    accuracy                           0.67     21625
   macro avg       0.63      0.68      0.63     21625
weighted avg       0.76      0.67      0.69     21625

Confusion matrix Test saved as: outputs_without_artist/0/lyrics_bert/conf_matrix_test_svm.png
Modelo guardado como: outputs_without_artist/0/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv': 0.7888, 'precision_cv': 0.8157, 'recall_cv': 0.7465, 'f1_score_cv': 0.7795}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.91      0.85      0.88     16465
           1       0.61      0.75      0.67      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.80      0.78     21625
weighted avg       0.84      0.82      0.83     21625

Confusion matrix Test saved as: outputs_without_artist/0/lyrics_bert/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_without_artist/0/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv': 0.8509, 'precision_cv': 0.888, 'recall_cv': 0.803, 'f1_score_cv': 0.8434}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.90      0.92     16465
           1       0.71      0.81      0.76      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.85      0.84     21625
weighted avg       0.88      0.88      0.88     21625

Confusion matrix Test saved as: outputs_without_artist/0/lyrics_bert/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_without_artist/0/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv': 0.875, 'precision_cv': 0.8946, 'recall_cv': 0.8501, 'f1_score_cv': 0.8718}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.91      0.93     16465
           1       0.74      0.86      0.79      5160

    accuracy                           0.89     21625
   macro avg       0.85      0.88      0.86     21625
weighted avg       0.90      0.89      0.90     21625

Confusion matrix Test saved as: outputs_without_artist/0/lyrics_bert/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_without_artist/0/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv': 0.7743, 'precision_cv': 0.7582, 'recall_cv': 0.8055, 'f1_score_cv': 0.7811}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.74      0.82     16465
           1       0.49      0.81      0.61      5160

    accuracy                           0.76     21625
   macro avg       0.71      0.77      0.72     21625
weighted avg       0.82      0.76      0.77     21625

Confusion matrix Test saved as: outputs_without_artist/0/lyrics_bert/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_without_artist/0/lyrics_bert/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv': 0.875, 'precision_cv': 0.8946, 'recall_cv': 0.8501, 'f1_score_cv': 0.8718, 'accuracy_test': 0.8937, 'precision_test': 0.7389, 'recall_test': 0.8574, 'f1_score_test': 0.7938}
Random Forest: {'accuracy_cv': 0.8509, 'precision_cv': 0.888, 'recall_cv': 0.803, 'f1_score_cv': 0.8434, 'accuracy_test': 0.8771, 'precision_test': 0.7128, 'recall_test': 0.8124, 'f1_score_test': 0.7594}
Logistic Regression: {'accuracy_cv': 0.8484, 'precision_cv': 0.863, 'recall_cv': 0.8282, 'f1_score_cv': 0.8453, 'accuracy_test': 0.861, 'precision_test': 0.6688, 'recall_test': 0.8269, 'f1_score_test': 0.7395}
Decision Tree: {'accuracy_cv': 0.7888, 'precision_cv': 0.8157, 'recall_cv': 0.7465, 'f1_score_cv': 0.7795, 'accuracy_test': 0.8249, 'precision_test': 0.6084, 'recall_test': 0.7473, 'f1_score_test': 0.6707}
Naive Bayes: {'accuracy_cv': 0.7743, 'precision_cv': 0.7582, 'recall_cv': 0.8055, 'f1_score_cv': 0.7811, 'accuracy_test': 0.7559, 'precision_test': 0.493, 'recall_test': 0.8107, 'f1_score_test': 0.6131}
SVM: {'accuracy_cv': 0.6485, 'precision_cv': 0.6268, 'recall_cv': 0.757, 'f1_score_cv': 0.6827, 'accuracy_test': 0.6661, 'precision_test': 0.3904, 'recall_test': 0.7114, 'f1_score_test': 0.5042}
resultados_globales con ML {'tfidf': {'MLP_160705': {'f1_cv_mean': 0.8873, 'f1_cv_std': 0.0032, 'params': 160705, 'f1_score_test': 0.8252}, 'MLP_5840897': {'f1_cv_mean': 0.8957, 'f1_cv_std': 0.0016, 'params': 5840897, 'f1_score_test': 0.8349}, 'Logistic Regression': {'accuracy_cv': 0.8786, 'precision_cv': 0.8929, 'recall_cv': 0.8605, 'f1_score_cv': 0.8764, 'accuracy_test': 0.8908, 'precision_test': 0.7281, 'recall_test': 0.8657, 'f1_score_test': 0.791}, 'SVM': {'accuracy_cv': 0.734, 'precision_cv': 0.6924, 'recall_cv': 0.85, 'f1_score_cv': 0.7618, 'accuracy_test': 0.7332, 'precision_test': 0.4652, 'recall_test': 0.7882, 'f1_score_test': 0.5851}, 'Decision Tree': {'accuracy_cv': 0.8254, 'precision_cv': 0.883, 'recall_cv': 0.7506, 'f1_score_cv': 0.8113, 'accuracy_test': 0.8732, 'precision_test': 0.7232, 'recall_test': 0.7589, 'f1_score_test': 0.7406}, 'Random Forest': {'accuracy_cv': 0.8204, 'precision_cv': 0.8566, 'recall_cv': 0.7698, 'f1_score_cv': 0.8108, 'accuracy_test': 0.8491, 'precision_test': 0.6573, 'recall_test': 0.7678, 'f1_score_test': 0.7083}, 'XGBoost': {'accuracy_cv': 0.8785, 'precision_cv': 0.9036, 'recall_cv': 0.8474, 'f1_score_cv': 0.8746, 'accuracy_test': 0.8962, 'precision_test': 0.7486, 'recall_test': 0.8506, 'f1_score_test': 0.7963}, 'Naive Bayes': {'accuracy_cv': 0.8467, 'precision_cv': 0.8729, 'recall_cv': 0.8116, 'f1_score_cv': 0.8411, 'accuracy_test': 0.866, 'precision_test': 0.6847, 'recall_test': 0.813, 'f1_score_test': 0.7433}}, 'lyrics_bert': {'MLP_10305': {'f1_cv_mean': 0.869, 'f1_cv_std': 0.0026, 'params': 10305, 'f1_score_test': 0.7872}, 'MLP_1028097': {'f1_cv_mean': 0.8806, 'f1_cv_std': 0.0032, 'params': 1028097, 'f1_score_test': 0.8069}, 'Logistic Regression': {'accuracy_cv': 0.8484, 'precision_cv': 0.863, 'recall_cv': 0.8282, 'f1_score_cv': 0.8453, 'accuracy_test': 0.861, 'precision_test': 0.6688, 'recall_test': 0.8269, 'f1_score_test': 0.7395}, 'SVM': {'accuracy_cv': 0.6485, 'precision_cv': 0.6268, 'recall_cv': 0.757, 'f1_score_cv': 0.6827, 'accuracy_test': 0.6661, 'precision_te/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_without_artist.py:278: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
st': 0.3904, 'recall_test': 0.7114, 'f1_score_test': 0.5042}, 'Decision Tree': {'accuracy_cv': 0.7888, 'precision_cv': 0.8157, 'recall_cv': 0.7465, 'f1_score_cv': 0.7795, 'accuracy_test': 0.8249, 'precision_test': 0.6084, 'recall_test': 0.7473, 'f1_score_test': 0.6707}, 'Random Forest': {'accuracy_cv': 0.8509, 'precision_cv': 0.888, 'recall_cv': 0.803, 'f1_score_cv': 0.8434, 'accuracy_test': 0.8771, 'precision_test': 0.7128, 'recall_test': 0.8124, 'f1_score_test': 0.7594}, 'XGBoost': {'accuracy_cv': 0.875, 'precision_cv': 0.8946, 'recall_cv': 0.8501, 'f1_score_cv': 0.8718, 'accuracy_test': 0.8937, 'precision_test': 0.7389, 'recall_test': 0.8574, 'f1_score_test': 0.7938}, 'Naive Bayes': {'accuracy_cv': 0.7743, 'precision_cv': 0.7582, 'recall_cv': 0.8055, 'f1_score_cv': 0.7811, 'accuracy_test': 0.7559, 'precision_test': 0.493, 'recall_test': 0.8107, 'f1_score_test': 0.6131}}}

##################################################
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
E eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El último valor de eliminar es:  98849
Shape original y: (108138,)
Shape filtrado  y: (108125,)
Label distribution: {0: 82336, 1: 25802}
X shape: (108125, 1536)
y shape: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1556)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1556)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [1556, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4232, Test Loss: 0.3903, F1: 0.8385, AUC: 0.9140
Epoch [10/30] Train Loss: 0.3240, Test Loss: 0.3303, F1: 0.8482, AUC: 0.9377
Epoch [20/30] Train Loss: 0.3057, Test Loss: 0.3274, F1: 0.8637, AUC: 0.9384
Mejores resultados en la época:  25
f1-score 0.8685352409991791
AUC según el mejor F1-score 0.9414412550796976

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4236, Test Loss: 0.4603, F1: 0.7494, AUC: 0.9117
Epoch [10/30] Train Loss: 0.3229, Test Loss: 0.3294, F1: 0.8591, AUC: 0.9338
Epoch [20/30] Train Loss: 0.3055, Test Loss: 0.3169, F1: 0.8654, AUC: 0.9384
Mejores resultados en la época:  26
f1-score 0.8682170542635659
AUC según el mejor F1-score 0.9401603860570279

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4402, Test Loss: 0.3961, F1: 0.8320, AUC: 0.9069
Epoch [10/30] Train Loss: 0.3263, Test Loss: 0.3391, F1: 0.8532, AUC: 0.9292
Epoch [20/30] Train Loss: 0.3111, Test Loss: 0.3269, F1: 0.8602, AUC: 0.9342
Mejores resultados en la época:  19
f1-score 0.862139178344712
AUC según el mejor F1-score 0.9339170408366383

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4154, Test Loss: 0.3746, F1: 0.8376, AUC: 0.9140
Epoch [10/30] Train Loss: 0.3257, Test Loss: 0.3515, F1: 0.8378, AUC: 0.9330
Epoch [20/30] Train Loss: 0.3090, Test Loss: 0.3321, F1: 0.8646, AUC: 0.9362
Mejores resultados en la época:  28
f1-score 0.8674418604651163
AUC según el mejor F1-score 0.9401555130423023

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4237, Test Loss: 0.3777, F1: 0.8332, AUC: 0.9136
Epoch [10/30] Train Loss: 0.3282, Test Loss: 0.3239, F1: 0.8621, AUC: 0.9362
Epoch [20/30] Train Loss: 0.3107, Test Loss: 0.3140, F1: 0.8694, AUC: 0.9397
Mejores resultados en la época:  25
f1-score 0.8720570749108204
AUC según el mejor F1-score 0.9409794323353675
Epoch [0/30] Train Loss: 0.4177, Test Loss: 0.3660, F1: 0.7196, AUC: 0.9143
Epoch [10/30] Train Loss: 0.3200, Test Loss: 0.2927, F1: 0.7634, AUC: 0.9350
Epoch [20/30] Train Loss: 0.3050, Test Loss: 0.2814, F1: 0.7699, AUC: 0.9383
Mejores resultados en la época:  28
f1-score 0.7726986977997305
AUC según el mejor F1-score 0.9396969729070591
Confusion matrix Test saved: outputs_without_artist/0/gpt/cm_mlp_1.png

========================================
Entrenando red 6 con capas [1556, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4333, Test Loss: 0.4121, F1: 0.8168, AUC: 0.9178
Epoch [10/30] Train Loss: 0.3154, Test Loss: 0.3265, F1: 0.8663, AUC: 0.9403
Epoch [20/30] Train Loss: 0.2781, Test Loss: 0.3216, F1: 0.8536, AUC: 0.9389
Mejores resultados en la época:  27
f1-score 0.8803107550376305
AUC según el mejor F1-score 0.9472054150892375

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4177, Test Loss: 0.3799, F1: 0.8281, AUC: 0.9183
Epoch [10/30] Train Loss: 0.3146, Test Loss: 0.3317, F1: 0.8555, AUC: 0.9391
Epoch [20/30] Train Loss: 0.2796, Test Loss: 0.3382, F1: 0.8668, AUC: 0.9411
Mejores resultados en la época:  26
f1-score 0.8784653175810037
AUC según el mejor F1-score 0.9473965786217026

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4263, Test Loss: 0.4152, F1: 0.7979, AUC: 0.9120
Epoch [10/30] Train Loss: 0.3168, Test Loss: 0.3280, F1: 0.8535, AUC: 0.9339
Epoch [20/30] Train Loss: 0.2811, Test Loss: 0.3183, F1: 0.8675, AUC: 0.9408
Mejores resultados en la época:  25
f1-score 0.8763525835866262
AUC según el mejor F1-score 0.9446942921004597

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4230, Test Loss: 0.3930, F1: 0.8282, AUC: 0.9142
Epoch [10/30] Train Loss: 0.3151, Test Loss: 0.3209, F1: 0.8620, AUC: 0.9381
Epoch [20/30] Train Loss: 0.2779, Test Loss: 0.3229, F1: 0.8644, AUC: 0.9382
Mejores resultados en la época:  29
f1-score 0.8770764119601329
AUC según el mejor F1-score 0.9458880519287804

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4249, Test Loss: 0.3893, F1: 0.8115, AUC: 0.9178
Epoch [10/30] Train Loss: 0.3149, Test Loss: 0.3118, F1: 0.8701, AUC: 0.9419
Epoch [20/30] Train Loss: 0.2834, Test Loss: 0.3043, F1: 0.8739, AUC: 0.9442
Mejores resultados en la época:  25
f1-score 0.8826370441922241
AUC según el mejor F1-score 0.948861357800681
Epoch [0/30] Train Loss: 0.4123, Test Loss: 0.3607, F1: 0.7196, AUC: 0.9193
Epoch [10/30] Train Loss: 0.3150, Test Loss: 0.2662, F1: 0.7803, AUC: 0.9401
Epoch [20/30] Train Loss: 0.2793, Test Loss: 0.2780, F1: 0.7795, AUC: 0.9405
Mejores resultados en la época:  23
f1-score 0.7851785714285714
AUC según el mejor F1-score 0.9462841310084581
Confusion matrix Test saved: outputs_without_artist/0/gpt/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160705': {'f1_cv_mean': 0.8873, 'f1_cv_std': 0.0032, 'params': 160705, 'f1_score_test': 0.8252}, 'MLP_5840897': {'f1_cv_mean': 0.8957, 'f1_cv_std': 0.0016, 'params': 5840897, 'f1_score_test': 0.8349}, 'Logistic Regression': {'accuracy_cv': 0.8786, 'precision_cv': 0.8929, 'recall_cv': 0.8605, 'f1_score_cv': 0.8764, 'accuracy_test': 0.8908, 'precision_test': 0.7281, 'recall_test': 0.8657, 'f1_score_test': 0.791}, 'SVM': {'accuracy_cv': 0.734, 'precision_cv': 0.6924, 'recall_cv': 0.85, 'f1_score_cv': 0.7618, 'accuracy_test': 0.7332, 'precision_test': 0.4652, 'recall_test': 0.7882, 'f1_score_test': 0.5851}, 'Decision Tree': {'accuracy_cv': 0.8254, 'precision_cv': 0.883, 'recall_cv': 0.7506, 'f1_score_cv': 0.8113, 'accuracy_test': 0.8732, 'precision_test': 0.7232, 'recall_test': 0.7589, 'f1_score_test': 0.7406}, 'Random Forest': {'accuracy_cv': 0.8204, 'precision_cv': 0.8566, 'recall_cv': 0.7698, 'f1_score_cv': 0.8108, 'accuracy_test': 0.8491, 'precision_test': 0.6573, 'recall_test': 0.7678, 'f1_score_test': 0.7083}, 'XGBoost': {'accuracy_cv': 0.8785, 'precision_cv': 0.9036, 'recall_cv': 0.8474, 'f1_score_cv': 0.8746, 'accuracy_test': 0.8962, 'precision_test': 0.7486, 'recall_test': 0.8506, 'f1_score_test': 0.7963}, 'Naive Bayes': {'accuracy_cv': 0.8467, 'precision_cv': 0.8729, 'recall_cv': 0.8116, 'f1_score_cv': 0.8411, 'accuracy_test': 0.866, 'precision_test': 0.6847, 'recall_test': 0.813, 'f1_score_test': 0.7433}}, 'lyrics_bert': {'MLP_10305': {'f1_cv_mean': 0.869, 'f1_cv_std': 0.0026, 'params': 10305, 'f1_score_test': 0.7872}, 'MLP_1028097': {'f1_cv_mean': 0.8806, 'f1_cv_std': 0.0032, 'params': 1028097, 'f1_score_test': 0.8069}, 'Logistic Regression': {'accuracy_cv': 0.8484, 'precision_cv': 0.863, 'recall_cv': 0.8282, 'f1_score_cv': 0.8453, 'accuracy_test': 0.861, 'precision_test': 0.6688, 'recall_test': 0.8269, 'f1_score_test': 0.7395}, 'SVM': {'accuracy_cv': 0.6485, 'precision_cv': 0.6268, 'recall_cv': 0.757, 'f1_score_cv': 0.6827, 'accuracy_test': 0.6661, 'precision_test': 0.3904, 'recall_test': 0.7114, 'f1_score_test': 0.5042}, 'Decision Tree': {'accuracy_cv': 0.7888, 'precision_cv': 0.8157, 'recall_cv': 0.7465, 'f1_score_cv': 0.7795, 'accuracy_test': 0.8249, 'precision_test': 0.6084, 'recall_test': 0.7473, 'f1_score_test': 0.6707}, 'Random Forest': {'accuracy_cv': 0.8509, 'precision_cv': 0.888, 'recall_cv': 0.803, 'f1_score_cv': 0.8434, 'accuracy_test': 0.8771, 'precision_test': 0.7128, 'recall_test': 0.8124, 'f1_score_test': 0.7594}, 'XGBoost': {'accuracy_cv': 0.875, 'precision_cv': 0.8946, 'recall_cv': 0.8501, 'f1_score_cv': 0.8718, 'accuracy_test': 0.8937, 'precision_test': 0.7389, 'recall_test': 0.8574, 'f1_score_test': 0.7938}, 'Naive Bayes': {'accuracy_cv': 0.7743, 'precision_cv': 0.7582, 'recall_cv': 0.8055, 'f1_score_cv': 0.7811, 'accuracy_test': 0.7559, 'precision_test': 0.493, 'recall_test': 0.8107, 'f1_score_test': 0.6131}}, 'gpt': {'MLP_49857': {'f1_cv_mean': 0.8677, 'f1_cv_std': 0.0032, 'params': 49857, 'f1_score_test': 0.7727}, 'MLP_2293761': {'f1_cv_mean':/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [22:33:18] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [22:36:35] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [22:39:52] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [22:43:08] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [22:46:19] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [22:49:34] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
 0.879, 'f1_cv_std': 0.0023, 'params': 2293761, 'f1_score_test': 0.7852}}}
Saved on: outputs_without_artist/0/gpt

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv': 0.8519, 'precision_cv': 0.8563, 'recall_cv': 0.8459, 'f1_score_cv': 0.851}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.86      0.90     16465
           1       0.66      0.85      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.85      0.82     21625
weighted avg       0.88      0.86      0.86     21625

Confusion matrix Test saved as: outputs_without_artist/0/gpt/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_without_artist/0/gpt/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv': 0.7164, 'precision_cv': 0.6849, 'recall_cv': 0.8108, 'f1_score_cv': 0.7411}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.90      0.74      0.81     16465
           1       0.48      0.75      0.58      5160

    accuracy                           0.74     21625
   macro avg       0.69      0.74      0.70     21625
weighted avg       0.80      0.74      0.76     21625

Confusion matrix Test saved as: outputs_without_artist/0/gpt/conf_matrix_test_svm.png
Modelo guardado como: outputs_without_artist/0/gpt/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv': 0.7818, 'precision_cv': 0.7913, 'recall_cv': 0.766, 'f1_score_cv': 0.7783}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.79      0.85     16465
           1       0.54      0.79      0.64      5160

    accuracy                           0.79     21625
   macro avg       0.73      0.79      0.75     21625
weighted avg       0.83      0.79      0.80     21625

Confusion matrix Test saved as: outputs_without_artist/0/gpt/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_without_artist/0/gpt/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv': 0.8219, 'precision_cv': 0.859, 'recall_cv': 0.7703, 'f1_score_cv': 0.8122}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.87      0.90     16465
           1       0.66      0.76      0.71      5160

    accuracy                           0.85     21625
   macro avg       0.79      0.82      0.80     21625
weighted avg       0.86      0.85      0.85     21625

Confusion matrix Test saved as: outputs_without_artist/0/gpt/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_without_artist/0/gpt/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv': 0.8665, 'precision_cv': 0.8848, 'recall_cv': 0.8427, 'f1_score_cv': 0.8632}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.89      0.92     16465
           1       0.71      0.84      0.77      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.87      0.84     21625
weighted avg       0.89      0.88      0.88     21625

Confusion matrix Test saved as: outputs_without_artist/0/gpt/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_without_artist/0/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv': 0.8016, 'precision_cv': 0.8597, 'recall_cv': 0.7208, 'f1_score_cv': 0.7841}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.91      0.88      0.89     16465
           1       0.65      0.72      0.68      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.80      0.79     21625
weighted avg       0.85      0.84      0.84     21625

Confusion matrix Test saved as: outputs_without_artist/0/gpt/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_without_artist/0/gpt/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv': 0.8665, 'precision_cv': 0.8848, 'recall_cv': 0.8427, 'f1_score_cv': 0.8632, 'accuracy_test': 0.8797, 'precision_test': 0.7092, 'recall_test': 0.8403, 'f1_score_test': 0.7692}
Logistic Regression: {'accuracy_cv': 0.8519, 'precision_cv': 0.8563, 'recall_cv': 0.8459, 'f1_score_cv': 0.851, 'accuracy_test': 0.8585, 'precision_test': 0.6586, 'recall_test': 0.8455, 'f1_score_test': 0.7404}
Random Forest: {'accuracy_cv': 0.8219, 'precision_cv': 0.859, 'recall_cv': 0.7703, 'f1_score_cv': 0.8122, 'accuracy_test': 0.8478, 'precision_test': 0.6555, 'recall_test': 0.763, 'f1_score_test': 0.7052}
Naive Bayes: {'accuracy_cv': 0.8016, 'precision_cv': 0.8597, 'recall_cv': 0.7208, 'f1_score_cv': 0.7841, 'accuracy_test': 0.8423, 'precision_test': 0.6549, 'recall_test': 0.7169, 'f1_score_test': 0.6845}
Decision Tree: {'accuracy_cv': 0.7818, 'precision_cv': 0.7913, 'recall_cv': 0.766, 'f1_score_cv': 0.7783, 'accuracy_test': 0.7887, 'precision_test': 0.5392, 'recall_test': 0.7862, 'f1_score_test': 0.6397}
SVM: {'accuracy_cv': 0.7164, 'precision_cv': 0.6849, 'recall_cv': 0.8108, 'f1_score_cv': 0.7411, 'accuracy_test': 0.7426, 'precision_test': 0.4751, 'recall_test': 0.7492, 'f1_score_test': 0.5814}
resultados_globales con ML {'tfidf': {'MLP_160705': {'f1_cv_mean': 0.8873, 'f1_cv_std': 0.0032, 'params': 160705, 'f1_score_test': 0.8252}, 'MLP_5840897': {'f1_cv_mean': 0.8957, 'f1_cv_std': 0.0016, 'params': 5840897, 'f1_score_test': 0.8349}, 'Logistic Regression': {'accuracy_cv': 0.8786, 'precision_cv': 0.8929, 'recall_cv': 0.8605, 'f1_score_cv': 0.8764, 'accuracy_test': 0.8908, 'precision_test': 0.7281, 'recall_test': 0.8657, 'f1_score_test': 0.791}, 'SVM': {'accuracy_cv': 0.734, 'precision_cv': 0.6924, 'recall_cv': 0.85, 'f1_score_cv': 0.7618, 'accuracy_test': 0.7332, 'precision_test': 0.4652, 'recall_test': 0.7882, 'f1_score_test': 0.5851}, 'Decision Tree': {'accuracy_cv': 0.8254, 'precision_cv': 0.883, 'recall_cv': 0.7506, 'f1_score_cv': 0.8113, 'accuracy_test': 0.8732, 'precision_test': 0.7232, 'recall_test': 0.7589, 'f1_score_test': 0.7406}, 'Random Forest': {'accuracy_cv': 0.8204, 'precision_cv': 0.8566, 'recall_cv': 0.7698, 'f1_score_cv': 0.8108, 'accuracy_test': 0.8491, 'precision_test': 0.6573, 'recall_test': 0.7678, 'f1_score_test': 0.7083}, 'XGBoost': {'accuracy_cv': 0.8785, 'precision_cv': 0.9036, 'recall_cv': 0.8474, 'f1_score_cv': 0.8746, 'accuracy_test': 0.8962, 'precision_test': 0.7486, 'recall_test': 0.8506, 'f1_score_test': 0.7963}, 'Naive Bayes': {'accuracy_cv': 0.8467, 'precision_cv': 0.8729, 'recall_cv': 0.8116, 'f1_score_cv': 0.8411, 'accuracy_test': 0.866, 'precision_test': 0.6847, 'recall_test': 0.813, 'f1_score_test': 0.7433}}, 'lyrics_bert': {'MLP_10305': {'f1_cv_mean': 0.869, 'f1_cv_std': 0.0026, 'params': 10305, 'f1_score_test': 0.7872}, 'MLP_1028097': {'f1_cv_mean': 0.8806, 'f1_cv_std': 0.0032, 'params': 1028097, 'f1_score_test': 0.8069}, 'Logistic Regression': {'accuracy_cv': 0.8484, 'precision_cv': 0.863, 'recall_cv': 0.8282, 'f1_score_cv': 0.8453, 'accuracy_test': 0.861, 'precision_test': 0.6688, 'recall_test': 0.8269, 'f1_score_test': 0.7395}, 'SVM': {'accuracy_cv': 0.6485, 'precision_cv': 0.6268, 'recall_cv': 0.757, 'f1_score_cv': 0.6827, 'accuracy_test': 0.6661, 'precision_test': 0.3904, 'recall_test': 0.7114, 'f1_score_test': 0.5042}, 'Decision Tree': {'accuracy_cv': 0.7888, 'precision_cv': 0.8157, 'recall_cv': 0.7465, 'f1_score_cv': 0.7795, 'accuracy_test': 0.8249, 'precision_test': 0.6084, 'recall_test': 0.7473, 'f1_score_test': 0.6707}, 'Random Forest': {'accuracy_cv': 0.8509, 'precision_cv': 0.888, 'recall_cv': 0.803, 'f1_score_cv': 0.8434, 'accuracy_test': 0.8771, 'precision_test': 0.7128, 'recall_test': 0.8124, 'f1_score_test': 0.7594}, 'XGBoost': {'accuracy_cv': 0.875, 'precision_cv': 0.8946, 'recall_cv': 0.8501, 'f1_score_cv': 0.8718, 'accuracy_test': 0.8937, 'precision_test': 0.7389, 'recall_test': 0.8574, 'f1_score_test': 0.7938}, 'Naive Bayes': {'accuracy_cv': 0.7743, 'precision_cv': 0.7582, 'recall_cv': 0.8055, 'f1_score_cv': 0.7811, 'accuracy_test': 0.7559, 'precision_test': 0.493, 'recall_test': 0.8107, 'f1_score_test': 0.6131}}, 'gpt': {'MLP_49857': {'f1_cv_mean': 0.8677, 'f1_cv_std': 0.0032, 'params': 49857, 'f1_score_test': 0.7727}, 'MLP_2293761': {'f1_cv_mean': 0.879, 'f1_cv_std': 0.0023, 'params': 2293761, 'f1_score_test': 0.7852}, 'Logistic Regression': {'accuracy_cv': 0.8519, 'precision_cv': 0.8563, 'recall_cv': 0.8459, 'f1_score_cv': 0.851, 'accuracy_test': 0.8585, 'precision_test': 0.6586, 'recall_test': 0.8455, 'f1_score_test': 0.7404}, 'SVM': {'accuracy_cv': 0.7164, 'precision_cv': 0.6849, 'recall_cv': 0.8108, 'f1_score_cv': 0.7411, 'accuracy_test': 0.7426, 'precision_test': 0.4751, 'recall_test': 0.7492, 'f1_score_test': 0.5814}, 'Decision Tree': {'accuracy_cv': 0.7818, 'precision_cv': 0.7913, 'recall_cv': 0.766, 'f1_score_cv': 0.7783, 'accuracy_test': 0.7887, 'precision_test': 0.5392, 'recall_test': 0.7862, 'f1_score_test': 0.6397}, 'Random Forest': {'accuracy_cv': 0.8219, 'precision_cv': 0.859, 'recall_cv': 0.7703, 'f1_score_cv': 0.8122, 'accuracy_test': 0.8478, 'precision_test': 0.6555, 'recall_test': 0.763, 'f1_score_test': 0.7052}, 'XGBoost': {'accuracy_cv': 0.8665, 'precision_cv': 0.8848, 'recall_cv': 0.8427, 'f1_score_cv': 0.8632, 'accuracy_test': 0.8797, 'precision_test': 0.7092, 'recall_test': 0.8403, 'f1_score_test': 0.7692}, 'Naive Bayes': {'accuracy_cv': 0.8016, 'precision_cv': 0.8597, 'recall_cv': 0.7208, 'f1_score_cv': 0.7841, 'accuracy_test': 0.8423, 'precision_test': 0.6549, 'recall_test': 0.7169, 'f1_score_test': 0.6845}}}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
MLP_5840897: {'f1_cv_mean': 0.8957, 'f1_cv_std': 0.0016, 'params': 5840897, 'f1_score_test': 0.8349}
MLP_160705: {'f1_cv_mean': 0.8873, 'f1_cv_std': 0.0032, 'params': 160705, 'f1_score_test': 0.8252}
XGBoost: {'accuracy_cv': 0.8785, 'precision_cv': 0.9036, 'recall_cv': 0.8474, 'f1_score_cv': 0.8746, 'accuracy_test': 0.8962, 'precision_test': 0.7486, 'recall_test': 0.8506, 'f1_score_test': 0.7963}
Logistic Regression: {'accuracy_cv': 0.8786, 'precision_cv': 0.8929, 'recall_cv': 0.8605, 'f1_score_cv': 0.8764, 'accuracy_test': 0.8908, 'precision_test': 0.7281, 'recall_test': 0.8657, 'f1_score_test': 0.791}
Naive Bayes: {'accuracy_cv': 0.8467, 'precision_cv': 0.8729, 'recall_cv': 0.8116, 'f1_score_cv': 0.8411, 'accuracy_test': 0.866, 'precision_test': 0.6847, 'recall_test': 0.813, 'f1_score_test': 0.7433}
Decision Tree: {'accuracy_cv': 0.8254, 'precision_cv': 0.883, 'recall_cv': 0.7506, 'f1_score_cv': 0.8113, 'accuracy_test': 0.8732, 'precision_test': 0.7232, 'recall_test': 0.7589, 'f1_score_test': 0.7406}
Random Forest: {'accuracy_cv': 0.8204, 'precision_cv': 0.8566, 'recall_cv': 0.7698, 'f1_score_cv': 0.8108, 'accuracy_test': 0.8491, 'precision_test': 0.6573, 'recall_test': 0.7678, 'f1_score_test': 0.7083}
SVM: {'accuracy_cv': 0.734, 'precision_cv': 0.6924, 'recall_cv': 0.85, 'f1_score_cv': 0.7618, 'accuracy_test': 0.7332, 'precision_test': 0.4652, 'recall_test': 0.7882, 'f1_score_test': 0.5851}


EMBEDDINGS TYPE: LYRICS_BERT
MLP_1028097: {'f1_cv_mean': 0.8806, 'f1_cv_std': 0.0032, 'params': 1028097, 'f1_score_test': 0.8069}
XGBoost: {'accuracy_cv': 0.875, 'precision_cv': 0.8946, 'recall_cv': 0.8501, 'f1_score_cv': 0.8718, 'accuracy_test': 0.8937, 'precision_test': 0.7389, 'recall_test': 0.8574, 'f1_score_test': 0.7938}
MLP_10305: {'f1_cv_mean': 0.869, 'f1_cv_std': 0.0026, 'params': 10305, 'f1_score_test': 0.7872}
Random Forest: {'accuracy_cv': 0.8509, 'precision_cv': 0.888, 'recall_cv': 0.803, 'f1_score_cv': 0.8434, 'accuracy_test': 0.8771, 'precision_test': 0.7128, 'recall_test': 0.8124, 'f1_score_test': 0.7594}
Logistic Regression: {'accuracy_cv': 0.8484, 'precision_cv': 0.863, 'recall_cv': 0.8282, 'f1_score_cv': 0.8453, 'accuracy_test': 0.861, 'precision_test': 0.6688, 'recall_test': 0.8269, 'f1_score_test': 0.7395}
Decision Tree: {'accuracy_cv': 0.7888, 'precision_cv': 0.8157, 'recall_cv': 0.7465, 'f1_score_cv': 0.7795, 'accuracy_test': 0.8249, 'precision_test': 0.6084, 'recall_test': 0.7473, 'f1_score_test': 0.6707}
Naive Bayes: {'accuracy_cv': 0.7743, 'precision_cv': 0.7582, 'recall_cv': 0.8055, 'f1_score_cv': 0.7811, 'accuracy_test': 0.7559, 'precision_test': 0.493, 'recall_test': 0.8107, 'f1_score_test': 0.6131}
SVM: {'accuracy_cv': 0.6485, 'precision_cv': 0.6268, 'recall_cv': 0.757, 'f1_score_cv': 0.6827, 'accuracy_test': 0.6661, 'precision_test': 0.3904, 'recall_test': 0.7114, 'f1_score_test': 0.5042}


EMBEDDINGS TYPE: GPT
MLP_2293761: {'f1_cv_mean': 0.879, 'f1_cv_std': 0.0023, 'params': 2293761, 'f1_score_test': 0.7852}
MLP_49857: {'f1_cv_mean': 0.8677, 'f1_cv_std': 0.0032, 'params': 49857, 'f1_score_test': 0.7727}
XGBoost: {'accuracy_cv': 0.8665, 'precision_cv': 0.8848, 'recall_cv': 0.8427, 'f1_score_cv': 0.8632, 'accuracy_test': 0.8797, 'precision_test': 0.7092, 'recall_test': 0.8403, 'f1_score_test': 0.7692}
Logistic Regression: {'accuracy_cv': 0.8519, 'precision_cv': 0.8563, 'recall_cv': 0.8459, 'f1_score_cv': 0.851, 'accuracy_test': 0.8585, 'precision_test': 0.6586, 'recall_test': 0.8455, 'f1_score_test': 0.7404}
Random Forest: {'accuracy_cv': 0.8219, 'precision_cv': 0.859, 'recall_cv': 0.7703, 'f1_score_cv': 0.8122, 'accuracy_test': 0.8478, 'precision_test': 0.6555, 'recall_test': 0.763, 'f1_score_test': 0.7052}
Naive Bayes: {'accuracy_cv': 0.8016, 'precision_cv': 0.8597, 'recall_cv': 0.7208, 'f1_score_cv': 0.7841, 'accuracy_test': 0.8423, 'precision_test': 0.6549, 'recall_test': 0.7169, 'f1_score_test': 0.6845}
Decision Tree: {'accuracy_cv': 0.7818, 'precision_cv': 0.7913, 'recall_cv': 0.766, 'f1_score_cv': 0.7783, 'accuracy_test': 0.7887, 'precision_test': 0.5392, 'recall_test': 0.7862, 'f1_score_test': 0.6397}
SVM: {'accuracy_cv': 0.7164, 'precision_cv': 0.6849, 'recall_cv': 0.8108, 'f1_score_cv': 0.7411, 'accuracy_test': 0.7426, 'precision_test': 0.4751, 'recall_test': 0.7492, 'f1_score_test': 0.5814}
Diccionario global guardado en: outputs_without_artist/0/gpt/global_metrics.json

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['emotion', 'Time signature', 'song', 'Genre', 'Album', 'Release Date', 'Key', 'Similar Song 1', 'Similar Song 2', 'Similar Song 3', 'song_normalized', 'artist_normalized']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
====================================

