2025-09-19 10:57:47.768461: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-09-19 10:57:47.833125: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-09-19 10:57:50.458294: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_without_artist_8.py:288: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
For TF-IDF embbedings you are selecteing this columns:
--> ['emotion', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Release Date', 'Key', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
After removing some columns that doesn't love me, for TF-IDF embbedings you are selecteing this columns:
--> ['emotion', 'Time signature', 'song', 'Genre', 'Album', 'Release Date', 'Key', 'Similar Song 1', 'Similar Song 2', 'Similar Song 3', 'song_normalized', 'artist_normalized']
After removing some columns that doesn't love me, for numeric cols you are selecteing this columns:
--> ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
--> PaTH:  ../../data/embbedings_khipu/LB_fuss/lb_khipu_B.npy

##################################################
Running experiment with TFIDF embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [5020, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4718, Test Loss: 0.4256, F1: 0.8253, AUC: 0.8946
Epoch [10/30] Train Loss: 0.2292, Test Loss: 0.2854, F1: 0.8696, AUC: 0.9537
Epoch [20/30] Train Loss: 0.1954, Test Loss: 0.2817, F1: 0.8797, AUC: 0.9551
Mejores resultados en la época:  13
f1-score 0.885159434188444
AUC según el mejor F1-score 0.9554577232531849

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4637, Test Loss: 0.4122, F1: 0.8027, AUC: 0.8951
Epoch [10/30] Train Loss: 0.2273, Test Loss: 0.2949, F1: 0.8796, AUC: 0.9520
Epoch [20/30] Train Loss: 0.1754, Test Loss: 0.3027, F1: 0.8820, AUC: 0.9545
Mejores resultados en la época:  23
f1-score 0.8833879383569955
AUC según el mejor F1-score 0.9548786283219909

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4656, Test Loss: 0.4183, F1: 0.8095, AUC: 0.8920
Epoch [10/30] Train Loss: 0.2299, Test Loss: 0.4005, F1: 0.8170, AUC: 0.9522
Epoch [20/30] Train Loss: 0.1852, Test Loss: 0.2964, F1: 0.8823, AUC: 0.9556
Mejores resultados en la época:  26
f1-score 0.8866571018651362
AUC según el mejor F1-score 0.9571340645844602

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4648, Test Loss: 0.4153, F1: 0.8241, AUC: 0.8984
Epoch [10/30] Train Loss: 0.2276, Test Loss: 0.2845, F1: 0.8828, AUC: 0.9565
Epoch [20/30] Train Loss: 0.1845, Test Loss: 0.2777, F1: 0.8918, AUC: 0.9577
Mejores resultados en la época:  28
f1-score 0.893606813002279
AUC según el mejor F1-score 0.9587392323759399

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4574, Test Loss: 0.3994, F1: 0.8350, AUC: 0.9089
Epoch [10/30] Train Loss: 0.2235, Test Loss: 0.3324, F1: 0.8485, AUC: 0.9533
Epoch [20/30] Train Loss: 0.1840, Test Loss: 0.2836, F1: 0.8830, AUC: 0.9558
Mejores resultados en la época:  20
f1-score 0.8829915560916767
AUC según el mejor F1-score 0.955771884385865
Epoch [0/30] Train Loss: 0.4498, Test Loss: 0.3431, F1: 0.7359, AUC: 0.9092
Epoch [10/30] Train Loss: 0.2272, Test Loss: 0.2846, F1: 0.7862, AUC: 0.9567
Epoch [20/30] Train Loss: 0.1846, Test Loss: 0.2436, F1: 0.8060, AUC: 0.9586
Mejores resultados en la época:  27
f1-score 0.8233225900966892
AUC según el mejor F1-score 0.9616235107592568
Confusion matrix Test saved: outputs_without_artist/8/tfidf/cm_mlp_1.png

========================================
Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4592, Test Loss: 0.3915, F1: 0.8034, AUC: 0.9089
Epoch [10/30] Train Loss: 0.2159, Test Loss: 0.2741, F1: 0.8817, AUC: 0.9573
Epoch [20/30] Train Loss: 0.1359, Test Loss: 0.2711, F1: 0.8927, AUC: 0.9605
Mejores resultados en la época:  24
f1-score 0.8979841172877214
AUC según el mejor F1-score 0.963542282849964

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4611, Test Loss: 0.3814, F1: 0.8227, AUC: 0.9138
Epoch [10/30] Train Loss: 0.2121, Test Loss: 0.2767, F1: 0.8864, AUC: 0.9557
Epoch [20/30] Train Loss: 0.1426, Test Loss: 0.3830, F1: 0.8598, AUC: 0.9544
Mejores resultados en la época:  28
f1-score 0.8934876651467322
AUC según el mejor F1-score 0.9590218448128869

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4564, Test Loss: 0.5319, F1: 0.6120, AUC: 0.9005
Epoch [10/30] Train Loss: 0.2089, Test Loss: 0.2988, F1: 0.8821, AUC: 0.9559
Epoch [20/30] Train Loss: 0.1423, Test Loss: 0.3793, F1: 0.8754, AUC: 0.9596
Mejores resultados en la época:  22
f1-score 0.8936271590232281
AUC según el mejor F1-score 0.9607902908760743

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4608, Test Loss: 0.3847, F1: 0.8136, AUC: 0.9103
Epoch [10/30] Train Loss: 0.2098, Test Loss: 0.2690, F1: 0.8892, AUC: 0.9571
Epoch [20/30] Train Loss: 0.1478, Test Loss: 0.2605, F1: 0.9003, AUC: 0.9628
Mejores resultados en la época:  20
f1-score 0.9002961500493584
AUC según el mejor F1-score 0.9627574861518869

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4686, Test Loss: 0.4830, F1: 0.6919, AUC: 0.9061
Epoch [10/30] Train Loss: 0.2106, Test Loss: 0.3002, F1: 0.8794, AUC: 0.9551
Epoch [20/30] Train Loss: 0.1431, Test Loss: 0.2766, F1: 0.8857, AUC: 0.9575
Mejores resultados en la época:  29
f1-score 0.8915867427461455
AUC según el mejor F1-score 0.9597053484051894
Epoch [0/30] Train Loss: 0.4439, Test Loss: 0.3257, F1: 0.7441, AUC: 0.9165
Epoch [10/30] Train Loss: 0.2152, Test Loss: 0.2348, F1: 0.8214, AUC: 0.9617
Epoch [20/30] Train Loss: 0.1420, Test Loss: 0.2770, F1: 0.7786, AUC: 0.9541
Mejores resultados en la época:  24
f1-score 0.8243684992570579
AUC según el mejor F1-score 0.9622744922869041
Confusion matrix Test saved: outputs_without_artist/8/tfidf/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160705': {'accuracy_cv_mean': 0.8856, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.8801, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8927, 'recall_cv_std': 0.0076, 'f1_cv_mean': 0.8864, 'f1_cv_std': 0.0039, 'params': 160705, 'accuracy_test': 0.9163, 'precision_test': 0.8299, 'recall_test': 0.8169, 'f1_score_test': 0.8233}, 'MLP_5840897': {'accuracy_cv_mean': 0.8949, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8919, 'precision_cv_std': 0.019, 'recall_cv_mean': 0.8996, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.8954, 'f1_cv_std': 0.0032, 'params': 5840897, 'accuracy_test': 0.918, 'precision_test': 0.8432, 'recall_test': 0.8064, 'f1_score_test': 0.8244}}}
Saved on: outputs_without_artist/8/tfidf

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8795, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8921, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.8636, 'recall_cv_std': 0.0063, 'f1_cv_mean': 0.8776, 'f1_cv_std': 0.0038}
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [11:42:18] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [11:45:04] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [11:47:50] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [11:50:36] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [11:53:21] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [11:56:16] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 50, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.90      0.92     16465
           1       0.72      0.86      0.79      5160

    accuracy                           0.89     21625
   macro avg       0.84      0.88      0.86     21625
weighted avg       0.90      0.89      0.89     21625

Confusion matrix Test saved as: outputs_without_artist/8/tfidf/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_without_artist/8/tfidf/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7551, 'accuracy_cv_std': 0.0122, 'precision_cv_mean': 0.7257, 'precision_cv_std': 0.0274, 'recall_cv_mean': 0.8261, 'recall_cv_std': 0.045, 'f1_cv_mean': 0.7711, 'f1_cv_std': 0.0109}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 50, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.67      0.77     16465
           1       0.43      0.81      0.56      5160

    accuracy                           0.70     21625
   macro avg       0.68      0.74      0.67     21625
weighted avg       0.80      0.70      0.72     21625

Confusion matrix Test saved as: outputs_without_artist/8/tfidf/conf_matrix_test_svm.png
Modelo guardado como: outputs_without_artist/8/tfidf/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8261, 'accuracy_cv_std': 0.0037, 'precision_cv_mean': 0.8795, 'precision_cv_std': 0.009, 'recall_cv_mean': 0.756, 'recall_cv_std': 0.0176, 'f1_cv_mean': 0.8129, 'f1_cv_std': 0.0067}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 50, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.75      0.73      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.82     21625
weighted avg       0.87      0.87      0.87     21625

Confusion matrix Test saved as: outputs_without_artist/8/tfidf/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_without_artist/8/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8154, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.8485, 'precision_cv_std': 0.0065, 'recall_cv_mean': 0.7681, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8063, 'f1_cv_std': 0.0046}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 50, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.86      0.89     16465
           1       0.64      0.77      0.70      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.82      0.80     21625
weighted avg       0.86      0.84      0.85     21625

Confusion matrix Test saved as: outputs_without_artist/8/tfidf/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_without_artist/8/tfidf/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8779, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.902, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.8481, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0044}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 50, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.92      0.93     16465
           1       0.76      0.84      0.80      5160

    accuracy                           0.90     21625
   macro avg       0.85      0.88      0.87     21625
weighted avg       0.90      0.90      0.90     21625

Confusion matrix Test saved as: outputs_without_artist/8/tfidf/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_without_artist/8/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8455, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.8688, 'precision_cv_std': 0.0018, 'recall_cv_mean': 0.814, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.8405, 'f1_cv_std': 0.0025}
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.88      0.91     16465
           1       0.68      0.81      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

Confusion matrix Test saved as: outputs_without_artist/8/tfidf/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_without_artist/8/tfidf/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.8779, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.902, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.8481, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0044, 'accuracy_test': 0.8983, 'precision_test': 0.7572, 'recall_test': 0.8444, 'f1_score_test': 0.7984}
Logistic Regression: {'accuracy_cv_mean': 0.8795, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8921, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.8636, 'recall_cv_std': 0.0063, 'f1_cv_mean': 0.8776, 'f1_cv_std': 0.0038, 'accuracy_test': 0.8892, 'precision_test': 0.7248, 'recall_test': 0.8636, 'f1_score_test': 0.7881}
Naive Bayes: {'accuracy_cv_mean': 0.8455, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.8688, 'precision_cv_std': 0.0018, 'recall_cv_mean': 0.814, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.8405, 'f1_cv_std': 0.0025, 'accuracy_test': 0.8632, 'precision_test': 0.6798, 'recall_test': 0.8064, 'f1_score_test': 0.7377}
Decision Tree: {'accuracy_cv_mean': 0.8261, 'accuracy_cv_std': 0.0037, 'precision_cv_mean': 0.8795, 'precision_cv_std': 0.009, 'recall_cv_mean': 0.756, 'recall_cv_std': 0.0176, 'f1_cv_mean': 0.8129, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8713, 'precision_test': 0.7226, 'recall_test': 0.7473, 'f1_score_test': 0.7348}
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_without_artist_8.py:288: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Random Forest: {'accuracy_cv_mean': 0.8154, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.8485, 'precision_cv_std': 0.0065, 'recall_cv_mean': 0.7681, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8063, 'f1_cv_std': 0.0046, 'accuracy_test': 0.8425, 'precision_test': 0.6409, 'recall_test': 0.7736, 'f1_score_test': 0.701}
SVM: {'accuracy_cv_mean': 0.7551, 'accuracy_cv_std': 0.0122, 'precision_cv_mean': 0.7257, 'precision_cv_std': 0.0274, 'recall_cv_mean': 0.8261, 'recall_cv_std': 0.045, 'f1_cv_mean': 0.7711, 'f1_cv_std': 0.0109, 'accuracy_test': 0.7026, 'precision_test': 0.4336, 'recall_test': 0.805, 'f1_score_test': 0.5636}
resultados_globales con ML {'tfidf': {'MLP_160705': {'accuracy_cv_mean': 0.8856, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.8801, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8927, 'recall_cv_std': 0.0076, 'f1_cv_mean': 0.8864, 'f1_cv_std': 0.0039, 'params': 160705, 'accuracy_test': 0.9163, 'precision_test': 0.8299, 'recall_test': 0.8169, 'f1_score_test': 0.8233}, 'MLP_5840897': {'accuracy_cv_mean': 0.8949, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8919, 'precision_cv_std': 0.019, 'recall_cv_mean': 0.8996, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.8954, 'f1_cv_std': 0.0032, 'params': 5840897, 'accuracy_test': 0.918, 'precision_test': 0.8432, 'recall_test': 0.8064, 'f1_score_test': 0.8244}, 'Logistic Regression': {'accuracy_cv_mean': 0.8795, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8921, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.8636, 'recall_cv_std': 0.0063, 'f1_cv_mean': 0.8776, 'f1_cv_std': 0.0038, 'accuracy_test': 0.8892, 'precision_test': 0.7248, 'recall_test': 0.8636, 'f1_score_test': 0.7881}, 'SVM': {'accuracy_cv_mean': 0.7551, 'accuracy_cv_std': 0.0122, 'precision_cv_mean': 0.7257, 'precision_cv_std': 0.0274, 'recall_cv_mean': 0.8261, 'recall_cv_std': 0.045, 'f1_cv_mean': 0.7711, 'f1_cv_std': 0.0109, 'accuracy_test': 0.7026, 'precision_test': 0.4336, 'recall_test': 0.805, 'f1_score_test': 0.5636}, 'Decision Tree': {'accuracy_cv_mean': 0.8261, 'accuracy_cv_std': 0.0037, 'precision_cv_mean': 0.8795, 'precision_cv_std': 0.009, 'recall_cv_mean': 0.756, 'recall_cv_std': 0.0176, 'f1_cv_mean': 0.8129, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8713, 'precision_test': 0.7226, 'recall_test': 0.7473, 'f1_score_test': 0.7348}, 'Random Forest': {'accuracy_cv_mean': 0.8154, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.8485, 'precision_cv_std': 0.0065, 'recall_cv_mean': 0.7681, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8063, 'f1_cv_std': 0.0046, 'accuracy_test': 0.8425, 'precision_test': 0.6409, 'recall_test': 0.7736, 'f1_score_test': 0.701}, 'XGBoost': {'accuracy_cv_mean': 0.8779, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.902, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.8481, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0044, 'accuracy_test': 0.8983, 'precision_test': 0.7572, 'recall_test': 0.8444, 'f1_score_test': 0.7984}, 'Naive Bayes': {'accuracy_cv_mean': 0.8455, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.8688, 'precision_cv_std': 0.0018, 'recall_cv_mean': 0.814, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.8405, 'f1_cv_std': 0.0025, 'accuracy_test': 0.8632, 'precision_test': 0.6798, 'recall_test': 0.8064, 'f1_score_test': 0.7377}}}

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 320)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 320)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [320, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4517, Test Loss: 0.3989, F1: 0.8289, AUC: 0.9029
Epoch [10/30] Train Loss: 0.3200, Test Loss: 0.3427, F1: 0.8392, AUC: 0.9364
Epoch [20/30] Train Loss: 0.2931, Test Loss: 0.3244, F1: 0.8542, AUC: 0.9395
Mejores resultados en la época:  26
f1-score 0.8707994662137571
AUC según el mejor F1-score 0.9435937593894599

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.5073, Test Loss: 0.4105, F1: 0.8228, AUC: 0.8947
Epoch [10/30] Train Loss: 0.3364, Test Loss: 0.3751, F1: 0.8429, AUC: 0.9223
Epoch [20/30] Train Loss: 0.3098, Test Loss: 0.3404, F1: 0.8541, AUC: 0.9303
Mejores resultados en la época:  28
f1-score 0.8598175708113298
AUC según el mejor F1-score 0.9350632297957605

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4704, Test Loss: 0.4051, F1: 0.8302, AUC: 0.9023
Epoch [10/30] Train Loss: 0.3262, Test Loss: 0.3447, F1: 0.8539, AUC: 0.9282
Epoch [20/30] Train Loss: 0.3013, Test Loss: 0.3143, F1: 0.8666, AUC: 0.9393
Mejores resultados en la época:  27
f1-score 0.8692826191622532
AUC según el mejor F1-score 0.9402949487523286

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4470, Test Loss: 0.4019, F1: 0.8133, AUC: 0.8995
Epoch [10/30] Train Loss: 0.3257, Test Loss: 0.3385, F1: 0.8480, AUC: 0.9325
Epoch [20/30] Train Loss: 0.3033, Test Loss: 0.3203, F1: 0.8632, AUC: 0.9372
Mejores resultados en la época:  29
f1-score 0.8690591658583899
AUC según el mejor F1-score 0.9423229493616437

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4456, Test Loss: 0.4335, F1: 0.7711, AUC: 0.9068
Epoch [10/30] Train Loss: 0.3229, Test Loss: 0.3315, F1: 0.8451, AUC: 0.9356
Epoch [20/30] Train Loss: 0.2959, Test Loss: 0.3140, F1: 0.8600, AUC: 0.9416
Mejores resultados en la época:  29
f1-score 0.867929079725003
AUC según el mejor F1-score 0.9433369632388714
Epoch [0/30] Train Loss: 0.4281, Test Loss: 0.3440, F1: 0.7333, AUC: 0.9085
Epoch [10/30] Train Loss: 0.3176, Test Loss: 0.2884, F1: 0.7724, AUC: 0.9375
Epoch [20/30] Train Loss: 0.2899, Test Loss: 0.4598, F1: 0.6822, AUC: 0.9421
Mejores resultados en la época:  26
f1-score 0.7891382654474923
AUC según el mejor F1-score 0.9455421942716169
Confusion matrix Test saved: outputs_without_artist/8/lyrics_bert/cm_mlp_1.png

========================================
Entrenando red 6 con capas [320, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4367, Test Loss: 0.3783, F1: 0.8304, AUC: 0.9123
Epoch [10/30] Train Loss: 0.2956, Test Loss: 0.3120, F1: 0.8587, AUC: 0.9416
Epoch [20/30] Train Loss: 0.2409, Test Loss: 0.2920, F1: 0.8765, AUC: 0.9496
Mejores resultados en la época:  25
f1-score 0.8818897637795275
AUC según el mejor F1-score 0.9518417836123879

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4451, Test Loss: 0.3986, F1: 0.7983, AUC: 0.9043
Epoch [10/30] Train Loss: 0.2905, Test Loss: 0.3293, F1: 0.8631, AUC: 0.9399
Epoch [20/30] Train Loss: 0.2416, Test Loss: 0.3542, F1: 0.8625, AUC: 0.9413
Mejores resultados en la época:  29
f1-score 0.8775461298825785
AUC según el mejor F1-score 0.9504089813938464

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4380, Test Loss: 0.4263, F1: 0.8229, AUC: 0.9074
Epoch [10/30] Train Loss: 0.2917, Test Loss: 0.3099, F1: 0.8685, AUC: 0.9424
Epoch [20/30] Train Loss: 0.2400, Test Loss: 0.3234, F1: 0.8773, AUC: 0.9488
Mejores resultados en la época:  26
f1-score 0.8816844120110061
AUC según el mejor F1-score 0.9501809055476684

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4504, Test Loss: 0.4248, F1: 0.8158, AUC: 0.9065
Epoch [10/30] Train Loss: 0.2933, Test Loss: 0.3138, F1: 0.8597, AUC: 0.9394
Epoch [20/30] Train Loss: 0.2360, Test Loss: 0.3231, F1: 0.8715, AUC: 0.9466
Mejores resultados en la época:  26
f1-score 0.8820853313660396
AUC según el mejor F1-score 0.9512565730404616

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4417, Test Loss: 0.3844, F1: 0.8012, AUC: 0.9113
Epoch [10/30] Train Loss: 0.2938, Test Loss: 0.3156, F1: 0.8688, AUC: 0.9429
Epoch [20/30] Train Loss: 0.2441, Test Loss: 0.2922, F1: 0.8719, AUC: 0.9504
Mejores resultados en la época:  25
f1-score 0.877893447642376
AUC según el mejor F1-score 0.9492040093785864
Epoch [0/30] Train Loss: 0.4381, Test Loss: 0.3570, F1: 0.7128, AUC: 0.9148
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Epoch [10/30] Train Loss: 0.2838, Test Loss: 0.2720, F1: 0.7836, AUC: 0.9460
Epoch [20/30] Train Loss: 0.2342, Test Loss: 0.2883, F1: 0.7835, AUC: 0.9429
Mejores resultados en la época:  28
f1-score 0.8105083459787557
AUC según el mejor F1-score 0.9528588537583833
Confusion matrix Test saved: outputs_without_artist/8/lyrics_bert/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160705': {'accuracy_cv_mean': 0.8856, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.8801, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8927, 'recall_cv_std': 0.0076, 'f1_cv_mean': 0.8864, 'f1_cv_std': 0.0039, 'params': 160705, 'accuracy_test': 0.9163, 'precision_test': 0.8299, 'recall_test': 0.8169, 'f1_score_test': 0.8233}, 'MLP_5840897': {'accuracy_cv_mean': 0.8949, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8919, 'precision_cv_std': 0.019, 'recall_cv_mean': 0.8996, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.8954, 'f1_cv_std': 0.0032, 'params': 5840897, 'accuracy_test': 0.918, 'precision_test': 0.8432, 'recall_test': 0.8064, 'f1_score_test': 0.8244}, 'Logistic Regression': {'accuracy_cv_mean': 0.8795, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8921, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.8636, 'recall_cv_std': 0.0063, 'f1_cv_mean': 0.8776, 'f1_cv_std': 0.0038, 'accuracy_test': 0.8892, 'precision_test': 0.7248, 'recall_test': 0.8636, 'f1_score_test': 0.7881}, 'SVM': {'accuracy_cv_mean': 0.7551, 'accuracy_cv_std': 0.0122, 'precision_cv_mean': 0.7257, 'precision_cv_std': 0.0274, 'recall_cv_mean': 0.8261, 'recall_cv_std': 0.045, 'f1_cv_mean': 0.7711, 'f1_cv_std': 0.0109, 'accuracy_test': 0.7026, 'precision_test': 0.4336, 'recall_test': 0.805, 'f1_score_test': 0.5636}, 'Decision Tree': {'accuracy_cv_mean': 0.8261, 'accuracy_cv_std': 0.0037, 'precision_cv_mean': 0.8795, 'precision_cv_std': 0.009, 'recall_cv_mean': 0.756, 'recall_cv_std': 0.0176, 'f1_cv_mean': 0.8129, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8713, 'precision_test': 0.7226, 'recall_test': 0.7473, 'f1_score_test': 0.7348}, 'Random Forest': {'accuracy_cv_mean': 0.8154, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.8485, 'precision_cv_std': 0.0065, 'recall_cv_mean': 0.7681, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8063, 'f1_cv_std': 0.0046, 'accuracy_test': 0.8425, 'precision_test': 0.6409, 'recall_test': 0.7736, 'f1_score_test': 0.701}, 'XGBoost': {'accuracy_cv_mean': 0.8779, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.902, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.8481, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0044, 'accuracy_test': 0.8983, 'precision_test': 0.7572, 'recall_test': 0.8444, 'f1_score_test': 0.7984}, 'Naive Bayes': {'accuracy_cv_mean': 0.8455, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.8688, 'precision_cv_std': 0.0018, 'recall_cv_mean': 0.814, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.8405, 'f1_cv_std': 0.0025, 'accuracy_test': 0.8632, 'precision_test': 0.6798, 'recall_test': 0.8064, 'f1_score_test': 0.7377}}, 'lyrics_bert': {'MLP_10305': {'accuracy_cv_mean': 0.8669, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8644, 'precision_cv_std': 0.0069, 'recall_cv_mean': 0.8704, 'recall_cv_std': 0.0025, 'f1_cv_mean': 0.8674, 'f1_cv_std': 0.0039, 'params': 10305, 'accuracy_test': 0.8944, 'precision_test': 0.7538, 'recall_test': 0.8279, 'f1_score_test': 0.7891}, 'MLP_1028097': {'accuracy_cv_mean': 0.8799, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8782, 'precision_cv_std': 0.0112, 'recall_cv_mean': 0.8825, 'recall_cv_std': 0.0116, 'f1_cv_mean': 0.8802, 'f1_cv_std': 0.002, 'params': 1028097, 'accuracy_test': 0.9076, 'precision_test': 0.7936, 'recall_test': 0.8281, 'f1_score_test': 0.8105}}}
Saved on: outputs_without_artist/8/lyrics_bert

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0021, 'precision_cv_mean': 0.8632, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8291, 'recall_cv_std': 0.0058, 'f1_cv_mean': 0.8458, 'f1_cv_std': 0.0022}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 50, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.87      0.90     16465
           1       0.67      0.82      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.85      0.82     21625
weighted avg       0.88      0.86      0.87     21625

Confusion matrix Test saved as: outputs_without_artist/8/lyrics_bert/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_without_artist/8/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.661, 'accuracy_cv_std': 0.035, 'precision_cv_mean': 0.6358, 'precision_cv_std': 0.0306, 'recall_cv_mean': 0.7553, 'recall_cv_std': 0.0312, 'f1_cv_mean': 0.6903, 'f1_cv_std': 0.0303}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 50, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.83      0.47      0.60     16465
           1       0.29      0.70      0.41      5160

    accuracy                           0.53     21625
   macro avg       0.56      0.59      0.51     21625
weighted avg       0.70      0.53      0.56     21625

Confusion matrix Test saved as: outputs_without_artist/8/lyrics_bert/conf_matrix_test_svm.png
Modelo guardado como: outputs_without_artist/8/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7873, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8125, 'precision_cv_std': 0.0092, 'recall_cv_mean': 0.7472, 'recall_cv_std': 0.0127, 'f1_cv_mean': 0.7784, 'f1_cv_std': 0.007}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 50, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.91      0.84      0.88     16465
           1       0.59      0.75      0.66      5160

    accuracy                           0.82     21625
   macro avg       0.75      0.79      0.77     21625
weighted avg       0.84      0.82      0.82     21625

Confusion matrix Test saved as: outputs_without_artist/8/lyrics_bert/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_without_artist/8/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8491, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8847, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8027, 'recall_cv_std': 0.0061, 'f1_cv_mean': 0.8417, 'f1_cv_std': 0.0055}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 50, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.90      0.92     16465
           1       0.71      0.81      0.76      5160

    accuracy                           0.88     21625
   macro avg       0.82      0.85      0.84     21625
weighted avg       0.88      0.88      0.88     21625

/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [12:21:34] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [12:22:05] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [12:22:36] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [12:23:08] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [12:23:39] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [12:24:10] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Confusion matrix Test saved as: outputs_without_artist/8/lyrics_bert/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_without_artist/8/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8756, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8955, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8504, 'recall_cv_std': 0.0038, 'f1_cv_mean': 0.8724, 'f1_cv_std': 0.0026}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 50, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.90      0.93     16465
           1       0.74      0.85      0.79      5160

    accuracy                           0.89     21625
   macro avg       0.84      0.88      0.86     21625
weighted avg       0.90      0.89      0.89     21625

Confusion matrix Test saved as: outputs_without_artist/8/lyrics_bert/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_without_artist/8/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7739, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.7555, 'precision_cv_std': 0.005, 'recall_cv_mean': 0.8101, 'recall_cv_std': 0.0055, 'f1_cv_mean': 0.7818, 'f1_cv_std': 0.0039}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.74      0.82     16465
           1       0.49      0.81      0.61      5160

    accuracy                           0.75     21625
   macro avg       0.71      0.77      0.72     21625
weighted avg       0.82      0.75      0.77     21625

Confusion matrix Test saved as: outputs_without_artist/8/lyrics_bert/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_without_artist/8/lyrics_bert/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.8756, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8955, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8504, 'recall_cv_std': 0.0038, 'f1_cv_mean': 0.8724, 'f1_cv_std': 0.0026, 'accuracy_test': 0.8916, 'precision_test': 0.7358, 'recall_test': 0.8514, 'f1_score_test': 0.7894}
Random Forest: {'accuracy_cv_mean': 0.8491, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8847, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8027, 'recall_cv_std': 0.0061, 'f1_cv_mean': 0.8417, 'f1_cv_std': 0.0055, 'accuracy_test': 0.875, 'precision_test': 0.7085, 'recall_test': 0.8089, 'f1_score_test': 0.7554}
Logistic Regression: {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0021, 'precision_cv_mean': 0.8632, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8291, 'recall_cv_std': 0.0058, 'f1_cv_mean': 0.8458, 'f1_cv_std': 0.0022, 'accuracy_test': 0.8605, 'precision_test': 0.6683, 'recall_test': 0.825, 'f1_score_test': 0.7384}
Decision Tree: {'accuracy_cv_mean': 0.7873, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8125, 'precision_cv_std': 0.0092, 'recall_cv_mean': 0.7472, 'recall_cv_std': 0.0127, 'f1_cv_mean': 0.7784, 'f1_cv_std': 0.007, 'accuracy_test': 0.8178, 'precision_test': 0.594, 'recall_test': 0.7479, 'f1_score_test': 0.6621}
Naive Bayes: {'accuracy_cv_mean': 0.7739, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.7555, 'precision_cv_std': 0.005, 'recall_cv_mean': 0.8101, 'recall_cv_std': 0.0055, 'f1_cv_mean': 0.7818, 'f1_cv_std': 0.0039, 'accuracy_test': 0.7539, 'precision_test': 0.4905, 'recall_test': 0.8068, 'f1_score_test': 0.6101}
SVM: {'accuracy_cv_mean': 0.661, 'accuracy_cv_std': 0.035, 'precision_cv_mean': 0.6358, 'precision_cv_std': 0.0306, 'recall_cv_mean': 0.7553, 'recall_cv_std': 0.0312, 'f1_cv_mean': 0.6903, 'f1_cv_std': 0.0303, 'accuracy_test': 0.5274, 'precision_test': 0.2933, 'recall_test': 0.6957, 'f1_score_test': 0.4126}
resultados_globales con ML {'tfidf': {'MLP_160705': {'accuracy_cv_mean': 0.8856, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.8801, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8927, 'recall_cv_std': 0.0076, 'f1_cv_mean': 0.8864, 'f1_cv_std': 0.0039, 'params': 160705, 'accuracy_test': 0.9163, 'precision_test': 0.8299, 'recall_test': 0.8169, 'f1_score_test': 0.8233}, 'MLP_5840897': {'accuracy_cv_mean': 0.8949, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8919, 'precision_cv_std': 0.019, 'recall_cv_mean': 0.8996, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.8954, 'f1_cv_std': 0.0032, 'params': 5840897, 'accuracy_test': 0.918, 'precision_test': 0.8432, 'recall_test': 0.8064, 'f1_score_test': 0.8244}, 'Logistic Regression': {'accuracy_cv_mean': 0.8795, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8921, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.8636, 'recall_cv_std': 0.0063, 'f1_cv_mean': 0.8776, 'f1_cv_std': 0.0038, 'accuracy_test': 0.8892, 'precision_test': 0.7248, 'recall_test': 0.8636, 'f1_score_test': 0.7881}, 'SVM': {'accuracy_cv_mean': 0.7551, 'accuracy_cv_std': 0.0122, 'precision_cv_mean': 0.7257, 'precision_cv_std': 0.0274, 'recall_cv_mean': 0.8261, 'recall_cv_std': 0.045, 'f1_cv_mean': 0.7711, 'f1_cv_std': 0.0109, 'accuracy_test': 0.7026, 'precision_test': 0.4336, 'recall_test': 0.805, 'f1_score_test': 0.5636}, 'Decision Tree': {'accuracy_cv_mean': 0.8261, 'accuracy_cv_std': 0.0037, 'precision_cv_mean': 0.8795, 'precision_cv_std': 0.009, 'recall_cv_mean': 0.756, 'recall_cv_std': 0.0176, 'f1_cv_mean': 0.8129, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8713, 'precision_test': 0.7226, 'recall_test': 0.7473, 'f1_score_test': 0.7348}, 'Random Forest': {'accuracy_cv_mean': 0.8154, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.8485, 'precision_cv_std': 0.0065, 'recall_cv_mean': 0.7681, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8063, 'f1_cv_std': 0.0046, 'accuracy_test': 0.8425, 'precision_test': 0.6409, 'recall_test': 0.7736, 'f1_score_test': 0.701}, 'XGBoost': {'accuracy_cv_mean': 0.8779, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.902, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.8481, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0044, 'accuracy_test': 0.8983, 'precision_test': 0.7572, 'recall_test': 0.8444, 'f1_score_test': 0.7984}, 'Naive Bayes': {'accuracy_cv_mean': 0.8455, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.8688, 'precision_cv_std': 0.0018, 'recall_cv_mean': 0.814, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.8405, 'f1_cv_std': 0.0025, 'accuracy_test': 0.8632, 'precision_test': 0.6798, 'recall_test': 0.8064, 'f1_score_test': 0.7377}}, 'lyrics_bert': {'MLP_10305': {'accuracy_cv_mean': 0.8669, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8644, 'precision_cv_std': 0.0069, 'recall_cv_mean': 0.8704, 'recall_cv_std': 0.0025, 'f1_cv_mean': 0.8674, 'f1_cv_std': 0.0039, 'params': 10305, 'accuracy_test': 0.8944, 'precision_test': 0.7538, 'recall_test': 0.8279, 'f1_score_test': 0.7891}, 'MLP_1028097': {'accuracy_cv_mean': 0.8799, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8782, 'precision_cv_std': 0.0112, 'recall_cv_mean': 0.8825, 'recall_cv_std': 0.0116, 'f1_cv_mean': 0.8802, 'f1_cv_std': 0.002, 'params': 1028097, 'accuracy_test': 0.9076, 'precision_test': 0.7936, 'recall_test': 0.8281, 'f1_score_test': 0.8105}, 'Logistic Regression': {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0021, 'precision_cv_mean': 0.8632, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8291, 'recall_cv_std': 0.0058, 'f1_cv_mean': 0.8458, 'f1_cv_std': 0.0022, 'accuracy_test': 0.8605, 'precision_test': 0.6683, 'recall_test': 0.825, 'f1_score_test': 0.7384}, 'SVM': {'accuracy_cv_mean': 0.661, 'accuracy_cv_std': 0.035, 'precision_cv_mean': 0.6358, 'precision_cv_std': 0.0306, 'recall_cv_mean': 0.7553, 'recall_cv_std': 0.0312, 'f1_cv_mean': 0.6903, 'f1_cv_std': 0.0303, 'accuracy_test': 0.5274, 'precision_test': 0.2933, 'recall_test': 0.6957, 'f1_score_test': 0.4126}, 'Decision Tree': {'accuracy_cv_mean': 0.7873, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8125, 'precision_cv_std': 0.0092, 'recall_cv_mean': 0.7472, 'recall_cv_std': 0.0127, 'f1_cv_mean': 0.7784, 'f1_cv_std': 0.007, 'accuracy_test': 0.8178, 'precision_test': 0.594, 'recall_test': 0.7479, 'f1_score_test': 0.6621}, 'Random Forest': {'accuracy_cv_mean': 0.8491, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8847, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8027, 'recall_cv_std': 0.0061, 'f1_cv_mean': 0.8417, 'f1_cv_std': 0.0055, 'accuracy_test': 0.875, 'precision_test': 0.7085, 'recall_test': 0.8089, 'f1_score_test': 0.7554}, 'XGBoost': {'accuracy_cv_mean': 0.8756, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8955, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8504, 'recall_cv_std': 0.0038, 'f1_cv_mean': 0.8724, 'f1_cv_std': 0.0026, 'accuracy_test': 0.8916, 'precision_test': 0.7358, 'recall_test': 0.8514, 'f1_score_test': 0.7894}, 'Naive Bayes': {'accuracy_cv_mean': 0.7739, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.7555, 'precision_cv_std': 0.005, 'recall_cv_mean': 0.8101, 'recall_cv_std': 0.0055, 'f1_/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_without_artist_8.py:288: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
cv_mean': 0.7818, 'f1_cv_std': 0.0039, 'accuracy_test': 0.7539, 'precision_test': 0.4905, 'recall_test': 0.8068, 'f1_score_test': 0.6101}}}

##################################################
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
E eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El último valor de eliminar es:  98849
Shape original y: (108138,)
Shape filtrado  y: (108125,)
Label distribution: {0: 82336, 1: 25802}
X shape: (108125, 1536)
y shape: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1556)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1556)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [1556, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4264, Test Loss: 0.3827, F1: 0.8402, AUC: 0.9156
Epoch [10/30] Train Loss: 0.3297, Test Loss: 0.3287, F1: 0.8589, AUC: 0.9341
Epoch [20/30] Train Loss: 0.3109, Test Loss: 0.3417, F1: 0.8625, AUC: 0.9389
Mejores resultados en la época:  29
f1-score 0.8723763570566948
AUC según el mejor F1-score 0.9411878276921458

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4272, Test Loss: 0.3904, F1: 0.8367, AUC: 0.9135
Epoch [10/30] Train Loss: 0.3349, Test Loss: 0.3425, F1: 0.8447, AUC: 0.9294
Epoch [20/30] Train Loss: 0.3130, Test Loss: 0.3356, F1: 0.8616, AUC: 0.9355
Mejores resultados en la época:  26
f1-score 0.8633682436499338
AUC según el mejor F1-score 0.9363158130708792

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4164, Test Loss: 0.3868, F1: 0.8175, AUC: 0.9086
Epoch [10/30] Train Loss: 0.3274, Test Loss: 0.3738, F1: 0.8503, AUC: 0.9268
Epoch [20/30] Train Loss: 0.3113, Test Loss: 0.3378, F1: 0.8516, AUC: 0.9315
Mejores resultados en la época:  14
f1-score 0.8590109367570138
AUC según el mejor F1-score 0.9295209544573643

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4220, Test Loss: 0.3953, F1: 0.8343, AUC: 0.9113
Epoch [10/30] Train Loss: 0.3304, Test Loss: 0.3405, F1: 0.8411, AUC: 0.9314
Epoch [20/30] Train Loss: 0.3090, Test Loss: 0.3386, F1: 0.8584, AUC: 0.9341
Mejores resultados en la época:  25
f1-score 0.8614718614718615
AUC según el mejor F1-score 0.934903508141695

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4261, Test Loss: 0.3668, F1: 0.8345, AUC: 0.9171
Epoch [10/30] Train Loss: 0.3303, Test Loss: 0.3211, F1: 0.8599, AUC: 0.9364
Epoch [20/30] Train Loss: 0.3144, Test Loss: 0.3165, F1: 0.8641, AUC: 0.9392
Mejores resultados en la época:  29
f1-score 0.8666666666666667
AUC según el mejor F1-score 0.9414987072276914
Epoch [0/30] Train Loss: 0.4145, Test Loss: 0.3455, F1: 0.7284, AUC: 0.9180
Epoch [10/30] Train Loss: 0.3247, Test Loss: 0.3405, F1: 0.7523, AUC: 0.9394
Epoch [20/30] Train Loss: 0.3102, Test Loss: 0.3565, F1: 0.7430, AUC: 0.9431
Mejores resultados en la época:  29
f1-score 0.7865663424481556
AUC según el mejor F1-score 0.9433127941110694
Confusion matrix Test saved: outputs_without_artist/8/gpt/cm_mlp_1.png

========================================
Entrenando red 6 con capas [1556, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4247, Test Loss: 0.3813, F1: 0.8407, AUC: 0.9205
Epoch [10/30] Train Loss: 0.3173, Test Loss: 0.3423, F1: 0.8533, AUC: 0.9351
Epoch [20/30] Train Loss: 0.2873, Test Loss: 0.2959, F1: 0.8744, AUC: 0.9466
Mejores resultados en la época:  27
f1-score 0.882401027677216
AUC según el mejor F1-score 0.9493886405376779

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4263, Test Loss: 0.3691, F1: 0.8384, AUC: 0.9183
Epoch [10/30] Train Loss: 0.3168, Test Loss: 0.3303, F1: 0.8502, AUC: 0.9369
Epoch [20/30] Train Loss: 0.2846, Test Loss: 0.3214, F1: 0.8658, AUC: 0.9379
Mejores resultados en la época:  21
f1-score 0.8767622751579971
AUC según el mejor F1-score 0.9453959781657502

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4249, Test Loss: 0.4167, F1: 0.8319, AUC: 0.9096
Epoch [10/30] Train Loss: 0.3162, Test Loss: 0.3479, F1: 0.8560, AUC: 0.9331
Epoch [20/30] Train Loss: 0.2813, Test Loss: 0.3330, F1: 0.8602, AUC: 0.9377
Mejores resultados en la época:  25
f1-score 0.8701253074130461
AUC según el mejor F1-score 0.9405631938810768

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4244, Test Loss: 0.3698, F1: 0.8253, AUC: 0.9169
Epoch [10/30] Train Loss: 0.3180, Test Loss: 0.3596, F1: 0.8307, AUC: 0.9333
Epoch [20/30] Train Loss: 0.2856, Test Loss: 0.3259, F1: 0.8557, AUC: 0.9406
Mejores resultados en la época:  23
f1-score 0.8764563106796116
AUC según el mejor F1-score 0.9422769298606455

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4337, Test Loss: 0.3678, F1: 0.8331, AUC: 0.9200
Epoch [10/30] Train Loss: 0.3188, Test Loss: 0.3136, F1: 0.8652, AUC: 0.9405
Epoch [20/30] Train Loss: 0.2888, Test Loss: 0.3143, F1: 0.8656, AUC: 0.9431
Mejores resultados en la época:  26
f1-score 0.8742092457420925
AUC según el mejor F1-score 0.9469269832526583
Epoch [0/30] Train Loss: 0.4151, Test Loss: 0.4020, F1: 0.7070, AUC: 0.9233
Epoch [10/30] Train Loss: 0.3148, Test Loss: 0.2764, F1: 0.7802, AUC: 0.9411
Epoch [20/30] Train Loss: 0.2816, Test Loss: 0.2616, F1: 0.7942, AUC: 0.9497
Mejores resultados en la época:  20
f1-score 0.7942218588171164
AUC según el mejor F1-score 0.9496763866034836
Confusion matrix Test saved: outputs_without_artist/8/gpt/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160705': {'accuracy_cv_mean': 0.8856, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.8801, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8927, 'recall_cv_std': 0.0076, 'f1_cv_mean': 0.8864, 'f1_cv_std': 0.0039, 'params': 160705, 'accuracy_test': 0.9163, 'precision_test': 0.8299, 'recall_test': 0.8169, 'f1_score_test': 0.8233}, 'MLP_5840897': {'accuracy_cv_mean': 0.8949, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8919, 'precision_cv_std': 0.019, 'recall_cv_mean': 0.8996, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.8954, 'f1_cv_std': 0.0032, 'params': 5840897, 'accuracy_test': 0.918, 'precision_test': 0.8432, 'recall_test': 0.8064, 'f1_score_test': 0.8244}, 'Logistic Regression': {'accuracy_cv_mean': 0.8795, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8921, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.8636, 'recall_cv_std': 0.0063, 'f1_cv_mean': 0.8776, 'f1_cv_std': 0.0038, 'accuracy_test': 0.8892, 'precision_test': 0.7248, 'recall_test': 0.8636, 'f1_score_test': 0.7881}, 'SVM': {'accuracy_cv_mean': 0.7551, 'accuracy_cv_std': 0.0122, 'precision_cv_mean': 0.7257, 'precision_cv_std': 0.0274, 'recall_cv_mean': 0.8261, 'recall_cv_std': 0.045, 'f1_cv_mean': 0.7711, 'f1_cv_std': 0.0109, 'accuracy_test': 0.7026, 'precision_test': 0.4336, 'recall_test': 0.805, 'f1_score_test': 0.5636}, 'Decision Tree': {'accuracy_cv_mean': 0.8261, 'accuracy_cv_std': 0.0037, 'precision_cv_mean': 0.8795, 'precision_cv_std': 0.009, 'recall_cv_mean': 0.756, 'recall_cv_std': 0.0176, 'f1_cv_mean': 0.8129, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8713, 'precision_test': 0.7226, 'recall_test': 0.7473, 'f1_score_test': 0.7348}, 'Random Forest': {'accuracy_cv_mean': 0.8154, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.8485, 'precision_cv_std': 0.0065, 'recall_cv_mean': 0.7681, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8063, 'f1_cv_std': 0.0046, 'accuracy_test': 0.8425, 'precision_test': 0.6409, 'recall_test': 0.7736, 'f1_score_test': 0.701}, 'XGBoost': {'accuracy_cv_mean': 0.8779, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.902, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.8481, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0044, 'accuracy_test': 0.8983, 'precision_test': 0.7572, 'recall_test': 0.8444, 'f1_score_test': 0.7984}, 'Naive Bayes': {'accuracy_cv_mean': 0.8455, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.8688, 'precision_cv_std': 0.0018, 'recall_cv_mean': 0.814, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.8405, 'f1_cv_std': 0.0025, 'accuracy_test': 0.8632, 'precision_test': 0.6798, 'recall_test': 0.8064, 'f1_score_test': 0.7377}}, 'lyrics_bert': {'MLP_10305': {'accuracy_cv_mean': 0.8669, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8644, 'precision_cv_std': 0.0069, 'recall_cv_mean': 0.8704, 'recall_cv_std': 0.0025, 'f1_cv_mean': 0.8674, 'f1_cv_std': 0.0039, 'params': 10305, 'accuracy_test': 0.8944, 'precision_test': 0.7538, 'recall_test': 0.8279, 'f1_score_test': 0.7891}, 'MLP_1028097': {'accuracy_cv_mean': 0.8799, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8782, 'precision_cv_std': 0.0112, 'recall_cv_mean': 0.8825, 'recall_cv_std': 0.0116, 'f1_cv_mean': 0.8802, 'f1_cv_std': 0.002, 'params': 1028097, 'accuracy_test': 0.9076, 'precision_test': 0.7936, 'recall_test': 0.8281, 'f1_score_test': 0.8105}, 'Logistic Regression': {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0021, 'precision_cv_mean': 0.8632, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8291, 'recall_cv_std': 0.0058, 'f1_cv_mean': 0.8458, 'f1_cv_std': 0.0022, 'accuracy_test': 0.8605, 'precision_test': 0.6683, 'recall_test': 0.825, 'f1_score_test': 0.7384}, 'SVM': {'accuracy_cv_mean': 0.661, 'accuracy_cv_std': 0.035, 'precision_cv_mean': 0.6358, 'precision_cv_std': 0.0306, 'recall_cv_mean': 0.7553, 'recall_cv_std': 0.0312, 'f1_cv_mean': 0.6903, 'f1_cv_std': 0.0303, 'accuracy_test': 0.5274, 'precision_test': 0.2933, 'recall_test': 0.6957, 'f1_score_test': 0.4126}, 'Decision Tree': {'accuracy_cv_mean': 0.7873, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8125, 'precision_cv_std': 0.0092, 'recall_cv_mean': 0.7472, 'recall_cv_std': 0.0127, 'f1_cv_mean': 0.7784, 'f1_cv_std': 0.007, 'accuracy_test': 0.8178, 'precision_test': 0.594, 'recall_test': 0.7479, 'f1_score_test': 0.6621}, 'Random Forest': {'accuracy_cv_mean': 0.8491, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8847, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8027, 'recall_cv_std': 0.0061, 'f1_cv_mean': 0.8417, 'f1_cv_std': 0.0055, 'accuracy_test': 0.875, 'precision_test': 0.7085, 'recall_test': 0.8089, 'f1_score_test': 0.7554}, 'XGBoost': {'accuracy_cv_mean': 0.8756, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8955, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8504, 'recall_cv_std': 0.0038, 'f1_cv_mean': 0.8724, 'f1_cv_std': 0.0026, 'accuracy_test': 0.8916, 'precision_test': 0.7358, 'recall_test': 0.8514, 'f1_score_test': 0.7894}, 'Naive Bayes': {'accuracy_cv_mean': 0.7739, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.7555, 'precision_cv_std': 0.005, 'recall_cv_mean': 0.8101, 'recall_cv_std': 0.0055, 'f1_/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [13:16:42] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [13:19:28] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [13:22:14] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [13:25:01] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [13:27:48] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [13:30:37] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
cv_mean': 0.7818, 'f1_cv_std': 0.0039, 'accuracy_test': 0.7539, 'precision_test': 0.4905, 'recall_test': 0.8068, 'f1_score_test': 0.6101}}, 'gpt': {'MLP_49857': {'accuracy_cv_mean': 0.8633, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8567, 'precision_cv_std': 0.0081, 'recall_cv_mean': 0.8727, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.8646, 'f1_cv_std': 0.0046, 'params': 49857, 'accuracy_test': 0.8924, 'precision_test': 0.747, 'recall_test': 0.8306, 'f1_score_test': 0.7866}, 'MLP_2293761': {'accuracy_cv_mean': 0.8745, 'accuracy_cv_std': 0.0045, 'precision_cv_mean': 0.866, 'precision_cv_std': 0.0158, 'recall_cv_mean': 0.8869, 'recall_cv_std': 0.0177, 'f1_cv_mean': 0.876, 'f1_cv_std': 0.004, 'params': 2293761, 'accuracy_test': 0.8953, 'precision_test': 0.7476, 'recall_test': 0.8471, 'f1_score_test': 0.7942}}}
Saved on: outputs_without_artist/8/gpt

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8544, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8415, 'recall_cv_std': 0.0059, 'f1_cv_mean': 0.8479, 'f1_cv_std': 0.0052}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 50, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.86      0.90     16465
           1       0.66      0.84      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.85      0.82     21625
weighted avg       0.88      0.86      0.86     21625

Confusion matrix Test saved as: outputs_without_artist/8/gpt/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_without_artist/8/gpt/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7119, 'accuracy_cv_std': 0.0279, 'precision_cv_mean': 0.6874, 'precision_cv_std': 0.0433, 'recall_cv_mean': 0.7885, 'recall_cv_std': 0.0339, 'f1_cv_mean': 0.7328, 'f1_cv_std': 0.0176}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 50, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.91      0.61      0.73     16465
           1       0.39      0.81      0.53      5160

    accuracy                           0.66     21625
   macro avg       0.65      0.71      0.63     21625
weighted avg       0.79      0.66      0.68     21625

Confusion matrix Test saved as: outputs_without_artist/8/gpt/conf_matrix_test_svm.png
Modelo guardado como: outputs_without_artist/8/gpt/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7811, 'accuracy_cv_std': 0.0074, 'precision_cv_mean': 0.7846, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.7749, 'recall_cv_std': 0.0102, 'f1_cv_mean': 0.7797, 'f1_cv_std': 0.0079}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 50, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.81      0.86     16465
           1       0.56      0.79      0.65      5160

    accuracy                           0.80     21625
   macro avg       0.74      0.80      0.76     21625
weighted avg       0.84      0.80      0.81     21625

Confusion matrix Test saved as: outputs_without_artist/8/gpt/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_without_artist/8/gpt/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8177, 'accuracy_cv_std': 0.0038, 'precision_cv_mean': 0.855, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.7652, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.8076, 'f1_cv_std': 0.004}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 50, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.87      0.90     16465
           1       0.66      0.78      0.72      5160

    accuracy                           0.85     21625
   macro avg       0.79      0.83      0.81     21625
weighted avg       0.86      0.85      0.86     21625

Confusion matrix Test saved as: outputs_without_artist/8/gpt/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_without_artist/8/gpt/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8631, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8806, 'precision_cv_std': 0.005, 'recall_cv_mean': 0.8402, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.8599, 'f1_cv_std': 0.0051}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 50, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.90      0.92     16465
           1       0.72      0.85      0.78      5160

    accuracy                           0.89     21625
   macro avg       0.84      0.88      0.85     21625
weighted avg       0.90      0.89      0.89     21625

Confusion matrix Test saved as: outputs_without_artist/8/gpt/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_without_artist/8/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7981, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.8547, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.7185, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.7807, 'f1_cv_std': 0.0035}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.91      0.88      0.90     16465
           1       0.66      0.73      0.69      5160

    accuracy                           0.85     21625
   macro avg       0.79      0.81      0.80     21625
weighted avg       0.85      0.85      0.85     21625

Confusion matrix Test saved as: outputs_without_artist/8/gpt/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_without_artist/8/gpt/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.8631, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8806, 'precision_cv_std': 0.005, 'recall_cv_mean': 0.8402, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.8599, 'f1_cv_std': 0.0051, 'accuracy_test': 0.8875, 'precision_test': 0.7243, 'recall_test': 0.8533, 'f1_score_test': 0.7835}
Logistic Regression: {'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8544, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8415, 'recall_cv_std': 0.0059, 'f1_cv_mean': 0.8479, 'f1_cv_std': 0.0052, 'accuracy_test': 0.8586, 'precision_test': 0.6589, 'recall_test': 0.845, 'f1_score_test': 0.7404}
Random Forest: {'accuracy_cv_mean': 0.8177, 'accuracy_cv_std': 0.0038, 'precision_cv_mean': 0.855, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.7652, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.8076, 'f1_cv_std': 0.004, 'accuracy_test': 0.8521, 'precision_test': 0.6602, 'recall_test': 0.7831, 'f1_score_test': 0.7164}
Naive Bayes: {'accuracy_cv_mean': 0.7981, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.8547, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.7185, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.7807, 'f1_cv_std': 0.0035, 'accuracy_test': 0.8458, 'precision_test': 0.6596, 'recall_test': 0.7312, 'f1_score_test': 0.6936}
Decision Tree: {'accuracy_cv_mean': 0.7811, 'accuracy_cv_std': 0.0074, 'precision_cv_mean': 0.7846, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.7749, 'recall_cv_std': 0.0102, 'f1_cv_mean': 0.7797, 'f1_cv_std': 0.0079, 'accuracy_test': 0.8011, 'precision_test': 0.5592, 'recall_test': 0.7851, 'f1_score_test': 0.6532}
SVM: {'accuracy_cv_mean': 0.7119, 'accuracy_cv_std': 0.0279, 'precision_cv_mean': 0.6874, 'precision_cv_std': 0.0433, 'recall_cv_mean': 0.7885, 'recall_cv_std': 0.0339, 'f1_cv_mean': 0.7328, 'f1_cv_std': 0.0176, 'accuracy_test': 0.6571, 'precision_test': 0.394, 'recall_test': 0.8118, 'f1_score_test': 0.5305}
resultados_globales con ML {'tfidf': {'MLP_160705': {'accuracy_cv_mean': 0.8856, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.8801, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8927, 'recall_cv_std': 0.0076, 'f1_cv_mean': 0.8864, 'f1_cv_std': 0.0039, 'params': 160705, 'accuracy_test': 0.9163, 'precision_test': 0.8299, 'recall_test': 0.8169, 'f1_score_test': 0.8233}, 'MLP_5840897': {'accuracy_cv_mean': 0.8949, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8919, 'precision_cv_std': 0.019, 'recall_cv_mean': 0.8996, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.8954, 'f1_cv_std': 0.0032, 'params': 5840897, 'accuracy_test': 0.918, 'precision_test': 0.8432, 'recall_test': 0.8064, 'f1_score_test': 0.8244}, 'Logistic Regression': {'accuracy_cv_mean': 0.8795, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8921, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.8636, 'recall_cv_std': 0.0063, 'f1_cv_mean': 0.8776, 'f1_cv_std': 0.0038, 'accuracy_test': 0.8892, 'precision_test': 0.7248, 'recall_test': 0.8636, 'f1_score_test': 0.7881}, 'SVM': {'accuracy_cv_mean': 0.7551, 'accuracy_cv_std': 0.0122, 'precision_cv_mean': 0.7257, 'precision_cv_std': 0.0274, 'recall_cv_mean': 0.8261, 'recall_cv_std': 0.045, 'f1_cv_mean': 0.7711, 'f1_cv_std': 0.0109, 'accuracy_test': 0.7026, 'precision_test': 0.4336, 'recall_test': 0.805, 'f1_score_test': 0.5636}, 'Decision Tree': {'accuracy_cv_mean': 0.8261, 'accuracy_cv_std': 0.0037, 'precision_cv_mean': 0.8795, 'precision_cv_std': 0.009, 'recall_cv_mean': 0.756, 'recall_cv_std': 0.0176, 'f1_cv_mean': 0.8129, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8713, 'precision_test': 0.7226, 'recall_test': 0.7473, 'f1_score_test': 0.7348}, 'Random Forest': {'accuracy_cv_mean': 0.8154, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.8485, 'precision_cv_std': 0.0065, 'recall_cv_mean': 0.7681, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8063, 'f1_cv_std': 0.0046, 'accuracy_test': 0.8425, 'precision_test': 0.6409, 'recall_test': 0.7736, 'f1_score_test': 0.701}, 'XGBoost': {'accuracy_cv_mean': 0.8779, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.902, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.8481, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0044, 'accuracy_test': 0.8983, 'precision_test': 0.7572, 'recall_test': 0.8444, 'f1_score_test': 0.7984}, 'Naive Bayes': {'accuracy_cv_mean': 0.8455, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.8688, 'precision_cv_std': 0.0018, 'recall_cv_mean': 0.814, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.8405, 'f1_cv_std': 0.0025, 'accuracy_test': 0.8632, 'precision_test': 0.6798, 'recall_test': 0.8064, 'f1_score_test': 0.7377}}, 'lyrics_bert': {'MLP_10305': {'accuracy_cv_mean': 0.8669, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8644, 'precision_cv_std': 0.0069, 'recall_cv_mean': 0.8704, 'recall_cv_std': 0.0025, 'f1_cv_mean': 0.8674, 'f1_cv_std': 0.0039, 'params': 10305, 'accuracy_test': 0.8944, 'precision_test': 0.7538, 'recall_test': 0.8279, 'f1_score_test': 0.7891}, 'MLP_1028097': {'accuracy_cv_mean': 0.8799, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8782, 'precision_cv_std': 0.0112, 'recall_cv_mean': 0.8825, 'recall_cv_std': 0.0116, 'f1_cv_mean': 0.8802, 'f1_cv_std': 0.002, 'params': 1028097, 'accuracy_test': 0.9076, 'precision_test': 0.7936, 'recall_test': 0.8281, 'f1_score_test': 0.8105}, 'Logistic Regression': {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0021, 'precision_cv_mean': 0.8632, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8291, 'recall_cv_std': 0.0058, 'f1_cv_mean': 0.8458, 'f1_cv_std': 0.0022, 'accuracy_test': 0.8605, 'precision_test': 0.6683, 'recall_test': 0.825, 'f1_score_test': 0.7384}, 'SVM': {'accuracy_cv_mean': 0.661, 'accuracy_cv_std': 0.035, 'precision_cv_mean': 0.6358, 'precision_cv_std': 0.0306, 'recall_cv_mean': 0.7553, 'recall_cv_std': 0.0312, 'f1_cv_mean': 0.6903, 'f1_cv_std': 0.0303, 'accuracy_test': 0.5274, 'precision_test': 0.2933, 'recall_test': 0.6957, 'f1_score_test': 0.4126}, 'Decision Tree': {'accuracy_cv_mean': 0.7873, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8125, 'precision_cv_std': 0.0092, 'recall_cv_mean': 0.7472, 'recall_cv_std': 0.0127, 'f1_cv_mean': 0.7784, 'f1_cv_std': 0.007, 'accuracy_test': 0.8178, 'precision_test': 0.594, 'recall_test': 0.7479, 'f1_score_test': 0.6621}, 'Random Forest': {'accuracy_cv_mean': 0.8491, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8847, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8027, 'recall_cv_std': 0.0061, 'f1_cv_mean': 0.8417, 'f1_cv_std': 0.0055, 'accuracy_test': 0.875, 'precision_test': 0.7085, 'recall_test': 0.8089, 'f1_score_test': 0.7554}, 'XGBoost': {'accuracy_cv_mean': 0.8756, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8955, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8504, 'recall_cv_std': 0.0038, 'f1_cv_mean': 0.8724, 'f1_cv_std': 0.0026, 'accuracy_test': 0.8916, 'precision_test': 0.7358, 'recall_test': 0.8514, 'f1_score_test': 0.7894}, 'Naive Bayes': {'accuracy_cv_mean': 0.7739, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.7555, 'precision_cv_std': 0.005, 'recall_cv_mean': 0.8101, 'recall_cv_std': 0.0055, 'f1_cv_mean': 0.7818, 'f1_cv_std': 0.0039, 'accuracy_test': 0.7539, 'precision_test': 0.4905, 'recall_test': 0.8068, 'f1_score_test': 0.6101}}, 'gpt': {'MLP_49857': {'accuracy_cv_mean': 0.8633, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8567, 'precision_cv_std': 0.0081, 'recall_cv_mean': 0.8727, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.8646, 'f1_cv_std': 0.0046, 'params': 49857, 'accuracy_test': 0.8924, 'precision_test': 0.747, 'recall_test': 0.8306, 'f1_score_test': 0.7866}, 'MLP_2293761': {'accuracy_cv_mean': 0.8745, 'accuracy_cv_std': 0.0045, 'precision_cv_mean': 0.866, 'precision_cv_std': 0.0158, 'recall_cv_mean': 0.8869, 'recall_cv_std': 0.0177, 'f1_cv_mean': 0.876, 'f1_cv_std': 0.004, 'params': 2293761, 'accuracy_test': 0.8953, 'precision_test': 0.7476, 'recall_test': 0.8471, 'f1_score_test': 0.7942}, 'Logistic Regression': {'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8544, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8415, 'recall_cv_std': 0.0059, 'f1_cv_mean': 0.8479, 'f1_cv_std': 0.0052, 'accuracy_test': 0.8586, 'precision_test': 0.6589, 'recall_test': 0.845, 'f1_score_test': 0.7404}, 'SVM': {'accuracy_cv_mean': 0.7119, 'accuracy_cv_std': 0.0279, 'precision_cv_mean': 0.6874, 'precision_cv_std': 0.0433, 'recall_cv_mean': 0.7885, 'recall_cv_std': 0.0339, 'f1_cv_mean': 0.7328, 'f1_cv_std': 0.0176, 'accuracy_test': 0.6571, 'precision_test': 0.394, 'recall_test': 0.8118, 'f1_score_test': 0.5305}, 'Decision Tree': {'accuracy_cv_mean': 0.7811, 'accuracy_cv_std': 0.0074, 'precision_cv_mean': 0.7846, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.7749, 'recall_cv_std': 0.0102, 'f1_cv_mean': 0.7797, 'f1_cv_std': 0.0079, 'accuracy_test': 0.8011, 'precision_test': 0.5592, 'recall_test': 0.7851, 'f1_score_test': 0.6532}, 'Random Forest': {'accuracy_cv_mean': 0.8177, 'accuracy_cv_std': 0.0038, 'precision_cv_mean': 0.855, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.7652, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.8076, 'f1_cv_std': 0.004, 'accuracy_test': 0.8521, 'precision_test': 0.6602, 'recall_test': 0.7831, 'f1_score_test': 0.7164}, 'XGBoost': {'accuracy_cv_mean': 0.8631, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8806, 'precision_cv_std': 0.005, 'recall_cv_mean': 0.8402, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.8599, 'f1_cv_std': 0.0051, 'accuracy_test': 0.8875, 'precision_test': 0.7243, 'recall_test': 0.8533, 'f1_score_test': 0.7835}, 'Naive Bayes': {'accuracy_cv_mean': 0.7981, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.8547, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.7185, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.7807, 'f1_cv_std': 0.0035, 'accuracy_test': 0.8458, 'precision_test': 0.6596, 'recall_test': 0.7312, 'f1_score_test': 0.6936}}}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
MLP_5840897: {'accuracy_cv_mean': 0.8949, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8919, 'precision_cv_std': 0.019, 'recall_cv_mean': 0.8996, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.8954, 'f1_cv_std': 0.0032, 'params': 5840897, 'accuracy_test': 0.918, 'precision_test': 0.8432, 'recall_test': 0.8064, 'f1_score_test': 0.8244}
MLP_160705: {'accuracy_cv_mean': 0.8856, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.8801, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8927, 'recall_cv_std': 0.0076, 'f1_cv_mean': 0.8864, 'f1_cv_std': 0.0039, 'params': 160705, 'accuracy_test': 0.9163, 'precision_test': 0.8299, 'recall_test': 0.8169, 'f1_score_test': 0.8233}
XGBoost: {'accuracy_cv_mean': 0.8779, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.902, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.8481, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0044, 'accuracy_test': 0.8983, 'precision_test': 0.7572, 'recall_test': 0.8444, 'f1_score_test': 0.7984}
Logistic Regression: {'accuracy_cv_mean': 0.8795, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8921, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.8636, 'recall_cv_std': 0.0063, 'f1_cv_mean': 0.8776, 'f1_cv_std': 0.0038, 'accuracy_test': 0.8892, 'precision_test': 0.7248, 'recall_test': 0.8636, 'f1_score_test': 0.7881}
Naive Bayes: {'accuracy_cv_mean': 0.8455, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.8688, 'precision_cv_std': 0.0018, 'recall_cv_mean': 0.814, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.8405, 'f1_cv_std': 0.0025, 'accuracy_test': 0.8632, 'precision_test': 0.6798, 'recall_test': 0.8064, 'f1_score_test': 0.7377}
Decision Tree: {'accuracy_cv_mean': 0.8261, 'accuracy_cv_std': 0.0037, 'precision_cv_mean': 0.8795, 'precision_cv_std': 0.009, 'recall_cv_mean': 0.756, 'recall_cv_std': 0.0176, 'f1_cv_mean': 0.8129, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8713, 'precision_test': 0.7226, 'recall_test': 0.7473, 'f1_score_test': 0.7348}
Random Forest: {'accuracy_cv_mean': 0.8154, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.8485, 'precision_cv_std': 0.0065, 'recall_cv_mean': 0.7681, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8063, 'f1_cv_std': 0.0046, 'accuracy_test': 0.8425, 'precision_test': 0.6409, 'recall_test': 0.7736, 'f1_score_test': 0.701}
SVM: {'accuracy_cv_mean': 0.7551, 'accuracy_cv_std': 0.0122, 'precision_cv_mean': 0.7257, 'precision_cv_std': 0.0274, 'recall_cv_mean': 0.8261, 'recall_cv_std': 0.045, 'f1_cv_mean': 0.7711, 'f1_cv_std': 0.0109, 'accuracy_test': 0.7026, 'precision_test': 0.4336, 'recall_test': 0.805, 'f1_score_test': 0.5636}


EMBEDDINGS TYPE: LYRICS_BERT
MLP_1028097: {'accuracy_cv_mean': 0.8799, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8782, 'precision_cv_std': 0.0112, 'recall_cv_mean': 0.8825, 'recall_cv_std': 0.0116, 'f1_cv_mean': 0.8802, 'f1_cv_std': 0.002, 'params': 1028097, 'accuracy_test': 0.9076, 'precision_test': 0.7936, 'recall_test': 0.8281, 'f1_score_test': 0.8105}
XGBoost: {'accuracy_cv_mean': 0.8756, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8955, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8504, 'recall_cv_std': 0.0038, 'f1_cv_mean': 0.8724, 'f1_cv_std': 0.0026, 'accuracy_test': 0.8916, 'precision_test': 0.7358, 'recall_test': 0.8514, 'f1_score_test': 0.7894}
MLP_10305: {'accuracy_cv_mean': 0.8669, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8644, 'precision_cv_std': 0.0069, 'recall_cv_mean': 0.8704, 'recall_cv_std': 0.0025, 'f1_cv_mean': 0.8674, 'f1_cv_std': 0.0039, 'params': 10305, 'accuracy_test': 0.8944, 'precision_test': 0.7538, 'recall_test': 0.8279, 'f1_score_test': 0.7891}
Random Forest: {'accuracy_cv_mean': 0.8491, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8847, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8027, 'recall_cv_std': 0.0061, 'f1_cv_mean': 0.8417, 'f1_cv_std': 0.0055, 'accuracy_test': 0.875, 'precision_test': 0.7085, 'recall_test': 0.8089, 'f1_score_test': 0.7554}
Logistic Regression: {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0021, 'precision_cv_mean': 0.8632, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8291, 'recall_cv_std': 0.0058, 'f1_cv_mean': 0.8458, 'f1_cv_std': 0.0022, 'accuracy_test': 0.8605, 'precision_test': 0.6683, 'recall_test': 0.825, 'f1_score_test': 0.7384}
Decision Tree: {'accuracy_cv_mean': 0.7873, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8125, 'precision_cv_std': 0.0092, 'recall_cv_mean': 0.7472, 'recall_cv_std': 0.0127, 'f1_cv_mean': 0.7784, 'f1_cv_std': 0.007, 'accuracy_test': 0.8178, 'precision_test': 0.594, 'recall_test': 0.7479, 'f1_score_test': 0.6621}
Naive Bayes: {'accuracy_cv_mean': 0.7739, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.7555, 'precision_cv_std': 0.005, 'recall_cv_mean': 0.8101, 'recall_cv_std': 0.0055, 'f1_cv_mean': 0.7818, 'f1_cv_std': 0.0039, 'accuracy_test': 0.7539, 'precision_test': 0.4905, 'recall_test': 0.8068, 'f1_score_test': 0.6101}
SVM: {'accuracy_cv_mean': 0.661, 'accuracy_cv_std': 0.035, 'precision_cv_mean': 0.6358, 'precision_cv_std': 0.0306, 'recall_cv_mean': 0.7553, 'recall_cv_std': 0.0312, 'f1_cv_mean': 0.6903, 'f1_cv_std': 0.0303, 'accuracy_test': 0.5274, 'precision_test': 0.2933, 'recall_test': 0.6957, 'f1_score_test': 0.4126}


EMBEDDINGS TYPE: GPT
MLP_2293761: {'accuracy_cv_mean': 0.8745, 'accuracy_cv_std': 0.0045, 'precision_cv_mean': 0.866, 'precision_cv_std': 0.0158, 'recall_cv_mean': 0.8869, 'recall_cv_std': 0.0177, 'f1_cv_mean': 0.876, 'f1_cv_std': 0.004, 'params': 2293761, 'accuracy_test': 0.8953, 'precision_test': 0.7476, 'recall_test': 0.8471, 'f1_score_test': 0.7942}
MLP_49857: {'accuracy_cv_mean': 0.8633, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8567, 'precision_cv_std': 0.0081, 'recall_cv_mean': 0.8727, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.8646, 'f1_cv_std': 0.0046, 'params': 49857, 'accuracy_test': 0.8924, 'precision_test': 0.747, 'recall_test': 0.8306, 'f1_score_test': 0.7866}
XGBoost: {'accuracy_cv_mean': 0.8631, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8806, 'precision_cv_std': 0.005, 'recall_cv_mean': 0.8402, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.8599, 'f1_cv_std': 0.0051, 'accuracy_test': 0.8875, 'precision_test': 0.7243, 'recall_test': 0.8533, 'f1_score_test': 0.7835}
Logistic Regression: {'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8544, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8415, 'recall_cv_std': 0.0059, 'f1_cv_mean': 0.8479, 'f1_cv_std': 0.0052, 'accuracy_test': 0.8586, 'precision_test': 0.6589, 'recall_test': 0.845, 'f1_score_test': 0.7404}
Random Forest: {'accuracy_cv_mean': 0.8177, 'accuracy_cv_std': 0.0038, 'precision_cv_mean': 0.855, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.7652, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.8076, 'f1_cv_std': 0.004, 'accuracy_test': 0.8521, 'precision_test': 0.6602, 'recall_test': 0.7831, 'f1_score_test': 0.7164}
Naive Bayes: {'accuracy_cv_mean': 0.7981, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.8547, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.7185, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.7807, 'f1_cv_std': 0.0035, 'accuracy_test': 0.8458, 'precision_test': 0.6596, 'recall_test': 0.7312, 'f1_score_test': 0.6936}
Decision Tree: {'accuracy_cv_mean': 0.7811, 'accuracy_cv_std': 0.0074, 'precision_cv_mean': 0.7846, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.7749, 'recall_cv_std': 0.0102, 'f1_cv_mean': 0.7797, 'f1_cv_std': 0.0079, 'accuracy_test': 0.8011, 'precision_test': 0.5592, 'recall_test': 0.7851, 'f1_score_test': 0.6532}
SVM: {'accuracy_cv_mean': 0.7119, 'accuracy_cv_std': 0.0279, 'precision_cv_mean': 0.6874, 'precision_cv_std': 0.0433, 'recall_cv_mean': 0.7885, 'recall_cv_std': 0.0339, 'f1_cv_mean': 0.7328, 'f1_cv_std': 0.0176, 'accuracy_test': 0.6571, 'precision_test': 0.394, 'recall_test': 0.8118, 'f1_score_test': 0.5305}
Diccionario global guardado en: outputs_without_artist/8/gpt/global_metrics.json

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['emotion', 'Time signature', 'song', 'Genre', 'Album', 'Release Date', 'Key', 'Similar Song 1', 'Similar Song 2', 'Similar Song 3', 'song_normalized', 'artist_normalized']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
====================================

