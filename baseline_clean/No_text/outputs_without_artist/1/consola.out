2025-09-19 02:57:46.090465: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-09-19 02:57:46.158624: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-09-19 02:57:51.723593: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_without_artist_1.py:288: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
For TF-IDF embbedings you are selecteing this columns:
--> ['emotion', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Release Date', 'Key', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
After removing some columns that doesn't love me, for TF-IDF embbedings you are selecteing this columns:
--> ['emotion', 'Time signature', 'song', 'Genre', 'Album', 'Release Date', 'Key', 'Similar Song 1', 'Similar Song 2', 'Similar Song 3', 'song_normalized', 'artist_normalized']
After removing some columns that doesn't love me, for numeric cols you are selecteing this columns:
--> ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
--> PaTH:  ../../data/embbedings_khipu/LB_fuss/lb_khipu_B.npy

##################################################
Running experiment with TFIDF embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [5020, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4678, Test Loss: 0.4132, F1: 0.8169, AUC: 0.8942
Epoch [10/30] Train Loss: 0.2335, Test Loss: 0.2691, F1: 0.8823, AUC: 0.9561
Epoch [20/30] Train Loss: 0.1967, Test Loss: 0.2732, F1: 0.8827, AUC: 0.9577
Mejores resultados en la época:  16
f1-score 0.8878977820636451
AUC según el mejor F1-score 0.9571706834775554

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4807, Test Loss: 0.4216, F1: 0.7997, AUC: 0.8926
Epoch [10/30] Train Loss: 0.2335, Test Loss: 0.2756, F1: 0.8834, AUC: 0.9541
Epoch [20/30] Train Loss: 0.1926, Test Loss: 0.2763, F1: 0.8848, AUC: 0.9559
Mejores resultados en la época:  25
f1-score 0.8889422499699844
AUC según el mejor F1-score 0.9578013030692265

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4731, Test Loss: 0.4231, F1: 0.7970, AUC: 0.8901
Epoch [10/30] Train Loss: 0.2206, Test Loss: 0.3437, F1: 0.8641, AUC: 0.9543
Epoch [20/30] Train Loss: 0.1768, Test Loss: 0.2927, F1: 0.8815, AUC: 0.9549
Mejores resultados en la época:  23
f1-score 0.8865830115830116
AUC según el mejor F1-score 0.9570854741301604

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4612, Test Loss: 0.4180, F1: 0.7847, AUC: 0.9041
Epoch [10/30] Train Loss: 0.2232, Test Loss: 0.2683, F1: 0.8833, AUC: 0.9572
Epoch [20/30] Train Loss: 0.1765, Test Loss: 0.2677, F1: 0.8893, AUC: 0.9580
Mejores resultados en la época:  27
f1-score 0.8908540278619019
AUC según el mejor F1-score 0.959168258565732

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4651, Test Loss: 0.4271, F1: 0.7839, AUC: 0.8931
Epoch [10/30] Train Loss: 0.2230, Test Loss: 0.2968, F1: 0.8621, AUC: 0.9521
Epoch [20/30] Train Loss: 0.1817, Test Loss: 0.3615, F1: 0.8641, AUC: 0.9540
Mejores resultados en la época:  18
f1-score 0.8840873821177032
AUC según el mejor F1-score 0.9550345451488873
Epoch [0/30] Train Loss: 0.4526, Test Loss: 0.3662, F1: 0.7237, AUC: 0.9083
Epoch [10/30] Train Loss: 0.2233, Test Loss: 0.2519, F1: 0.7706, AUC: 0.9532
Epoch [20/30] Train Loss: 0.1847, Test Loss: 0.3426, F1: 0.7506, AUC: 0.9570
Mejores resultados en la época:  26
f1-score 0.8142050349382598
AUC según el mejor F1-score 0.9583759301501658
Confusion matrix Test saved: outputs_without_artist/1/tfidf/cm_mlp_1.png

========================================
Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4566, Test Loss: 0.3744, F1: 0.8345, AUC: 0.9166
Epoch [10/30] Train Loss: 0.2077, Test Loss: 0.2820, F1: 0.8763, AUC: 0.9566
Epoch [20/30] Train Loss: 0.1379, Test Loss: 0.2646, F1: 0.8919, AUC: 0.9621
Mejores resultados en la época:  21
f1-score 0.8948050418188244
AUC según el mejor F1-score 0.9628275102251217

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4623, Test Loss: 0.4141, F1: 0.8236, AUC: 0.9171
Epoch [10/30] Train Loss: 0.2092, Test Loss: 0.2873, F1: 0.8716, AUC: 0.9577
Epoch [20/30] Train Loss: 0.1438, Test Loss: 0.2831, F1: 0.8948, AUC: 0.9628
Mejores resultados en la época:  25
f1-score 0.8969770253929867
AUC según el mejor F1-score 0.9612488192754342

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4711, Test Loss: 0.4273, F1: 0.8208, AUC: 0.9040
Epoch [10/30] Train Loss: 0.2061, Test Loss: 0.2838, F1: 0.8900, AUC: 0.9583
Epoch [20/30] Train Loss: 0.1405, Test Loss: 0.3364, F1: 0.8860, AUC: 0.9612
Mejores resultados en la época:  22
f1-score 0.8932863906762171
AUC según el mejor F1-score 0.9611570959903252

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4589, Test Loss: 0.4720, F1: 0.8012, AUC: 0.9236
Epoch [10/30] Train Loss: 0.2131, Test Loss: 0.4094, F1: 0.8530, AUC: 0.9554
Epoch [20/30] Train Loss: 0.1358, Test Loss: 0.3362, F1: 0.8833, AUC: 0.9582
Mejores resultados en la época:  24
f1-score 0.8951827644638102
AUC según el mejor F1-score 0.9602440230999112

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4688, Test Loss: 0.4307, F1: 0.7911, AUC: 0.9062
Epoch [10/30] Train Loss: 0.2119, Test Loss: 0.3099, F1: 0.8818, AUC: 0.9544
Epoch [20/30] Train Loss: 0.1393, Test Loss: 0.3281, F1: 0.8827, AUC: 0.9582
Mejores resultados en la época:  22
f1-score 0.8919980363279333
AUC según el mejor F1-score 0.9605388355281818
Epoch [0/30] Train Loss: 0.4465, Test Loss: 0.3134, F1: 0.7287, AUC: 0.9201
Epoch [10/30] Train Loss: 0.2135, Test Loss: 0.4689, F1: 0.6888, AUC: 0.9551
Epoch [20/30] Train Loss: 0.1361, Test Loss: 0.4136, F1: 0.7393, AUC: 0.9607
Mejores resultados en la época:  19
f1-score 0.8295800365185636
AUC según el mejor F1-score 0.9642136479306587
Confusion matrix Test saved: outputs_without_artist/1/tfidf/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160705': {'accuracy_cv_mean': 0.8869, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8821, 'precision_cv_std': 0.0062, 'recall_cv_mean': 0.8934, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.8877, 'f1_cv_std': 0.0023, 'params': 160705, 'accuracy_test': 0.9102, 'precision_test': 0.8044, 'recall_test': 0.8242, 'f1_score_test': 0.8142}, 'MLP_5840897': {'accuracy_cv_mean': 0.8941, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.892, 'precision_cv_std': 0.0111, 'recall_cv_mean': 0.8972, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8944, 'f1_cv_std': 0.0017, 'params': 5840897, 'accuracy_test': 0.9223, 'precision_test': 0.8704, 'recall_test': 0.7924, 'f1_score_test': 0.8296}}}
Saved on: outputs_without_artist/1/tfidf

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8787, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8921, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8617, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.8766, 'f1_cv_std': 0.0044}
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:44:01] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:46:49] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:49:36] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:52:23] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:55:10] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:58:05] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 43, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.90      0.93     16465
           1       0.73      0.85      0.79      5160

    accuracy                           0.89     21625
   macro avg       0.84      0.88      0.86     21625
weighted avg       0.90      0.89      0.89     21625

Confusion matrix Test saved as: outputs_without_artist/1/tfidf/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_without_artist/1/tfidf/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7306, 'accuracy_cv_std': 0.0181, 'precision_cv_mean': 0.6946, 'precision_cv_std': 0.035, 'recall_cv_mean': 0.8328, 'recall_cv_std': 0.0389, 'f1_cv_mean': 0.7557, 'f1_cv_std': 0.0082}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 43, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.63      0.75     16465
           1       0.42      0.85      0.56      5160

    accuracy                           0.68     21625
   macro avg       0.67      0.74      0.65     21625
weighted avg       0.81      0.68      0.70     21625

Confusion matrix Test saved as: outputs_without_artist/1/tfidf/conf_matrix_test_svm.png
Modelo guardado como: outputs_without_artist/1/tfidf/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8287, 'accuracy_cv_std': 0.0058, 'precision_cv_mean': 0.8801, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.7611, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8162, 'f1_cv_std': 0.007}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 43, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.91      0.91     16465
           1       0.71      0.74      0.73      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.82      0.82     21625
weighted avg       0.87      0.87      0.87     21625

Confusion matrix Test saved as: outputs_without_artist/1/tfidf/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_without_artist/1/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8194, 'accuracy_cv_std': 0.0079, 'precision_cv_mean': 0.8515, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.7738, 'recall_cv_std': 0.0109, 'f1_cv_mean': 0.8108, 'f1_cv_std': 0.0088}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 43, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.87      0.89     16465
           1       0.64      0.76      0.70      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.81      0.80     21625
weighted avg       0.85      0.84      0.85     21625

Confusion matrix Test saved as: outputs_without_artist/1/tfidf/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_without_artist/1/tfidf/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9034, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8471, 'recall_cv_std': 0.0097, 'f1_cv_mean': 0.8743, 'f1_cv_std': 0.0054}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 43, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.92      0.93     16465
           1       0.76      0.84      0.80      5160

    accuracy                           0.90     21625
   macro avg       0.85      0.88      0.86     21625
weighted avg       0.90      0.90      0.90     21625

Confusion matrix Test saved as: outputs_without_artist/1/tfidf/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_without_artist/1/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8459, 'accuracy_cv_std': 0.0062, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8402, 'f1_cv_std': 0.0071}
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.89      0.91     16465
           1       0.69      0.81      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.85      0.83     21625
weighted avg       0.88      0.87      0.87     21625

Confusion matrix Test saved as: outputs_without_artist/1/tfidf/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_without_artist/1/tfidf/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9034, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8471, 'recall_cv_std': 0.0097, 'f1_cv_mean': 0.8743, 'f1_cv_std': 0.0054, 'accuracy_test': 0.897, 'precision_test': 0.7561, 'recall_test': 0.8391, 'f1_score_test': 0.7954}
Logistic Regression: {'accuracy_cv_mean': 0.8787, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8921, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8617, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.8766, 'f1_cv_std': 0.0044, 'accuracy_test': 0.8899, 'precision_test': 0.7313, 'recall_test': 0.8514, 'f1_score_test': 0.7868}
Naive Bayes: {'accuracy_cv_mean': 0.8459, 'accuracy_cv_std': 0.0062, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8402, 'f1_cv_std': 0.0071, 'accuracy_test': 0.8681, 'precision_test': 0.6917, 'recall_test': 0.8066, 'f1_score_test': 0.7447}
Decision Tree: {'accuracy_cv_mean': 0.8287, 'accuracy_cv_std': 0.0058, 'precision_cv_mean': 0.8801, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.7611, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8162, 'f1_cv_std': 0.007, 'accuracy_test': 0.8667, 'precision_test': 0.7132, 'recall_test': 0.7382, 'f1_score_test': 0.7255}
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_without_artist_1.py:288: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Random Forest: {'accuracy_cv_mean': 0.8194, 'accuracy_cv_std': 0.0079, 'precision_cv_mean': 0.8515, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.7738, 'recall_cv_std': 0.0109, 'f1_cv_mean': 0.8108, 'f1_cv_std': 0.0088, 'accuracy_test': 0.8422, 'precision_test': 0.6433, 'recall_test': 0.7605, 'f1_score_test': 0.697}
SVM: {'accuracy_cv_mean': 0.7306, 'accuracy_cv_std': 0.0181, 'precision_cv_mean': 0.6946, 'precision_cv_std': 0.035, 'recall_cv_mean': 0.8328, 'recall_cv_std': 0.0389, 'f1_cv_mean': 0.7557, 'f1_cv_std': 0.0082, 'accuracy_test': 0.6794, 'precision_test': 0.4158, 'recall_test': 0.8484, 'f1_score_test': 0.5581}
resultados_globales con ML {'tfidf': {'MLP_160705': {'accuracy_cv_mean': 0.8869, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8821, 'precision_cv_std': 0.0062, 'recall_cv_mean': 0.8934, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.8877, 'f1_cv_std': 0.0023, 'params': 160705, 'accuracy_test': 0.9102, 'precision_test': 0.8044, 'recall_test': 0.8242, 'f1_score_test': 0.8142}, 'MLP_5840897': {'accuracy_cv_mean': 0.8941, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.892, 'precision_cv_std': 0.0111, 'recall_cv_mean': 0.8972, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8944, 'f1_cv_std': 0.0017, 'params': 5840897, 'accuracy_test': 0.9223, 'precision_test': 0.8704, 'recall_test': 0.7924, 'f1_score_test': 0.8296}, 'Logistic Regression': {'accuracy_cv_mean': 0.8787, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8921, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8617, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.8766, 'f1_cv_std': 0.0044, 'accuracy_test': 0.8899, 'precision_test': 0.7313, 'recall_test': 0.8514, 'f1_score_test': 0.7868}, 'SVM': {'accuracy_cv_mean': 0.7306, 'accuracy_cv_std': 0.0181, 'precision_cv_mean': 0.6946, 'precision_cv_std': 0.035, 'recall_cv_mean': 0.8328, 'recall_cv_std': 0.0389, 'f1_cv_mean': 0.7557, 'f1_cv_std': 0.0082, 'accuracy_test': 0.6794, 'precision_test': 0.4158, 'recall_test': 0.8484, 'f1_score_test': 0.5581}, 'Decision Tree': {'accuracy_cv_mean': 0.8287, 'accuracy_cv_std': 0.0058, 'precision_cv_mean': 0.8801, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.7611, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8162, 'f1_cv_std': 0.007, 'accuracy_test': 0.8667, 'precision_test': 0.7132, 'recall_test': 0.7382, 'f1_score_test': 0.7255}, 'Random Forest': {'accuracy_cv_mean': 0.8194, 'accuracy_cv_std': 0.0079, 'precision_cv_mean': 0.8515, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.7738, 'recall_cv_std': 0.0109, 'f1_cv_mean': 0.8108, 'f1_cv_std': 0.0088, 'accuracy_test': 0.8422, 'precision_test': 0.6433, 'recall_test': 0.7605, 'f1_score_test': 0.697}, 'XGBoost': {'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9034, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8471, 'recall_cv_std': 0.0097, 'f1_cv_mean': 0.8743, 'f1_cv_std': 0.0054, 'accuracy_test': 0.897, 'precision_test': 0.7561, 'recall_test': 0.8391, 'f1_score_test': 0.7954}, 'Naive Bayes': {'accuracy_cv_mean': 0.8459, 'accuracy_cv_std': 0.0062, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8402, 'f1_cv_std': 0.0071, 'accuracy_test': 0.8681, 'precision_test': 0.6917, 'recall_test': 0.8066, 'f1_score_test': 0.7447}}}

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 320)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 320)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [320, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4776, Test Loss: 0.4044, F1: 0.8119, AUC: 0.8981
Epoch [10/30] Train Loss: 0.3211, Test Loss: 0.3294, F1: 0.8558, AUC: 0.9329
Epoch [20/30] Train Loss: 0.2964, Test Loss: 0.3198, F1: 0.8576, AUC: 0.9375
Mejores resultados en la época:  27
f1-score 0.8667239896818573
AUC según el mejor F1-score 0.9392713802693649

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4804, Test Loss: 0.4064, F1: 0.8211, AUC: 0.8973
Epoch [10/30] Train Loss: 0.3283, Test Loss: 0.3385, F1: 0.8579, AUC: 0.9306
Epoch [20/30] Train Loss: 0.3025, Test Loss: 0.3208, F1: 0.8637, AUC: 0.9369
Mejores resultados en la época:  28
f1-score 0.8657670454545454
AUC según el mejor F1-score 0.9387497664371884

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4549, Test Loss: 0.4127, F1: 0.8271, AUC: 0.9037
Epoch [10/30] Train Loss: 0.3201, Test Loss: 0.3376, F1: 0.8625, AUC: 0.9373
Epoch [20/30] Train Loss: 0.2969, Test Loss: 0.3109, F1: 0.8667, AUC: 0.9411
Mejores resultados en la época:  25
f1-score 0.8735438933589528
AUC según el mejor F1-score 0.9449190816357189

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4497, Test Loss: 0.3945, F1: 0.8178, AUC: 0.9038
Epoch [10/30] Train Loss: 0.3212, Test Loss: 0.3284, F1: 0.8605, AUC: 0.9367
Epoch [20/30] Train Loss: 0.2978, Test Loss: 0.3471, F1: 0.8556, AUC: 0.9389
Mejores resultados en la época:  28
f1-score 0.8701314076003315
AUC según el mejor F1-score 0.944551960242908

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4415, Test Loss: 0.4055, F1: 0.8228, AUC: 0.8986
Epoch [10/30] Train Loss: 0.3254, Test Loss: 0.3352, F1: 0.8536, AUC: 0.9324
Epoch [20/30] Train Loss: 0.2970, Test Loss: 0.3175, F1: 0.8582, AUC: 0.9376
Mejores resultados en la época:  27
f1-score 0.8667837431008529
AUC según el mejor F1-score 0.941056063022298
Epoch [0/30] Train Loss: 0.4657, Test Loss: 0.3988, F1: 0.7116, AUC: 0.8993
Epoch [10/30] Train Loss: 0.3243, Test Loss: 0.3107, F1: 0.7554, AUC: 0.9302
Epoch [20/30] Train Loss: 0.3031, Test Loss: 0.3193, F1: 0.7534, AUC: 0.9365
Mejores resultados en la época:  25
f1-score 0.7773444753946147
AUC según el mejor F1-score 0.937693892612236
Confusion matrix Test saved: outputs_without_artist/1/lyrics_bert/cm_mlp_1.png

========================================
Entrenando red 6 con capas [320, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4445, Test Loss: 0.4077, F1: 0.8004, AUC: 0.9070
Epoch [10/30] Train Loss: 0.2883, Test Loss: 0.3428, F1: 0.8672, AUC: 0.9421
Epoch [20/30] Train Loss: 0.2408, Test Loss: 0.2995, F1: 0.8770, AUC: 0.9495
Mejores resultados en la época:  29
f1-score 0.8819860877908371
AUC según el mejor F1-score 0.9484960843605401

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4428, Test Loss: 0.3864, F1: 0.8279, AUC: 0.9074
Epoch [10/30] Train Loss: 0.2945, Test Loss: 0.3184, F1: 0.8659, AUC: 0.9412
Epoch [20/30] Train Loss: 0.2394, Test Loss: 0.3226, F1: 0.8780, AUC: 0.9484
Mejores resultados en la época:  22
f1-score 0.8799336256963376
AUC según el mejor F1-score 0.9487854264317048

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4469, Test Loss: 0.3858, F1: 0.8088, AUC: 0.9111
Epoch [10/30] Train Loss: 0.2900, Test Loss: 0.3160, F1: 0.8632, AUC: 0.9420
Epoch [20/30] Train Loss: 0.2435, Test Loss: 0.3045, F1: 0.8696, AUC: 0.9442
Mejores resultados en la época:  26
f1-score 0.8776350860189
AUC según el mejor F1-score 0.9486113106371311

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4510, Test Loss: 0.4046, F1: 0.8253, AUC: 0.9101
Epoch [10/30] Train Loss: 0.2943, Test Loss: 0.3040, F1: 0.8732, AUC: 0.9456
Epoch [20/30] Train Loss: 0.2418, Test Loss: 0.2964, F1: 0.8753, AUC: 0.9488
Mejores resultados en la época:  29
f1-score 0.8826426426426427
AUC según el mejor F1-score 0.9525845056566419

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4479, Test Loss: 0.3891, F1: 0.8302, AUC: 0.9076
Epoch [10/30] Train Loss: 0.2903, Test Loss: 0.3081, F1: 0.8636, AUC: 0.9412
Epoch [20/30] Train Loss: 0.2389, Test Loss: 0.3087, F1: 0.8761, AUC: 0.9476
Mejores resultados en la época:  20
f1-score 0.8761225242957313
AUC según el mejor F1-score 0.9475570806167741
Epoch [0/30] Train Loss: 0.4349, Test Loss: 0.4257, F1: 0.7134, AUC: 0.9098
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Epoch [10/30] Train Loss: 0.2827, Test Loss: 0.3402, F1: 0.7450, AUC: 0.9412
Epoch [20/30] Train Loss: 0.2339, Test Loss: 0.3066, F1: 0.7758, AUC: 0.9492
Mejores resultados en la época:  17
f1-score 0.7969528274245532
AUC según el mejor F1-score 0.947170089478033
Confusion matrix Test saved: outputs_without_artist/1/lyrics_bert/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160705': {'accuracy_cv_mean': 0.8869, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8821, 'precision_cv_std': 0.0062, 'recall_cv_mean': 0.8934, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.8877, 'f1_cv_std': 0.0023, 'params': 160705, 'accuracy_test': 0.9102, 'precision_test': 0.8044, 'recall_test': 0.8242, 'f1_score_test': 0.8142}, 'MLP_5840897': {'accuracy_cv_mean': 0.8941, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.892, 'precision_cv_std': 0.0111, 'recall_cv_mean': 0.8972, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8944, 'f1_cv_std': 0.0017, 'params': 5840897, 'accuracy_test': 0.9223, 'precision_test': 0.8704, 'recall_test': 0.7924, 'f1_score_test': 0.8296}, 'Logistic Regression': {'accuracy_cv_mean': 0.8787, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8921, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8617, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.8766, 'f1_cv_std': 0.0044, 'accuracy_test': 0.8899, 'precision_test': 0.7313, 'recall_test': 0.8514, 'f1_score_test': 0.7868}, 'SVM': {'accuracy_cv_mean': 0.7306, 'accuracy_cv_std': 0.0181, 'precision_cv_mean': 0.6946, 'precision_cv_std': 0.035, 'recall_cv_mean': 0.8328, 'recall_cv_std': 0.0389, 'f1_cv_mean': 0.7557, 'f1_cv_std': 0.0082, 'accuracy_test': 0.6794, 'precision_test': 0.4158, 'recall_test': 0.8484, 'f1_score_test': 0.5581}, 'Decision Tree': {'accuracy_cv_mean': 0.8287, 'accuracy_cv_std': 0.0058, 'precision_cv_mean': 0.8801, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.7611, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8162, 'f1_cv_std': 0.007, 'accuracy_test': 0.8667, 'precision_test': 0.7132, 'recall_test': 0.7382, 'f1_score_test': 0.7255}, 'Random Forest': {'accuracy_cv_mean': 0.8194, 'accuracy_cv_std': 0.0079, 'precision_cv_mean': 0.8515, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.7738, 'recall_cv_std': 0.0109, 'f1_cv_mean': 0.8108, 'f1_cv_std': 0.0088, 'accuracy_test': 0.8422, 'precision_test': 0.6433, 'recall_test': 0.7605, 'f1_score_test': 0.697}, 'XGBoost': {'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9034, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8471, 'recall_cv_std': 0.0097, 'f1_cv_mean': 0.8743, 'f1_cv_std': 0.0054, 'accuracy_test': 0.897, 'precision_test': 0.7561, 'recall_test': 0.8391, 'f1_score_test': 0.7954}, 'Naive Bayes': {'accuracy_cv_mean': 0.8459, 'accuracy_cv_std': 0.0062, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8402, 'f1_cv_std': 0.0071, 'accuracy_test': 0.8681, 'precision_test': 0.6917, 'recall_test': 0.8066, 'f1_score_test': 0.7447}}, 'lyrics_bert': {'MLP_10305': {'accuracy_cv_mean': 0.8684, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8683, 'precision_cv_std': 0.0192, 'recall_cv_mean': 0.8698, 'recall_cv_std': 0.0206, 'f1_cv_mean': 0.8686, 'f1_cv_std': 0.0029, 'params': 10305, 'accuracy_test': 0.8891, 'precision_test': 0.7462, 'recall_test': 0.8112, 'f1_score_test': 0.7773}, 'MLP_1028097': {'accuracy_cv_mean': 0.8791, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8756, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.8841, 'recall_cv_std': 0.0128, 'f1_cv_mean': 0.8797, 'f1_cv_std': 0.0025, 'params': 1028097, 'accuracy_test': 0.9039, 'precision_test': 0.8033, 'recall_test': 0.7907, 'f1_score_test': 0.797}}}
Saved on: outputs_without_artist/1/lyrics_bert

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.863, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8298, 'recall_cv_std': 0.0088, 'f1_cv_mean': 0.8461, 'f1_cv_std': 0.0058}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 43, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.87      0.90     16465
           1       0.66      0.82      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.85      0.82     21625
weighted avg       0.87      0.86      0.86     21625

Confusion matrix Test saved as: outputs_without_artist/1/lyrics_bert/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_without_artist/1/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.6579, 'accuracy_cv_std': 0.0382, 'precision_cv_mean': 0.6313, 'precision_cv_std': 0.0428, 'recall_cv_mean': 0.7772, 'recall_cv_std': 0.0252, 'f1_cv_mean': 0.6952, 'f1_cv_std': 0.0202}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 43, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.86      0.56      0.68     16465
           1       0.34      0.71      0.46      5160

    accuracy                           0.60     21625
   macro avg       0.60      0.64      0.57     21625
weighted avg       0.74      0.60      0.63     21625

Confusion matrix Test saved as: outputs_without_artist/1/lyrics_bert/conf_matrix_test_svm.png
Modelo guardado como: outputs_without_artist/1/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.785, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8145, 'precision_cv_std': 0.016, 'recall_cv_mean': 0.7392, 'recall_cv_std': 0.0187, 'f1_cv_mean': 0.7746, 'f1_cv_std': 0.0049}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 43, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.91      0.82      0.87     16465
           1       0.57      0.74      0.64      5160

    accuracy                           0.80     21625
   macro avg       0.74      0.78      0.75     21625
weighted avg       0.83      0.80      0.81     21625

Confusion matrix Test saved as: outputs_without_artist/1/lyrics_bert/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_without_artist/1/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8506, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8866, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8043, 'recall_cv_std': 0.0113, 'f1_cv_mean': 0.8433, 'f1_cv_std': 0.0054}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 43, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.72      0.80      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.85      0.84     21625
weighted avg       0.88      0.88      0.88     21625

/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:24:06] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:24:37] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:25:09] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:25:41] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:26:12] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:26:44] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Confusion matrix Test saved as: outputs_without_artist/1/lyrics_bert/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_without_artist/1/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8752, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8943, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.851, 'recall_cv_std': 0.0118, 'f1_cv_mean': 0.872, 'f1_cv_std': 0.0062}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 43, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.90      0.92     16465
           1       0.73      0.84      0.78      5160

    accuracy                           0.89     21625
   macro avg       0.84      0.87      0.85     21625
weighted avg       0.90      0.89      0.89     21625

Confusion matrix Test saved as: outputs_without_artist/1/lyrics_bert/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_without_artist/1/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7742, 'accuracy_cv_std': 0.0057, 'precision_cv_mean': 0.7565, 'precision_cv_std': 0.0062, 'recall_cv_mean': 0.8089, 'recall_cv_std': 0.01, 'f1_cv_mean': 0.7817, 'f1_cv_std': 0.0059}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.74      0.82     16465
           1       0.49      0.80      0.61      5160

    accuracy                           0.76     21625
   macro avg       0.71      0.77      0.72     21625
weighted avg       0.82      0.76      0.77     21625

Confusion matrix Test saved as: outputs_without_artist/1/lyrics_bert/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_without_artist/1/lyrics_bert/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.8752, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8943, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.851, 'recall_cv_std': 0.0118, 'f1_cv_mean': 0.872, 'f1_cv_std': 0.0062, 'accuracy_test': 0.8881, 'precision_test': 0.7301, 'recall_test': 0.8426, 'f1_score_test': 0.7824}
Random Forest: {'accuracy_cv_mean': 0.8506, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8866, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8043, 'recall_cv_std': 0.0113, 'f1_cv_mean': 0.8433, 'f1_cv_std': 0.0054, 'accuracy_test': 0.8763, 'precision_test': 0.7172, 'recall_test': 0.7952, 'f1_score_test': 0.7542}
Logistic Regression: {'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.863, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8298, 'recall_cv_std': 0.0088, 'f1_cv_mean': 0.8461, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8581, 'precision_test': 0.6639, 'recall_test': 0.8205, 'f1_score_test': 0.734}
Decision Tree: {'accuracy_cv_mean': 0.785, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8145, 'precision_cv_std': 0.016, 'recall_cv_mean': 0.7392, 'recall_cv_std': 0.0187, 'f1_cv_mean': 0.7746, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8047, 'precision_test': 0.5699, 'recall_test': 0.7403, 'f1_score_test': 0.644}
Naive Bayes: {'accuracy_cv_mean': 0.7742, 'accuracy_cv_std': 0.0057, 'precision_cv_mean': 0.7565, 'precision_cv_std': 0.0062, 'recall_cv_mean': 0.8089, 'recall_cv_std': 0.01, 'f1_cv_mean': 0.7817, 'f1_cv_std': 0.0059, 'accuracy_test': 0.755, 'precision_test': 0.4918, 'recall_test': 0.8004, 'f1_score_test': 0.6092}
SVM: {'accuracy_cv_mean': 0.6579, 'accuracy_cv_std': 0.0382, 'precision_cv_mean': 0.6313, 'precision_cv_std': 0.0428, 'recall_cv_mean': 0.7772, 'recall_cv_std': 0.0252, 'f1_cv_mean': 0.6952, 'f1_cv_std': 0.0202, 'accuracy_test': 0.6003, 'precision_test': 0.3394, 'recall_test': 0.7136, 'f1_score_test': 0.46}
resultados_globales con ML {'tfidf': {'MLP_160705': {'accuracy_cv_mean': 0.8869, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8821, 'precision_cv_std': 0.0062, 'recall_cv_mean': 0.8934, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.8877, 'f1_cv_std': 0.0023, 'params': 160705, 'accuracy_test': 0.9102, 'precision_test': 0.8044, 'recall_test': 0.8242, 'f1_score_test': 0.8142}, 'MLP_5840897': {'accuracy_cv_mean': 0.8941, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.892, 'precision_cv_std': 0.0111, 'recall_cv_mean': 0.8972, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8944, 'f1_cv_std': 0.0017, 'params': 5840897, 'accuracy_test': 0.9223, 'precision_test': 0.8704, 'recall_test': 0.7924, 'f1_score_test': 0.8296}, 'Logistic Regression': {'accuracy_cv_mean': 0.8787, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8921, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8617, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.8766, 'f1_cv_std': 0.0044, 'accuracy_test': 0.8899, 'precision_test': 0.7313, 'recall_test': 0.8514, 'f1_score_test': 0.7868}, 'SVM': {'accuracy_cv_mean': 0.7306, 'accuracy_cv_std': 0.0181, 'precision_cv_mean': 0.6946, 'precision_cv_std': 0.035, 'recall_cv_mean': 0.8328, 'recall_cv_std': 0.0389, 'f1_cv_mean': 0.7557, 'f1_cv_std': 0.0082, 'accuracy_test': 0.6794, 'precision_test': 0.4158, 'recall_test': 0.8484, 'f1_score_test': 0.5581}, 'Decision Tree': {'accuracy_cv_mean': 0.8287, 'accuracy_cv_std': 0.0058, 'precision_cv_mean': 0.8801, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.7611, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8162, 'f1_cv_std': 0.007, 'accuracy_test': 0.8667, 'precision_test': 0.7132, 'recall_test': 0.7382, 'f1_score_test': 0.7255}, 'Random Forest': {'accuracy_cv_mean': 0.8194, 'accuracy_cv_std': 0.0079, 'precision_cv_mean': 0.8515, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.7738, 'recall_cv_std': 0.0109, 'f1_cv_mean': 0.8108, 'f1_cv_std': 0.0088, 'accuracy_test': 0.8422, 'precision_test': 0.6433, 'recall_test': 0.7605, 'f1_score_test': 0.697}, 'XGBoost': {'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9034, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8471, 'recall_cv_std': 0.0097, 'f1_cv_mean': 0.8743, 'f1_cv_std': 0.0054, 'accuracy_test': 0.897, 'precision_test': 0.7561, 'recall_test': 0.8391, 'f1_score_test': 0.7954}, 'Naive Bayes': {'accuracy_cv_mean': 0.8459, 'accuracy_cv_std': 0.0062, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8402, 'f1_cv_std': 0.0071, 'accuracy_test': 0.8681, 'precision_test': 0.6917, 'recall_test': 0.8066, 'f1_score_test': 0.7447}}, 'lyrics_bert': {'MLP_10305': {'accuracy_cv_mean': 0.8684, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8683, 'precision_cv_std': 0.0192, 'recall_cv_mean': 0.8698, 'recall_cv_std': 0.0206, 'f1_cv_mean': 0.8686, 'f1_cv_std': 0.0029, 'params': 10305, 'accuracy_test': 0.8891, 'precision_test': 0.7462, 'recall_test': 0.8112, 'f1_score_test': 0.7773}, 'MLP_1028097': {'accuracy_cv_mean': 0.8791, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8756, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.8841, 'recall_cv_std': 0.0128, 'f1_cv_mean': 0.8797, 'f1_cv_std': 0.0025, 'params': 1028097, 'accuracy_test': 0.9039, 'precision_test': 0.8033, 'recall_test': 0.7907, 'f1_score_test': 0.797}, 'Logistic Regression': {'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.863, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8298, 'recall_cv_std': 0.0088, 'f1_cv_mean': 0.8461, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8581, 'precision_test': 0.6639, 'recall_test': 0.8205, 'f1_score_test': 0.734}, 'SVM': {'accuracy_cv_mean': 0.6579, 'accuracy_cv_std': 0.0382, 'precision_cv_mean': 0.6313, 'precision_cv_std': 0.0428, 'recall_cv_mean': 0.7772, 'recall_cv_std': 0.0252, 'f1_cv_mean': 0.6952, 'f1_cv_std': 0.0202, 'accuracy_test': 0.6003, 'precision_test': 0.3394, 'recall_test': 0.7136, 'f1_score_test': 0.46}, 'Decision Tree': {'accuracy_cv_mean': 0.785, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8145, 'precision_cv_std': 0.016, 'recall_cv_mean': 0.7392, 'recall_cv_std': 0.0187, 'f1_cv_mean': 0.7746, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8047, 'precision_test': 0.5699, 'recall_test': 0.7403, 'f1_score_test': 0.644}, 'Random Forest': {'accuracy_cv_mean': 0.8506, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8866, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8043, 'recall_cv_std': 0.0113, 'f1_cv_mean': 0.8433, 'f1_cv_std': 0.0054, 'accuracy_test': 0.8763, 'precision_test': 0.7172, 'recall_test': 0.7952, 'f1_score_test': 0.7542}, 'XGBoost': {'accuracy_cv_mean': 0.8752, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8943, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.851, 'recall_cv_std': 0.0118, 'f1_cv_mean': 0.872, 'f1_cv_std': 0.0062, 'accuracy_test': 0.8881, 'precision_test': 0.7301, 'recall_test': 0.8426, 'f1_score_test': 0.7824}, 'Naive Bayes': {'accuracy_cv_mean': 0.7742, 'accuracy_cv_std': 0.0057, 'precision_cv_mean': 0.7565, 'precision_cv_std': 0.0062, 'recall_cv_mean': 0.8089, 'recall_cv_std': 0.01, 'f1_cv/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_without_artist_1.py:288: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
_mean': 0.7817, 'f1_cv_std': 0.0059, 'accuracy_test': 0.755, 'precision_test': 0.4918, 'recall_test': 0.8004, 'f1_score_test': 0.6092}}}

##################################################
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
E eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El último valor de eliminar es:  98849
Shape original y: (108138,)
Shape filtrado  y: (108125,)
Label distribution: {0: 82336, 1: 25802}
X shape: (108125, 1536)
y shape: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1556)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1556)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [1556, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4427, Test Loss: 0.4102, F1: 0.7948, AUC: 0.9089
Epoch [10/30] Train Loss: 0.3262, Test Loss: 0.3537, F1: 0.8334, AUC: 0.9314
Epoch [20/30] Train Loss: 0.3118, Test Loss: 0.3280, F1: 0.8536, AUC: 0.9360
Mejores resultados en la época:  26
f1-score 0.8688039749201467
AUC según el mejor F1-score 0.9379830877050658

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4260, Test Loss: 0.3778, F1: 0.8337, AUC: 0.9149
Epoch [10/30] Train Loss: 0.3228, Test Loss: 0.3269, F1: 0.8502, AUC: 0.9365
Epoch [20/30] Train Loss: 0.3068, Test Loss: 0.3215, F1: 0.8549, AUC: 0.9380
Mejores resultados en la época:  25
f1-score 0.8664688427299704
AUC según el mejor F1-score 0.9411347772444564

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4204, Test Loss: 0.4016, F1: 0.8340, AUC: 0.9120
Epoch [10/30] Train Loss: 0.3262, Test Loss: 0.3366, F1: 0.8588, AUC: 0.9330
Epoch [20/30] Train Loss: 0.3081, Test Loss: 0.3220, F1: 0.8599, AUC: 0.9372
Mejores resultados en la época:  29
f1-score 0.864775413711584
AUC según el mejor F1-score 0.9378618756478728

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4193, Test Loss: 0.3712, F1: 0.8290, AUC: 0.9159
Epoch [10/30] Train Loss: 0.3317, Test Loss: 0.3332, F1: 0.8634, AUC: 0.9368
Epoch [20/30] Train Loss: 0.3107, Test Loss: 0.3125, F1: 0.8659, AUC: 0.9403
Mejores resultados en la época:  19
f1-score 0.8700840131498844
AUC según el mejor F1-score 0.9394882596269979

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4302, Test Loss: 0.3979, F1: 0.8043, AUC: 0.9073
Epoch [10/30] Train Loss: 0.3264, Test Loss: 0.3626, F1: 0.8321, AUC: 0.9308
Epoch [20/30] Train Loss: 0.3043, Test Loss: 0.3294, F1: 0.8541, AUC: 0.9347
Mejores resultados en la época:  23
f1-score 0.862279755483639
AUC según el mejor F1-score 0.9361109917578135
Epoch [0/30] Train Loss: 0.4293, Test Loss: 0.3830, F1: 0.7141, AUC: 0.9122
Epoch [10/30] Train Loss: 0.3270, Test Loss: 0.2819, F1: 0.7647, AUC: 0.9322
Epoch [20/30] Train Loss: 0.3070, Test Loss: 0.3078, F1: 0.7574, AUC: 0.9345
Mejores resultados en la época:  21
f1-score 0.7732292460015232
AUC según el mejor F1-score 0.9346454247558245
Confusion matrix Test saved: outputs_without_artist/1/gpt/cm_mlp_1.png

========================================
Entrenando red 6 con capas [1556, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4290, Test Loss: 0.3871, F1: 0.8172, AUC: 0.9176
Epoch [10/30] Train Loss: 0.3163, Test Loss: 0.3208, F1: 0.8534, AUC: 0.9384
Epoch [20/30] Train Loss: 0.2771, Test Loss: 0.3004, F1: 0.8753, AUC: 0.9449
Mejores resultados en la época:  24
f1-score 0.8793514544587506
AUC según el mejor F1-score 0.9465140867717536

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4293, Test Loss: 0.3750, F1: 0.8213, AUC: 0.9162
Epoch [10/30] Train Loss: 0.3150, Test Loss: 0.3513, F1: 0.8572, AUC: 0.9375
Epoch [20/30] Train Loss: 0.2816, Test Loss: 0.3046, F1: 0.8711, AUC: 0.9439
Mejores resultados en la época:  29
f1-score 0.8795373446536238
AUC según el mejor F1-score 0.9467776078285559

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4213, Test Loss: 0.3741, F1: 0.8426, AUC: 0.9172
Epoch [10/30] Train Loss: 0.3195, Test Loss: 0.3659, F1: 0.8568, AUC: 0.9372
Epoch [20/30] Train Loss: 0.2805, Test Loss: 0.3162, F1: 0.8676, AUC: 0.9441
Mejores resultados en la época:  22
f1-score 0.8727756930153734
AUC según el mejor F1-score 0.9414460378357671

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4236, Test Loss: 0.3699, F1: 0.8187, AUC: 0.9176
Epoch [10/30] Train Loss: 0.3149, Test Loss: 0.3213, F1: 0.8692, AUC: 0.9409
Epoch [20/30] Train Loss: 0.2827, Test Loss: 0.3118, F1: 0.8574, AUC: 0.9421
Mejores resultados en la época:  27
f1-score 0.8833516550629046
AUC según el mejor F1-score 0.9486048166921182

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4224, Test Loss: 0.3978, F1: 0.8150, AUC: 0.9087
Epoch [10/30] Train Loss: 0.3152, Test Loss: 0.3320, F1: 0.8495, AUC: 0.9335
Epoch [20/30] Train Loss: 0.2783, Test Loss: 0.3514, F1: 0.8362, AUC: 0.9383
Mejores resultados en la época:  16
f1-score 0.870836328780254
AUC según el mejor F1-score 0.9405600913721888
Epoch [0/30] Train Loss: 0.4163, Test Loss: 0.5412, F1: 0.6460, AUC: 0.9176
Epoch [10/30] Train Loss: 0.3141, Test Loss: 0.2841, F1: 0.7655, AUC: 0.9353
Epoch [20/30] Train Loss: 0.2740, Test Loss: 0.3606, F1: 0.7495, AUC: 0.9444
Mejores resultados en la época:  26
f1-score 0.8014643762320474
AUC según el mejor F1-score 0.9483019830648521
Confusion matrix Test saved: outputs_without_artist/1/gpt/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160705': {'accuracy_cv_mean': 0.8869, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8821, 'precision_cv_std': 0.0062, 'recall_cv_mean': 0.8934, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.8877, 'f1_cv_std': 0.0023, 'params': 160705, 'accuracy_test': 0.9102, 'precision_test': 0.8044, 'recall_test': 0.8242, 'f1_score_test': 0.8142}, 'MLP_5840897': {'accuracy_cv_mean': 0.8941, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.892, 'precision_cv_std': 0.0111, 'recall_cv_mean': 0.8972, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8944, 'f1_cv_std': 0.0017, 'params': 5840897, 'accuracy_test': 0.9223, 'precision_test': 0.8704, 'recall_test': 0.7924, 'f1_score_test': 0.8296}, 'Logistic Regression': {'accuracy_cv_mean': 0.8787, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8921, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8617, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.8766, 'f1_cv_std': 0.0044, 'accuracy_test': 0.8899, 'precision_test': 0.7313, 'recall_test': 0.8514, 'f1_score_test': 0.7868}, 'SVM': {'accuracy_cv_mean': 0.7306, 'accuracy_cv_std': 0.0181, 'precision_cv_mean': 0.6946, 'precision_cv_std': 0.035, 'recall_cv_mean': 0.8328, 'recall_cv_std': 0.0389, 'f1_cv_mean': 0.7557, 'f1_cv_std': 0.0082, 'accuracy_test': 0.6794, 'precision_test': 0.4158, 'recall_test': 0.8484, 'f1_score_test': 0.5581}, 'Decision Tree': {'accuracy_cv_mean': 0.8287, 'accuracy_cv_std': 0.0058, 'precision_cv_mean': 0.8801, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.7611, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8162, 'f1_cv_std': 0.007, 'accuracy_test': 0.8667, 'precision_test': 0.7132, 'recall_test': 0.7382, 'f1_score_test': 0.7255}, 'Random Forest': {'accuracy_cv_mean': 0.8194, 'accuracy_cv_std': 0.0079, 'precision_cv_mean': 0.8515, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.7738, 'recall_cv_std': 0.0109, 'f1_cv_mean': 0.8108, 'f1_cv_std': 0.0088, 'accuracy_test': 0.8422, 'precision_test': 0.6433, 'recall_test': 0.7605, 'f1_score_test': 0.697}, 'XGBoost': {'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9034, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8471, 'recall_cv_std': 0.0097, 'f1_cv_mean': 0.8743, 'f1_cv_std': 0.0054, 'accuracy_test': 0.897, 'precision_test': 0.7561, 'recall_test': 0.8391, 'f1_score_test': 0.7954}, 'Naive Bayes': {'accuracy_cv_mean': 0.8459, 'accuracy_cv_std': 0.0062, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8402, 'f1_cv_std': 0.0071, 'accuracy_test': 0.8681, 'precision_test': 0.6917, 'recall_test': 0.8066, 'f1_score_test': 0.7447}}, 'lyrics_bert': {'MLP_10305': {'accuracy_cv_mean': 0.8684, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8683, 'precision_cv_std': 0.0192, 'recall_cv_mean': 0.8698, 'recall_cv_std': 0.0206, 'f1_cv_mean': 0.8686, 'f1_cv_std': 0.0029, 'params': 10305, 'accuracy_test': 0.8891, 'precision_test': 0.7462, 'recall_test': 0.8112, 'f1_score_test': 0.7773}, 'MLP_1028097': {'accuracy_cv_mean': 0.8791, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8756, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.8841, 'recall_cv_std': 0.0128, 'f1_cv_mean': 0.8797, 'f1_cv_std': 0.0025, 'params': 1028097, 'accuracy_test': 0.9039, 'precision_test': 0.8033, 'recall_test': 0.7907, 'f1_score_test': 0.797}, 'Logistic Regression': {'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.863, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8298, 'recall_cv_std': 0.0088, 'f1_cv_mean': 0.8461, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8581, 'precision_test': 0.6639, 'recall_test': 0.8205, 'f1_score_test': 0.734}, 'SVM': {'accuracy_cv_mean': 0.6579, 'accuracy_cv_std': 0.0382, 'precision_cv_mean': 0.6313, 'precision_cv_std': 0.0428, 'recall_cv_mean': 0.7772, 'recall_cv_std': 0.0252, 'f1_cv_mean': 0.6952, 'f1_cv_std': 0.0202, 'accuracy_test': 0.6003, 'precision_test': 0.3394, 'recall_test': 0.7136, 'f1_score_test': 0.46}, 'Decision Tree': {'accuracy_cv_mean': 0.785, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8145, 'precision_cv_std': 0.016, 'recall_cv_mean': 0.7392, 'recall_cv_std': 0.0187, 'f1_cv_mean': 0.7746, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8047, 'precision_test': 0.5699, 'recall_test': 0.7403, 'f1_score_test': 0.644}, 'Random Forest': {'accuracy_cv_mean': 0.8506, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8866, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8043, 'recall_cv_std': 0.0113, 'f1_cv_mean': 0.8433, 'f1_cv_std': 0.0054, 'accuracy_test': 0.8763, 'precision_test': 0.7172, 'recall_test': 0.7952, 'f1_score_test': 0.7542}, 'XGBoost': {'accuracy_cv_mean': 0.8752, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8943, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.851, 'recall_cv_std': 0.0118, 'f1_cv_mean': 0.872, 'f1_cv_std': 0.0062, 'accuracy_test': 0.8881, 'precision_test': 0.7301, 'recall_test': 0.8426, 'f1_score_test': 0.7824}, 'Naive Bayes': {'accuracy_cv_mean': 0.7742, 'accuracy_cv_std': 0.0057, 'precision_cv_mean': 0.7565, 'precision_cv_std': 0.0062, 'recall_cv_mean': 0.8089, 'recall_cv_std': 0.01, 'f1_cv/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:20:04] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:22:49] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:25:38] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:28:26] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:31:15] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:34:06] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
_mean': 0.7817, 'f1_cv_std': 0.0059, 'accuracy_test': 0.755, 'precision_test': 0.4918, 'recall_test': 0.8004, 'f1_score_test': 0.6092}}, 'gpt': {'MLP_49857': {'accuracy_cv_mean': 0.8645, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.8541, 'precision_cv_std': 0.0105, 'recall_cv_mean': 0.8794, 'recall_cv_std': 0.0092, 'f1_cv_mean': 0.8665, 'f1_cv_std': 0.0028, 'params': 49857, 'accuracy_test': 0.8898, 'precision_test': 0.7599, 'recall_test': 0.787, 'f1_score_test': 0.7732}, 'MLP_2293761': {'accuracy_cv_mean': 0.8771, 'accuracy_cv_std': 0.0055, 'precision_cv_mean': 0.8768, 'precision_cv_std': 0.0131, 'recall_cv_mean': 0.8778, 'recall_cv_std': 0.0091, 'f1_cv_mean': 0.8772, 'f1_cv_std': 0.0046, 'params': 2293761, 'accuracy_test': 0.9022, 'precision_test': 0.7772, 'recall_test': 0.8273, 'f1_score_test': 0.8015}}}
Saved on: outputs_without_artist/1/gpt

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.851, 'accuracy_cv_std': 0.0047, 'precision_cv_mean': 0.856, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.844, 'recall_cv_std': 0.0067, 'f1_cv_mean': 0.85, 'f1_cv_std': 0.005}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 43, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.87      0.90     16465
           1       0.67      0.83      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.85      0.82     21625
weighted avg       0.88      0.86      0.87     21625

Confusion matrix Test saved as: outputs_without_artist/1/gpt/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_without_artist/1/gpt/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7221, 'accuracy_cv_std': 0.0237, 'precision_cv_mean': 0.6947, 'precision_cv_std': 0.0316, 'recall_cv_mean': 0.798, 'recall_cv_std': 0.0268, 'f1_cv_mean': 0.7419, 'f1_cv_std': 0.0164}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 43, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.90      0.58      0.70     16465
           1       0.37      0.79      0.50      5160

    accuracy                           0.63     21625
   macro avg       0.63      0.68      0.60     21625
weighted avg       0.77      0.63      0.66     21625

Confusion matrix Test saved as: outputs_without_artist/1/gpt/conf_matrix_test_svm.png
Modelo guardado como: outputs_without_artist/1/gpt/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7831, 'accuracy_cv_std': 0.0063, 'precision_cv_mean': 0.788, 'precision_cv_std': 0.0059, 'recall_cv_mean': 0.7746, 'recall_cv_std': 0.0166, 'f1_cv_mean': 0.7811, 'f1_cv_std': 0.0083}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 43, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.91      0.81      0.86     16465
           1       0.55      0.76      0.64      5160

    accuracy                           0.80     21625
   macro avg       0.73      0.78      0.75     21625
weighted avg       0.83      0.80      0.81     21625

Confusion matrix Test saved as: outputs_without_artist/1/gpt/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_without_artist/1/gpt/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8201, 'accuracy_cv_std': 0.0081, 'precision_cv_mean': 0.8572, 'precision_cv_std': 0.0078, 'recall_cv_mean': 0.7681, 'recall_cv_std': 0.0119, 'f1_cv_mean': 0.8102, 'f1_cv_std': 0.0092}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 43, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.88      0.90     16465
           1       0.66      0.76      0.71      5160

    accuracy                           0.85     21625
   macro avg       0.79      0.82      0.80     21625
weighted avg       0.86      0.85      0.85     21625

Confusion matrix Test saved as: outputs_without_artist/1/gpt/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_without_artist/1/gpt/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8647, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.883, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8408, 'recall_cv_std': 0.0096, 'f1_cv_mean': 0.8614, 'f1_cv_std': 0.0068}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 43, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.90      0.92     16465
           1       0.72      0.83      0.77      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.86      0.85     21625
weighted avg       0.89      0.88      0.89     21625

Confusion matrix Test saved as: outputs_without_artist/1/gpt/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_without_artist/1/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7999, 'accuracy_cv_std': 0.0083, 'precision_cv_mean': 0.8562, 'precision_cv_std': 0.0085, 'recall_cv_mean': 0.7208, 'recall_cv_std': 0.0113, 'f1_cv_mean': 0.7827, 'f1_cv_std': 0.0096}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.91      0.89      0.90     16465
           1       0.66      0.71      0.69      5160

    accuracy                           0.84     21625
   macro avg       0.79      0.80      0.79     21625
weighted avg       0.85      0.84      0.85     21625

Confusion matrix Test saved as: outputs_without_artist/1/gpt/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_without_artist/1/gpt/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.8647, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.883, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8408, 'recall_cv_std': 0.0096, 'f1_cv_mean': 0.8614, 'f1_cv_std': 0.0068, 'accuracy_test': 0.8832, 'precision_test': 0.7228, 'recall_test': 0.8279, 'f1_score_test': 0.7718}
Logistic Regression: {'accuracy_cv_mean': 0.851, 'accuracy_cv_std': 0.0047, 'precision_cv_mean': 0.856, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.844, 'recall_cv_std': 0.0067, 'f1_cv_mean': 0.85, 'f1_cv_std': 0.005, 'accuracy_test': 0.8603, 'precision_test': 0.6661, 'recall_test': 0.8314, 'f1_score_test': 0.7397}
Random Forest: {'accuracy_cv_mean': 0.8201, 'accuracy_cv_std': 0.0081, 'precision_cv_mean': 0.8572, 'precision_cv_std': 0.0078, 'recall_cv_mean': 0.7681, 'recall_cv_std': 0.0119, 'f1_cv_mean': 0.8102, 'f1_cv_std': 0.0092, 'accuracy_test': 0.8506, 'precision_test': 0.6643, 'recall_test': 0.7562, 'f1_score_test': 0.7073}
Naive Bayes: {'accuracy_cv_mean': 0.7999, 'accuracy_cv_std': 0.0083, 'precision_cv_mean': 0.8562, 'precision_cv_std': 0.0085, 'recall_cv_mean': 0.7208, 'recall_cv_std': 0.0113, 'f1_cv_mean': 0.7827, 'f1_cv_std': 0.0096, 'accuracy_test': 0.845, 'precision_test': 0.6642, 'recall_test': 0.7085, 'f1_score_test': 0.6857}
Decision Tree: {'accuracy_cv_mean': 0.7831, 'accuracy_cv_std': 0.0063, 'precision_cv_mean': 0.788, 'precision_cv_std': 0.0059, 'recall_cv_mean': 0.7746, 'recall_cv_std': 0.0166, 'f1_cv_mean': 0.7811, 'f1_cv_std': 0.0083, 'accuracy_test': 0.7969, 'precision_test': 0.5546, 'recall_test': 0.757, 'f1_score_test': 0.6402}
SVM: {'accuracy_cv_mean': 0.7221, 'accuracy_cv_std': 0.0237, 'precision_cv_mean': 0.6947, 'precision_cv_std': 0.0316, 'recall_cv_mean': 0.798, 'recall_cv_std': 0.0268, 'f1_cv_mean': 0.7419, 'f1_cv_std': 0.0164, 'accuracy_test': 0.629, 'precision_test': 0.3698, 'recall_test': 0.788, 'f1_score_test': 0.5034}
resultados_globales con ML {'tfidf': {'MLP_160705': {'accuracy_cv_mean': 0.8869, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8821, 'precision_cv_std': 0.0062, 'recall_cv_mean': 0.8934, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.8877, 'f1_cv_std': 0.0023, 'params': 160705, 'accuracy_test': 0.9102, 'precision_test': 0.8044, 'recall_test': 0.8242, 'f1_score_test': 0.8142}, 'MLP_5840897': {'accuracy_cv_mean': 0.8941, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.892, 'precision_cv_std': 0.0111, 'recall_cv_mean': 0.8972, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8944, 'f1_cv_std': 0.0017, 'params': 5840897, 'accuracy_test': 0.9223, 'precision_test': 0.8704, 'recall_test': 0.7924, 'f1_score_test': 0.8296}, 'Logistic Regression': {'accuracy_cv_mean': 0.8787, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8921, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8617, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.8766, 'f1_cv_std': 0.0044, 'accuracy_test': 0.8899, 'precision_test': 0.7313, 'recall_test': 0.8514, 'f1_score_test': 0.7868}, 'SVM': {'accuracy_cv_mean': 0.7306, 'accuracy_cv_std': 0.0181, 'precision_cv_mean': 0.6946, 'precision_cv_std': 0.035, 'recall_cv_mean': 0.8328, 'recall_cv_std': 0.0389, 'f1_cv_mean': 0.7557, 'f1_cv_std': 0.0082, 'accuracy_test': 0.6794, 'precision_test': 0.4158, 'recall_test': 0.8484, 'f1_score_test': 0.5581}, 'Decision Tree': {'accuracy_cv_mean': 0.8287, 'accuracy_cv_std': 0.0058, 'precision_cv_mean': 0.8801, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.7611, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8162, 'f1_cv_std': 0.007, 'accuracy_test': 0.8667, 'precision_test': 0.7132, 'recall_test': 0.7382, 'f1_score_test': 0.7255}, 'Random Forest': {'accuracy_cv_mean': 0.8194, 'accuracy_cv_std': 0.0079, 'precision_cv_mean': 0.8515, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.7738, 'recall_cv_std': 0.0109, 'f1_cv_mean': 0.8108, 'f1_cv_std': 0.0088, 'accuracy_test': 0.8422, 'precision_test': 0.6433, 'recall_test': 0.7605, 'f1_score_test': 0.697}, 'XGBoost': {'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9034, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8471, 'recall_cv_std': 0.0097, 'f1_cv_mean': 0.8743, 'f1_cv_std': 0.0054, 'accuracy_test': 0.897, 'precision_test': 0.7561, 'recall_test': 0.8391, 'f1_score_test': 0.7954}, 'Naive Bayes': {'accuracy_cv_mean': 0.8459, 'accuracy_cv_std': 0.0062, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8402, 'f1_cv_std': 0.0071, 'accuracy_test': 0.8681, 'precision_test': 0.6917, 'recall_test': 0.8066, 'f1_score_test': 0.7447}}, 'lyrics_bert': {'MLP_10305': {'accuracy_cv_mean': 0.8684, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8683, 'precision_cv_std': 0.0192, 'recall_cv_mean': 0.8698, 'recall_cv_std': 0.0206, 'f1_cv_mean': 0.8686, 'f1_cv_std': 0.0029, 'params': 10305, 'accuracy_test': 0.8891, 'precision_test': 0.7462, 'recall_test': 0.8112, 'f1_score_test': 0.7773}, 'MLP_1028097': {'accuracy_cv_mean': 0.8791, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8756, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.8841, 'recall_cv_std': 0.0128, 'f1_cv_mean': 0.8797, 'f1_cv_std': 0.0025, 'params': 1028097, 'accuracy_test': 0.9039, 'precision_test': 0.8033, 'recall_test': 0.7907, 'f1_score_test': 0.797}, 'Logistic Regression': {'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.863, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8298, 'recall_cv_std': 0.0088, 'f1_cv_mean': 0.8461, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8581, 'precision_test': 0.6639, 'recall_test': 0.8205, 'f1_score_test': 0.734}, 'SVM': {'accuracy_cv_mean': 0.6579, 'accuracy_cv_std': 0.0382, 'precision_cv_mean': 0.6313, 'precision_cv_std': 0.0428, 'recall_cv_mean': 0.7772, 'recall_cv_std': 0.0252, 'f1_cv_mean': 0.6952, 'f1_cv_std': 0.0202, 'accuracy_test': 0.6003, 'precision_test': 0.3394, 'recall_test': 0.7136, 'f1_score_test': 0.46}, 'Decision Tree': {'accuracy_cv_mean': 0.785, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8145, 'precision_cv_std': 0.016, 'recall_cv_mean': 0.7392, 'recall_cv_std': 0.0187, 'f1_cv_mean': 0.7746, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8047, 'precision_test': 0.5699, 'recall_test': 0.7403, 'f1_score_test': 0.644}, 'Random Forest': {'accuracy_cv_mean': 0.8506, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8866, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8043, 'recall_cv_std': 0.0113, 'f1_cv_mean': 0.8433, 'f1_cv_std': 0.0054, 'accuracy_test': 0.8763, 'precision_test': 0.7172, 'recall_test': 0.7952, 'f1_score_test': 0.7542}, 'XGBoost': {'accuracy_cv_mean': 0.8752, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8943, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.851, 'recall_cv_std': 0.0118, 'f1_cv_mean': 0.872, 'f1_cv_std': 0.0062, 'accuracy_test': 0.8881, 'precision_test': 0.7301, 'recall_test': 0.8426, 'f1_score_test': 0.7824}, 'Naive Bayes': {'accuracy_cv_mean': 0.7742, 'accuracy_cv_std': 0.0057, 'precision_cv_mean': 0.7565, 'precision_cv_std': 0.0062, 'recall_cv_mean': 0.8089, 'recall_cv_std': 0.01, 'f1_cv_mean': 0.7817, 'f1_cv_std': 0.0059, 'accuracy_test': 0.755, 'precision_test': 0.4918, 'recall_test': 0.8004, 'f1_score_test': 0.6092}}, 'gpt': {'MLP_49857': {'accuracy_cv_mean': 0.8645, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.8541, 'precision_cv_std': 0.0105, 'recall_cv_mean': 0.8794, 'recall_cv_std': 0.0092, 'f1_cv_mean': 0.8665, 'f1_cv_std': 0.0028, 'params': 49857, 'accuracy_test': 0.8898, 'precision_test': 0.7599, 'recall_test': 0.787, 'f1_score_test': 0.7732}, 'MLP_2293761': {'accuracy_cv_mean': 0.8771, 'accuracy_cv_std': 0.0055, 'precision_cv_mean': 0.8768, 'precision_cv_std': 0.0131, 'recall_cv_mean': 0.8778, 'recall_cv_std': 0.0091, 'f1_cv_mean': 0.8772, 'f1_cv_std': 0.0046, 'params': 2293761, 'accuracy_test': 0.9022, 'precision_test': 0.7772, 'recall_test': 0.8273, 'f1_score_test': 0.8015}, 'Logistic Regression': {'accuracy_cv_mean': 0.851, 'accuracy_cv_std': 0.0047, 'precision_cv_mean': 0.856, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.844, 'recall_cv_std': 0.0067, 'f1_cv_mean': 0.85, 'f1_cv_std': 0.005, 'accuracy_test': 0.8603, 'precision_test': 0.6661, 'recall_test': 0.8314, 'f1_score_test': 0.7397}, 'SVM': {'accuracy_cv_mean': 0.7221, 'accuracy_cv_std': 0.0237, 'precision_cv_mean': 0.6947, 'precision_cv_std': 0.0316, 'recall_cv_mean': 0.798, 'recall_cv_std': 0.0268, 'f1_cv_mean': 0.7419, 'f1_cv_std': 0.0164, 'accuracy_test': 0.629, 'precision_test': 0.3698, 'recall_test': 0.788, 'f1_score_test': 0.5034}, 'Decision Tree': {'accuracy_cv_mean': 0.7831, 'accuracy_cv_std': 0.0063, 'precision_cv_mean': 0.788, 'precision_cv_std': 0.0059, 'recall_cv_mean': 0.7746, 'recall_cv_std': 0.0166, 'f1_cv_mean': 0.7811, 'f1_cv_std': 0.0083, 'accuracy_test': 0.7969, 'precision_test': 0.5546, 'recall_test': 0.757, 'f1_score_test': 0.6402}, 'Random Forest': {'accuracy_cv_mean': 0.8201, 'accuracy_cv_std': 0.0081, 'precision_cv_mean': 0.8572, 'precision_cv_std': 0.0078, 'recall_cv_mean': 0.7681, 'recall_cv_std': 0.0119, 'f1_cv_mean': 0.8102, 'f1_cv_std': 0.0092, 'accuracy_test': 0.8506, 'precision_test': 0.6643, 'recall_test': 0.7562, 'f1_score_test': 0.7073}, 'XGBoost': {'accuracy_cv_mean': 0.8647, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.883, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8408, 'recall_cv_std': 0.0096, 'f1_cv_mean': 0.8614, 'f1_cv_std': 0.0068, 'accuracy_test': 0.8832, 'precision_test': 0.7228, 'recall_test': 0.8279, 'f1_score_test': 0.7718}, 'Naive Bayes': {'accuracy_cv_mean': 0.7999, 'accuracy_cv_std': 0.0083, 'precision_cv_mean': 0.8562, 'precision_cv_std': 0.0085, 'recall_cv_mean': 0.7208, 'recall_cv_std': 0.0113, 'f1_cv_mean': 0.7827, 'f1_cv_std': 0.0096, 'accuracy_test': 0.845, 'precision_test': 0.6642, 'recall_test': 0.7085, 'f1_score_test': 0.6857}}}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
MLP_5840897: {'accuracy_cv_mean': 0.8941, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.892, 'precision_cv_std': 0.0111, 'recall_cv_mean': 0.8972, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8944, 'f1_cv_std': 0.0017, 'params': 5840897, 'accuracy_test': 0.9223, 'precision_test': 0.8704, 'recall_test': 0.7924, 'f1_score_test': 0.8296}
MLP_160705: {'accuracy_cv_mean': 0.8869, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8821, 'precision_cv_std': 0.0062, 'recall_cv_mean': 0.8934, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.8877, 'f1_cv_std': 0.0023, 'params': 160705, 'accuracy_test': 0.9102, 'precision_test': 0.8044, 'recall_test': 0.8242, 'f1_score_test': 0.8142}
XGBoost: {'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9034, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8471, 'recall_cv_std': 0.0097, 'f1_cv_mean': 0.8743, 'f1_cv_std': 0.0054, 'accuracy_test': 0.897, 'precision_test': 0.7561, 'recall_test': 0.8391, 'f1_score_test': 0.7954}
Logistic Regression: {'accuracy_cv_mean': 0.8787, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8921, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8617, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.8766, 'f1_cv_std': 0.0044, 'accuracy_test': 0.8899, 'precision_test': 0.7313, 'recall_test': 0.8514, 'f1_score_test': 0.7868}
Naive Bayes: {'accuracy_cv_mean': 0.8459, 'accuracy_cv_std': 0.0062, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8402, 'f1_cv_std': 0.0071, 'accuracy_test': 0.8681, 'precision_test': 0.6917, 'recall_test': 0.8066, 'f1_score_test': 0.7447}
Decision Tree: {'accuracy_cv_mean': 0.8287, 'accuracy_cv_std': 0.0058, 'precision_cv_mean': 0.8801, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.7611, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8162, 'f1_cv_std': 0.007, 'accuracy_test': 0.8667, 'precision_test': 0.7132, 'recall_test': 0.7382, 'f1_score_test': 0.7255}
Random Forest: {'accuracy_cv_mean': 0.8194, 'accuracy_cv_std': 0.0079, 'precision_cv_mean': 0.8515, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.7738, 'recall_cv_std': 0.0109, 'f1_cv_mean': 0.8108, 'f1_cv_std': 0.0088, 'accuracy_test': 0.8422, 'precision_test': 0.6433, 'recall_test': 0.7605, 'f1_score_test': 0.697}
SVM: {'accuracy_cv_mean': 0.7306, 'accuracy_cv_std': 0.0181, 'precision_cv_mean': 0.6946, 'precision_cv_std': 0.035, 'recall_cv_mean': 0.8328, 'recall_cv_std': 0.0389, 'f1_cv_mean': 0.7557, 'f1_cv_std': 0.0082, 'accuracy_test': 0.6794, 'precision_test': 0.4158, 'recall_test': 0.8484, 'f1_score_test': 0.5581}


EMBEDDINGS TYPE: LYRICS_BERT
MLP_1028097: {'accuracy_cv_mean': 0.8791, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8756, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.8841, 'recall_cv_std': 0.0128, 'f1_cv_mean': 0.8797, 'f1_cv_std': 0.0025, 'params': 1028097, 'accuracy_test': 0.9039, 'precision_test': 0.8033, 'recall_test': 0.7907, 'f1_score_test': 0.797}
XGBoost: {'accuracy_cv_mean': 0.8752, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8943, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.851, 'recall_cv_std': 0.0118, 'f1_cv_mean': 0.872, 'f1_cv_std': 0.0062, 'accuracy_test': 0.8881, 'precision_test': 0.7301, 'recall_test': 0.8426, 'f1_score_test': 0.7824}
MLP_10305: {'accuracy_cv_mean': 0.8684, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8683, 'precision_cv_std': 0.0192, 'recall_cv_mean': 0.8698, 'recall_cv_std': 0.0206, 'f1_cv_mean': 0.8686, 'f1_cv_std': 0.0029, 'params': 10305, 'accuracy_test': 0.8891, 'precision_test': 0.7462, 'recall_test': 0.8112, 'f1_score_test': 0.7773}
Random Forest: {'accuracy_cv_mean': 0.8506, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8866, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8043, 'recall_cv_std': 0.0113, 'f1_cv_mean': 0.8433, 'f1_cv_std': 0.0054, 'accuracy_test': 0.8763, 'precision_test': 0.7172, 'recall_test': 0.7952, 'f1_score_test': 0.7542}
Logistic Regression: {'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.863, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8298, 'recall_cv_std': 0.0088, 'f1_cv_mean': 0.8461, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8581, 'precision_test': 0.6639, 'recall_test': 0.8205, 'f1_score_test': 0.734}
Decision Tree: {'accuracy_cv_mean': 0.785, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8145, 'precision_cv_std': 0.016, 'recall_cv_mean': 0.7392, 'recall_cv_std': 0.0187, 'f1_cv_mean': 0.7746, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8047, 'precision_test': 0.5699, 'recall_test': 0.7403, 'f1_score_test': 0.644}
Naive Bayes: {'accuracy_cv_mean': 0.7742, 'accuracy_cv_std': 0.0057, 'precision_cv_mean': 0.7565, 'precision_cv_std': 0.0062, 'recall_cv_mean': 0.8089, 'recall_cv_std': 0.01, 'f1_cv_mean': 0.7817, 'f1_cv_std': 0.0059, 'accuracy_test': 0.755, 'precision_test': 0.4918, 'recall_test': 0.8004, 'f1_score_test': 0.6092}
SVM: {'accuracy_cv_mean': 0.6579, 'accuracy_cv_std': 0.0382, 'precision_cv_mean': 0.6313, 'precision_cv_std': 0.0428, 'recall_cv_mean': 0.7772, 'recall_cv_std': 0.0252, 'f1_cv_mean': 0.6952, 'f1_cv_std': 0.0202, 'accuracy_test': 0.6003, 'precision_test': 0.3394, 'recall_test': 0.7136, 'f1_score_test': 0.46}


EMBEDDINGS TYPE: GPT
MLP_2293761: {'accuracy_cv_mean': 0.8771, 'accuracy_cv_std': 0.0055, 'precision_cv_mean': 0.8768, 'precision_cv_std': 0.0131, 'recall_cv_mean': 0.8778, 'recall_cv_std': 0.0091, 'f1_cv_mean': 0.8772, 'f1_cv_std': 0.0046, 'params': 2293761, 'accuracy_test': 0.9022, 'precision_test': 0.7772, 'recall_test': 0.8273, 'f1_score_test': 0.8015}
MLP_49857: {'accuracy_cv_mean': 0.8645, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.8541, 'precision_cv_std': 0.0105, 'recall_cv_mean': 0.8794, 'recall_cv_std': 0.0092, 'f1_cv_mean': 0.8665, 'f1_cv_std': 0.0028, 'params': 49857, 'accuracy_test': 0.8898, 'precision_test': 0.7599, 'recall_test': 0.787, 'f1_score_test': 0.7732}
XGBoost: {'accuracy_cv_mean': 0.8647, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.883, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8408, 'recall_cv_std': 0.0096, 'f1_cv_mean': 0.8614, 'f1_cv_std': 0.0068, 'accuracy_test': 0.8832, 'precision_test': 0.7228, 'recall_test': 0.8279, 'f1_score_test': 0.7718}
Logistic Regression: {'accuracy_cv_mean': 0.851, 'accuracy_cv_std': 0.0047, 'precision_cv_mean': 0.856, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.844, 'recall_cv_std': 0.0067, 'f1_cv_mean': 0.85, 'f1_cv_std': 0.005, 'accuracy_test': 0.8603, 'precision_test': 0.6661, 'recall_test': 0.8314, 'f1_score_test': 0.7397}
Random Forest: {'accuracy_cv_mean': 0.8201, 'accuracy_cv_std': 0.0081, 'precision_cv_mean': 0.8572, 'precision_cv_std': 0.0078, 'recall_cv_mean': 0.7681, 'recall_cv_std': 0.0119, 'f1_cv_mean': 0.8102, 'f1_cv_std': 0.0092, 'accuracy_test': 0.8506, 'precision_test': 0.6643, 'recall_test': 0.7562, 'f1_score_test': 0.7073}
Naive Bayes: {'accuracy_cv_mean': 0.7999, 'accuracy_cv_std': 0.0083, 'precision_cv_mean': 0.8562, 'precision_cv_std': 0.0085, 'recall_cv_mean': 0.7208, 'recall_cv_std': 0.0113, 'f1_cv_mean': 0.7827, 'f1_cv_std': 0.0096, 'accuracy_test': 0.845, 'precision_test': 0.6642, 'recall_test': 0.7085, 'f1_score_test': 0.6857}
Decision Tree: {'accuracy_cv_mean': 0.7831, 'accuracy_cv_std': 0.0063, 'precision_cv_mean': 0.788, 'precision_cv_std': 0.0059, 'recall_cv_mean': 0.7746, 'recall_cv_std': 0.0166, 'f1_cv_mean': 0.7811, 'f1_cv_std': 0.0083, 'accuracy_test': 0.7969, 'precision_test': 0.5546, 'recall_test': 0.757, 'f1_score_test': 0.6402}
SVM: {'accuracy_cv_mean': 0.7221, 'accuracy_cv_std': 0.0237, 'precision_cv_mean': 0.6947, 'precision_cv_std': 0.0316, 'recall_cv_mean': 0.798, 'recall_cv_std': 0.0268, 'f1_cv_mean': 0.7419, 'f1_cv_std': 0.0164, 'accuracy_test': 0.629, 'precision_test': 0.3698, 'recall_test': 0.788, 'f1_score_test': 0.5034}
Diccionario global guardado en: outputs_without_artist/1/gpt/global_metrics.json

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['emotion', 'Time signature', 'song', 'Genre', 'Album', 'Release Date', 'Key', 'Similar Song 1', 'Similar Song 2', 'Similar Song 3', 'song_normalized', 'artist_normalized']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
====================================

