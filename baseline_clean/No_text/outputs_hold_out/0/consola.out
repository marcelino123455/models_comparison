2025-09-08 11:26:55.430907: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-09-08 11:26:55.493380: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-09-08 11:27:00.829980: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
For TF-IDF embbedings you are selecteing this columns:
--> ['emotion', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Release Date', 'Key', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
--> PaTH:  ../../data/embbedings_khipu/LB_fuss/lb_khipu_B.npy
Applying Hold Out with tf-idf embbedings only
You are using 36 columns for TF-IDF

##################################################
Running experiment without EMOTION embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5020, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4322, Test Loss: 0.3244, F1: 0.7688, AUC: 0.9358
Epoch [10/30] Train Loss: 0.1274, Test Loss: 0.1504, F1: 0.8823, AUC: 0.9847
Epoch [20/30] Train Loss: 0.1089, Test Loss: 0.1571, F1: 0.8772, AUC: 0.9855
Mejores resultados en la época:  19
f1-score 0.8926752839796318
AUC según el mejor F1-score 0.9854248382168423
Confusion Matrix:
 [[15971   494]
 [  602  4558]]
Matriz de confusión guardada en: outputs_hold_out/0/emotion/confusion_matrix_param_160705.png
Accuracy:   0.9493
Precision:  0.9022
Recall:     0.8833
F1-score:   0.8927

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4375, Test Loss: 0.3324, F1: 0.7701, AUC: 0.9306
Epoch [10/30] Train Loss: 0.1313, Test Loss: 0.2011, F1: 0.8503, AUC: 0.9848
Epoch [20/30] Train Loss: 0.1082, Test Loss: 0.1630, F1: 0.8736, AUC: 0.9851
Mejores resultados en la época:  21
f1-score 0.8924026224450443
AUC según el mejor F1-score 0.9853094242661791
Confusion Matrix:
 [[15881   584]
 [  532  4628]]
Matriz de confusión guardada en: outputs_hold_out/0/emotion/confusion_matrix_param_160705.png
Accuracy:   0.9484
Precision:  0.8880
Recall:     0.8969
F1-score:   0.8924

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4287, Test Loss: 0.2965, F1: 0.7772, AUC: 0.9393
Epoch [10/30] Train Loss: 0.1311, Test Loss: 0.1331, F1: 0.8918, AUC: 0.9851
Epoch [20/30] Train Loss: 0.0992, Test Loss: 0.1372, F1: 0.8917, AUC: 0.9857
Mejores resultados en la época:  27
f1-score 0.8959810874704491
AUC según el mejor F1-score 0.9852897383926911
Confusion Matrix:
 [[16021   444]
 [  612  4548]]
Matriz de confusión guardada en: outputs_hold_out/0/emotion/confusion_matrix_param_160705.png
Accuracy:   0.9512
Precision:  0.9111
Recall:     0.8814
F1-score:   0.8960
Tiempo total para red 1: 205.34 segundos

Entrenando red 3 con capas [5020, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3959, Test Loss: 0.2378, F1: 0.8254, AUC: 0.9709
Epoch [10/30] Train Loss: 0.1207, Test Loss: 0.1254, F1: 0.8977, AUC: 0.9861
Epoch [20/30] Train Loss: 0.0722, Test Loss: 0.1561, F1: 0.8915, AUC: 0.9871
Mejores resultados en la época:  29
f1-score 0.9090736881985224
AUC según el mejor F1-score 0.9890291539252867
Confusion Matrix:
 [[15866   599]
 [  361  4799]]
Matriz de confusión guardada en: outputs_hold_out/0/emotion/confusion_matrix_param_653057.png
Accuracy:   0.9556
Precision:  0.8890
Recall:     0.9300
F1-score:   0.9091

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3928, Test Loss: 0.1890, F1: 0.8321, AUC: 0.9706
Epoch [10/30] Train Loss: 0.1255, Test Loss: 0.1847, F1: 0.8587, AUC: 0.9855
Epoch [20/30] Train Loss: 0.0747, Test Loss: 0.1374, F1: 0.9047, AUC: 0.9886
Mejores resultados en la época:  25
f1-score 0.907416199791093
AUC según el mejor F1-score 0.9888227023731337
Confusion Matrix:
 [[15872   593]
 [  382  4778]]
Matriz de confusión guardada en: outputs_hold_out/0/emotion/confusion_matrix_param_653057.png
Accuracy:   0.9549
Precision:  0.8896
Recall:     0.9260
F1-score:   0.9074

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3934, Test Loss: 0.2325, F1: 0.8302, AUC: 0.9664
Epoch [10/30] Train Loss: 0.1229, Test Loss: 0.1326, F1: 0.8986, AUC: 0.9869
Epoch [20/30] Train Loss: 0.0731, Test Loss: 0.1392, F1: 0.9028, AUC: 0.9880
Mejores resultados en la época:  28
f1-score 0.9108497723823976
AUC según el mejor F1-score 0.9892396662405808
Confusion Matrix:
 [[15883   582]
 [  358  4802]]
Matriz de confusión guardada en: outputs_hold_out/0/emotion/confusion_matrix_param_653057.png
Accuracy:   0.9565
Precision:  0.8919
Recall:     0.9306
F1-score:   0.9108
Tiempo total para red 3: 221.34 segundos

Entrenando red 5 con capas [5020, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3886, Test Loss: 0.4045, F1: 0.7294, AUC: 0.9655
Epoch [10/30] Train Loss: 0.1187, Test Loss: 0.1692, F1: 0.8749, AUC: 0.9853
Epoch [20/30] Train Loss: 0.0743, Test Loss: 0.1569, F1: 0.8981, AUC: 0.9890
Mejores resultados en la época:  24
f1-score 0.9103854186443896
AUC según el mejor F1-score 0.9893662620027918
Confusion Matrix:
 [[15886   579]
 [  365  4795]]
Matriz de confusión guardada en: outputs_hold_out/0/emotion/confusion_matrix_param_2743297.png
Accuracy:   0.9563
Precision:  0.8923
Recall:     0.9293
F1-score:   0.9104

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4119, Test Loss: 0.2459, F1: 0.8165, AUC: 0.9613
Epoch [10/30] Train Loss: 0.1191, Test Loss: 0.2264, F1: 0.8450, AUC: 0.9855
Epoch [20/30] Train Loss: 0.0668, Test Loss: 0.1490, F1: 0.8933, AUC: 0.9881
Mejores resultados en la época:  17
f1-score 0.9098258107978058
AUC según el mejor F1-score 0.9886595362019976
Confusion Matrix:
 [[15961   504]
 [  433  4727]]
Matriz de confusión guardada en: outputs_hold_out/0/emotion/confusion_matrix_param_2743297.png
Accuracy:   0.9567
Precision:  0.9037
Recall:     0.9161
F1-score:   0.9098

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3839, Test Loss: 0.3312, F1: 0.7652, AUC: 0.9707
Epoch [10/30] Train Loss: 0.1162, Test Loss: 0.2484, F1: 0.8213, AUC: 0.9861
Epoch [20/30] Train Loss: 0.0739, Test Loss: 0.1461, F1: 0.9056, AUC: 0.9881
Mejores resultados en la época:  20
f1-score 0.9056166763173132
AUC según el mejor F1-score 0.9880830608502412
Confusion Matrix:
 [[15955   510]
 [  468  4692]]
Matriz de confusión guardada en: outputs_hold_out/0/emotion/confusion_matrix_param_2743297.png
Accuracy:   0.9548
Precision:  0.9020
Recall:     0.9093
F1-score:   0.9056
Tiempo total para red 5: 244.28 segundos

Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4033, Test Loss: 0.2128, F1: 0.8234, AUC: 0.9633
Epoch [10/30] Train Loss: 0.1284, Test Loss: 0.1795, F1: 0.8593, AUC: 0.9870
Epoch [20/30] Train Loss: 0.0754, Test Loss: 0.2100, F1: 0.8597, AUC: 0.9883
Mejores resultados en la época:  27
f1-score 0.9107963246554365
AUC según el mejor F1-score 0.9894156973801604
Confusion Matrix:
 [[15935   530]
 [  402  4758]]
Matriz de confusión guardada en: outputs_hold_out/0/emotion/confusion_matrix_param_5840897.png
Accuracy:   0.9569
Precision:  0.8998
Recall:     0.9221
F1-score:   0.9108

--- Iteración 2 de 3 para la red 6 ---
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [11:50:10] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [0/30] Train Loss: 0.3969, Test Loss: 0.2432, F1: 0.8199, AUC: 0.9620
Epoch [10/30] Train Loss: 0.1229, Test Loss: 0.1573, F1: 0.8443, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0813, Test Loss: 0.1789, F1: 0.8787, AUC: 0.9882
Mejores resultados en la época:  28
f1-score 0.9149302236666029
AUC según el mejor F1-score 0.9899990524886003
Confusion Matrix:
 [[15949   516]
 [  374  4786]]
Matriz de confusión guardada en: outputs_hold_out/0/emotion/confusion_matrix_param_5840897.png
Accuracy:   0.9588
Precision:  0.9027
Recall:     0.9275
F1-score:   0.9149

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4110, Test Loss: 0.3950, F1: 0.7212, AUC: 0.9678
Epoch [10/30] Train Loss: 0.1181, Test Loss: 0.1478, F1: 0.8875, AUC: 0.9864
Epoch [20/30] Train Loss: 0.0721, Test Loss: 0.1432, F1: 0.8937, AUC: 0.9880
Mejores resultados en la época:  18
f1-score 0.9070094773388383
AUC según el mejor F1-score 0.9891125231581204
Confusion Matrix:
 [[15801   664]
 [  327  4833]]
Matriz de confusión guardada en: outputs_hold_out/0/emotion/confusion_matrix_param_5840897.png
Accuracy:   0.9542
Precision:  0.8792
Recall:     0.9366
F1-score:   0.9070
Tiempo total para red 6: 276.98 segundos
Saved on: outputs_hold_out/0/emotion

==============================
Model: Logistic Regression
Accuracy:  0.9355
Precision: 0.8287
Recall:    0.9198
F1-score:  0.8719
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15484   981]
 [  414  4746]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/emotion/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/emotion/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8068
Precision: 0.5612
Recall:    0.8723
F1-score:  0.6830
              precision    recall  f1-score   support

           0       0.95      0.79      0.86     16465
           1       0.56      0.87      0.68      5160

    accuracy                           0.81     21625
   macro avg       0.76      0.83      0.77     21625
weighted avg       0.86      0.81      0.82     21625

[[12946  3519]
 [  659  4501]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/emotion/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/emotion/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8779
Precision: 0.7395
Recall:    0.7539
F1-score:  0.7466
              precision    recall  f1-score   support

           0       0.92      0.92      0.92     16465
           1       0.74      0.75      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.84      0.83     21625
weighted avg       0.88      0.88      0.88     21625

[[15095  1370]
 [ 1270  3890]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/emotion/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/emotion/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8627
Precision: 0.6870
Recall:    0.7798
F1-score:  0.7305
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.78      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.83      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14632  1833]
 [ 1136  4024]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/emotion/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/emotion/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9209
Precision: 0.8032
Recall:    0.8857
F1-score:  0.8424
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15345  1120]
 [  590  4570]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/emotion/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/emotion/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7860
Precision: 0.5281
Recall:    0.9680
F1-score:  0.6834
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12002  4463]
 [  165  4995]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/emotion/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/emotion/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9355, 'precision': 0.8287, 'recall': 0.9198, 'f1_score': 0.8719}
XGBoost: {'accuracy': 0.9209, 'precision': 0.8032, 'recall': 0.8857, 'f1_score': 0.8424}
Decision Tree: {'accuracy': 0.8779, 'precision': 0.7395, 'recall': 0.7539, 'f1_score': 0.7466}
Random Forest: {'accuracy': 0.8627, 'precision': 0.687, 'recall': 0.7798, 'f1_score': 0.7305}
Naive Bayes: {'accuracy': 0.786, 'precision': 0.5281, 'recall': 0.968, 'f1_score': 0.6834}
SVM: {'accuracy': 0.8068, 'precision': 0.5612, 'recall': 0.8723, 'f1_score': 0.683}

##################################################
Running experiment without TIME SIGNATURE embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5020, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4294, Test Loss: 0.2700, F1: 0.7467, AUC: 0.9370
Epoch [10/30] Train Loss: 0.1294, Test Loss: 0.1394, F1: 0.8871, AUC: 0.9836
Epoch [20/30] Train Loss: 0.1033, Test Loss: 0.1719, F1: 0.8743, AUC: 0.9851
Mejores resultados en la época:  28
f1-score 0.8974408312487974
AUC según el mejor F1-score 0.9861169511554929
Confusion Matrix:
 [[15895   570]
 [  496  4664]]
Matriz de confusión guardada en: outputs_hold_out/0/Time signature/confusion_matrix_param_160705.png
Accuracy:   0.9507
Precision:  0.8911
Recall:     0.9039
F1-score:   0.8974

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4333, Test Loss: 0.5242, F1: 0.6438, AUC: 0.9304
Epoch [10/30] Train Loss: 0.1341, Test Loss: 0.1672, F1: 0.8692, AUC: 0.9838
Epoch [20/30] Train Loss: 0.1035, Test Loss: 0.1330, F1: 0.8930, AUC: 0.9853
Mejores resultados en la época:  23
f1-score 0.8963151705213642
AUC según el mejor F1-score 0.9852166211154976
Confusion Matrix:
 [[15994   471]
 [  587  4573]]
Matriz de confusión guardada en: outputs_hold_out/0/Time signature/confusion_matrix_param_160705.png
Accuracy:   0.9511
Precision:  0.9066
Recall:     0.8862
F1-score:   0.8963

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4287, Test Loss: 0.3532, F1: 0.7589, AUC: 0.9376
Epoch [10/30] Train Loss: 0.1322, Test Loss: 0.1311, F1: 0.8893, AUC: 0.9852
Epoch [20/30] Train Loss: 0.0973, Test Loss: 0.1481, F1: 0.8879, AUC: 0.9854
Mejores resultados en la época:  21
f1-score 0.8946969696969697
AUC según el mejor F1-score 0.9858828334475055
Confusion Matrix:
 [[15789   676]
 [  436  4724]]
Matriz de confusión guardada en: outputs_hold_out/0/Time signature/confusion_matrix_param_160705.png
Accuracy:   0.9486
Precision:  0.8748
Recall:     0.9155
F1-score:   0.8947
Tiempo total para red 1: 207.85 segundos

Entrenando red 3 con capas [5020, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3985, Test Loss: 0.2193, F1: 0.8362, AUC: 0.9665
Epoch [10/30] Train Loss: 0.1227, Test Loss: 0.1556, F1: 0.8813, AUC: 0.9861
Epoch [20/30] Train Loss: 0.0809, Test Loss: 0.1304, F1: 0.9016, AUC: 0.9881
Mejores resultados en la época:  29
f1-score 0.9096725630410237
AUC según el mejor F1-score 0.9893775203214712
Confusion Matrix:
 [[15831   634]
 [  326  4834]]
Matriz de confusión guardada en: outputs_hold_out/0/Time signature/confusion_matrix_param_653057.png
Accuracy:   0.9556
Precision:  0.8841
Recall:     0.9368
F1-score:   0.9097

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3912, Test Loss: 0.7618, F1: 0.5324, AUC: 0.9600
Epoch [10/30] Train Loss: 0.1205, Test Loss: 0.1290, F1: 0.8972, AUC: 0.9863
Epoch [20/30] Train Loss: 0.0770, Test Loss: 0.1383, F1: 0.9021, AUC: 0.9884
Mejores resultados en la época:  27
f1-score 0.9094827586206896
AUC según el mejor F1-score 0.988377607421898
Confusion Matrix:
 [[16059   406]
 [  518  4642]]
Matriz de confusión guardada en: outputs_hold_out/0/Time signature/confusion_matrix_param_653057.png
Accuracy:   0.9573
Precision:  0.9196
Recall:     0.8996
F1-score:   0.9095

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4152, Test Loss: 0.3364, F1: 0.7606, AUC: 0.9688
Epoch [10/30] Train Loss: 0.1264, Test Loss: 0.3591, F1: 0.7709, AUC: 0.9851
Epoch [20/30] Train Loss: 0.0727, Test Loss: 0.1669, F1: 0.8936, AUC: 0.9872
Mejores resultados en la época:  23
f1-score 0.91044633470324
AUC según el mejor F1-score 0.9885676687923879
Confusion Matrix:
 [[15925   540]
 [  397  4763]]
Matriz de confusión guardada en: outputs_hold_out/0/Time signature/confusion_matrix_param_653057.png
Accuracy:   0.9567
Precision:  0.8982
Recall:     0.9231
F1-score:   0.9104
Tiempo total para red 3: 222.47 segundos

Entrenando red 5 con capas [5020, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3891, Test Loss: 0.2741, F1: 0.8161, AUC: 0.9685
Epoch [10/30] Train Loss: 0.1157, Test Loss: 0.1851, F1: 0.8700, AUC: 0.9862
Epoch [20/30] Train Loss: 0.0869, Test Loss: 0.1315, F1: 0.9030, AUC: 0.9879
Mejores resultados en la época:  26
f1-score 0.9094844549390004
AUC según el mejor F1-score 0.9887868793800332
Confusion Matrix:
 [[16083   382]
 [  538  4622]]
Matriz de confusión guardada en: outputs_hold_out/0/Time signature/confusion_matrix_param_2743297.png
Accuracy:   0.9575
Precision:  0.9237
Recall:     0.8957
F1-score:   0.9095

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3956, Test Loss: 0.3056, F1: 0.7965, AUC: 0.9623
Epoch [10/30] Train Loss: 0.1158, Test Loss: 0.2066, F1: 0.8582, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0723, Test Loss: 0.1442, F1: 0.8965, AUC: 0.9881
Mejores resultados en la época:  26
f1-score 0.9081349775356085
AUC según el mejor F1-score 0.9886921341252409
Confusion Matrix:
 [[15914   551]
 [  410  4750]]
Matriz de confusión guardada en: outputs_hold_out/0/Time signature/confusion_matrix_param_2743297.png
Accuracy:   0.9556
Precision:  0.8961
Recall:     0.9205
F1-score:   0.9081

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3969, Test Loss: 0.5044, F1: 0.6684, AUC: 0.9614
Epoch [10/30] Train Loss: 0.1167, Test Loss: 0.2836, F1: 0.7876, AUC: 0.9842
Epoch [20/30] Train Loss: 0.0741, Test Loss: 0.1388, F1: 0.9014, AUC: 0.9885
Mejores resultados en la época:  22
f1-score 0.9120015523430678
AUC según el mejor F1-score 0.9883812032570851
Confusion Matrix:
 [[16018   447]
 [  460  4700]]
Matriz de confusión guardada en: outputs_hold_out/0/Time signature/confusion_matrix_param_2743297.png
Accuracy:   0.9581
Precision:  0.9132
Recall:     0.9109
F1-score:   0.9120
Tiempo total para red 5: 245.40 segundos

Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4093, Test Loss: 0.1907, F1: 0.8346, AUC: 0.9680
Epoch [10/30] Train Loss: 0.1203, Test Loss: 0.2252, F1: 0.8403, AUC: 0.9867
Epoch [20/30] Train Loss: 0.0812, Test Loss: 0.5292, F1: 0.7449, AUC: 0.9878
Mejores resultados en la época:  26
f1-score 0.910851226993865
AUC según el mejor F1-score 0.9894232833565209
Confusion Matrix:
 [[15944   521]
 [  409  4751]]
Matriz de confusión guardada en: outputs_hold_out/0/Time signature/confusion_matrix_param_5840897.png
Accuracy:   0.9570
Precision:  0.9012
Recall:     0.9207
F1-score:   0.9109

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4090, Test Loss: 0.3730, F1: 0.7679, AUC: 0.9674
Epoch [10/30] Train Loss: 0.1196, Test Loss: 0.2700, F1: 0.8100, AUC: 0.9871
Epoch [20/30] Train Loss: 0.0762, Test Loss: 0.1376, F1: 0.8845, AUC: 0.9862
Mejores resultados en la época:  25
f1-score 0.9116333268595844
AUC según el mejor F1-score 0.9894159445570473
Confusion Matrix:
 [[16021   444]
 [  466  4694]]
Matriz de confusión guardada en: outputs_hold_out/0/Time signature/confusion_matrix_param_5840897.png
Accuracy:   0.9579
Precision:  0.9136
Recall:     0.9097
F1-score:   0.9116

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4010, Test Loss: 0.3125, F1: 0.7960, AUC: 0.9697
Epoch [10/30] Train Loss: 0.1243, Test Loss: 0.1263, F1: 0.8973, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0677, Test Loss: 0.1669, F1: 0.8968, AUC: 0.9885
Mejores resultados en la época:  28
f1-score 0.9128458498023715
AUC según el mejor F1-score 0.9892322568191395
Confusion Matrix:
 [[16124   341]
 [  541  4619]]
Matriz de confusión guardada en: outputs_hold_out/0/Time signature/confusion_matrix_param_5840897.png
Accuracy:   0.9592
Precision:  0.9313
Recall:     0.8952
F1-score:   0.9128
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [12:16:34] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Tiempo total para red 6: 279.24 segundos
Saved on: outputs_hold_out/0/Time signature

==============================
Model: Logistic Regression
Accuracy:  0.9354
Precision: 0.8278
Recall:    0.9205
F1-score:  0.8717
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15477   988]
 [  410  4750]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Time signature/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Time signature/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7568
Precision: 0.4948
Recall:    0.9291
F1-score:  0.6457
              precision    recall  f1-score   support

           0       0.97      0.70      0.81     16465
           1       0.49      0.93      0.65      5160

    accuracy                           0.76     21625
   macro avg       0.73      0.82      0.73     21625
weighted avg       0.86      0.76      0.77     21625

[[11571  4894]
 [  366  4794]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Time signature/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Time signature/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8726
Precision: 0.7197
Recall:    0.7638
F1-score:  0.7411
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14930  1535]
 [ 1219  3941]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Time signature/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Time signature/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8566
Precision: 0.6702
Recall:    0.7853
F1-score:  0.7232
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.79      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

[[14471  1994]
 [ 1108  4052]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Time signature/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Time signature/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9198
Precision: 0.8001
Recall:    0.8851
F1-score:  0.8404
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15324  1141]
 [  593  4567]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Time signature/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Time signature/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7871
Precision: 0.5294
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12022  4443]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Time signature/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Time signature/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.8278, 'recall': 0.9205, 'f1_score': 0.8717}
XGBoost: {'accuracy': 0.9198, 'precision': 0.8001, 'recall': 0.8851, 'f1_score': 0.8404}
Decision Tree: {'accuracy': 0.8726, 'precision': 0.7197, 'recall': 0.7638, 'f1_score': 0.7411}
Random Forest: {'accuracy': 0.8566, 'precision': 0.6702, 'recall': 0.7853, 'f1_score': 0.7232}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7568, 'precision': 0.4948, 'recall': 0.9291, 'f1_score': 0.6457}

##################################################
Running experiment without ARTIST(S) embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5020, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4286, Test Loss: 0.3290, F1: 0.7757, AUC: 0.9407
Epoch [10/30] Train Loss: 0.1274, Test Loss: 0.1366, F1: 0.8909, AUC: 0.9857
Epoch [20/30] Train Loss: 0.0956, Test Loss: 0.1808, F1: 0.8692, AUC: 0.9868
Mejores resultados en la época:  29
f1-score 0.9021292848276521
AUC según el mejor F1-score 0.9872602972714025
Confusion Matrix:
 [[15876   589]
 [  436  4724]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Matriz de confusión guardada en: outputs_hold_out/0/Artist(s)/confusion_matrix_param_160705.png
Accuracy:   0.9526
Precision:  0.8891
Recall:     0.9155
F1-score:   0.9021

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4292, Test Loss: 0.3362, F1: 0.7619, AUC: 0.9367
Epoch [10/30] Train Loss: 0.1306, Test Loss: 0.1654, F1: 0.8730, AUC: 0.9854
Epoch [20/30] Train Loss: 0.1026, Test Loss: 0.1523, F1: 0.8796, AUC: 0.9842
Mejores resultados en la época:  29
f1-score 0.8957127440400274
AUC según el mejor F1-score 0.9863412936061225
Confusion Matrix:
 [[15997   468]
 [  595  4565]]
Matriz de confusión guardada en: outputs_hold_out/0/Artist(s)/confusion_matrix_param_160705.png
Accuracy:   0.9508
Precision:  0.9070
Recall:     0.8847
F1-score:   0.8957

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4269, Test Loss: 0.4318, F1: 0.7177, AUC: 0.9367
Epoch [10/30] Train Loss: 0.1256, Test Loss: 0.2396, F1: 0.8287, AUC: 0.9850
Epoch [20/30] Train Loss: 0.1055, Test Loss: 0.2149, F1: 0.8502, AUC: 0.9857
Mejores resultados en la época:  29
f1-score 0.8960136999334031
AUC según el mejor F1-score 0.9859906496514806
Confusion Matrix:
 [[15823   642]
 [  451  4709]]
Matriz de confusión guardada en: outputs_hold_out/0/Artist(s)/confusion_matrix_param_160705.png
Accuracy:   0.9495
Precision:  0.8800
Recall:     0.9126
F1-score:   0.8960
Tiempo total para red 1: 209.11 segundos

Entrenando red 3 con capas [5020, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3963, Test Loss: 0.2097, F1: 0.8094, AUC: 0.9620
Epoch [10/30] Train Loss: 0.1170, Test Loss: 0.2816, F1: 0.8103, AUC: 0.9866
Epoch [20/30] Train Loss: 0.0750, Test Loss: 0.1445, F1: 0.9041, AUC: 0.9884
Mejores resultados en la época:  24
f1-score 0.9113478643338873
AUC según el mejor F1-score 0.9886480189361035
Confusion Matrix:
 [[16056   409]
 [  498  4662]]
Matriz de confusión guardada en: outputs_hold_out/0/Artist(s)/confusion_matrix_param_653057.png
Accuracy:   0.9581
Precision:  0.9193
Recall:     0.9035
F1-score:   0.9113

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4040, Test Loss: 0.2174, F1: 0.8211, AUC: 0.9587
Epoch [10/30] Train Loss: 0.1226, Test Loss: 0.1535, F1: 0.8848, AUC: 0.9866
Epoch [20/30] Train Loss: 0.0820, Test Loss: 0.1457, F1: 0.9016, AUC: 0.9884
Mejores resultados en la época:  29
f1-score 0.9130600571973307
AUC según el mejor F1-score 0.9893843765374991
Confusion Matrix:
 [[15924   541]
 [  371  4789]]
Matriz de confusión guardada en: outputs_hold_out/0/Artist(s)/confusion_matrix_param_653057.png
Accuracy:   0.9578
Precision:  0.8985
Recall:     0.9281
F1-score:   0.9131

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3928, Test Loss: 0.1878, F1: 0.8432, AUC: 0.9702
Epoch [10/30] Train Loss: 0.1275, Test Loss: 0.1777, F1: 0.8667, AUC: 0.9863
Epoch [20/30] Train Loss: 0.0726, Test Loss: 0.1443, F1: 0.9077, AUC: 0.9888
Mejores resultados en la época:  20
f1-score 0.9076546924957682
AUC según el mejor F1-score 0.9888452778621317
Confusion Matrix:
 [[15817   648]
 [  334  4826]]
Matriz de confusión guardada en: outputs_hold_out/0/Artist(s)/confusion_matrix_param_653057.png
Accuracy:   0.9546
Precision:  0.8816
Recall:     0.9353
F1-score:   0.9077
Tiempo total para red 3: 222.21 segundos

Entrenando red 5 con capas [5020, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4132, Test Loss: 0.2511, F1: 0.8139, AUC: 0.9561
Epoch [10/30] Train Loss: 0.1143, Test Loss: 0.1607, F1: 0.8762, AUC: 0.9866
Epoch [20/30] Train Loss: 0.0680, Test Loss: 0.1418, F1: 0.8910, AUC: 0.9889
Mejores resultados en la época:  18
f1-score 0.9134253450439147
AUC según el mejor F1-score 0.9893178271032989
Confusion Matrix:
 [[15996   469]
 [  428  4732]]
Matriz de confusión guardada en: outputs_hold_out/0/Artist(s)/confusion_matrix_param_2743297.png
Accuracy:   0.9585
Precision:  0.9098
Recall:     0.9171
F1-score:   0.9134

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4026, Test Loss: 0.2063, F1: 0.8215, AUC: 0.9594
Epoch [10/30] Train Loss: 0.1211, Test Loss: 0.1308, F1: 0.8935, AUC: 0.9864
Epoch [20/30] Train Loss: 0.0731, Test Loss: 0.1486, F1: 0.9097, AUC: 0.9889
Mejores resultados en la época:  26
f1-score 0.9152672488244891
AUC según el mejor F1-score 0.9896786994729246
Confusion Matrix:
 [[15973   492]
 [  391  4769]]
Matriz de confusión guardada en: outputs_hold_out/0/Artist(s)/confusion_matrix_param_2743297.png
Accuracy:   0.9592
Precision:  0.9065
Recall:     0.9242
F1-score:   0.9153

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4161, Test Loss: 0.3361, F1: 0.7782, AUC: 0.9603
Epoch [10/30] Train Loss: 0.1178, Test Loss: 0.1332, F1: 0.8923, AUC: 0.9855
Epoch [20/30] Train Loss: 0.0724, Test Loss: 0.3214, F1: 0.8161, AUC: 0.9890
Mejores resultados en la época:  18
f1-score 0.9087893864013267
AUC según el mejor F1-score 0.9887731551776496
Confusion Matrix:
 [[16032   433]
 [  502  4658]]
Matriz de confusión guardada en: outputs_hold_out/0/Artist(s)/confusion_matrix_param_2743297.png
Accuracy:   0.9568
Precision:  0.9149
Recall:     0.9027
F1-score:   0.9088
Tiempo total para red 5: 245.34 segundos

Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4038, Test Loss: 0.1925, F1: 0.8261, AUC: 0.9678
Epoch [10/30] Train Loss: 0.1232, Test Loss: 0.1713, F1: 0.8722, AUC: 0.9847
Epoch [20/30] Train Loss: 0.0768, Test Loss: 0.1361, F1: 0.9078, AUC: 0.9893
Mejores resultados en la época:  21
f1-score 0.9155634010002942
AUC según el mejor F1-score 0.9895649274830095
Confusion Matrix:
 [[16096   369]
 [  492  4668]]
Matriz de confusión guardada en: outputs_hold_out/0/Artist(s)/confusion_matrix_param_5840897.png
Accuracy:   0.9602
Precision:  0.9267
Recall:     0.9047
F1-score:   0.9156

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3906, Test Loss: 0.1864, F1: 0.8472, AUC: 0.9705
Epoch [10/30] Train Loss: 0.1171, Test Loss: 0.1611, F1: 0.8699, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0805, Test Loss: 0.2200, F1: 0.8641, AUC: 0.9892
Mejores resultados en la época:  28
f1-score 0.9092979127134725
AUC según el mejor F1-score 0.9893307214975623
Confusion Matrix:
 [[15877   588]
 [  368  4792]]
Matriz de confusión guardada en: outputs_hold_out/0/Artist(s)/confusion_matrix_param_5840897.png
Accuracy:   0.9558
Precision:  0.8907
Recall:     0.9287
F1-score:   0.9093

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4013, Test Loss: 0.2341, F1: 0.7630, AUC: 0.9530
Epoch [10/30] Train Loss: 0.1193, Test Loss: 0.1447, F1: 0.8881, AUC: 0.9873
Epoch [20/30] Train Loss: 0.0760, Test Loss: 0.2956, F1: 0.8229, AUC: 0.9869
Mejores resultados en la época:  21
f1-score 0.9120527306967985
AUC según el mejor F1-score 0.9894525914731036
Confusion Matrix:
 [[15848   617]
 [  317  4843]]
Matriz de confusión guardada en: outputs_hold_out/0/Artist(s)/confusion_matrix_param_5840897.png
Accuracy:   0.9568
Precision:  0.8870
Recall:     0.9386
F1-score:   0.9121
Tiempo total para red 6: 279.74 segundos
Saved on: outputs_hold_out/0/Artist(s)

==============================
Model: Logistic Regression
Accuracy:  0.9351
Precision: 0.8266
Recall:    0.9213
F1-score:  0.8714
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15468   997]
 [  406  4754]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Artist(s)/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Artist(s)/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8225
Precision: 0.5862
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [12:42:58] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Recall:    0.8713
F1-score:  0.7009
              precision    recall  f1-score   support

           0       0.95      0.81      0.87     16465
           1       0.59      0.87      0.70      5160

    accuracy                           0.82     21625
   macro avg       0.77      0.84      0.79     21625
weighted avg       0.87      0.82      0.83     21625

[[13291  3174]
 [  664  4496]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Artist(s)/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Artist(s)/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8713
Precision: 0.7160
Recall:    0.7632
F1-score:  0.7388
              precision    recall  f1-score   support

           0       0.92      0.91      0.91     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14903  1562]
 [ 1222  3938]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Artist(s)/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Artist(s)/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8658
Precision: 0.6950
Recall:    0.7795
F1-score:  0.7348
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.70      0.78      0.73      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.87      0.87     21625

[[14700  1765]
 [ 1138  4022]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Artist(s)/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Artist(s)/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9202
Precision: 0.8005
Recall:    0.8866
F1-score:  0.8414
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15325  1140]
 [  585  4575]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Artist(s)/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Artist(s)/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7754
Precision: 0.5156
Recall:    0.9713
F1-score:  0.6736
              precision    recall  f1-score   support

           0       0.99      0.71      0.83     16465
           1       0.52      0.97      0.67      5160

    accuracy                           0.78     21625
   macro avg       0.75      0.84      0.75     21625
weighted avg       0.87      0.78      0.79     21625

[[11756  4709]
 [  148  5012]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Artist(s)/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Artist(s)/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9351, 'precision': 0.8266, 'recall': 0.9213, 'f1_score': 0.8714}
XGBoost: {'accuracy': 0.9202, 'precision': 0.8005, 'recall': 0.8866, 'f1_score': 0.8414}
Decision Tree: {'accuracy': 0.8713, 'precision': 0.716, 'recall': 0.7632, 'f1_score': 0.7388}
Random Forest: {'accuracy': 0.8658, 'precision': 0.695, 'recall': 0.7795, 'f1_score': 0.7348}
SVM: {'accuracy': 0.8225, 'precision': 0.5862, 'recall': 0.8713, 'f1_score': 0.7009}
Naive Bayes: {'accuracy': 0.7754, 'precision': 0.5156, 'recall': 0.9713, 'f1_score': 0.6736}

##################################################
Running experiment without SONG embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5020, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4294, Test Loss: 0.5237, F1: 0.6597, AUC: 0.9376
Epoch [10/30] Train Loss: 0.1279, Test Loss: 0.1395, F1: 0.8927, AUC: 0.9849
Epoch [20/30] Train Loss: 0.1002, Test Loss: 0.1506, F1: 0.8877, AUC: 0.9867
Mejores resultados en la época:  25
f1-score 0.9012546690929988
AUC según el mejor F1-score 0.9868151728943472
Confusion Matrix:
 [[15889   576]
 [  455  4705]]
Matriz de confusión guardada en: outputs_hold_out/0/song/confusion_matrix_param_160705.png
Accuracy:   0.9523
Precision:  0.8909
Recall:     0.9118
F1-score:   0.9013

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4347, Test Loss: 0.3338, F1: 0.7673, AUC: 0.9328
Epoch [10/30] Train Loss: 0.1258, Test Loss: 0.1866, F1: 0.8600, AUC: 0.9860
Epoch [20/30] Train Loss: 0.1011, Test Loss: 0.1865, F1: 0.8614, AUC: 0.9865
Mejores resultados en la época:  28
f1-score 0.901611374407583
AUC según el mejor F1-score 0.9873312723489102
Confusion Matrix:
 [[15831   634]
 [  404  4756]]
Matriz de confusión guardada en: outputs_hold_out/0/song/confusion_matrix_param_160705.png
Accuracy:   0.9520
Precision:  0.8824
Recall:     0.9217
F1-score:   0.9016

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4312, Test Loss: 0.2968, F1: 0.7768, AUC: 0.9382
Epoch [10/30] Train Loss: 0.1230, Test Loss: 0.1592, F1: 0.8808, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0989, Test Loss: 0.1492, F1: 0.8838, AUC: 0.9853
Mejores resultados en la época:  22
f1-score 0.9018522054611419
AUC según el mejor F1-score 0.9870931821552411
Confusion Matrix:
 [[15874   591]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
 [  437  4723]]
Matriz de confusión guardada en: outputs_hold_out/0/song/confusion_matrix_param_160705.png
Accuracy:   0.9525
Precision:  0.8888
Recall:     0.9153
F1-score:   0.9019
Tiempo total para red 1: 209.11 segundos

Entrenando red 3 con capas [5020, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3982, Test Loss: 0.2020, F1: 0.8413, AUC: 0.9694
Epoch [10/30] Train Loss: 0.1207, Test Loss: 0.1571, F1: 0.8797, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0720, Test Loss: 0.1258, F1: 0.8981, AUC: 0.9884
Mejores resultados en la época:  25
f1-score 0.9140900195694717
AUC según el mejor F1-score 0.9893032142411552
Confusion Matrix:
 [[16076   389]
 [  489  4671]]
Matriz de confusión guardada en: outputs_hold_out/0/song/confusion_matrix_param_653057.png
Accuracy:   0.9594
Precision:  0.9231
Recall:     0.9052
F1-score:   0.9141

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3895, Test Loss: 0.1967, F1: 0.8427, AUC: 0.9699
Epoch [10/30] Train Loss: 0.1170, Test Loss: 0.1528, F1: 0.8818, AUC: 0.9877
Epoch [20/30] Train Loss: 0.0772, Test Loss: 0.6456, F1: 0.7028, AUC: 0.9874
Mejores resultados en la época:  18
f1-score 0.9071701066223222
AUC según el mejor F1-score 0.9884573808195445
Confusion Matrix:
 [[16039   426]
 [  523  4637]]
Matriz de confusión guardada en: outputs_hold_out/0/song/confusion_matrix_param_653057.png
Accuracy:   0.9561
Precision:  0.9159
Recall:     0.8986
F1-score:   0.9072

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3962, Test Loss: 0.3058, F1: 0.7920, AUC: 0.9715
Epoch [10/30] Train Loss: 0.1222, Test Loss: 0.2012, F1: 0.8545, AUC: 0.9875
Epoch [20/30] Train Loss: 0.0876, Test Loss: 0.1176, F1: 0.9111, AUC: 0.9890
Mejores resultados en la época:  27
f1-score 0.9126847054308194
AUC según el mejor F1-score 0.9892592932624288
Confusion Matrix:
 [[15959   506]
 [  404  4756]]
Matriz de confusión guardada en: outputs_hold_out/0/song/confusion_matrix_param_653057.png
Accuracy:   0.9579
Precision:  0.9038
Recall:     0.9217
F1-score:   0.9127
Tiempo total para red 3: 222.67 segundos

Entrenando red 5 con capas [5020, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3828, Test Loss: 0.2330, F1: 0.8214, AUC: 0.9748
Epoch [10/30] Train Loss: 0.1220, Test Loss: 0.2014, F1: 0.8681, AUC: 0.9875
Epoch [20/30] Train Loss: 0.0691, Test Loss: 0.1271, F1: 0.9124, AUC: 0.9888
Mejores resultados en la época:  20
f1-score 0.9123896381100223
AUC según el mejor F1-score 0.9887899690911188
Confusion Matrix:
 [[16020   445]
 [  458  4702]]
Matriz de confusión guardada en: outputs_hold_out/0/song/confusion_matrix_param_2743297.png
Accuracy:   0.9582
Precision:  0.9135
Recall:     0.9112
F1-score:   0.9124

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3822, Test Loss: 0.2117, F1: 0.8031, AUC: 0.9677
Epoch [10/30] Train Loss: 0.1149, Test Loss: 0.1415, F1: 0.8884, AUC: 0.9853
Epoch [20/30] Train Loss: 0.0695, Test Loss: 0.1357, F1: 0.9071, AUC: 0.9878
Mejores resultados en la época:  28
f1-score 0.9150845376405025
AUC según el mejor F1-score 0.989828741728402
Confusion Matrix:
 [[15882   583]
 [  316  4844]]
Matriz de confusión guardada en: outputs_hold_out/0/song/confusion_matrix_param_2743297.png
Accuracy:   0.9584
Precision:  0.8926
Recall:     0.9388
F1-score:   0.9151

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3903, Test Loss: 0.1804, F1: 0.8450, AUC: 0.9716
Epoch [10/30] Train Loss: 0.1124, Test Loss: 0.1221, F1: 0.9055, AUC: 0.9884
Epoch [20/30] Train Loss: 0.0691, Test Loss: 0.1449, F1: 0.9022, AUC: 0.9897
Mejores resultados en la época:  28
f1-score 0.9134398588373689
AUC según el mejor F1-score 0.9894293627309044
Confusion Matrix:
 [[16083   382]
 [  501  4659]]
Matriz de confusión guardada en: outputs_hold_out/0/song/confusion_matrix_param_2743297.png
Accuracy:   0.9592
Precision:  0.9242
Recall:     0.9029
F1-score:   0.9134
Tiempo total para red 5: 244.54 segundos

Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4020, Test Loss: 0.1937, F1: 0.8514, AUC: 0.9718
Epoch [10/30] Train Loss: 0.1194, Test Loss: 0.1479, F1: 0.8963, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0699, Test Loss: 0.1540, F1: 0.8938, AUC: 0.9898
Mejores resultados en la época:  24
f1-score 0.9111848526254148
AUC según el mejor F1-score 0.9895558231343441
Confusion Matrix:
 [[16047   418]
 [  492  4668]]
Matriz de confusión guardada en: outputs_hold_out/0/song/confusion_matrix_param_5840897.png
Accuracy:   0.9579
Precision:  0.9178
Recall:     0.9047
F1-score:   0.9112

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3906, Test Loss: 0.1808, F1: 0.8534, AUC: 0.9732
Epoch [10/30] Train Loss: 0.1255, Test Loss: 0.1258, F1: 0.9020, AUC: 0.9870
Epoch [20/30] Train Loss: 0.0741, Test Loss: 0.1422, F1: 0.8945, AUC: 0.9902
Mejores resultados en la época:  23
f1-score 0.9123420796890185
AUC según el mejor F1-score 0.9892613471846553
Confusion Matrix:
 [[16029   436]
 [  466  4694]]
Matriz de confusión guardada en: outputs_hold_out/0/song/confusion_matrix_param_5840897.png
Accuracy:   0.9583
Precision:  0.9150
Recall:     0.9097
F1-score:   0.9123

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4062, Test Loss: 0.2212, F1: 0.8386, AUC: 0.9657
Epoch [10/30] Train Loss: 0.1217, Test Loss: 0.1334, F1: 0.8957, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0764, Test Loss: 0.1529, F1: 0.8988, AUC: 0.9897
Mejores resultados en la época:  18
f1-score 0.9130601302183071
AUC según el mejor F1-score 0.9897119388790411
Confusion Matrix:
 [[15949   516]
 [  392  4768]]
Matriz de confusión guardada en: outputs_hold_out/0/song/confusion_matrix_param_5840897.png
Accuracy:   0.9580
Precision:  0.9023
Recall:     0.9240
F1-score:   0.9131
Tiempo total para red 6: 279.76 segundos
Saved on: outputs_hold_out/0/song

==============================
Model: Logistic Regression
Accuracy:  0.9379
Precision: 0.8325
Recall:    0.9260
F1-score:  0.8768
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.88      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15504   961]
 [  382  4778]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/song/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/song/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8436
Precision: 0.6253
Recall:    0.8595
F1-score:  0.7239
              precision    recall  f1-score   support

           0       0.95      0.84      0.89     16465
           1       0.63      0.86      0.72      5160

    accuracy                           0.84     21625
   macro avg       0.79      0.85      0.81     21625
weighted avg       0.87      0.84      0.85     21625

[[13807  2658]
 [  725  4435]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/song/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/song/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8738
Precision: 0.7223
Recall:    0.7653
F1-score:  0.7432
              precision    recall  f1-score   support

           0       0.93      0.91      0.92     16465
           1       0.72      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.88     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [13:09:13] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[14947  1518]
 [ 1211  3949]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/song/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/song/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8722
Precision: 0.7085
Recall:    0.7891
F1-score:  0.7467
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.79      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14790  1675]
 [ 1088  4072]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/song/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/song/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9207
Precision: 0.8039
Recall:    0.8833
F1-score:  0.8417
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15353  1112]
 [  602  4558]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/song/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/song/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7993
Precision: 0.5446
Recall:    0.9694
F1-score:  0.6974
              precision    recall  f1-score   support

           0       0.99      0.75      0.85     16465
           1       0.54      0.97      0.70      5160

    accuracy                           0.80     21625
   macro avg       0.77      0.86      0.77     21625
weighted avg       0.88      0.80      0.81     21625

[[12282  4183]
 [  158  5002]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/song/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/song/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9379, 'precision': 0.8325, 'recall': 0.926, 'f1_score': 0.8768}
XGBoost: {'accuracy': 0.9207, 'precision': 0.8039, 'recall': 0.8833, 'f1_score': 0.8417}
Random Forest: {'accuracy': 0.8722, 'precision': 0.7085, 'recall': 0.7891, 'f1_score': 0.7467}
Decision Tree: {'accuracy': 0.8738, 'precision': 0.7223, 'recall': 0.7653, 'f1_score': 0.7432}
SVM: {'accuracy': 0.8436, 'precision': 0.6253, 'recall': 0.8595, 'f1_score': 0.7239}
Naive Bayes: {'accuracy': 0.7993, 'precision': 0.5446, 'recall': 0.9694, 'f1_score': 0.6974}

##################################################
Running experiment without GENRE embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5020, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4419, Test Loss: 0.4601, F1: 0.6945, AUC: 0.9276
Epoch [10/30] Train Loss: 0.1401, Test Loss: 0.1676, F1: 0.8732, AUC: 0.9841
Epoch [20/30] Train Loss: 0.1014, Test Loss: 0.2608, F1: 0.8224, AUC: 0.9858
Mejores resultados en la época:  29
f1-score 0.8940188877229801
AUC según el mejor F1-score 0.9858290783597813
Confusion Matrix:
 [[15828   637]
 [  474  4686]]
Matriz de confusión guardada en: outputs_hold_out/0/Genre/confusion_matrix_param_160705.png
Accuracy:   0.9486
Precision:  0.8803
Recall:     0.9081
F1-score:   0.8940

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4345, Test Loss: 0.3159, F1: 0.7694, AUC: 0.9377
Epoch [10/30] Train Loss: 0.1365, Test Loss: 0.1312, F1: 0.8899, AUC: 0.9848
Epoch [20/30] Train Loss: 0.1017, Test Loss: 0.2917, F1: 0.8110, AUC: 0.9857
Mejores resultados en la época:  24
f1-score 0.8920205319645357
AUC según el mejor F1-score 0.9859294674868232
Confusion Matrix:
 [[15689   776]
 [  381  4779]]
Matriz de confusión guardada en: outputs_hold_out/0/Genre/confusion_matrix_param_160705.png
Accuracy:   0.9465
Precision:  0.8603
Recall:     0.9262
F1-score:   0.8920

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4504, Test Loss: 0.4366, F1: 0.7118, AUC: 0.9244
Epoch [10/30] Train Loss: 0.1310, Test Loss: 0.2239, F1: 0.8399, AUC: 0.9850
Epoch [20/30] Train Loss: 0.1008, Test Loss: 0.1404, F1: 0.8908, AUC: 0.9853
Mejores resultados en la época:  29
f1-score 0.8948279177081331
AUC según el mejor F1-score 0.9858754240260643
Confusion Matrix:
 [[15877   588]
 [  506  4654]]
Matriz de confusión guardada en: outputs_hold_out/0/Genre/confusion_matrix_param_160705.png
Accuracy:   0.9494
Precision:  0.8878
Recall:     0.9019
F1-score:   0.8948
Tiempo total para red 1: 207.36 segundos

Entrenando red 3 con capas [5020, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4094, Test Loss: 0.2770, F1: 0.8075, AUC: 0.9634
Epoch [10/30] Train Loss: 0.1278, Test Loss: 0.1260, F1: 0.8966, AUC: 0.9862
Epoch [20/30] Train Loss: 0.0746, Test Loss: 0.2353, F1: 0.8554, AUC: 0.9858
Mejores resultados en la época:  28
f1-score 0.9016686531585221
AUC según el mejor F1-score 0.9873234215401709
Confusion Matrix:
 [[16096   369]
 [  621  4539]]
Matriz de confusión guardada en: outputs_hold_out/0/Genre/confusion_matrix_param_653057.png
Accuracy:   0.9542
Precision:  0.9248
Recall:     0.8797
F1-score:   0.9017

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4152, Test Loss: 0.2501, F1: 0.8108, AUC: 0.9623
Epoch [10/30] Train Loss: 0.1187, Test Loss: 0.1690, F1: 0.8732, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0827, Test Loss: 0.1159, F1: 0.9087, AUC: 0.9891
Mejores resultados en la época:  29
f1-score 0.9123914759273876
AUC según el mejor F1-score 0.9890990990990992
Confusion Matrix:
 [[16113   352]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
 [  536  4624]]
Matriz de confusión guardada en: outputs_hold_out/0/Genre/confusion_matrix_param_653057.png
Accuracy:   0.9589
Precision:  0.9293
Recall:     0.8961
F1-score:   0.9124

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4141, Test Loss: 0.2217, F1: 0.8068, AUC: 0.9538
Epoch [10/30] Train Loss: 0.1209, Test Loss: 0.1727, F1: 0.8713, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0790, Test Loss: 0.1558, F1: 0.8914, AUC: 0.9881
Mejores resultados en la época:  29
f1-score 0.9102008664828672
AUC según el mejor F1-score 0.9885576581284707
Confusion Matrix:
 [[16091   374]
 [  538  4622]]
Matriz de confusión guardada en: outputs_hold_out/0/Genre/confusion_matrix_param_653057.png
Accuracy:   0.9578
Precision:  0.9251
Recall:     0.8957
F1-score:   0.9102
Tiempo total para red 3: 221.82 segundos

Entrenando red 5 con capas [5020, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4230, Test Loss: 0.2270, F1: 0.8352, AUC: 0.9664
Epoch [10/30] Train Loss: 0.1211, Test Loss: 0.1316, F1: 0.8976, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0702, Test Loss: 0.1250, F1: 0.9033, AUC: 0.9892
Mejores resultados en la época:  28
f1-score 0.9160365314740253
AUC según el mejor F1-score 0.990041884711992
Confusion Matrix:
 [[16106   359]
 [  496  4664]]
Matriz de confusión guardada en: outputs_hold_out/0/Genre/confusion_matrix_param_2743297.png
Accuracy:   0.9605
Precision:  0.9285
Recall:     0.9039
F1-score:   0.9160

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4122, Test Loss: 0.4239, F1: 0.7129, AUC: 0.9575
Epoch [10/30] Train Loss: 0.1239, Test Loss: 0.2094, F1: 0.8542, AUC: 0.9867
Epoch [20/30] Train Loss: 0.0686, Test Loss: 0.1451, F1: 0.9064, AUC: 0.9887
Mejores resultados en la época:  28
f1-score 0.9083349831716492
AUC según el mejor F1-score 0.9878554521336073
Confusion Matrix:
 [[16111   354]
 [  572  4588]]
Matriz de confusión guardada en: outputs_hold_out/0/Genre/confusion_matrix_param_2743297.png
Accuracy:   0.9572
Precision:  0.9284
Recall:     0.8891
F1-score:   0.9083

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4192, Test Loss: 0.2648, F1: 0.8112, AUC: 0.9622
Epoch [10/30] Train Loss: 0.1179, Test Loss: 0.2198, F1: 0.8160, AUC: 0.9834
Epoch [20/30] Train Loss: 0.0699, Test Loss: 0.1363, F1: 0.8922, AUC: 0.9862
Mejores resultados en la época:  19
f1-score 0.9079874706342992
AUC según el mejor F1-score 0.9881001984477291
Confusion Matrix:
 [[16047   418]
 [  522  4638]]
Matriz de confusión guardada en: outputs_hold_out/0/Genre/confusion_matrix_param_2743297.png
Accuracy:   0.9565
Precision:  0.9173
Recall:     0.8988
F1-score:   0.9080
Tiempo total para red 5: 245.38 segundos

Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4211, Test Loss: 0.1985, F1: 0.8260, AUC: 0.9655
Epoch [10/30] Train Loss: 0.1163, Test Loss: 0.1870, F1: 0.8655, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0784, Test Loss: 0.1861, F1: 0.8713, AUC: 0.9893
Mejores resultados en la época:  29
f1-score 0.9134801081498648
AUC según el mejor F1-score 0.9893581522468379
Confusion Matrix:
 [[15999   466]
 [  430  4730]]
Matriz de confusión guardada en: outputs_hold_out/0/Genre/confusion_matrix_param_5840897.png
Accuracy:   0.9586
Precision:  0.9103
Recall:     0.9167
F1-score:   0.9135

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4310, Test Loss: 0.2546, F1: 0.8007, AUC: 0.9548
Epoch [10/30] Train Loss: 0.1206, Test Loss: 0.1926, F1: 0.8523, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0736, Test Loss: 0.1387, F1: 0.9075, AUC: 0.9888
Mejores resultados en la época:  21
f1-score 0.9101251422070534
AUC según el mejor F1-score 0.9890055014512814
Confusion Matrix:
 [[15877   588]
 [  360  4800]]
Matriz de confusión guardada en: outputs_hold_out/0/Genre/confusion_matrix_param_5840897.png
Accuracy:   0.9562
Precision:  0.8909
Recall:     0.9302
F1-score:   0.9101

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4194, Test Loss: 0.4773, F1: 0.6860, AUC: 0.9554
Epoch [10/30] Train Loss: 0.1218, Test Loss: 0.1536, F1: 0.8768, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0767, Test Loss: 0.4892, F1: 0.7553, AUC: 0.9874
Mejores resultados en la época:  27
f1-score 0.9137361569846513
AUC según el mejor F1-score 0.9893382309667913
Confusion Matrix:
 [[16034   431]
 [  457  4703]]
Matriz de confusión guardada en: outputs_hold_out/0/Genre/confusion_matrix_param_5840897.png
Accuracy:   0.9589
Precision:  0.9160
Recall:     0.9114
F1-score:   0.9137
Tiempo total para red 6: 279.02 segundos
Saved on: outputs_hold_out/0/Genre

==============================
Model: Logistic Regression
Accuracy:  0.9340
Precision: 0.8249
Recall:    0.9184
F1-score:  0.8691
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.82      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.94     21625

[[15459  1006]
 [  421  4739]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Genre/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Genre/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8521
Precision: 0.6555
Recall:    0.8010
F1-score:  0.7210
              precision    recall  f1-score   support

           0       0.93      0.87      0.90     16465
           1       0.66      0.80      0.72      5160

    accuracy                           0.85     21625
   macro avg       0.79      0.83      0.81     21625
weighted avg       0.87      0.85      0.86     21625

[[14293  2172]
 [ 1027  4133]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Genre/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Genre/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8243
Precision: 0.5994
Recall:    0.7946
F1-score:  0.6833
              precision    recall  f1-score   support

           0       0.93      0.83      0.88     16465
           1       0.60      0.79      0.68      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.81      0.78     21625
weighted avg       0.85      0.82      0.83     21625

[[13725  2740]
 [ 1060  4100]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Genre/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Genre/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8833
Precision: 0.7433
Recall:    0.7806
F1-score:  0.7615
              precision    recall  f1-score   support

           0       0.93      0.92      0.92     16465
           1       0.74      0.78      0.76      5160

    accuracy                           0.88     21625
   macro avg       0.84      0.85      0.84     21625
weighted avg       0.89      0.88      0.88     21625

[[15074  1391]
 [ 1132  4028]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [13:35:31] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Genre/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Genre/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9134
Precision: 0.7824
Recall:    0.8824
F1-score:  0.8294
              precision    recall  f1-score   support

           0       0.96      0.92      0.94     16465
           1       0.78      0.88      0.83      5160

    accuracy                           0.91     21625
   macro avg       0.87      0.90      0.89     21625
weighted avg       0.92      0.91      0.92     21625

[[15199  1266]
 [  607  4553]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Genre/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Genre/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7802
Precision: 0.5212
Recall:    0.9694
F1-score:  0.6779
              precision    recall  f1-score   support

           0       0.99      0.72      0.83     16465
           1       0.52      0.97      0.68      5160

    accuracy                           0.78     21625
   macro avg       0.75      0.85      0.76     21625
weighted avg       0.88      0.78      0.80     21625

[[11870  4595]
 [  158  5002]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Genre/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Genre/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.934, 'precision': 0.8249, 'recall': 0.9184, 'f1_score': 0.8691}
XGBoost: {'accuracy': 0.9134, 'precision': 0.7824, 'recall': 0.8824, 'f1_score': 0.8294}
Random Forest: {'accuracy': 0.8833, 'precision': 0.7433, 'recall': 0.7806, 'f1_score': 0.7615}
SVM: {'accuracy': 0.8521, 'precision': 0.6555, 'recall': 0.801, 'f1_score': 0.721}
Decision Tree: {'accuracy': 0.8243, 'precision': 0.5994, 'recall': 0.7946, 'f1_score': 0.6833}
Naive Bayes: {'accuracy': 0.7802, 'precision': 0.5212, 'recall': 0.9694, 'f1_score': 0.6779}

##################################################
Running experiment without ALBUM embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5020, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4350, Test Loss: 0.2934, F1: 0.7708, AUC: 0.9322
Epoch [10/30] Train Loss: 0.1311, Test Loss: 0.1440, F1: 0.8877, AUC: 0.9849
Epoch [20/30] Train Loss: 0.1049, Test Loss: 0.1972, F1: 0.8589, AUC: 0.9863
Mejores resultados en la época:  24
f1-score 0.898889653139432
AUC según el mejor F1-score 0.9858943624837275
Confusion Matrix:
 [[16022   443]
 [  586  4574]]
Matriz de confusión guardada en: outputs_hold_out/0/Album/confusion_matrix_param_160705.png
Accuracy:   0.9524
Precision:  0.9117
Recall:     0.8864
F1-score:   0.8989

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4389, Test Loss: 0.3004, F1: 0.7702, AUC: 0.9300
Epoch [10/30] Train Loss: 0.1311, Test Loss: 0.1473, F1: 0.8849, AUC: 0.9852
Epoch [20/30] Train Loss: 0.1114, Test Loss: 0.1340, F1: 0.8907, AUC: 0.9856
Mejores resultados en la época:  25
f1-score 0.8918996278981013
AUC según el mejor F1-score 0.9854655753218595
Confusion Matrix:
 [[15818   647]
 [  486  4674]]
Matriz de confusión guardada en: outputs_hold_out/0/Album/confusion_matrix_param_160705.png
Accuracy:   0.9476
Precision:  0.8784
Recall:     0.9058
F1-score:   0.8919

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4286, Test Loss: 0.3124, F1: 0.7778, AUC: 0.9369
Epoch [10/30] Train Loss: 0.1330, Test Loss: 0.2126, F1: 0.8448, AUC: 0.9848
Epoch [20/30] Train Loss: 0.1021, Test Loss: 0.1316, F1: 0.8934, AUC: 0.9859
Mejores resultados en la época:  21
f1-score 0.897590939629523
AUC según el mejor F1-score 0.9862543461935935
Confusion Matrix:
 [[15882   583]
 [  484  4676]]
Matriz de confusión guardada en: outputs_hold_out/0/Album/confusion_matrix_param_160705.png
Accuracy:   0.9507
Precision:  0.8891
Recall:     0.9062
F1-score:   0.8976
Tiempo total para red 1: 206.93 segundos

Entrenando red 3 con capas [5020, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3891, Test Loss: 0.7145, F1: 0.5730, AUC: 0.9674
Epoch [10/30] Train Loss: 0.1205, Test Loss: 0.1567, F1: 0.8869, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0767, Test Loss: 0.1494, F1: 0.8968, AUC: 0.9885
Mejores resultados en la época:  28
f1-score 0.9099090473910962
AUC según el mejor F1-score 0.9889996162873089
Confusion Matrix:
 [[15932   533]
 [  408  4752]]
Matriz de confusión guardada en: outputs_hold_out/0/Album/confusion_matrix_param_653057.png
Accuracy:   0.9565
Precision:  0.8991
Recall:     0.9209
F1-score:   0.9099

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4058, Test Loss: 0.2164, F1: 0.7970, AUC: 0.9630
Epoch [10/30] Train Loss: 0.1189, Test Loss: 0.1442, F1: 0.8878, AUC: 0.9867
Epoch [20/30] Train Loss: 0.0772, Test Loss: 0.1250, F1: 0.9063, AUC: 0.9887
Mejores resultados en la época:  26
f1-score 0.9115791517033873
AUC según el mejor F1-score 0.9893276376716408
Confusion Matrix:
 [[16018   447]
 [  464  4696]]
Matriz de confusión guardada en: outputs_hold_out/0/Album/confusion_matrix_param_653057.png
Accuracy:   0.9579
Precision:  0.9131
Recall:     0.9101
F1-score:   0.9116

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4042, Test Loss: 0.1920, F1: 0.8405, AUC: 0.9686
Epoch [10/30] Train Loss: 0.1249, Test Loss: 0.1402, F1: 0.8908, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0788, Test Loss: 0.2156, F1: 0.8548, AUC: 0.9881
Mejores resultados en la época:  25
f1-score 0.9128294236895453
AUC según el mejor F1-score 0.988891146830133
Confusion Matrix:
 [[15994   471]
 [  432  4728]]
Matriz de confusión guardada en: outputs_hold_out/0/Album/confusion_matrix_param_653057.png
Accuracy:   0.9582
Precision:  0.9094
Recall:     0.9163
F1-score:   0.9128
Tiempo total para red 3: 220.88 segundos

Entrenando red 5 con capas [5020, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3898, Test Loss: 0.3807, F1: 0.7527, AUC: 0.9729
Epoch [10/30] Train Loss: 0.1247, Test Loss: 0.1340, F1: 0.8952, AUC: 0.9866
Epoch [20/30] Train Loss: 0.0824, Test Loss: 0.1465, F1: 0.9045, AUC: 0.9887
Mejores resultados en la época:  21
f1-score 0.9109023626530032
AUC según el mejor F1-score 0.9887707658010767
Confusion Matrix:
 [[15886   579]
 [  360  4800]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [14:01:43] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Matriz de confusión guardada en: outputs_hold_out/0/Album/confusion_matrix_param_2743297.png
Accuracy:   0.9566
Precision:  0.8924
Recall:     0.9302
F1-score:   0.9109

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3942, Test Loss: 0.1996, F1: 0.8193, AUC: 0.9702
Epoch [10/30] Train Loss: 0.1162, Test Loss: 0.1467, F1: 0.8867, AUC: 0.9857
Epoch [20/30] Train Loss: 0.0791, Test Loss: 0.1516, F1: 0.8946, AUC: 0.9889
Mejores resultados en la época:  21
f1-score 0.9091616417039486
AUC según el mejor F1-score 0.9891476811276915
Confusion Matrix:
 [[16017   448]
 [  486  4674]]
Matriz de confusión guardada en: outputs_hold_out/0/Album/confusion_matrix_param_2743297.png
Accuracy:   0.9568
Precision:  0.9125
Recall:     0.9058
F1-score:   0.9092

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4089, Test Loss: 0.2779, F1: 0.8222, AUC: 0.9636
Epoch [10/30] Train Loss: 0.1308, Test Loss: 0.1799, F1: 0.8652, AUC: 0.9844
Epoch [20/30] Train Loss: 0.0693, Test Loss: 0.2976, F1: 0.8223, AUC: 0.9867
Mejores resultados en la época:  29
f1-score 0.9083798882681564
AUC según el mejor F1-score 0.9897270755207782
Confusion Matrix:
 [[15763   702]
 [  282  4878]]
Matriz de confusión guardada en: outputs_hold_out/0/Album/confusion_matrix_param_2743297.png
Accuracy:   0.9545
Precision:  0.8742
Recall:     0.9453
F1-score:   0.9084
Tiempo total para red 5: 243.09 segundos

Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4144, Test Loss: 0.2619, F1: 0.8162, AUC: 0.9600
Epoch [10/30] Train Loss: 0.1211, Test Loss: 0.1319, F1: 0.8882, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0702, Test Loss: 0.1773, F1: 0.8806, AUC: 0.9890
Mejores resultados en la época:  24
f1-score 0.9124929124929125
AUC según el mejor F1-score 0.989821261684993
Confusion Matrix:
 [[15871   594]
 [  332  4828]]
Matriz de confusión guardada en: outputs_hold_out/0/Album/confusion_matrix_param_5840897.png
Accuracy:   0.9572
Precision:  0.8904
Recall:     0.9357
F1-score:   0.9125

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4085, Test Loss: 0.3301, F1: 0.7973, AUC: 0.9653
Epoch [10/30] Train Loss: 0.1215, Test Loss: 0.1217, F1: 0.9005, AUC: 0.9871
Epoch [20/30] Train Loss: 0.0710, Test Loss: 0.6901, F1: 0.7046, AUC: 0.9878
Mejores resultados en la época:  27
f1-score 0.9120973106528556
AUC según el mejor F1-score 0.9898822908353873
Confusion Matrix:
 [[15901   564]
 [  361  4799]]
Matriz de confusión guardada en: outputs_hold_out/0/Album/confusion_matrix_param_5840897.png
Accuracy:   0.9572
Precision:  0.8948
Recall:     0.9300
F1-score:   0.9121

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4009, Test Loss: 0.2123, F1: 0.8368, AUC: 0.9654
Epoch [10/30] Train Loss: 0.1181, Test Loss: 0.1322, F1: 0.8985, AUC: 0.9874
Epoch [20/30] Train Loss: 0.0789, Test Loss: 0.1692, F1: 0.8994, AUC: 0.9884
Mejores resultados en la época:  28
f1-score 0.9131905298759865
AUC según el mejor F1-score 0.9898457380819545
Confusion Matrix:
 [[15841   624]
 [  300  4860]]
Matriz de confusión guardada en: outputs_hold_out/0/Album/confusion_matrix_param_5840897.png
Accuracy:   0.9573
Precision:  0.8862
Recall:     0.9419
F1-score:   0.9132
Tiempo total para red 6: 278.78 segundos
Saved on: outputs_hold_out/0/Album

==============================
Model: Logistic Regression
Accuracy:  0.9360
Precision: 0.8299
Recall:    0.9207
F1-score:  0.8729
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15491   974]
 [  409  4751]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Album/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Album/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7440
Precision: 0.4808
Recall:    0.9101
F1-score:  0.6292
              precision    recall  f1-score   support

           0       0.96      0.69      0.80     16465
           1       0.48      0.91      0.63      5160

    accuracy                           0.74     21625
   macro avg       0.72      0.80      0.72     21625
weighted avg       0.85      0.74      0.76     21625

[[11393  5072]
 [  464  4696]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Album/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Album/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8716
Precision: 0.7166
Recall:    0.7643
F1-score:  0.7397
              precision    recall  f1-score   support

           0       0.92      0.91      0.91     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14905  1560]
 [ 1216  3944]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Album/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Album/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8608
Precision: 0.6818
Recall:    0.7812
F1-score:  0.7281
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.68      0.78      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14584  1881]
 [ 1129  4031]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Album/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Album/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9199
Precision: 0.8020
Recall:    0.8818
F1-score:  0.8400
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15342  1123]
 [  610  4550]]
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Album/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Album/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7942
Precision: 0.5382
Recall:    0.9690
F1-score:  0.6920
              precision    recall  f1-score   support

           0       0.99      0.74      0.85     16465
           1       0.54      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.77     21625
weighted avg       0.88      0.79      0.81     21625

[[12174  4291]
 [  160  5000]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Album/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Album/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.936, 'precision': 0.8299, 'recall': 0.9207, 'f1_score': 0.8729}
XGBoost: {'accuracy': 0.9199, 'precision': 0.802, 'recall': 0.8818, 'f1_score': 0.84}
Decision Tree: {'accuracy': 0.8716, 'precision': 0.7166, 'recall': 0.7643, 'f1_score': 0.7397}
Random Forest: {'accuracy': 0.8608, 'precision': 0.6818, 'recall': 0.7812, 'f1_score': 0.7281}
Naive Bayes: {'accuracy': 0.7942, 'precision': 0.5382, 'recall': 0.969, 'f1_score': 0.692}
SVM: {'accuracy': 0.744, 'precision': 0.4808, 'recall': 0.9101, 'f1_score': 0.6292}

##################################################
Running experiment without RELEASE DATE embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5020, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4354, Test Loss: 0.3102, F1: 0.7735, AUC: 0.9362
Epoch [10/30] Train Loss: 0.1433, Test Loss: 0.1743, F1: 0.8665, AUC: 0.9846
Epoch [20/30] Train Loss: 0.1035, Test Loss: 0.2108, F1: 0.8433, AUC: 0.9846
Mejores resultados en la época:  25
f1-score 0.8957657923175164
AUC según el mejor F1-score 0.986028355897052
Confusion Matrix:
 [[16005   460]
 [  601  4559]]
Matriz de confusión guardada en: outputs_hold_out/0/Release Date/confusion_matrix_param_160705.png
Accuracy:   0.9509
Precision:  0.9083
Recall:     0.8835
F1-score:   0.8958

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4263, Test Loss: 0.3201, F1: 0.7666, AUC: 0.9372
Epoch [10/30] Train Loss: 0.1288, Test Loss: 0.1625, F1: 0.8772, AUC: 0.9844
Epoch [20/30] Train Loss: 0.1059, Test Loss: 0.1809, F1: 0.8645, AUC: 0.9847
Mejores resultados en la época:  19
f1-score 0.8952567237163814
AUC según el mejor F1-score 0.9851922271108319
Confusion Matrix:
 [[15977   488]
 [  583  4577]]
Matriz de confusión guardada en: outputs_hold_out/0/Release Date/confusion_matrix_param_160705.png
Accuracy:   0.9505
Precision:  0.9037
Recall:     0.8870
F1-score:   0.8953

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4225, Test Loss: 0.3128, F1: 0.7714, AUC: 0.9378
Epoch [10/30] Train Loss: 0.1316, Test Loss: 0.1863, F1: 0.8455, AUC: 0.9835
Epoch [20/30] Train Loss: 0.1060, Test Loss: 0.2070, F1: 0.8537, AUC: 0.9860
Mejores resultados en la época:  22
f1-score 0.8951935263722336
AUC según el mejor F1-score 0.9855469141731227
Confusion Matrix:
 [[15959   506]
 [  569  4591]]
Matriz de confusión guardada en: outputs_hold_out/0/Release Date/confusion_matrix_param_160705.png
Accuracy:   0.9503
Precision:  0.9007
Recall:     0.8897
F1-score:   0.8952
Tiempo total para red 1: 210.38 segundos

Entrenando red 3 con capas [5020, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4040, Test Loss: 0.2255, F1: 0.8271, AUC: 0.9644
Epoch [10/30] Train Loss: 0.1317, Test Loss: 0.2510, F1: 0.8224, AUC: 0.9849
Epoch [20/30] Train Loss: 0.0759, Test Loss: 0.3382, F1: 0.7882, AUC: 0.9870
Mejores resultados en la época:  28
f1-score 0.908455489758276
AUC según el mejor F1-score 0.9876839525702865
Confusion Matrix:
 [[16003   462]
 [  481  4679]]
Matriz de confusión guardada en: outputs_hold_out/0/Release Date/confusion_matrix_param_653057.png
Accuracy:   0.9564
Precision:  0.9101
Recall:     0.9068
F1-score:   0.9085

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4125, Test Loss: 0.4162, F1: 0.7200, AUC: 0.9594
Epoch [10/30] Train Loss: 0.1146, Test Loss: 0.1332, F1: 0.8875, AUC: 0.9856
Epoch [20/30] Train Loss: 0.0740, Test Loss: 0.2657, F1: 0.8458, AUC: 0.9870
Mejores resultados en la época:  23
f1-score 0.9071136386311484
AUC según el mejor F1-score 0.9881336320642566
Confusion Matrix:
 [[16148   317]
 [  614  4546]]
Matriz de confusión guardada en: outputs_hold_out/0/Release Date/confusion_matrix_param_653057.png
Accuracy:   0.9569
Precision:  0.9348
Recall:     0.8810
F1-score:   0.9071

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3968, Test Loss: 0.1899, F1: 0.8304, AUC: 0.9685
Epoch [10/30] Train Loss: 0.1156, Test Loss: 0.1920, F1: 0.8504, AUC: 0.9834
Epoch [20/30] Train Loss: 0.0806, Test Loss: 0.1267, F1: 0.9026, AUC: 0.9874
Mejores resultados en la época:  23
f1-score 0.9109667558272831
AUC según el mejor F1-score 0.988499542134243
Confusion Matrix:
 [[15925   540]
 [  392  4768]]
Matriz de confusión guardada en: outputs_hold_out/0/Release Date/confusion_matrix_param_653057.png
Accuracy:   0.9569
Precision:  0.8983
Recall:     0.9240
F1-score:   0.9110
Tiempo total para red 3: 223.10 segundos

Entrenando red 5 con capas [5020, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4160, Test Loss: 0.3450, F1: 0.7710, AUC: 0.9666
Epoch [10/30] Train Loss: 0.1146, Test Loss: 0.1665, F1: 0.8745, AUC: 0.9856
Epoch [20/30] Train Loss: 0.0689, Test Loss: 0.2073, F1: 0.8691, AUC: 0.9881
Mejores resultados en la época:  21
f1-score 0.9124878993223621
AUC según el mejor F1-score 0.9889726092698394
Confusion Matrix:
 [[16008   457]
 [  447  4713]]
Matriz de confusión guardada en: outputs_hold_out/0/Release Date/confusion_matrix_param_2743297.png
Accuracy:   0.9582
Precision:  0.9116
Recall:     0.9134
F1-score:   0.9125

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4036, Test Loss: 0.2338, F1: 0.8224, AUC: 0.9640
Epoch [10/30] Train Loss: 0.1197, Test Loss: 0.1724, F1: 0.8789, AUC: 0.9871
Epoch [20/30] Train Loss: 0.0778, Test Loss: 0.2469, F1: 0.8557, AUC: 0.9882
Mejores resultados en la época:  23
f1-score 0.9150943396226415
AUC según el mejor F1-score 0.9896576129304115
Confusion Matrix:
 [[15875   590]
 [  310  4850]]
Matriz de confusión guardada en: outputs_hold_out/0/Release Date/confusion_matrix_param_2743297.png
Accuracy:   0.9584
Precision:  0.8915
Recall:     0.9399
F1-score:   0.9151

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3894, Test Loss: 0.2250, F1: 0.8336, AUC: 0.9707
Epoch [10/30] Train Loss: 0.1216, Test Loss: 0.1348, F1: 0.8898, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0755, Test Loss: 0.1739, F1: 0.8872, AUC: 0.9884
Mejores resultados en la época:  18
f1-score 0.9096964506613379
AUC según el mejor F1-score 0.9887087891392831
Confusion Matrix:
 [[15896   569]
 [  380  4780]]
Matriz de confusión guardada en: outputs_hold_out/0/Release Date/confusion_matrix_param_2743297.png
Accuracy:   0.9561
Precision:  0.8936
Recall:     0.9264
F1-score:   0.9097
Tiempo total para red 5: 245.31 segundos

Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3956, Test Loss: 0.1780, F1: 0.8506, AUC: 0.9711
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [14:28:08] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [10/30] Train Loss: 0.1266, Test Loss: 0.1651, F1: 0.8765, AUC: 0.9866
Epoch [20/30] Train Loss: 0.0794, Test Loss: 0.1302, F1: 0.9111, AUC: 0.9891
Mejores resultados en la época:  22
f1-score 0.9119625621526762
AUC según el mejor F1-score 0.98853386441053
Confusion Matrix:
 [[16045   420]
 [  483  4677]]
Matriz de confusión guardada en: outputs_hold_out/0/Release Date/confusion_matrix_param_5840897.png
Accuracy:   0.9582
Precision:  0.9176
Recall:     0.9064
F1-score:   0.9120

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4117, Test Loss: 0.2519, F1: 0.8280, AUC: 0.9642
Epoch [10/30] Train Loss: 0.1204, Test Loss: 0.1427, F1: 0.8826, AUC: 0.9871
Epoch [20/30] Train Loss: 0.0788, Test Loss: 0.2385, F1: 0.8382, AUC: 0.9867
Mejores resultados en la época:  24
f1-score 0.9114578347851058
AUC según el mejor F1-score 0.9889519288036404
Confusion Matrix:
 [[15939   526]
 [  399  4761]]
Matriz de confusión guardada en: outputs_hold_out/0/Release Date/confusion_matrix_param_5840897.png
Accuracy:   0.9572
Precision:  0.9005
Recall:     0.9227
F1-score:   0.9115

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4052, Test Loss: 0.2837, F1: 0.7959, AUC: 0.9664
Epoch [10/30] Train Loss: 0.1278, Test Loss: 0.1368, F1: 0.8943, AUC: 0.9866
Epoch [20/30] Train Loss: 0.0774, Test Loss: 0.1730, F1: 0.8609, AUC: 0.9864
Mejores resultados en la época:  22
f1-score 0.9108346785680372
AUC según el mejor F1-score 0.9891998001398314
Confusion Matrix:
 [[15890   575]
 [  364  4796]]
Matriz de confusión guardada en: outputs_hold_out/0/Release Date/confusion_matrix_param_5840897.png
Accuracy:   0.9566
Precision:  0.8929
Recall:     0.9295
F1-score:   0.9108
Tiempo total para red 6: 280.10 segundos
Saved on: outputs_hold_out/0/Release Date

==============================
Model: Logistic Regression
Accuracy:  0.9364
Precision: 0.8298
Recall:    0.9225
F1-score:  0.8737
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15489   976]
 [  400  4760]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Release Date/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Release Date/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7982
Precision: 0.5464
Recall:    0.9064
F1-score:  0.6818
              precision    recall  f1-score   support

           0       0.96      0.76      0.85     16465
           1       0.55      0.91      0.68      5160

    accuracy                           0.80     21625
   macro avg       0.75      0.84      0.77     21625
weighted avg       0.86      0.80      0.81     21625

[[12583  3882]
 [  483  4677]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Release Date/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Release Date/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8732
Precision: 0.7204
Recall:    0.7661
F1-score:  0.7426
              precision    recall  f1-score   support

           0       0.93      0.91      0.92     16465
           1       0.72      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14931  1534]
 [ 1207  3953]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Release Date/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Release Date/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8647
Precision: 0.6909
Recall:    0.7839
F1-score:  0.7345
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.78      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14655  1810]
 [ 1115  4045]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Release Date/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Release Date/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9201
Precision: 0.8014
Recall:    0.8843
F1-score:  0.8408
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15334  1131]
 [  597  4563]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Release Date/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Release Date/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7877
Precision: 0.5302
Recall:    0.9684
F1-score:  0.6853
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12038  4427]
 [  163  4997]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Release Date/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Release Date/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9364, 'precision': 0.8298, 'recall': 0.9225, 'f1_score': 0.8737}
XGBoost: {'accuracy': 0.9201, 'precision': 0.8014, 'recall': 0.8843, 'f1_score': 0.8408}
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Decision Tree: {'accuracy': 0.8732, 'precision': 0.7204, 'recall': 0.7661, 'f1_score': 0.7426}
Random Forest: {'accuracy': 0.8647, 'precision': 0.6909, 'recall': 0.7839, 'f1_score': 0.7345}
Naive Bayes: {'accuracy': 0.7877, 'precision': 0.5302, 'recall': 0.9684, 'f1_score': 0.6853}
SVM: {'accuracy': 0.7982, 'precision': 0.5464, 'recall': 0.9064, 'f1_score': 0.6818}

##################################################
Running experiment without KEY embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5020, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4227, Test Loss: 0.2882, F1: 0.7848, AUC: 0.9407
Epoch [10/30] Train Loss: 0.1332, Test Loss: 0.1906, F1: 0.8580, AUC: 0.9845
Epoch [20/30] Train Loss: 0.1060, Test Loss: 0.1692, F1: 0.8771, AUC: 0.9852
Mejores resultados en la época:  28
f1-score 0.895562875411742
AUC según el mejor F1-score 0.9859778965011523
Confusion Matrix:
 [[15925   540]
 [  538  4622]]
Matriz de confusión guardada en: outputs_hold_out/0/Key/confusion_matrix_param_160705.png
Accuracy:   0.9502
Precision:  0.8954
Recall:     0.8957
F1-score:   0.8956

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4530, Test Loss: 0.3785, F1: 0.7391, AUC: 0.9231
Epoch [10/30] Train Loss: 0.1345, Test Loss: 0.1426, F1: 0.8852, AUC: 0.9843
Epoch [20/30] Train Loss: 0.1057, Test Loss: 0.1434, F1: 0.8889, AUC: 0.9855
Mejores resultados en la época:  24
f1-score 0.8934187536400698
AUC según el mejor F1-score 0.9851887077827763
Confusion Matrix:
 [[15925   540]
 [  558  4602]]
Matriz de confusión guardada en: outputs_hold_out/0/Key/confusion_matrix_param_160705.png
Accuracy:   0.9492
Precision:  0.8950
Recall:     0.8919
F1-score:   0.8934

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4296, Test Loss: 0.2921, F1: 0.7779, AUC: 0.9376
Epoch [10/30] Train Loss: 0.1294, Test Loss: 0.3160, F1: 0.7876, AUC: 0.9853
Epoch [20/30] Train Loss: 0.1110, Test Loss: 0.1822, F1: 0.8601, AUC: 0.9850
Mejores resultados en la época:  28
f1-score 0.8952623335943618
AUC según el mejor F1-score 0.9861562699359929
Confusion Matrix:
 [[15982   483]
 [  587  4573]]
Matriz de confusión guardada en: outputs_hold_out/0/Key/confusion_matrix_param_160705.png
Accuracy:   0.9505
Precision:  0.9045
Recall:     0.8862
F1-score:   0.8953
Tiempo total para red 1: 210.11 segundos

Entrenando red 3 con capas [5020, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3936, Test Loss: 0.2042, F1: 0.8446, AUC: 0.9712
Epoch [10/30] Train Loss: 0.1193, Test Loss: 0.4284, F1: 0.7576, AUC: 0.9842
Epoch [20/30] Train Loss: 0.0703, Test Loss: 0.2593, F1: 0.8377, AUC: 0.9872
Mejores resultados en la época:  22
f1-score 0.9089280382203643
AUC según el mejor F1-score 0.9883488937068764
Confusion Matrix:
 [[16144   321]
 [  594  4566]]
Matriz de confusión guardada en: outputs_hold_out/0/Key/confusion_matrix_param_653057.png
Accuracy:   0.9577
Precision:  0.9343
Recall:     0.8849
F1-score:   0.9089

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3920, Test Loss: 0.2234, F1: 0.8388, AUC: 0.9700
Epoch [10/30] Train Loss: 0.1286, Test Loss: 0.2054, F1: 0.8553, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0781, Test Loss: 0.1811, F1: 0.8764, AUC: 0.9882
Mejores resultados en la época:  27
f1-score 0.9096448633723698
AUC según el mejor F1-score 0.9884061445820004
Confusion Matrix:
 [[15899   566]
 [  383  4777]]
Matriz de confusión guardada en: outputs_hold_out/0/Key/confusion_matrix_param_653057.png
Accuracy:   0.9561
Precision:  0.8941
Recall:     0.9258
F1-score:   0.9096

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4024, Test Loss: 0.6099, F1: 0.6259, AUC: 0.9659
Epoch [10/30] Train Loss: 0.1240, Test Loss: 0.1347, F1: 0.8810, AUC: 0.9850
Epoch [20/30] Train Loss: 0.0795, Test Loss: 0.1275, F1: 0.9046, AUC: 0.9882
Mejores resultados en la época:  29
f1-score 0.9047143427886536
AUC según el mejor F1-score 0.9873431780356264
Confusion Matrix:
 [[16142   323]
 [  631  4529]]
Matriz de confusión guardada en: outputs_hold_out/0/Key/confusion_matrix_param_653057.png
Accuracy:   0.9559
Precision:  0.9334
Recall:     0.8777
F1-score:   0.9047
Tiempo total para red 3: 222.21 segundos

Entrenando red 5 con capas [5020, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3968, Test Loss: 0.2309, F1: 0.7787, AUC: 0.9627
Epoch [10/30] Train Loss: 0.1203, Test Loss: 0.1609, F1: 0.8788, AUC: 0.9862
Epoch [20/30] Train Loss: 0.0685, Test Loss: 0.1522, F1: 0.8979, AUC: 0.9871
Mejores resultados en la época:  25
f1-score 0.9049731055959234
AUC según el mejor F1-score 0.9889324606812194
Confusion Matrix:
 [[15823   642]
 [  365  4795]]
Matriz de confusión guardada en: outputs_hold_out/0/Key/confusion_matrix_param_2743297.png
Accuracy:   0.9534
Precision:  0.8819
Recall:     0.9293
F1-score:   0.9050

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3915, Test Loss: 0.2296, F1: 0.7776, AUC: 0.9584
Epoch [10/30] Train Loss: 0.1173, Test Loss: 0.2587, F1: 0.8342, AUC: 0.9832
Epoch [20/30] Train Loss: 0.0835, Test Loss: 0.1378, F1: 0.9008, AUC: 0.9879
Mejores resultados en la época:  24
f1-score 0.9152311435523114
AUC según el mejor F1-score 0.9890954679529281
Confusion Matrix:
 [[16052   413]
 [  458  4702]]
Matriz de confusión guardada en: outputs_hold_out/0/Key/confusion_matrix_param_2743297.png
Accuracy:   0.9597
Precision:  0.9193
Recall:     0.9112
F1-score:   0.9152

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3954, Test Loss: 0.1925, F1: 0.8246, AUC: 0.9699
Epoch [10/30] Train Loss: 0.1203, Test Loss: 0.1326, F1: 0.8869, AUC: 0.9848
Epoch [20/30] Train Loss: 0.0787, Test Loss: 0.1929, F1: 0.8752, AUC: 0.9868
Mejores resultados en la época:  25
f1-score 0.9086606720302887
AUC según el mejor F1-score 0.9895157804786758
Confusion Matrix:
 [[15860   605]
 [  360  4800]]
Matriz de confusión guardada en: outputs_hold_out/0/Key/confusion_matrix_param_2743297.png
Accuracy:   0.9554
Precision:  0.8881
Recall:     0.9302
F1-score:   0.9087
Tiempo total para red 5: 244.79 segundos

Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4028, Test Loss: 0.1923, F1: 0.8375, AUC: 0.9663
Epoch [10/30] Train Loss: 0.1195, Test Loss: 0.1949, F1: 0.8508, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0791, Test Loss: 0.1230, F1: 0.9067, AUC: 0.9883
Mejores resultados en la época:  28
f1-score 0.9141950203352842
AUC según el mejor F1-score 0.9897118388312537
Confusion Matrix:
 [[16152   313]
 [  552  4608]]
Matriz de confusión guardada en: outputs_hold_out/0/Key/confusion_matrix_param_5840897.png
Accuracy:   0.9600
Precision:  0.9364
Recall:     0.8930
F1-score:   0.9142

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3994, Test Loss: 0.2499, F1: 0.8149, AUC: 0.9713
Epoch [10/30] Train Loss: 0.1192, Test Loss: 0.1271, F1: 0.8949, AUC: 0.9862
Epoch [20/30] Train Loss: 0.0814, Test Loss: 0.1569, F1: 0.9012, AUC: 0.9874
Mejores resultados en la época:  27
f1-score 0.9088636800615266
AUC según el mejor F1-score 0.9879721725906728
Confusion Matrix:
 [[15950   515]
 [  433  4727]]
Matriz de confusión guardada en: outputs_hold_out/0/Key/confusion_matrix_param_5840897.png
Accuracy:   0.9562
Precision:  0.9018
Recall:     0.9161
F1-score:   0.9089

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4022, Test Loss: 0.1969, F1: 0.8307, AUC: 0.9676
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [14:54:33] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Epoch [10/30] Train Loss: 0.1161, Test Loss: 0.1442, F1: 0.8853, AUC: 0.9862
Epoch [20/30] Train Loss: 0.0752, Test Loss: 0.3846, F1: 0.8017, AUC: 0.9882
Mejores resultados en la época:  24
f1-score 0.9090216977540921
AUC según el mejor F1-score 0.9890119221651754
Confusion Matrix:
 [[15893   572]
 [  384  4776]]
Matriz de confusión guardada en: outputs_hold_out/0/Key/confusion_matrix_param_5840897.png
Accuracy:   0.9558
Precision:  0.8930
Recall:     0.9256
F1-score:   0.9090
Tiempo total para red 6: 280.43 segundos
Saved on: outputs_hold_out/0/Key

==============================
Model: Logistic Regression
Accuracy:  0.9350
Precision: 0.8269
Recall:    0.9203
F1-score:  0.8711
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15471   994]
 [  411  4749]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Key/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Key/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7970
Precision: 0.5453
Recall:    0.8994
F1-score:  0.6790
              precision    recall  f1-score   support

           0       0.96      0.76      0.85     16465
           1       0.55      0.90      0.68      5160

    accuracy                           0.80     21625
   macro avg       0.75      0.83      0.77     21625
weighted avg       0.86      0.80      0.81     21625

[[12595  3870]
 [  519  4641]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Key/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Key/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8726
Precision: 0.7198
Recall:    0.7636
F1-score:  0.7410
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14931  1534]
 [ 1220  3940]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Key/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Key/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8634
Precision: 0.6874
Recall:    0.7843
F1-score:  0.7327
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.78      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14625  1840]
 [ 1113  4047]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Key/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Key/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9208
Precision: 0.8025
Recall:    0.8864
F1-score:  0.8424
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15339  1126]
 [  586  4574]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Key/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Key/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7852
Precision: 0.5271
Recall:    0.9690
F1-score:  0.6828
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[11980  4485]
 [  160  5000]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Key/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Key/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.935, 'precision': 0.8269, 'recall': 0.9203, 'f1_score': 0.8711}
XGBoost: {'accuracy': 0.9208, 'precision': 0.8025, 'recall': 0.8864, 'f1_score': 0.8424}
Decision Tree: {'accuracy': 0.8726, 'precision': 0.7198, 'recall': 0.7636, 'f1_score': 0.741}
Random Forest: {'accuracy': 0.8634, 'precision': 0.6874, 'recall': 0.7843, 'f1_score': 0.7327}
Naive Bayes: {'accuracy': 0.7852, 'precision': 0.5271, 'recall': 0.969, 'f1_score': 0.6828}
SVM: {'accuracy': 0.797, 'precision': 0.5453, 'recall': 0.8994, 'f1_score': 0.679}

##################################################
Running experiment without SIMILAR ARTIST 1 embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5020, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4223, Test Loss: 0.3425, F1: 0.7647, AUC: 0.9392
Epoch [10/30] Train Loss: 0.1499, Test Loss: 0.2632, F1: 0.8074, AUC: 0.9801
Epoch [20/30] Train Loss: 0.1232, Test Loss: 0.2765, F1: 0.8092, AUC: 0.9810
Mejores resultados en la época:  29
f1-score 0.8818506751389992
AUC según el mejor F1-score 0.9818515314373688
Confusion Matrix:
 [[15994   471]
 [  719  4441]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 1/confusion_matrix_param_160705.png
Accuracy:   0.9450
Precision:  0.9041
Recall:     0.8607
F1-score:   0.8819

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4440, Test Loss: 0.4094, F1: 0.7258, AUC: 0.9211
Epoch [10/30] Train Loss: 0.1531, Test Loss: 0.1714, F1: 0.8679, AUC: 0.9788
Epoch [20/30] Train Loss: 0.1291, Test Loss: 0.2074, F1: 0.8433, AUC: 0.9796
Mejores resultados en la época:  26
f1-score 0.8787937367098395
AUC según el mejor F1-score 0.9804984263071539
Confusion Matrix:
 [[15825   640]
 [  614  4546]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 1/confusion_matrix_param_160705.png
Accuracy:   0.9420
Precision:  0.8766
Recall:     0.8810
F1-score:   0.8788

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4259, Test Loss: 0.3643, F1: 0.7603, AUC: 0.9389
Epoch [10/30] Train Loss: 0.1498, Test Loss: 0.1772, F1: 0.8622, AUC: 0.9804
Epoch [20/30] Train Loss: 0.1187, Test Loss: 0.2054, F1: 0.8500, AUC: 0.9808
Mejores resultados en la época:  25
f1-score 0.8804036007518053
AUC según el mejor F1-score 0.9814569723891647
Confusion Matrix:
 [[15966   499]
 [  710  4450]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 1/confusion_matrix_param_160705.png
Accuracy:   0.9441
Precision:  0.8992
Recall:     0.8624
F1-score:   0.8804
Tiempo total para red 1: 211.55 segundos

Entrenando red 3 con capas [5020, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4099, Test Loss: 0.2167, F1: 0.8214, AUC: 0.9609
Epoch [10/30] Train Loss: 0.1430, Test Loss: 0.1902, F1: 0.8467, AUC: 0.9762
Epoch [20/30] Train Loss: 0.0980, Test Loss: 0.1660, F1: 0.8720, AUC: 0.9844
Mejores resultados en la época:  16
f1-score 0.8865550193963478
AUC según el mejor F1-score 0.9835893732771184
Confusion Matrix:
 [[15741   724]
 [  475  4685]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 1/confusion_matrix_param_653057.png
Accuracy:   0.9446
Precision:  0.8661
Recall:     0.9079
F1-score:   0.8866

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4190, Test Loss: 0.2795, F1: 0.7957, AUC: 0.9485
Epoch [10/30] Train Loss: 0.1395, Test Loss: 0.1453, F1: 0.8795, AUC: 0.9816
Epoch [20/30] Train Loss: 0.0955, Test Loss: 0.1582, F1: 0.8828, AUC: 0.9845
Mejores resultados en la época:  21
f1-score 0.8960734518460637
AUC según el mejor F1-score 0.9852189516404305
Confusion Matrix:
 [[15974   491]
 [  573  4587]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 1/confusion_matrix_param_653057.png
Accuracy:   0.9508
Precision:  0.9033
Recall:     0.8890
F1-score:   0.8961

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4052, Test Loss: 0.2386, F1: 0.8100, AUC: 0.9551
Epoch [10/30] Train Loss: 0.1363, Test Loss: 0.1542, F1: 0.8749, AUC: 0.9810
Epoch [20/30] Train Loss: 0.0906, Test Loss: 0.1536, F1: 0.8741, AUC: 0.9825
Mejores resultados en la época:  27
f1-score 0.8934754964296194
AUC según el mejor F1-score 0.9846150749652186
Confusion Matrix:
 [[15969   496]
 [  593  4567]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 1/confusion_matrix_param_653057.png
Accuracy:   0.9496
Precision:  0.9020
Recall:     0.8851
F1-score:   0.8935
Tiempo total para red 3: 224.96 segundos

Entrenando red 5 con capas [5020, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4066, Test Loss: 0.2289, F1: 0.7991, AUC: 0.9560
Epoch [10/30] Train Loss: 0.1367, Test Loss: 0.2135, F1: 0.8407, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0952, Test Loss: 0.1656, F1: 0.8763, AUC: 0.9830
Mejores resultados en la época:  16
f1-score 0.8916939680092504
AUC según el mejor F1-score 0.9842011125313974
Confusion Matrix:
 [[15874   591]
 [  533  4627]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 1/confusion_matrix_param_2743297.png
Accuracy:   0.9480
Precision:  0.8867
Recall:     0.8967
F1-score:   0.8917

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4052, Test Loss: 0.2211, F1: 0.8254, AUC: 0.9599
Epoch [10/30] Train Loss: 0.1421, Test Loss: 0.1611, F1: 0.8721, AUC: 0.9819
Epoch [20/30] Train Loss: 0.0861, Test Loss: 0.2590, F1: 0.8326, AUC: 0.9836
Mejores resultados en la época:  21
f1-score 0.8928847641144625
AUC según el mejor F1-score 0.9847588495210655
Confusion Matrix:
 [[15899   566]
 [  542  4618]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 1/confusion_matrix_param_2743297.png
Accuracy:   0.9488
Precision:  0.8908
Recall:     0.8950
F1-score:   0.8929

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4180, Test Loss: 0.5031, F1: 0.6798, AUC: 0.9559
Epoch [10/30] Train Loss: 0.1397, Test Loss: 0.1430, F1: 0.8811, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0890, Test Loss: 0.4054, F1: 0.7903, AUC: 0.9823
Mejores resultados en la época:  18
f1-score 0.8879892037786775
AUC según el mejor F1-score 0.9829674644594947
Confusion Matrix:
 [[15857   608]
 [  554  4606]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 1/confusion_matrix_param_2743297.png
Accuracy:   0.9463
Precision:  0.8834
Recall:     0.8926
F1-score:   0.8880
Tiempo total para red 5: 246.69 segundos

Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4192, Test Loss: 0.2748, F1: 0.7993, AUC: 0.9538
Epoch [10/30] Train Loss: 0.1412, Test Loss: 0.1535, F1: 0.8795, AUC: 0.9810
Epoch [20/30] Train Loss: 0.0900, Test Loss: 0.2272, F1: 0.8446, AUC: 0.9850
Mejores resultados en la época:  23
f1-score 0.8956922774997547
AUC según el mejor F1-score 0.9844931755638576
Confusion Matrix:
 [[15998   467]
 [  596  4564]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 1/confusion_matrix_param_5840897.png
Accuracy:   0.9508
Precision:  0.9072
Recall:     0.8845
F1-score:   0.8957

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4248, Test Loss: 0.3390, F1: 0.7687, AUC: 0.9539
Epoch [10/30] Train Loss: 0.1368, Test Loss: 0.3514, F1: 0.7614, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0871, Test Loss: 0.2094, F1: 0.8795, AUC: 0.9815
Mejores resultados en la época:  22
f1-score 0.8979911807937285
AUC según el mejor F1-score 0.9854587897277993
Confusion Matrix:
 [[16002   463]
 [  578  4582]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 1/confusion_matrix_param_5840897.png
Accuracy:   0.9519
Precision:  0.9082
Recall:     0.8880
F1-score:   0.8980

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4309, Test Loss: 0.2424, F1: 0.7887, AUC: 0.9461
Epoch [10/30] Train Loss: 0.1392, Test Loss: 0.1565, F1: 0.8797, AUC: 0.9819
Epoch [20/30] Train Loss: 0.0877, Test Loss: 0.2507, F1: 0.8596, AUC: 0.9849
Mejores resultados en la época:  28
f1-score 0.8976529434397845
AUC según el mejor F1-score 0.9856748870637033
Confusion Matrix:
 [[15895   570]
 [  494  4666]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 1/confusion_matrix_param_5840897.png
Accuracy:   0.9508
Precision:  0.8911
Recall:     0.9043
F1-score:   0.8977
Tiempo total para red 6: 283.04 segundos
Saved on: outputs_hold_out/0/Similar Artist 1

==============================
Model: Logistic Regression
Accuracy:  0.9252
Precision: 0.8045
Recall:    0.9068
F1-score:  0.8526
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.91      0.85      5160

    accuracy                           0.93     21625
   macro avg       0.89      0.92      0.90     21625
weighted avg       0.93      0.93      0.93     21625

[[15328  1137]
 [  481  4679]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [15:21:06] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Artist 1/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Similar Artist 1/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7303
Precision: 0.4648
Recall:    0.8595
F1-score:  0.6033
              precision    recall  f1-score   support

           0       0.94      0.69      0.80     16465
           1       0.46      0.86      0.60      5160

    accuracy                           0.73     21625
   macro avg       0.70      0.77      0.70     21625
weighted avg       0.83      0.73      0.75     21625

[[11358  5107]
 [  725  4435]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Artist 1/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Similar Artist 1/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8728
Precision: 0.7227
Recall:    0.7576
F1-score:  0.7397
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14965  1500]
 [ 1251  3909]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Similar Artist 1/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Similar Artist 1/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8564
Precision: 0.6717
Recall:    0.7787
F1-score:  0.7212
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.78      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

[[14501  1964]
 [ 1142  4018]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Artist 1/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Similar Artist 1/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9157
Precision: 0.7917
Recall:    0.8775
F1-score:  0.8324
              precision    recall  f1-score   support

           0       0.96      0.93      0.94     16465
           1       0.79      0.88      0.83      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.90      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15274  1191]
 [  632  4528]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Artist 1/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Similar Artist 1/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7488
Precision: 0.4867
Recall:    0.9692
F1-score:  0.6480
              precision    recall  f1-score   support

           0       0.99      0.68      0.80     16465
           1       0.49      0.97      0.65      5160

    accuracy                           0.75     21625
   macro avg       0.74      0.82      0.73     21625
weighted avg       0.87      0.75      0.77     21625

[[11191  5274]
 [  159  5001]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Similar Artist 1/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Similar Artist 1/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9252, 'precision': 0.8045, 'recall': 0.9068, 'f1_score': 0.8526}
XGBoost: {'accuracy': 0.9157, 'precision': 0.7917, 'recall': 0.8775, 'f1_score': 0.8324}
Decision Tree: {'accuracy': 0.8728, 'precision': 0.7227, 'recall': 0.7576, 'f1_score': 0.7397}
Random Forest: {'accuracy': 0.8564, 'precision': 0.6717, 'recall': 0.7787, 'f1_score': 0.7212}
Naive Bayes: {'accuracy': 0.7488, 'precision': 0.4867, 'recall': 0.9692, 'f1_score': 0.648}
SVM: {'accuracy': 0.7303, 'precision': 0.4648, 'recall': 0.8595, 'f1_score': 0.6033}

##################################################
Running experiment without SIMILAR SONG 1 embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5020, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4454, Test Loss: 0.4050, F1: 0.7223, AUC: 0.9250
Epoch [10/30] Train Loss: 0.1388, Test Loss: 0.1907, F1: 0.8548, AUC: 0.9840
Epoch [20/30] Train Loss: 0.1152, Test Loss: 0.1385, F1: 0.8880, AUC: 0.9841
Mejores resultados en la época:  27
f1-score 0.8915571523116196
AUC según el mejor F1-score 0.9847579137799938
Confusion Matrix:
 [[15763   702]
 [  445  4715]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 1/confusion_matrix_param_160705.png
Accuracy:   0.9470
Precision:  0.8704
Recall:     0.9138
F1-score:   0.8916

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4249, Test Loss: 0.2775, F1: 0.7868, AUC: 0.9408
Epoch [10/30] Train Loss: 0.1351, Test Loss: 0.2093, F1: 0.8459, AUC: 0.9844
Epoch [20/30] Train Loss: 0.1104, Test Loss: 0.1587, F1: 0.8801, AUC: 0.9849
Mejores resultados en la época:  24
f1-score 0.8953361718982077
AUC según el mejor F1-score 0.9850026012424759
Confusion Matrix:
 [[16047   418]
 [  639  4521]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 1/confusion_matrix_param_160705.png
Accuracy:   0.9511
Precision:  0.9154
Recall:     0.8762
F1-score:   0.8953

/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4386, Test Loss: 0.3616, F1: 0.7453, AUC: 0.9296
Epoch [10/30] Train Loss: 0.1388, Test Loss: 0.1670, F1: 0.8743, AUC: 0.9842
Epoch [20/30] Train Loss: 0.1141, Test Loss: 0.1415, F1: 0.8863, AUC: 0.9838
Mejores resultados en la época:  22
f1-score 0.8923828125
AUC según el mejor F1-score 0.984875311030916
Confusion Matrix:
 [[15954   511]
 [  591  4569]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 1/confusion_matrix_param_160705.png
Accuracy:   0.9490
Precision:  0.8994
Recall:     0.8855
F1-score:   0.8924
Tiempo total para red 1: 213.56 segundos

Entrenando red 3 con capas [5020, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3936, Test Loss: 0.3025, F1: 0.7968, AUC: 0.9695
Epoch [10/30] Train Loss: 0.1330, Test Loss: 0.1449, F1: 0.8885, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0827, Test Loss: 0.2536, F1: 0.8423, AUC: 0.9873
Mejores resultados en la época:  23
f1-score 0.9086005971299239
AUC según el mejor F1-score 0.9882250816272243
Confusion Matrix:
 [[15959   506]
 [  443  4717]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 1/confusion_matrix_param_653057.png
Accuracy:   0.9561
Precision:  0.9031
Recall:     0.9141
F1-score:   0.9086

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4071, Test Loss: 0.2535, F1: 0.8175, AUC: 0.9659
Epoch [10/30] Train Loss: 0.1289, Test Loss: 0.2930, F1: 0.7932, AUC: 0.9850
Epoch [20/30] Train Loss: 0.0798, Test Loss: 0.1863, F1: 0.8600, AUC: 0.9797
Mejores resultados en la época:  29
f1-score 0.9047021338225355
AUC según el mejor F1-score 0.9880295588245681
Confusion Matrix:
 [[15953   512]
 [  475  4685]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 1/confusion_matrix_param_653057.png
Accuracy:   0.9544
Precision:  0.9015
Recall:     0.9079
F1-score:   0.9047

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3956, Test Loss: 0.3045, F1: 0.7908, AUC: 0.9666
Epoch [10/30] Train Loss: 0.1259, Test Loss: 0.2982, F1: 0.7902, AUC: 0.9842
Epoch [20/30] Train Loss: 0.0879, Test Loss: 0.2980, F1: 0.8117, AUC: 0.9864
Mejores resultados en la época:  29
f1-score 0.9058730472612221
AUC según el mejor F1-score 0.9881652059689687
Confusion Matrix:
 [[16092   373]
 [  579  4581]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 1/confusion_matrix_param_653057.png
Accuracy:   0.9560
Precision:  0.9247
Recall:     0.8878
F1-score:   0.9059
Tiempo total para red 3: 224.68 segundos

Entrenando red 5 con capas [5020, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3970, Test Loss: 0.2054, F1: 0.8336, AUC: 0.9655
Epoch [10/30] Train Loss: 0.1291, Test Loss: 0.1301, F1: 0.8966, AUC: 0.9863
Epoch [20/30] Train Loss: 0.0815, Test Loss: 0.2461, F1: 0.8367, AUC: 0.9873
Mejores resultados en la época:  28
f1-score 0.9129762134983562
AUC según el mejor F1-score 0.9892462870500497
Confusion Matrix:
 [[16004   461]
 [  439  4721]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 1/confusion_matrix_param_2743297.png
Accuracy:   0.9584
Precision:  0.9110
Recall:     0.9149
F1-score:   0.9130

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4110, Test Loss: 0.2347, F1: 0.8245, AUC: 0.9641
Epoch [10/30] Train Loss: 0.1258, Test Loss: 0.1933, F1: 0.8555, AUC: 0.9856
Epoch [20/30] Train Loss: 0.0757, Test Loss: 0.1381, F1: 0.8994, AUC: 0.9874
Mejores resultados en la época:  28
f1-score 0.9115853658536586
AUC según el mejor F1-score 0.9889458200034368
Confusion Matrix:
 [[15913   552]
 [  376  4784]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 1/confusion_matrix_param_2743297.png
Accuracy:   0.9571
Precision:  0.8966
Recall:     0.9271
F1-score:   0.9116

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4068, Test Loss: 0.2110, F1: 0.8319, AUC: 0.9661
Epoch [10/30] Train Loss: 0.1344, Test Loss: 0.1518, F1: 0.8756, AUC: 0.9828
Epoch [20/30] Train Loss: 0.0793, Test Loss: 0.1403, F1: 0.9036, AUC: 0.9879
Mejores resultados en la época:  20
f1-score 0.9035880841343866
AUC según el mejor F1-score 0.9878732900656079
Confusion Matrix:
 [[15865   600]
 [  413  4747]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 1/confusion_matrix_param_2743297.png
Accuracy:   0.9532
Precision:  0.8878
Recall:     0.9200
F1-score:   0.9036
Tiempo total para red 5: 247.04 segundos

Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4066, Test Loss: 0.3169, F1: 0.7775, AUC: 0.9560
Epoch [10/30] Train Loss: 0.1240, Test Loss: 0.1776, F1: 0.8567, AUC: 0.9847
Epoch [20/30] Train Loss: 0.0847, Test Loss: 0.1567, F1: 0.9000, AUC: 0.9885
Mejores resultados en la época:  27
f1-score 0.9053041362530414
AUC según el mejor F1-score 0.98794927930282
Confusion Matrix:
 [[16001   464]
 [  509  4651]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 1/confusion_matrix_param_5840897.png
Accuracy:   0.9550
Precision:  0.9093
Recall:     0.9014
F1-score:   0.9053

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3982, Test Loss: 0.2118, F1: 0.8410, AUC: 0.9706
Epoch [10/30] Train Loss: 0.1219, Test Loss: 0.1547, F1: 0.8818, AUC: 0.9836
Epoch [20/30] Train Loss: 0.0853, Test Loss: 0.2363, F1: 0.8415, AUC: 0.9859
Mejores resultados en la época:  28
f1-score 0.907832353498957
AUC según el mejor F1-score 0.9887838426354236
Confusion Matrix:
 [[15866   599]
 [  373  4787]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 1/confusion_matrix_param_5840897.png
Accuracy:   0.9551
Precision:  0.8888
Recall:     0.9277
F1-score:   0.9078

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4100, Test Loss: 0.2816, F1: 0.7694, AUC: 0.9533
Epoch [10/30] Train Loss: 0.1270, Test Loss: 0.1718, F1: 0.8698, AUC: 0.9838
Epoch [20/30] Train Loss: 0.0818, Test Loss: 0.1834, F1: 0.8954, AUC: 0.9861
Mejores resultados en la época:  28
f1-score 0.9023111283744416
AUC según el mejor F1-score 0.9872096789760756
Confusion Matrix:
 [[15973   492]
 [  514  4646]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 1/confusion_matrix_param_5840897.png
Accuracy:   0.9535
Precision:  0.9042
Recall:     0.9004
F1-score:   0.9023
Tiempo total para red 6: 280.60 segundos
Saved on: outputs_hold_out/0/Similar Song 1

==============================
Model: Logistic Regression
Accuracy:  0.9351
Precision: 0.8268
Recall:    0.9207
F1-score:  0.8713
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15470   995]
 [  409  4751]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Song 1/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Similar Song 1/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7696
Precision: 0.5095
Recall:    0.9264
F1-score:  0.6574
              precision    recall  f1-score   support

           0       0.97      0.72      0.83     16465
           1       0.51      0.93      0.66      5160

    accuracy                           0.77     21625
   macro avg       0.74      0.82      0.74     21625
weighted avg       0.86      0.77      0.79     21625

[[11863  4602]
 [  380  4780]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [15:47:40] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Song 1/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Similar Song 1/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8731
Precision: 0.7210
Recall:    0.7636
F1-score:  0.7416
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14940  1525]
 [ 1220  3940]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Similar Song 1/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Similar Song 1/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8663
Precision: 0.6941
Recall:    0.7859
F1-score:  0.7371
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.87      0.87     21625

[[14678  1787]
 [ 1105  4055]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Song 1/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Similar Song 1/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9190
Precision: 0.7988
Recall:    0.8831
F1-score:  0.8388
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15317  1148]
 [  603  4557]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Song 1/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Similar Song 1/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7908
Precision: 0.5339
Recall:    0.9707
F1-score:  0.6889
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.77     21625
weighted avg       0.88      0.79      0.81     21625

[[12092  4373]
 [  151  5009]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Similar Song 1/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Similar Song 1/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9351, 'precision': 0.8268, 'recall': 0.9207, 'f1_score': 0.8713}
XGBoost: {'accuracy': 0.919, 'precision': 0.7988, 'recall': 0.8831, 'f1_score': 0.8388}
Decision Tree: {'accuracy': 0.8731, 'precision': 0.721, 'recall': 0.7636, 'f1_score': 0.7416}
Random Forest: {'accuracy': 0.8663, 'precision': 0.6941, 'recall': 0.7859, 'f1_score': 0.7371}
Naive Bayes: {'accuracy': 0.7908, 'precision': 0.5339, 'recall': 0.9707, 'f1_score': 0.6889}
SVM: {'accuracy': 0.7696, 'precision': 0.5095, 'recall': 0.9264, 'f1_score': 0.6574}

##################################################
Running experiment without SIMILAR ARTIST 2 embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5020, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4429, Test Loss: 0.3074, F1: 0.7610, AUC: 0.9223
Epoch [10/30] Train Loss: 0.1603, Test Loss: 0.1792, F1: 0.8603, AUC: 0.9791
Epoch [20/30] Train Loss: 0.1360, Test Loss: 0.1845, F1: 0.8591, AUC: 0.9803
Mejores resultados en la época:  28
f1-score 0.8763717587646888
AUC según el mejor F1-score 0.9807583798849804
Confusion Matrix:
 [[15840   625]
 [  648  4512]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 2/confusion_matrix_param_160705.png
Accuracy:   0.9411
Precision:  0.8783
Recall:     0.8744
F1-score:   0.8764

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4347, Test Loss: 0.3059, F1: 0.7705, AUC: 0.9323
Epoch [10/30] Train Loss: 0.1536, Test Loss: 0.1525, F1: 0.8708, AUC: 0.9793
Epoch [20/30] Train Loss: 0.1253, Test Loss: 0.1636, F1: 0.8691, AUC: 0.9803
Mejores resultados en la época:  25
f1-score 0.8768345894486315
AUC según el mejor F1-score 0.9805548650296495
Confusion Matrix:
 [[15962   503]
 [  739  4421]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 2/confusion_matrix_param_160705.png
Accuracy:   0.9426
Precision:  0.8978
Recall:     0.8568
F1-score:   0.8768

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4456, Test Loss: 0.3140, F1: 0.7385, AUC: 0.9156
Epoch [10/30] Train Loss: 0.1613, Test Loss: 0.3785, F1: 0.7403, AUC: 0.9792
Epoch [20/30] Train Loss: 0.1263, Test Loss: 0.1869, F1: 0.8527, AUC: 0.9799
Mejores resultados en la época:  26
f1-score 0.8756798756798757
AUC según el mejor F1-score 0.9801067156783122
Confusion Matrix:
 [[15837   628]
 [  652  4508]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 2/confusion_matrix_param_160705.png
Accuracy:   0.9408
Precision:  0.8777
Recall:     0.8736
F1-score:   0.8757
Tiempo total para red 1: 209.95 segundos

Entrenando red 3 con capas [5020, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4029, Test Loss: 0.2398, F1: 0.7833, AUC: 0.9502
Epoch [10/30] Train Loss: 0.1446, Test Loss: 0.4139, F1: 0.7290, AUC: 0.9802
Epoch [20/30] Train Loss: 0.1010, Test Loss: 0.1553, F1: 0.8777, AUC: 0.9820
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Mejores resultados en la época:  27
f1-score 0.8907846443805179
AUC según el mejor F1-score 0.9840938495328357
Confusion Matrix:
 [[16034   431]
 [  670  4490]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 2/confusion_matrix_param_653057.png
Accuracy:   0.9491
Precision:  0.9124
Recall:     0.8702
F1-score:   0.8908

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4123, Test Loss: 0.3692, F1: 0.7417, AUC: 0.9584
Epoch [10/30] Train Loss: 0.1379, Test Loss: 0.1692, F1: 0.8660, AUC: 0.9809
Epoch [20/30] Train Loss: 0.0936, Test Loss: 0.2187, F1: 0.8490, AUC: 0.9832
Mejores resultados en la época:  28
f1-score 0.8884186225471336
AUC según el mejor F1-score 0.9841090626817044
Confusion Matrix:
 [[15847   618]
 [  542  4618]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 2/confusion_matrix_param_653057.png
Accuracy:   0.9464
Precision:  0.8820
Recall:     0.8950
F1-score:   0.8884

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4290, Test Loss: 0.2356, F1: 0.8021, AUC: 0.9505
Epoch [10/30] Train Loss: 0.1475, Test Loss: 0.1587, F1: 0.8671, AUC: 0.9790
Epoch [20/30] Train Loss: 0.0949, Test Loss: 0.2504, F1: 0.8390, AUC: 0.9825
Mejores resultados en la época:  23
f1-score 0.8924680246328754
AUC según el mejor F1-score 0.9845869850775781
Confusion Matrix:
 [[15780   685]
 [  450  4710]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 2/confusion_matrix_param_653057.png
Accuracy:   0.9475
Precision:  0.8730
Recall:     0.9128
F1-score:   0.8925
Tiempo total para red 3: 222.87 segundos

Entrenando red 5 con capas [5020, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4198, Test Loss: 0.6033, F1: 0.6389, AUC: 0.9512
Epoch [10/30] Train Loss: 0.1445, Test Loss: 0.2365, F1: 0.8408, AUC: 0.9820
Epoch [20/30] Train Loss: 0.0917, Test Loss: 0.1580, F1: 0.8827, AUC: 0.9848
Mejores resultados en la época:  23
f1-score 0.8911866075824717
AUC según el mejor F1-score 0.9839641993705228
Confusion Matrix:
 [[15995   470]
 [  635  4525]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 2/confusion_matrix_param_2743297.png
Accuracy:   0.9489
Precision:  0.9059
Recall:     0.8769
F1-score:   0.8912

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4145, Test Loss: 0.2547, F1: 0.8116, AUC: 0.9568
Epoch [10/30] Train Loss: 0.1357, Test Loss: 0.1614, F1: 0.8770, AUC: 0.9818
Epoch [20/30] Train Loss: 0.0912, Test Loss: 0.2675, F1: 0.8485, AUC: 0.9848
Mejores resultados en la época:  21
f1-score 0.8927322191993782
AUC según el mejor F1-score 0.9842632010113068
Confusion Matrix:
 [[15927   538]
 [  566  4594]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 2/confusion_matrix_param_2743297.png
Accuracy:   0.9489
Precision:  0.8952
Recall:     0.8903
F1-score:   0.8927

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4091, Test Loss: 0.2444, F1: 0.7959, AUC: 0.9514
Epoch [10/30] Train Loss: 0.1430, Test Loss: 0.2124, F1: 0.8489, AUC: 0.9799
Epoch [20/30] Train Loss: 0.0858, Test Loss: 0.1512, F1: 0.8916, AUC: 0.9845
Mejores resultados en la época:  27
f1-score 0.8957632817753867
AUC según el mejor F1-score 0.9848007518885491
Confusion Matrix:
 [[15878   587]
 [  498  4662]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 2/confusion_matrix_param_2743297.png
Accuracy:   0.9498
Precision:  0.8882
Recall:     0.9035
F1-score:   0.8958
Tiempo total para red 5: 246.26 segundos

Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4258, Test Loss: 0.2725, F1: 0.7914, AUC: 0.9455
Epoch [10/30] Train Loss: 0.1419, Test Loss: 0.1447, F1: 0.8775, AUC: 0.9814
Epoch [20/30] Train Loss: 0.0923, Test Loss: 0.3499, F1: 0.8000, AUC: 0.9829
Mejores resultados en la época:  28
f1-score 0.8951936537564162
AUC según el mejor F1-score 0.9865779654752741
Confusion Matrix:
 [[15706   759]
 [  364  4796]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 2/confusion_matrix_param_5840897.png
Accuracy:   0.9481
Precision:  0.8634
Recall:     0.9295
F1-score:   0.8952

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4260, Test Loss: 0.2705, F1: 0.7237, AUC: 0.9438
Epoch [10/30] Train Loss: 0.1469, Test Loss: 0.2756, F1: 0.8064, AUC: 0.9815
Epoch [20/30] Train Loss: 0.0912, Test Loss: 0.1962, F1: 0.8656, AUC: 0.9845
Mejores resultados en la época:  22
f1-score 0.8911937377690803
AUC según el mejor F1-score 0.9845839306774765
Confusion Matrix:
 [[15959   506]
 [  606  4554]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 2/confusion_matrix_param_5840897.png
Accuracy:   0.9486
Precision:  0.9000
Recall:     0.8826
F1-score:   0.8912

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4303, Test Loss: 0.3566, F1: 0.4680, AUC: 0.9445
Epoch [10/30] Train Loss: 0.1341, Test Loss: 0.1592, F1: 0.8743, AUC: 0.9807
Epoch [20/30] Train Loss: 0.0924, Test Loss: 0.1679, F1: 0.8863, AUC: 0.9845
Mejores resultados en la época:  29
f1-score 0.9
AUC según el mejor F1-score 0.9859993714644876
Confusion Matrix:
 [[15949   516]
 [  516  4644]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 2/confusion_matrix_param_5840897.png
Accuracy:   0.9523
Precision:  0.9000
Recall:     0.9000
F1-score:   0.9000
Tiempo total para red 6: 281.17 segundos
Saved on: outputs_hold_out/0/Similar Artist 2

==============================
Model: Logistic Regression
Accuracy:  0.9238
Precision: 0.8013
Recall:    0.9052
F1-score:  0.8501
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.91      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.89      0.92      0.90     21625
weighted avg       0.93      0.92      0.93     21625

[[15307  1158]
 [  489  4671]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Artist 2/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Similar Artist 2/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7905
Precision: 0.5377
Recall:    0.8711
F1-score:  0.6649
              precision    recall  f1-score   support

           0       0.95      0.77      0.85     16465
           1       0.54      0.87      0.66      5160

    accuracy                           0.79     21625
   macro avg       0.74      0.82      0.76     21625
weighted avg       0.85      0.79      0.80     21625

[[12600  3865]
 [  665  4495]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Artist 2/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Similar Artist 2/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8720
Precision: 0.7215
Recall:    0.7550
F1-score:  0.7379
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14961  1504]
 [ 1264  3896]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [16:14:06] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Similar Artist 2/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Similar Artist 2/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8567
Precision: 0.6718
Recall:    0.7810
F1-score:  0.7223
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.78      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

[[14496  1969]
 [ 1130  4030]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Artist 2/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Similar Artist 2/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9134
Precision: 0.7862
Recall:    0.8752
F1-score:  0.8283
              precision    recall  f1-score   support

           0       0.96      0.93      0.94     16465
           1       0.79      0.88      0.83      5160

    accuracy                           0.91     21625
   macro avg       0.87      0.90      0.89     21625
weighted avg       0.92      0.91      0.91     21625

[[15237  1228]
 [  644  4516]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Artist 2/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Similar Artist 2/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7490
Precision: 0.4869
Recall:    0.9645
F1-score:  0.6472
              precision    recall  f1-score   support

           0       0.98      0.68      0.81     16465
           1       0.49      0.96      0.65      5160

    accuracy                           0.75     21625
   macro avg       0.74      0.82      0.73     21625
weighted avg       0.87      0.75      0.77     21625

[[11221  5244]
 [  183  4977]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Similar Artist 2/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Similar Artist 2/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9238, 'precision': 0.8013, 'recall': 0.9052, 'f1_score': 0.8501}
XGBoost: {'accuracy': 0.9134, 'precision': 0.7862, 'recall': 0.8752, 'f1_score': 0.8283}
Decision Tree: {'accuracy': 0.872, 'precision': 0.7215, 'recall': 0.755, 'f1_score': 0.7379}
Random Forest: {'accuracy': 0.8567, 'precision': 0.6718, 'recall': 0.781, 'f1_score': 0.7223}
SVM: {'accuracy': 0.7905, 'precision': 0.5377, 'recall': 0.8711, 'f1_score': 0.6649}
Naive Bayes: {'accuracy': 0.749, 'precision': 0.4869, 'recall': 0.9645, 'f1_score': 0.6472}

##################################################
Running experiment without SIMILAR SONG 2 embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5020, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4288, Test Loss: 0.4121, F1: 0.7267, AUC: 0.9320
Epoch [10/30] Train Loss: 0.1305, Test Loss: 0.1356, F1: 0.8916, AUC: 0.9844
Epoch [20/30] Train Loss: 0.1098, Test Loss: 0.1849, F1: 0.8634, AUC: 0.9852
Mejores resultados en la época:  25
f1-score 0.8923451700552797
AUC según el mejor F1-score 0.9857360162618851
Confusion Matrix:
 [[15714   751]
 [  398  4762]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 2/confusion_matrix_param_160705.png
Accuracy:   0.9469
Precision:  0.8638
Recall:     0.9229
F1-score:   0.8923

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4335, Test Loss: 0.3065, F1: 0.7743, AUC: 0.9364
Epoch [10/30] Train Loss: 0.1290, Test Loss: 0.1861, F1: 0.8595, AUC: 0.9839
Epoch [20/30] Train Loss: 0.1077, Test Loss: 0.1389, F1: 0.8903, AUC: 0.9848
Mejores resultados en la época:  22
f1-score 0.8912762359494678
AUC según el mejor F1-score 0.9845620437526629
Confusion Matrix:
 [[16052   413]
 [  680  4480]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 2/confusion_matrix_param_160705.png
Accuracy:   0.9495
Precision:  0.9156
Recall:     0.8682
F1-score:   0.8913

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4262, Test Loss: 0.2893, F1: 0.7838, AUC: 0.9412
Epoch [10/30] Train Loss: 0.1366, Test Loss: 0.2428, F1: 0.8247, AUC: 0.9846
Epoch [20/30] Train Loss: 0.1081, Test Loss: 0.1567, F1: 0.8794, AUC: 0.9849
Mejores resultados en la época:  28
f1-score 0.8943607570490537
AUC según el mejor F1-score 0.9848734101229528
Confusion Matrix:
 [[15900   565]
 [  529  4631]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 2/confusion_matrix_param_160705.png
Accuracy:   0.9494
Precision:  0.8913
Recall:     0.8975
F1-score:   0.8944
Tiempo total para red 1: 210.35 segundos

Entrenando red 3 con capas [5020, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4054, Test Loss: 0.1977, F1: 0.8391, AUC: 0.9655
Epoch [10/30] Train Loss: 0.1260, Test Loss: 0.1912, F1: 0.8600, AUC: 0.9852
Epoch [20/30] Train Loss: 0.0854, Test Loss: 0.2399, F1: 0.8462, AUC: 0.9860
Mejores resultados en la época:  25
f1-score 0.9004053271569196
AUC según el mejor F1-score 0.9864175476757132
Confusion Matrix:
 [[15928   537]
 [  495  4665]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 2/confusion_matrix_param_653057.png
Accuracy:   0.9523
Precision:  0.8968
Recall:     0.9041
F1-score:   0.9004

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3895, Test Loss: 0.1948, F1: 0.8308, AUC: 0.9680
Epoch [10/30] Train Loss: 0.1244, Test Loss: 0.2434, F1: 0.8211, AUC: 0.9856
Epoch [20/30] Train Loss: 0.0795, Test Loss: 0.2912, F1: 0.8177, AUC: 0.9855
Mejores resultados en la época:  29
f1-score 0.9046520569011919
AUC según el mejor F1-score 0.9872424828800579
Confusion Matrix:
 [[15927   538]
 [  454  4706]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 2/confusion_matrix_param_653057.png
Accuracy:   0.9541
Precision:  0.8974
Recall:     0.9120
F1-score:   0.9047

/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4205, Test Loss: 0.2372, F1: 0.8265, AUC: 0.9629
Epoch [10/30] Train Loss: 0.1224, Test Loss: 0.1384, F1: 0.8934, AUC: 0.9845
Epoch [20/30] Train Loss: 0.0788, Test Loss: 0.1539, F1: 0.8951, AUC: 0.9865
Mejores resultados en la época:  24
f1-score 0.9089669944503943
AUC según el mejor F1-score 0.9881180128390737
Confusion Matrix:
 [[16022   443]
 [  492  4668]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 2/confusion_matrix_param_653057.png
Accuracy:   0.9568
Precision:  0.9133
Recall:     0.9047
F1-score:   0.9090
Tiempo total para red 3: 223.83 segundos

Entrenando red 5 con capas [5020, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4088, Test Loss: 0.2298, F1: 0.8306, AUC: 0.9613
Epoch [10/30] Train Loss: 0.1183, Test Loss: 0.1602, F1: 0.8777, AUC: 0.9838
Epoch [20/30] Train Loss: 0.0744, Test Loss: 0.2355, F1: 0.8631, AUC: 0.9881
Mejores resultados en la época:  28
f1-score 0.9091085083728584
AUC según el mejor F1-score 0.9882947913944778
Confusion Matrix:
 [[15990   475]
 [  464  4696]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 2/confusion_matrix_param_2743297.png
Accuracy:   0.9566
Precision:  0.9081
Recall:     0.9101
F1-score:   0.9091

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3875, Test Loss: 0.3826, F1: 0.7309, AUC: 0.9675
Epoch [10/30] Train Loss: 0.1269, Test Loss: 0.2332, F1: 0.8394, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0826, Test Loss: 0.1841, F1: 0.8830, AUC: 0.9878
Mejores resultados en la época:  28
f1-score 0.9088422081094284
AUC según el mejor F1-score 0.9887065233511536
Confusion Matrix:
 [[16041   424]
 [  509  4651]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 2/confusion_matrix_param_2743297.png
Accuracy:   0.9569
Precision:  0.9165
Recall:     0.9014
F1-score:   0.9088

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4004, Test Loss: 0.1904, F1: 0.8353, AUC: 0.9661
Epoch [10/30] Train Loss: 0.1247, Test Loss: 0.2146, F1: 0.8543, AUC: 0.9852
Epoch [20/30] Train Loss: 0.0825, Test Loss: 0.1633, F1: 0.8834, AUC: 0.9853
Mejores resultados en la época:  23
f1-score 0.9028000377109456
AUC según el mejor F1-score 0.9872982742345168
Confusion Matrix:
 [[15806   659]
 [  372  4788]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 2/confusion_matrix_param_2743297.png
Accuracy:   0.9523
Precision:  0.8790
Recall:     0.9279
F1-score:   0.9028
Tiempo total para red 5: 245.60 segundos

Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4111, Test Loss: 0.5059, F1: 0.6473, AUC: 0.9574
Epoch [10/30] Train Loss: 0.1219, Test Loss: 0.2030, F1: 0.8392, AUC: 0.9856
Epoch [20/30] Train Loss: 0.0763, Test Loss: 0.2238, F1: 0.8644, AUC: 0.9831
Mejores resultados en la época:  26
f1-score 0.8997356495468278
AUC según el mejor F1-score 0.9879285870662928
Confusion Matrix:
 [[15798   667]
 [  395  4765]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 2/confusion_matrix_param_5840897.png
Accuracy:   0.9509
Precision:  0.8772
Recall:     0.9234
F1-score:   0.8997

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3980, Test Loss: 0.1894, F1: 0.8371, AUC: 0.9664
Epoch [10/30] Train Loss: 0.1327, Test Loss: 0.1382, F1: 0.8921, AUC: 0.9849
Epoch [20/30] Train Loss: 0.0857, Test Loss: 0.3285, F1: 0.8313, AUC: 0.9866
Mejores resultados en la época:  28
f1-score 0.9100272055965799
AUC según el mejor F1-score 0.9878751085812753
Confusion Matrix:
 [[16016   449]
 [  477  4683]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 2/confusion_matrix_param_5840897.png
Accuracy:   0.9572
Precision:  0.9125
Recall:     0.9076
F1-score:   0.9100

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3991, Test Loss: 0.2630, F1: 0.8187, AUC: 0.9708
Epoch [10/30] Train Loss: 0.1319, Test Loss: 0.1429, F1: 0.8903, AUC: 0.9847
Epoch [20/30] Train Loss: 0.0822, Test Loss: 0.1435, F1: 0.8855, AUC: 0.9869
Mejores resultados en la época:  24
f1-score 0.9065606361829026
AUC según el mejor F1-score 0.9884401726000891
Confusion Matrix:
 [[15850   615]
 [  372  4788]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 2/confusion_matrix_param_5840897.png
Accuracy:   0.9544
Precision:  0.8862
Recall:     0.9279
F1-score:   0.9066
Tiempo total para red 6: 281.45 segundos
Saved on: outputs_hold_out/0/Similar Song 2

==============================
Model: Logistic Regression
Accuracy:  0.9339
Precision: 0.8234
Recall:    0.9205
F1-score:  0.8692
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.82      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.94     21625

[[15446  1019]
 [  410  4750]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Song 2/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Similar Song 2/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8077
Precision: 0.5638
Recall:    0.8578
F1-score:  0.6804
              precision    recall  f1-score   support

           0       0.95      0.79      0.86     16465
           1       0.56      0.86      0.68      5160

    accuracy                           0.81     21625
   macro avg       0.76      0.82      0.77     21625
weighted avg       0.86      0.81      0.82     21625

[[13041  3424]
 [  734  4426]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Song 2/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Similar Song 2/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8756
Precision: 0.7324
Recall:    0.7543
F1-score:  0.7432
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.73      0.75      0.74      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.83      0.83     21625
weighted avg       0.88      0.88      0.88     21625

[[15043  1422]
 [ 1268  3892]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Similar Song 2/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Similar Song 2/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8617
Precision: 0.6813
Recall:    0.7897
F1-score:  0.7315
              precision    recall  f1-score   support

           0       0.93      0.88      0.91     16465
           1       0.68      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14559  1906]
 [ 1085  4075]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [16:40:33] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Song 2/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Similar Song 2/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9190
Precision: 0.7977
Recall:    0.8851
F1-score:  0.8391
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15307  1158]
 [  593  4567]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Song 2/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Similar Song 2/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7916
Precision: 0.5350
Recall:    0.9671
F1-score:  0.6889
              precision    recall  f1-score   support

           0       0.99      0.74      0.84     16465
           1       0.54      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.77     21625
weighted avg       0.88      0.79      0.81     21625

[[12128  4337]
 [  170  4990]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Similar Song 2/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Similar Song 2/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9339, 'precision': 0.8234, 'recall': 0.9205, 'f1_score': 0.8692}
XGBoost: {'accuracy': 0.919, 'precision': 0.7977, 'recall': 0.8851, 'f1_score': 0.8391}
Decision Tree: {'accuracy': 0.8756, 'precision': 0.7324, 'recall': 0.7543, 'f1_score': 0.7432}
Random Forest: {'accuracy': 0.8617, 'precision': 0.6813, 'recall': 0.7897, 'f1_score': 0.7315}
Naive Bayes: {'accuracy': 0.7916, 'precision': 0.535, 'recall': 0.9671, 'f1_score': 0.6889}
SVM: {'accuracy': 0.8077, 'precision': 0.5638, 'recall': 0.8578, 'f1_score': 0.6804}

##################################################
Running experiment without SIMILAR ARTIST 3 embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5020, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4448, Test Loss: 0.3397, F1: 0.7488, AUC: 0.9186
Epoch [10/30] Train Loss: 0.1556, Test Loss: 0.2573, F1: 0.8111, AUC: 0.9789
Epoch [20/30] Train Loss: 0.1302, Test Loss: 0.1838, F1: 0.8546, AUC: 0.9790
Mejores resultados en la época:  26
f1-score 0.8698752228163993
AUC según el mejor F1-score 0.9801979828011969
Confusion Matrix:
 [[15602   863]
 [  524  4636]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 3/confusion_matrix_param_160705.png
Accuracy:   0.9359
Precision:  0.8431
Recall:     0.8984
F1-score:   0.8699

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4414, Test Loss: 0.4477, F1: 0.6919, AUC: 0.9205
Epoch [10/30] Train Loss: 0.1592, Test Loss: 0.1862, F1: 0.8537, AUC: 0.9784
Epoch [20/30] Train Loss: 0.1369, Test Loss: 0.2363, F1: 0.8210, AUC: 0.9789
Mejores resultados en la época:  27
f1-score 0.8759295499021527
AUC según el mejor F1-score 0.9793445457477337
Confusion Matrix:
 [[15881   584]
 [  684  4476]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 3/confusion_matrix_param_160705.png
Accuracy:   0.9414
Precision:  0.8846
Recall:     0.8674
F1-score:   0.8759

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4382, Test Loss: 0.2912, F1: 0.7456, AUC: 0.9251
Epoch [10/30] Train Loss: 0.1603, Test Loss: 0.2467, F1: 0.8126, AUC: 0.9781
Epoch [20/30] Train Loss: 0.1298, Test Loss: 0.1867, F1: 0.8612, AUC: 0.9796
Mejores resultados en la época:  24
f1-score 0.877734375
AUC según el mejor F1-score 0.9801613888516163
Confusion Matrix:
 [[15879   586]
 [  666  4494]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 3/confusion_matrix_param_160705.png
Accuracy:   0.9421
Precision:  0.8846
Recall:     0.8709
F1-score:   0.8777
Tiempo total para red 1: 211.04 segundos

Entrenando red 3 con capas [5020, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4191, Test Loss: 0.3912, F1: 0.7255, AUC: 0.9545
Epoch [10/30] Train Loss: 0.1458, Test Loss: 0.3577, F1: 0.7624, AUC: 0.9801
Epoch [20/30] Train Loss: 0.1014, Test Loss: 0.2009, F1: 0.8601, AUC: 0.9830
Mejores resultados en la época:  25
f1-score 0.8940499040307102
AUC según el mejor F1-score 0.9836778508322798
Confusion Matrix:
 [[15863   602]
 [  502  4658]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 3/confusion_matrix_param_653057.png
Accuracy:   0.9489
Precision:  0.8856
Recall:     0.9027
F1-score:   0.8940

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4140, Test Loss: 0.2702, F1: 0.7966, AUC: 0.9548
Epoch [10/30] Train Loss: 0.1505, Test Loss: 0.2257, F1: 0.8307, AUC: 0.9802
Epoch [20/30] Train Loss: 0.0980, Test Loss: 0.2218, F1: 0.8333, AUC: 0.9813
Mejores resultados en la época:  19
f1-score 0.8806317707806014
AUC según el mejor F1-score 0.9814273405885634
Confusion Matrix:
 [[16097   368]
 [  811  4349]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 3/confusion_matrix_param_653057.png
Accuracy:   0.9455
Precision:  0.9220
Recall:     0.8428
F1-score:   0.8806

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4270, Test Loss: 0.2634, F1: 0.8049, AUC: 0.9525
Epoch [10/30] Train Loss: 0.1505, Test Loss: 0.1549, F1: 0.8697, AUC: 0.9791
Epoch [20/30] Train Loss: 0.0922, Test Loss: 0.1797, F1: 0.8714, AUC: 0.9823
Mejores resultados en la época:  23
f1-score 0.8831891838154886
AUC según el mejor F1-score 0.9817881599917138
Confusion Matrix:
 [[16008   457]
 [  718  4442]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 3/confusion_matrix_param_653057.png
Accuracy:   0.9457
Precision:  0.9067
Recall:     0.8609
F1-score:   0.8832
Tiempo total para red 3: 224.79 segundos

Entrenando red 5 con capas [5020, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4155, Test Loss: 0.2365, F1: 0.7834, AUC: 0.9521
Epoch [10/30] Train Loss: 0.1414, Test Loss: 0.2645, F1: 0.8143, AUC: 0.9789
Epoch [20/30] Train Loss: 0.0941, Test Loss: 0.1896, F1: 0.8666, AUC: 0.9765
Mejores resultados en la época:  27
f1-score 0.8842333038261041
AUC según el mejor F1-score 0.982750913965965
Confusion Matrix:
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:06:38] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
 [[15953   512]
 [  665  4495]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 3/confusion_matrix_param_2743297.png
Accuracy:   0.9456
Precision:  0.8977
Recall:     0.8711
F1-score:   0.8842

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4085, Test Loss: 0.6149, F1: 0.6398, AUC: 0.9491
Epoch [10/30] Train Loss: 0.1454, Test Loss: 0.1688, F1: 0.8624, AUC: 0.9804
Epoch [20/30] Train Loss: 0.1016, Test Loss: 0.1692, F1: 0.8822, AUC: 0.9818
Mejores resultados en la época:  23
f1-score 0.8860182370820668
AUC según el mejor F1-score 0.9834472230265279
Confusion Matrix:
 [[15761   704]
 [  496  4664]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 3/confusion_matrix_param_2743297.png
Accuracy:   0.9445
Precision:  0.8689
Recall:     0.9039
F1-score:   0.8860

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4115, Test Loss: 0.2879, F1: 0.8149, AUC: 0.9550
Epoch [10/30] Train Loss: 0.1437, Test Loss: 0.2085, F1: 0.8432, AUC: 0.9807
Epoch [20/30] Train Loss: 0.0929, Test Loss: 0.1615, F1: 0.8795, AUC: 0.9813
Mejores resultados en la época:  17
f1-score 0.8899188876013905
AUC según el mejor F1-score 0.9832316847812014
Confusion Matrix:
 [[15877   588]
 [  552  4608]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 3/confusion_matrix_param_2743297.png
Accuracy:   0.9473
Precision:  0.8868
Recall:     0.8930
F1-score:   0.8899
Tiempo total para red 5: 246.39 segundos

Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4176, Test Loss: 0.2341, F1: 0.8087, AUC: 0.9556
Epoch [10/30] Train Loss: 0.1465, Test Loss: 0.1509, F1: 0.8781, AUC: 0.9810
Epoch [20/30] Train Loss: 0.0871, Test Loss: 0.4084, F1: 0.7983, AUC: 0.9821
Mejores resultados en la época:  16
f1-score 0.8839941262848752
AUC según el mejor F1-score 0.9821196889337731
Confusion Matrix:
 [[15925   540]
 [  645  4515]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 3/confusion_matrix_param_5840897.png
Accuracy:   0.9452
Precision:  0.8932
Recall:     0.8750
F1-score:   0.8840

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4179, Test Loss: 0.2253, F1: 0.8098, AUC: 0.9547
Epoch [10/30] Train Loss: 0.1430, Test Loss: 0.1676, F1: 0.8553, AUC: 0.9759
Epoch [20/30] Train Loss: 0.0940, Test Loss: 0.1747, F1: 0.8763, AUC: 0.9814
Mejores resultados en la época:  26
f1-score 0.8873968923844235
AUC según el mejor F1-score 0.9835596002325817
Confusion Matrix:
 [[15825   640]
 [  534  4626]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 3/confusion_matrix_param_5840897.png
Accuracy:   0.9457
Precision:  0.8785
Recall:     0.8965
F1-score:   0.8874

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4045, Test Loss: 0.2449, F1: 0.8148, AUC: 0.9620
Epoch [10/30] Train Loss: 0.1434, Test Loss: 0.1880, F1: 0.8428, AUC: 0.9803
Epoch [20/30] Train Loss: 0.0958, Test Loss: 0.4732, F1: 0.7074, AUC: 0.9823
Mejores resultados en la época:  28
f1-score 0.8920727684041851
AUC según el mejor F1-score 0.9848919954707777
Confusion Matrix:
 [[15748   717]
 [  428  4732]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Artist 3/confusion_matrix_param_5840897.png
Accuracy:   0.9471
Precision:  0.8684
Recall:     0.9171
F1-score:   0.8921
Tiempo total para red 6: 264.24 segundos
Saved on: outputs_hold_out/0/Similar Artist 3

==============================
Model: Logistic Regression
Accuracy:  0.9214
Precision: 0.7956
Recall:    0.9023
F1-score:  0.8456
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.90      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15269  1196]
 [  504  4656]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Artist 3/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Similar Artist 3/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7459
Precision: 0.4824
Recall:    0.8897
F1-score:  0.6256
              precision    recall  f1-score   support

           0       0.95      0.70      0.81     16465
           1       0.48      0.89      0.63      5160

    accuracy                           0.75     21625
   macro avg       0.72      0.80      0.72     21625
weighted avg       0.84      0.75      0.76     21625

[[11539  4926]
 [  569  4591]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Artist 3/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Similar Artist 3/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8720
Precision: 0.7191
Recall:    0.7607
F1-score:  0.7393
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14932  1533]
 [ 1235  3925]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Similar Artist 3/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Similar Artist 3/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8503
Precision: 0.6571
Recall:    0.7789
F1-score:  0.7128
              precision    recall  f1-score   support

           0       0.93      0.87      0.90     16465
           1       0.66      0.78      0.71      5160

    accuracy                           0.85     21625
   macro avg       0.79      0.83      0.81     21625
weighted avg       0.86      0.85      0.85     21625

[[14368  2097]
 [ 1141  4019]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Artist 3/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Similar Artist 3/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9129
Precision: 0.7865
Recall:    0.8715
F1-score:  0.8268
              precision    recall  f1-score   support

           0       0.96      0.93      0.94     16465
           1       0.79      0.87      0.83      5160

    accuracy                           0.91     21625
   macro avg       0.87      0.90      0.88     21625
weighted avg       0.92      0.91      0.91     21625

[[15244  1221]
 [  663  4497]]
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Artist 3/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Similar Artist 3/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7454
Precision: 0.4832
Recall:    0.9643
F1-score:  0.6438
              precision    recall  f1-score   support

           0       0.98      0.68      0.80     16465
           1       0.48      0.96      0.64      5160

    accuracy                           0.75     21625
   macro avg       0.73      0.82      0.72     21625
weighted avg       0.86      0.75      0.76     21625

[[11143  5322]
 [  184  4976]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Similar Artist 3/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Similar Artist 3/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9214, 'precision': 0.7956, 'recall': 0.9023, 'f1_score': 0.8456}
XGBoost: {'accuracy': 0.9129, 'precision': 0.7865, 'recall': 0.8715, 'f1_score': 0.8268}
Decision Tree: {'accuracy': 0.872, 'precision': 0.7191, 'recall': 0.7607, 'f1_score': 0.7393}
Random Forest: {'accuracy': 0.8503, 'precision': 0.6571, 'recall': 0.7789, 'f1_score': 0.7128}
Naive Bayes: {'accuracy': 0.7454, 'precision': 0.4832, 'recall': 0.9643, 'f1_score': 0.6438}
SVM: {'accuracy': 0.7459, 'precision': 0.4824, 'recall': 0.8897, 'f1_score': 0.6256}

##################################################
Running experiment without SIMILAR SONG 3 embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5020, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4241, Test Loss: 0.3493, F1: 0.7745, AUC: 0.9436
Epoch [10/30] Train Loss: 0.1365, Test Loss: 0.1561, F1: 0.8786, AUC: 0.9842
Epoch [20/30] Train Loss: 0.1180, Test Loss: 0.1534, F1: 0.8798, AUC: 0.9851
Mejores resultados en la época:  29
f1-score 0.8900702487184355
AUC según el mejor F1-score 0.9853954830189477
Confusion Matrix:
 [[15779   686]
 [  472  4688]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 3/confusion_matrix_param_160705.png
Accuracy:   0.9465
Precision:  0.8723
Recall:     0.9085
F1-score:   0.8901

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4292, Test Loss: 0.2799, F1: 0.7635, AUC: 0.9349
Epoch [10/30] Train Loss: 0.1391, Test Loss: 0.2143, F1: 0.8396, AUC: 0.9838
Epoch [20/30] Train Loss: 0.1094, Test Loss: 0.1534, F1: 0.8829, AUC: 0.9844
Mejores resultados en la época:  29
f1-score 0.8911639002087267
AUC según el mejor F1-score 0.9849553551461051
Confusion Matrix:
 [[16047   418]
 [  677  4483]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 3/confusion_matrix_param_160705.png
Accuracy:   0.9494
Precision:  0.9147
Recall:     0.8688
F1-score:   0.8912

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4147, Test Loss: 0.2691, F1: 0.7959, AUC: 0.9492
Epoch [10/30] Train Loss: 0.1344, Test Loss: 0.1698, F1: 0.8706, AUC: 0.9842
Epoch [20/30] Train Loss: 0.1059, Test Loss: 0.1998, F1: 0.8562, AUC: 0.9851
Mejores resultados en la época:  22
f1-score 0.8907119835921476
AUC según el mejor F1-score 0.9853966894775622
Confusion Matrix:
 [[15946   519]
 [  600  4560]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 3/confusion_matrix_param_160705.png
Accuracy:   0.9483
Precision:  0.8978
Recall:     0.8837
F1-score:   0.8907
Tiempo total para red 1: 154.61 segundos

Entrenando red 3 con capas [5020, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4006, Test Loss: 0.2758, F1: 0.8063, AUC: 0.9696
Epoch [10/30] Train Loss: 0.1246, Test Loss: 0.1640, F1: 0.8724, AUC: 0.9854
Epoch [20/30] Train Loss: 0.0820, Test Loss: 0.1320, F1: 0.9029, AUC: 0.9879
Mejores resultados en la época:  20
f1-score 0.9028506053961293
AUC según el mejor F1-score 0.9879393628015262
Confusion Matrix:
 [[15871   594]
 [  425  4735]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 3/confusion_matrix_param_653057.png
Accuracy:   0.9529
Precision:  0.8885
Recall:     0.9176
F1-score:   0.9029

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4071, Test Loss: 0.2777, F1: 0.8126, AUC: 0.9571
Epoch [10/30] Train Loss: 0.1254, Test Loss: 0.2030, F1: 0.8447, AUC: 0.9845
Epoch [20/30] Train Loss: 0.0814, Test Loss: 0.1399, F1: 0.8972, AUC: 0.9871
Mejores resultados en la época:  27
f1-score 0.9083840095703319
AUC según el mejor F1-score 0.9890237925409078
Confusion Matrix:
 [[16150   315]
 [  604  4556]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 3/confusion_matrix_param_653057.png
Accuracy:   0.9575
Precision:  0.9353
Recall:     0.8829
F1-score:   0.9084

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4064, Test Loss: 0.2258, F1: 0.8311, AUC: 0.9648
Epoch [10/30] Train Loss: 0.1201, Test Loss: 0.2075, F1: 0.8514, AUC: 0.9857
Epoch [20/30] Train Loss: 0.0813, Test Loss: 0.1320, F1: 0.8991, AUC: 0.9876
Mejores resultados en la época:  16
f1-score 0.9011594753849078
AUC según el mejor F1-score 0.9869253313935832
Confusion Matrix:
 [[15844   621]
 [  419  4741]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 3/confusion_matrix_param_653057.png
Accuracy:   0.9519
Precision:  0.8842
Recall:     0.9188
F1-score:   0.9012
Tiempo total para red 3: 166.56 segundos

Entrenando red 5 con capas [5020, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4014, Test Loss: 0.2003, F1: 0.8405, AUC: 0.9659
Epoch [10/30] Train Loss: 0.1246, Test Loss: 0.1616, F1: 0.8839, AUC: 0.9849
Epoch [20/30] Train Loss: 0.0751, Test Loss: 0.1267, F1: 0.9047, AUC: 0.9881
Mejores resultados en la época:  25
f1-score 0.9070675517376025
AUC según el mejor F1-score 0.9887168400435973
Confusion Matrix:
 [[16027   438]
 [  514  4646]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 3/confusion_matrix_param_2743297.png
Accuracy:   0.9560
Precision:  0.9138
Recall:     0.9004
F1-score:   0.9071

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4115, Test Loss: 0.2114, F1: 0.8291, AUC: 0.9623
Epoch [10/30] Train Loss: 0.1305, Test Loss: 0.1636, F1: 0.8783, AUC: 0.9856
Epoch [20/30] Train Loss: 0.0814, Test Loss: 0.1687, F1: 0.8763, AUC: 0.9877
Mejores resultados en la época:  27
f1-score 0.9124647818906053
AUC según el mejor F1-score 0.9893267431267169
Confusion Matrix:
 [[16028   437]
 [  464  4696]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 3/confusion_matrix_param_2743297.png
Accuracy:   0.9583
Precision:  0.9149
Recall:     0.9101
F1-score:   0.9125

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4018, Test Loss: 0.2065, F1: 0.8396, AUC: 0.9684
Epoch [10/30] Train Loss: 0.1267, Test Loss: 0.1882, F1: 0.8583, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0752, Test Loss: 0.1722, F1: 0.8798, AUC: 0.9868
Mejores resultados en la época:  28
f1-score 0.9050450796086706
AUC según el mejor F1-score 0.9872696782227747
Confusion Matrix:
 [[15917   548]
 [  442  4718]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 3/confusion_matrix_param_2743297.png
Accuracy:   0.9542
Precision:  0.8959
Recall:     0.9143
F1-score:   0.9050
Tiempo total para red 5: 183.83 segundos

Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:29:00] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [0/30] Train Loss: 0.4084, Test Loss: 0.3050, F1: 0.8215, AUC: 0.9606
Epoch [10/30] Train Loss: 0.1282, Test Loss: 0.1454, F1: 0.8899, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0732, Test Loss: 0.3358, F1: 0.7851, AUC: 0.9870
Mejores resultados en la época:  26
f1-score 0.9059732428867533
AUC según el mejor F1-score 0.9892623770883504
Confusion Matrix:
 [[15819   646]
 [  352  4808]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 3/confusion_matrix_param_5840897.png
Accuracy:   0.9538
Precision:  0.8816
Recall:     0.9318
F1-score:   0.9060

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4075, Test Loss: 0.3604, F1: 0.7611, AUC: 0.9640
Epoch [10/30] Train Loss: 0.1255, Test Loss: 0.1526, F1: 0.8891, AUC: 0.9864
Epoch [20/30] Train Loss: 0.0815, Test Loss: 0.1460, F1: 0.8996, AUC: 0.9865
Mejores resultados en la época:  16
f1-score 0.9012782694198623
AUC según el mejor F1-score 0.9875521837489437
Confusion Matrix:
 [[16038   427]
 [  577  4583]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 3/confusion_matrix_param_5840897.png
Accuracy:   0.9536
Precision:  0.9148
Recall:     0.8882
F1-score:   0.9013

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3938, Test Loss: 0.2145, F1: 0.8442, AUC: 0.9679
Epoch [10/30] Train Loss: 0.1230, Test Loss: 0.1339, F1: 0.8922, AUC: 0.9853
Epoch [20/30] Train Loss: 0.0818, Test Loss: 0.1922, F1: 0.8674, AUC: 0.9870
Mejores resultados en la época:  25
f1-score 0.9034973444232889
AUC según el mejor F1-score 0.9883572682952091
Confusion Matrix:
 [[16154   311]
 [  652  4508]]
Matriz de confusión guardada en: outputs_hold_out/0/Similar Song 3/confusion_matrix_param_5840897.png
Accuracy:   0.9555
Precision:  0.9355
Recall:     0.8736
F1-score:   0.9035
Tiempo total para red 6: 213.39 segundos
Saved on: outputs_hold_out/0/Similar Song 3

==============================
Model: Logistic Regression
Accuracy:  0.9342
Precision: 0.8252
Recall:    0.9192
F1-score:  0.8696
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.94     21625

[[15460  1005]
 [  417  4743]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Song 3/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Similar Song 3/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7951
Precision: 0.5444
Recall:    0.8657
F1-score:  0.6684
              precision    recall  f1-score   support

           0       0.95      0.77      0.85     16465
           1       0.54      0.87      0.67      5160

    accuracy                           0.80     21625
   macro avg       0.75      0.82      0.76     21625
weighted avg       0.85      0.80      0.81     21625

[[12726  3739]
 [  693  4467]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Song 3/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Similar Song 3/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8721
Precision: 0.7198
Recall:    0.7597
F1-score:  0.7392
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14939  1526]
 [ 1240  3920]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Similar Song 3/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Similar Song 3/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8594
Precision: 0.6756
Recall:    0.7899
F1-score:  0.7283
              precision    recall  f1-score   support

           0       0.93      0.88      0.91     16465
           1       0.68      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.84      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14508  1957]
 [ 1084  4076]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Song 3/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Similar Song 3/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9202
Precision: 0.8009
Recall:    0.8859
F1-score:  0.8413
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15329  1136]
 [  589  4571]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Similar Song 3/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Similar Song 3/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7916
Precision: 0.5350
Recall:    0.9686
F1-score:  0.6893
              precision    recall  f1-score   support

           0       0.99      0.74      0.84     16465
           1       0.54      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.77     21625
weighted avg       0.88      0.79      0.81     21625

[[12121  4344]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Similar Song 3/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Similar Song 3/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9342, 'precision': 0.8252, 'recall': 0.9192, 'f1_score': 0.8696}
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
XGBoost: {'accuracy': 0.9202, 'precision': 0.8009, 'recall': 0.8859, 'f1_score': 0.8413}
Decision Tree: {'accuracy': 0.8721, 'precision': 0.7198, 'recall': 0.7597, 'f1_score': 0.7392}
Random Forest: {'accuracy': 0.8594, 'precision': 0.6756, 'recall': 0.7899, 'f1_score': 0.7283}
Naive Bayes: {'accuracy': 0.7916, 'precision': 0.535, 'recall': 0.9686, 'f1_score': 0.6893}
SVM: {'accuracy': 0.7951, 'precision': 0.5444, 'recall': 0.8657, 'f1_score': 0.6684}

##################################################
Running experiment without SONG_NORMALIZED embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5020, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4260, Test Loss: 0.2746, F1: 0.7743, AUC: 0.9355
Epoch [10/30] Train Loss: 0.1303, Test Loss: 0.1944, F1: 0.8574, AUC: 0.9854
Epoch [20/30] Train Loss: 0.1026, Test Loss: 0.2192, F1: 0.8459, AUC: 0.9866
Mejores resultados en la época:  25
f1-score 0.9039370078740158
AUC según el mejor F1-score 0.9870897040233334
Confusion Matrix:
 [[16057   408]
 [  568  4592]]
Matriz de confusión guardada en: outputs_hold_out/0/song_normalized/confusion_matrix_param_160705.png
Accuracy:   0.9549
Precision:  0.9184
Recall:     0.8899
F1-score:   0.9039

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4380, Test Loss: 0.2978, F1: 0.7792, AUC: 0.9339
Epoch [10/30] Train Loss: 0.1307, Test Loss: 0.2118, F1: 0.8430, AUC: 0.9858
Epoch [20/30] Train Loss: 0.1098, Test Loss: 0.1345, F1: 0.8928, AUC: 0.9857
Mejores resultados en la época:  23
f1-score 0.899288871804728
AUC según el mejor F1-score 0.9864737686471421
Confusion Matrix:
 [[15898   567]
 [  481  4679]]
Matriz de confusión guardada en: outputs_hold_out/0/song_normalized/confusion_matrix_param_160705.png
Accuracy:   0.9515
Precision:  0.8919
Recall:     0.9068
F1-score:   0.8993

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4239, Test Loss: 0.3036, F1: 0.7776, AUC: 0.9422
Epoch [10/30] Train Loss: 0.1267, Test Loss: 0.1508, F1: 0.8861, AUC: 0.9865
Epoch [20/30] Train Loss: 0.1003, Test Loss: 0.1801, F1: 0.8680, AUC: 0.9864
Mejores resultados en la época:  17
f1-score 0.8993917638847343
AUC según el mejor F1-score 0.9867763720082768
Confusion Matrix:
 [[16106   359]
 [  650  4510]]
Matriz de confusión guardada en: outputs_hold_out/0/song_normalized/confusion_matrix_param_160705.png
Accuracy:   0.9533
Precision:  0.9263
Recall:     0.8740
F1-score:   0.8994
Tiempo total para red 1: 154.44 segundos

Entrenando red 3 con capas [5020, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4136, Test Loss: 0.2425, F1: 0.7487, AUC: 0.9629
Epoch [10/30] Train Loss: 0.1190, Test Loss: 0.1358, F1: 0.8908, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0775, Test Loss: 0.1253, F1: 0.9076, AUC: 0.9882
Mejores resultados en la época:  20
f1-score 0.9076171875
AUC según el mejor F1-score 0.9882235514845916
Confusion Matrix:
 [[16032   433]
 [  513  4647]]
Matriz de confusión guardada en: outputs_hold_out/0/song_normalized/confusion_matrix_param_653057.png
Accuracy:   0.9563
Precision:  0.9148
Recall:     0.9006
F1-score:   0.9076

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4046, Test Loss: 0.3140, F1: 0.7954, AUC: 0.9673
Epoch [10/30] Train Loss: 0.1154, Test Loss: 0.1377, F1: 0.8920, AUC: 0.9866
Epoch [20/30] Train Loss: 0.0726, Test Loss: 0.1931, F1: 0.8727, AUC: 0.9886
Mejores resultados en la época:  27
f1-score 0.9119969555703549
AUC según el mejor F1-score 0.9898572200368647
Confusion Matrix:
 [[15907   558]
 [  367  4793]]
Matriz de confusión guardada en: outputs_hold_out/0/song_normalized/confusion_matrix_param_653057.png
Accuracy:   0.9572
Precision:  0.8957
Recall:     0.9289
F1-score:   0.9120

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4001, Test Loss: 0.3343, F1: 0.7774, AUC: 0.9692
Epoch [10/30] Train Loss: 0.1101, Test Loss: 0.1266, F1: 0.8993, AUC: 0.9871
Epoch [20/30] Train Loss: 0.0709, Test Loss: 0.1289, F1: 0.9067, AUC: 0.9881
Mejores resultados en la época:  24
f1-score 0.9156993339676499
AUC según el mejor F1-score 0.9902205288643752
Confusion Matrix:
 [[15927   538]
 [  348  4812]]
Matriz de confusión guardada en: outputs_hold_out/0/song_normalized/confusion_matrix_param_653057.png
Accuracy:   0.9590
Precision:  0.8994
Recall:     0.9326
F1-score:   0.9157
Tiempo total para red 3: 165.97 segundos

Entrenando red 5 con capas [5020, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4046, Test Loss: 0.3221, F1: 0.7826, AUC: 0.9698
Epoch [10/30] Train Loss: 0.1144, Test Loss: 0.3425, F1: 0.7957, AUC: 0.9871
Epoch [20/30] Train Loss: 0.0715, Test Loss: 0.1418, F1: 0.9094, AUC: 0.9895
Mejores resultados en la época:  24
f1-score 0.9127340823970037
AUC según el mejor F1-score 0.9906323079023628
Confusion Matrix:
 [[15819   646]
 [  286  4874]]
Matriz de confusión guardada en: outputs_hold_out/0/song_normalized/confusion_matrix_param_2743297.png
Accuracy:   0.9569
Precision:  0.8830
Recall:     0.9446
F1-score:   0.9127

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3822, Test Loss: 0.1757, F1: 0.8621, AUC: 0.9752
Epoch [10/30] Train Loss: 0.1195, Test Loss: 0.2349, F1: 0.8222, AUC: 0.9871
Epoch [20/30] Train Loss: 0.0735, Test Loss: 0.1660, F1: 0.8733, AUC: 0.9879
Mejores resultados en la época:  26
f1-score 0.9116426171180075
AUC según el mejor F1-score 0.9895973311958419
Confusion Matrix:
 [[16119   346]
 [  548  4612]]
Matriz de confusión guardada en: outputs_hold_out/0/song_normalized/confusion_matrix_param_2743297.png
Accuracy:   0.9587
Precision:  0.9302
Recall:     0.8938
F1-score:   0.9116

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3858, Test Loss: 0.2361, F1: 0.8363, AUC: 0.9674
Epoch [10/30] Train Loss: 0.1162, Test Loss: 0.1474, F1: 0.8888, AUC: 0.9876
Epoch [20/30] Train Loss: 0.0732, Test Loss: 0.1512, F1: 0.8946, AUC: 0.9865
Mejores resultados en la época:  28
f1-score 0.9180711154408183
AUC según el mejor F1-score 0.9903067935978833
Confusion Matrix:
 [[16072   393]
 [  448  4712]]
Matriz de confusión guardada en: outputs_hold_out/0/song_normalized/confusion_matrix_param_2743297.png
Accuracy:   0.9611
Precision:  0.9230
Recall:     0.9132
F1-score:   0.9181
Tiempo total para red 5: 183.35 segundos

Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3939, Test Loss: 0.1922, F1: 0.8190, AUC: 0.9693
Epoch [10/30] Train Loss: 0.1203, Test Loss: 0.2293, F1: 0.8605, AUC: 0.9880
Epoch [20/30] Train Loss: 0.0752, Test Loss: 0.2018, F1: 0.8917, AUC: 0.9894
Mejores resultados en la época:  25
f1-score 0.9136825490776364
AUC según el mejor F1-score 0.989638915764471
Confusion Matrix:
 [[16119   346]
 [  529  4631]]
Matriz de confusión guardada en: outputs_hold_out/0/song_normalized/confusion_matrix_param_5840897.png
Accuracy:   0.9595
Precision:  0.9305
Recall:     0.8975
F1-score:   0.9137

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4155, Test Loss: 0.2680, F1: 0.8104, AUC: 0.9643
Epoch [10/30] Train Loss: 0.1139, Test Loss: 0.1707, F1: 0.8737, AUC: 0.9878
Epoch [20/30] Train Loss: 0.0698, Test Loss: 0.1272, F1: 0.9018, AUC: 0.9901
Mejores resultados en la época:  21
f1-score 0.9136447152112949
AUC según el mejor F1-score 0.9894542981706557
Confusion Matrix:
 [[16008   457]
 [  436  4724]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:51:16] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Matriz de confusión guardada en: outputs_hold_out/0/song_normalized/confusion_matrix_param_5840897.png
Accuracy:   0.9587
Precision:  0.9118
Recall:     0.9155
F1-score:   0.9136

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4081, Test Loss: 0.1791, F1: 0.8541, AUC: 0.9731
Epoch [10/30] Train Loss: 0.1117, Test Loss: 0.3541, F1: 0.7592, AUC: 0.9866
Epoch [20/30] Train Loss: 0.0729, Test Loss: 0.1422, F1: 0.9057, AUC: 0.9882
Mejores resultados en la época:  28
f1-score 0.9173955296404276
AUC según el mejor F1-score 0.9904160281263755
Confusion Matrix:
 [[16055   410]
 [  440  4720]]
Matriz de confusión guardada en: outputs_hold_out/0/song_normalized/confusion_matrix_param_5840897.png
Accuracy:   0.9607
Precision:  0.9201
Recall:     0.9147
F1-score:   0.9174
Tiempo total para red 6: 213.02 segundos
Saved on: outputs_hold_out/0/song_normalized

==============================
Model: Logistic Regression
Accuracy:  0.9376
Precision: 0.8311
Recall:    0.9267
F1-score:  0.8763
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.88      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15493   972]
 [  378  4782]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/song_normalized/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/song_normalized/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7662
Precision: 0.5056
Recall:    0.9039
F1-score:  0.6485
              precision    recall  f1-score   support

           0       0.96      0.72      0.82     16465
           1       0.51      0.90      0.65      5160

    accuracy                           0.77     21625
   macro avg       0.73      0.81      0.74     21625
weighted avg       0.85      0.77      0.78     21625

[[11905  4560]
 [  496  4664]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/song_normalized/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/song_normalized/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8734
Precision: 0.7218
Recall:    0.7640
F1-score:  0.7423
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14946  1519]
 [ 1218  3942]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/song_normalized/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/song_normalized/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8679
Precision: 0.6979
Recall:    0.7870
F1-score:  0.7398
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.70      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14707  1758]
 [ 1099  4061]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/song_normalized/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/song_normalized/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9200
Precision: 0.8018
Recall:    0.8833
F1-score:  0.8406
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15338  1127]
 [  602  4558]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/song_normalized/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/song_normalized/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8017
Precision: 0.5476
Recall:    0.9702
F1-score:  0.7001
              precision    recall  f1-score   support

           0       0.99      0.75      0.85     16465
           1       0.55      0.97      0.70      5160

    accuracy                           0.80     21625
   macro avg       0.77      0.86      0.78     21625
weighted avg       0.88      0.80      0.82     21625

[[12330  4135]
 [  154  5006]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/song_normalized/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/song_normalized/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9376, 'precision': 0.8311, 'recall': 0.9267, 'f1_score': 0.8763}
XGBoost: {'accuracy': 0.92, 'precision': 0.8018, 'recall': 0.8833, 'f1_score': 0.8406}
Decision Tree: {'accuracy': 0.8734, 'precision': 0.7218, 'recall': 0.764, 'f1_score': 0.7423}
Random Forest: {'accuracy': 0.8679, 'precision': 0.6979, 'recall': 0.787, 'f1_score': 0.7398}
Naive Bayes: {'accuracy': 0.8017, 'precision': 0.5476, 'recall': 0.9702, 'f1_score': 0.7001}
SVM: {'accuracy': 0.7662, 'precision': 0.5056, 'recall': 0.9039, 'f1_score': 0.6485}

##################################################
Running experiment without ARTIST_NORMALIZED embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5020, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4343, Test Loss: 0.2888, F1: 0.7682, AUC: 0.9325
Epoch [10/30] Train Loss: 0.1308, Test Loss: 0.1341, F1: 0.8851, AUC: 0.9844
Epoch [20/30] Train Loss: 0.1039, Test Loss: 0.2953, F1: 0.8030, AUC: 0.9856
Mejores resultados en la época:  18
f1-score 0.894767218160831
AUC según el mejor F1-score 0.9856512875561738
Confusion Matrix:
 [[15880   585]
 [  509  4651]]
Matriz de confusión guardada en: outputs_hold_out/0/artist_normalized/confusion_matrix_param_160705.png
Accuracy:   0.9494
Precision:  0.8883
Recall:     0.9014
F1-score:   0.8948

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4249, Test Loss: 0.3386, F1: 0.7723, AUC: 0.9428
Epoch [10/30] Train Loss: 0.1358, Test Loss: 0.1280, F1: 0.8932, AUC: 0.9857
Epoch [20/30] Train Loss: 0.0961, Test Loss: 0.1478, F1: 0.8844, AUC: 0.9854
Mejores resultados en la época:  25
f1-score 0.90020366598778
AUC según el mejor F1-score 0.986762665461385
Confusion Matrix:
 [[15955   510]
 [  519  4641]]
Matriz de confusión guardada en: outputs_hold_out/0/artist_normalized/confusion_matrix_param_160705.png
Accuracy:   0.9524
Precision:  0.9010
Recall:     0.8994
F1-score:   0.9002

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4527, Test Loss: 0.3769, F1: 0.7308, AUC: 0.9171
Epoch [10/30] Train Loss: 0.1358, Test Loss: 0.1767, F1: 0.8660, AUC: 0.9849
Epoch [20/30] Train Loss: 0.1117, Test Loss: 0.1663, F1: 0.8751, AUC: 0.9853
Mejores resultados en la época:  28
f1-score 0.8940809968847352
AUC según el mejor F1-score 0.9854433235168797
Confusion Matrix:
 [[15945   520]
 [  568  4592]]
Matriz de confusión guardada en: outputs_hold_out/0/artist_normalized/confusion_matrix_param_160705.png
Accuracy:   0.9497
Precision:  0.8983
Recall:     0.8899
F1-score:   0.8941
Tiempo total para red 1: 156.08 segundos

Entrenando red 3 con capas [5020, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3886, Test Loss: 0.3598, F1: 0.7558, AUC: 0.9734
Epoch [10/30] Train Loss: 0.1172, Test Loss: 0.1339, F1: 0.8917, AUC: 0.9861
Epoch [20/30] Train Loss: 0.0791, Test Loss: 0.2640, F1: 0.8426, AUC: 0.9882
Mejores resultados en la época:  21
f1-score 0.9097978227060654
AUC según el mejor F1-score 0.9892621240262998
Confusion Matrix:
 [[16017   448]
 [  480  4680]]
Matriz de confusión guardada en: outputs_hold_out/0/artist_normalized/confusion_matrix_param_653057.png
Accuracy:   0.9571
Precision:  0.9126
Recall:     0.9070
F1-score:   0.9098

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4064, Test Loss: 0.1976, F1: 0.8300, AUC: 0.9667
Epoch [10/30] Train Loss: 0.1213, Test Loss: 0.1540, F1: 0.8836, AUC: 0.9866
Epoch [20/30] Train Loss: 0.0736, Test Loss: 0.2237, F1: 0.8555, AUC: 0.9869
Mejores resultados en la época:  28
f1-score 0.9147982062780269
AUC según el mejor F1-score 0.9893514725857292
Confusion Matrix:
 [[15938   527]
 [  366  4794]]
Matriz de confusión guardada en: outputs_hold_out/0/artist_normalized/confusion_matrix_param_653057.png
Accuracy:   0.9587
Precision:  0.9010
Recall:     0.9291
F1-score:   0.9148

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4043, Test Loss: 0.2219, F1: 0.8283, AUC: 0.9615
Epoch [10/30] Train Loss: 0.1260, Test Loss: 0.2483, F1: 0.8334, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0747, Test Loss: 0.1237, F1: 0.9087, AUC: 0.9882
Mejores resultados en la época:  20
f1-score 0.9087032370614461
AUC según el mejor F1-score 0.9882369461177928
Confusion Matrix:
 [[15995   470]
 [  472  4688]]
Matriz de confusión guardada en: outputs_hold_out/0/artist_normalized/confusion_matrix_param_653057.png
Accuracy:   0.9564
Precision:  0.9089
Recall:     0.9085
F1-score:   0.9087
Tiempo total para red 3: 167.21 segundos

Entrenando red 5 con capas [5020, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3905, Test Loss: 0.3657, F1: 0.7506, AUC: 0.9710
Epoch [10/30] Train Loss: 0.1207, Test Loss: 0.2041, F1: 0.8515, AUC: 0.9870
Epoch [20/30] Train Loss: 0.0692, Test Loss: 0.1642, F1: 0.8993, AUC: 0.9873
Mejores resultados en la época:  24
f1-score 0.9153066151617576
AUC según el mejor F1-score 0.9898070607843276
Confusion Matrix:
 [[16009   456]
 [  421  4739]]
Matriz de confusión guardada en: outputs_hold_out/0/artist_normalized/confusion_matrix_param_2743297.png
Accuracy:   0.9594
Precision:  0.9122
Recall:     0.9184
F1-score:   0.9153

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3996, Test Loss: 0.2095, F1: 0.8104, AUC: 0.9611
Epoch [10/30] Train Loss: 0.1172, Test Loss: 0.1553, F1: 0.8827, AUC: 0.9848
Epoch [20/30] Train Loss: 0.0724, Test Loss: 0.1949, F1: 0.8710, AUC: 0.9880
Mejores resultados en la época:  29
f1-score 0.9079205149734118
AUC según el mejor F1-score 0.9895522508398129
Confusion Matrix:
 [[15772   693]
 [  294  4866]]
Matriz de confusión guardada en: outputs_hold_out/0/artist_normalized/confusion_matrix_param_2743297.png
Accuracy:   0.9544
Precision:  0.8753
Recall:     0.9430
F1-score:   0.9079

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3892, Test Loss: 0.4118, F1: 0.7320, AUC: 0.9609
Epoch [10/30] Train Loss: 0.1164, Test Loss: 0.1953, F1: 0.8705, AUC: 0.9875
Epoch [20/30] Train Loss: 0.0747, Test Loss: 0.1203, F1: 0.9103, AUC: 0.9889
Mejores resultados en la época:  29
f1-score 0.9146919431279621
AUC según el mejor F1-score 0.9893192689684721
Confusion Matrix:
 [[16129   336]
 [  528  4632]]
Matriz de confusión guardada en: outputs_hold_out/0/artist_normalized/confusion_matrix_param_2743297.png
Accuracy:   0.9600
Precision:  0.9324
Recall:     0.8977
F1-score:   0.9147
Tiempo total para red 5: 184.03 segundos

Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4068, Test Loss: 0.3094, F1: 0.7747, AUC: 0.9703
Epoch [10/30] Train Loss: 0.1254, Test Loss: 0.1476, F1: 0.8873, AUC: 0.9867
Epoch [20/30] Train Loss: 0.0733, Test Loss: 0.2168, F1: 0.8729, AUC: 0.9890
Mejores resultados en la época:  28
f1-score 0.9109385485571854
AUC según el mejor F1-score 0.9881242099167367
Confusion Matrix:
 [[15945   520]
 [  409  4751]]
Matriz de confusión guardada en: outputs_hold_out/0/artist_normalized/confusion_matrix_param_5840897.png
Accuracy:   0.9570
Precision:  0.9013
Recall:     0.9207
F1-score:   0.9109

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4110, Test Loss: 0.3513, F1: 0.7931, AUC: 0.9708
Epoch [10/30] Train Loss: 0.1224, Test Loss: 0.1244, F1: 0.9004, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0745, Test Loss: 0.2360, F1: 0.8694, AUC: 0.9881
Mejores resultados en la época:  29
f1-score 0.9143695014662757
AUC según el mejor F1-score 0.9898561548221858
Confusion Matrix:
 [[16072   393]
 [  483  4677]]
Matriz de confusión guardada en: outputs_hold_out/0/artist_normalized/confusion_matrix_param_5840897.png
Accuracy:   0.9595
Precision:  0.9225
Recall:     0.9064
F1-score:   0.9144

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3997, Test Loss: 0.2422, F1: 0.8407, AUC: 0.9707
Epoch [10/30] Train Loss: 0.1213, Test Loss: 0.1244, F1: 0.8936, AUC: 0.9862
Epoch [20/30] Train Loss: 0.0776, Test Loss: 0.1268, F1: 0.9057, AUC: 0.9882
Mejores resultados en la época:  29
f1-score 0.9118254202300207
AUC según el mejor F1-score 0.9884552798160062
Confusion Matrix:
 [[16090   375]
 [  522  4638]]
Matriz de confusión guardada en: outputs_hold_out/0/artist_normalized/confusion_matrix_param_5840897.png
Accuracy:   0.9585
Precision:  0.9252
Recall:     0.8988
F1-score:   0.9118
Tiempo total para red 6: 214.12 segundos
Saved on: outputs_hold_out/0/artist_normalized

==============================
Model: Logistic Regression
Accuracy:  0.9351
Precision: 0.8266
Recall:    0.9211
F1-score:  0.8713
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [18:13:39] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[15468   997]
 [  407  4753]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/artist_normalized/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/artist_normalized/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8147
Precision: 0.5714
Recall:    0.8938
F1-score:  0.6972
              precision    recall  f1-score   support

           0       0.96      0.79      0.87     16465
           1       0.57      0.89      0.70      5160

    accuracy                           0.81     21625
   macro avg       0.77      0.84      0.78     21625
weighted avg       0.87      0.81      0.83     21625

[[13006  3459]
 [  548  4612]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/artist_normalized/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/artist_normalized/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8713
Precision: 0.7172
Recall:    0.7607
F1-score:  0.7383
              precision    recall  f1-score   support

           0       0.92      0.91      0.91     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14917  1548]
 [ 1235  3925]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/artist_normalized/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/artist_normalized/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8561
Precision: 0.6696
Recall:    0.7833
F1-score:  0.7220
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.78      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

[[14471  1994]
 [ 1118  4042]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/artist_normalized/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/artist_normalized/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9225
Precision: 0.8070
Recall:    0.8876
F1-score:  0.8454
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.81      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.89      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15370  1095]
 [  580  4580]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/artist_normalized/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/artist_normalized/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7755
Precision: 0.5158
Recall:    0.9709
F1-score:  0.6737
              precision    recall  f1-score   support

           0       0.99      0.71      0.83     16465
           1       0.52      0.97      0.67      5160

    accuracy                           0.78     21625
   macro avg       0.75      0.84      0.75     21625
weighted avg       0.87      0.78      0.79     21625

[[11761  4704]
 [  150  5010]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/artist_normalized/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/artist_normalized/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9351, 'precision': 0.8266, 'recall': 0.9211, 'f1_score': 0.8713}
XGBoost: {'accuracy': 0.9225, 'precision': 0.807, 'recall': 0.8876, 'f1_score': 0.8454}
Decision Tree: {'accuracy': 0.8713, 'precision': 0.7172, 'recall': 0.7607, 'f1_score': 0.7383}
Random Forest: {'accuracy': 0.8561, 'precision': 0.6696, 'recall': 0.7833, 'f1_score': 0.722}
SVM: {'accuracy': 0.8147, 'precision': 0.5714, 'recall': 0.8938, 'f1_score': 0.6972}
Naive Bayes: {'accuracy': 0.7755, 'precision': 0.5158, 'recall': 0.9709, 'f1_score': 0.6737}

##################################################
Running experiment without TEMPO embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4202, Test Loss: 0.2911, F1: 0.7858, AUC: 0.9441
Epoch [10/30] Train Loss: 0.1233, Test Loss: 0.1372, F1: 0.8913, AUC: 0.9854
Epoch [20/30] Train Loss: 0.0965, Test Loss: 0.1396, F1: 0.8918, AUC: 0.9846
Mejores resultados en la época:  21
f1-score 0.8958212979010206
AUC según el mejor F1-score 0.9855620037335479
Confusion Matrix:
 [[15891   574]
 [  508  4652]]
Matriz de confusión guardada en: outputs_hold_out/0/Tempo/confusion_matrix_param_160673.png
Accuracy:   0.9500
Precision:  0.8902
Recall:     0.9016
F1-score:   0.8958

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4139, Test Loss: 0.2818, F1: 0.7992, AUC: 0.9488
Epoch [10/30] Train Loss: 0.1259, Test Loss: 0.2684, F1: 0.8124, AUC: 0.9851
Epoch [20/30] Train Loss: 0.0939, Test Loss: 0.1512, F1: 0.8842, AUC: 0.9855
Mejores resultados en la época:  26
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
f1-score 0.9005094684225704
AUC según el mejor F1-score 0.986151579460307
Confusion Matrix:
 [[15906   559]
 [  476  4684]]
Matriz de confusión guardada en: outputs_hold_out/0/Tempo/confusion_matrix_param_160673.png
Accuracy:   0.9521
Precision:  0.8934
Recall:     0.9078
F1-score:   0.9005

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4074, Test Loss: 0.4549, F1: 0.6948, AUC: 0.9501
Epoch [10/30] Train Loss: 0.1206, Test Loss: 0.2921, F1: 0.8002, AUC: 0.9837
Epoch [20/30] Train Loss: 0.0926, Test Loss: 0.2418, F1: 0.8367, AUC: 0.9854
Mejores resultados en la época:  17
f1-score 0.8964320154291224
AUC según el mejor F1-score 0.9856588205660586
Confusion Matrix:
 [[15903   562]
 [  512  4648]]
Matriz de confusión guardada en: outputs_hold_out/0/Tempo/confusion_matrix_param_160673.png
Accuracy:   0.9503
Precision:  0.8921
Recall:     0.9008
F1-score:   0.8964
Tiempo total para red 1: 155.49 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3805, Test Loss: 0.2287, F1: 0.8378, AUC: 0.9693
Epoch [10/30] Train Loss: 0.1148, Test Loss: 0.1876, F1: 0.8613, AUC: 0.9862
Epoch [20/30] Train Loss: 0.0634, Test Loss: 0.1361, F1: 0.8961, AUC: 0.9867
Mejores resultados en la época:  26
f1-score 0.9117903930131004
AUC según el mejor F1-score 0.9892226934276842
Confusion Matrix:
 [[16018   447]
 [  462  4698]]
Matriz de confusión guardada en: outputs_hold_out/0/Tempo/confusion_matrix_param_652929.png
Accuracy:   0.9580
Precision:  0.9131
Recall:     0.9105
F1-score:   0.9118

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3897, Test Loss: 0.4802, F1: 0.6875, AUC: 0.9713
Epoch [10/30] Train Loss: 0.1175, Test Loss: 0.1533, F1: 0.8900, AUC: 0.9853
Epoch [20/30] Train Loss: 0.0754, Test Loss: 0.1683, F1: 0.8869, AUC: 0.9882
Mejores resultados en la época:  29
f1-score 0.9054003021148036
AUC según el mejor F1-score 0.9886630143339052
Confusion Matrix:
 [[15828   637]
 [  365  4795]]
Matriz de confusión guardada en: outputs_hold_out/0/Tempo/confusion_matrix_param_652929.png
Accuracy:   0.9537
Precision:  0.8827
Recall:     0.9293
F1-score:   0.9054

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3947, Test Loss: 0.1980, F1: 0.8404, AUC: 0.9700
Epoch [10/30] Train Loss: 0.1124, Test Loss: 0.1287, F1: 0.9007, AUC: 0.9866
Epoch [20/30] Train Loss: 0.0652, Test Loss: 0.2024, F1: 0.8697, AUC: 0.9882
Mejores resultados en la época:  28
f1-score 0.9171260032878832
AUC según el mejor F1-score 0.9894591240051129
Confusion Matrix:
 [[16026   439]
 [  418  4742]]
Matriz de confusión guardada en: outputs_hold_out/0/Tempo/confusion_matrix_param_652929.png
Accuracy:   0.9604
Precision:  0.9153
Recall:     0.9190
F1-score:   0.9171
Tiempo total para red 3: 167.25 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3733, Test Loss: 0.1742, F1: 0.8514, AUC: 0.9734
Epoch [10/30] Train Loss: 0.1156, Test Loss: 0.1287, F1: 0.9001, AUC: 0.9873
Epoch [20/30] Train Loss: 0.0667, Test Loss: 0.1801, F1: 0.8966, AUC: 0.9886
Mejores resultados en la época:  24
f1-score 0.9134340144112131
AUC según el mejor F1-score 0.9894025499238461
Confusion Matrix:
 [[16121   344]
 [  533  4627]]
Matriz de confusión guardada en: outputs_hold_out/0/Tempo/confusion_matrix_param_2742785.png
Accuracy:   0.9594
Precision:  0.9308
Recall:     0.8967
F1-score:   0.9134

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3748, Test Loss: 0.2741, F1: 0.8010, AUC: 0.9696
Epoch [10/30] Train Loss: 0.1138, Test Loss: 0.1273, F1: 0.8952, AUC: 0.9861
Epoch [20/30] Train Loss: 0.0662, Test Loss: 0.2502, F1: 0.8488, AUC: 0.9896
Mejores resultados en la época:  23
f1-score 0.9103887334812385
AUC según el mejor F1-score 0.9887689531705733
Confusion Matrix:
 [[15977   488]
 [  441  4719]]
Matriz de confusión guardada en: outputs_hold_out/0/Tempo/confusion_matrix_param_2742785.png
Accuracy:   0.9570
Precision:  0.9063
Recall:     0.9145
F1-score:   0.9104

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3694, Test Loss: 0.2760, F1: 0.8057, AUC: 0.9748
Epoch [10/30] Train Loss: 0.1076, Test Loss: 0.1605, F1: 0.8757, AUC: 0.9870
Epoch [20/30] Train Loss: 0.0604, Test Loss: 0.1539, F1: 0.9050, AUC: 0.9884
Mejores resultados en la época:  27
f1-score 0.910126221884787
AUC según el mejor F1-score 0.9885929279161576
Confusion Matrix:
 [[15883   582]
 [  365  4795]]
Matriz de confusión guardada en: outputs_hold_out/0/Tempo/confusion_matrix_param_2742785.png
Accuracy:   0.9562
Precision:  0.8918
Recall:     0.9293
F1-score:   0.9101
Tiempo total para red 5: 184.94 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3750, Test Loss: 0.1918, F1: 0.8495, AUC: 0.9718
Epoch [10/30] Train Loss: 0.1160, Test Loss: 0.1941, F1: 0.8561, AUC: 0.9871
Epoch [20/30] Train Loss: 0.0654, Test Loss: 0.2869, F1: 0.8330, AUC: 0.9837
Mejores resultados en la época:  29
f1-score 0.9129041654529566
AUC según el mejor F1-score 0.9886385732479279
Confusion Matrix:
 [[16027   438]
 [  459  4701]]
Matriz de confusión guardada en: outputs_hold_out/0/Tempo/confusion_matrix_param_5839873.png
Accuracy:   0.9585
Precision:  0.9148
Recall:     0.9110
F1-score:   0.9129

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3838, Test Loss: 0.3252, F1: 0.8034, AUC: 0.9721
Epoch [10/30] Train Loss: 0.1185, Test Loss: 0.1666, F1: 0.8715, AUC: 0.9867
Epoch [20/30] Train Loss: 0.0581, Test Loss: 0.1937, F1: 0.8842, AUC: 0.9895
Mejores resultados en la época:  29
f1-score 0.9127319132227348
AUC según el mejor F1-score 0.9893279731259872
Confusion Matrix:
 [[16087   378]
 [  511  4649]]
Matriz de confusión guardada en: outputs_hold_out/0/Tempo/confusion_matrix_param_5839873.png
Accuracy:   0.9589
Precision:  0.9248
Recall:     0.9010
F1-score:   0.9127

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3861, Test Loss: 0.1854, F1: 0.8520, AUC: 0.9718
Epoch [10/30] Train Loss: 0.1150, Test Loss: 0.2227, F1: 0.8445, AUC: 0.9867
Epoch [20/30] Train Loss: 0.0631, Test Loss: 0.1364, F1: 0.9103, AUC: 0.9885
Mejores resultados en la época:  20
f1-score 0.9103353867214237
AUC según el mejor F1-score 0.9884548090028884
Confusion Matrix:
 [[16053   412]
 [  505  4655]]
Matriz de confusión guardada en: outputs_hold_out/0/Tempo/confusion_matrix_param_5839873.png
Accuracy:   0.9576
Precision:  0.9187
Recall:     0.9021
F1-score:   0.9103
Tiempo total para red 6: 214.37 segundos
Saved on: outputs_hold_out/0/Tempo

==============================
Model: Logistic Regression
Accuracy:  0.9352
Precision: 0.8272
Recall:    0.9205
F1-score:  0.8714
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15473   992]
 [  410  4750]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Tempo/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Tempo/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7730
Precision: 0.5136
Recall:    0.9196
F1-score:  0.6591
              precision    recall  f1-score   support

           0       0.97      0.73      0.83     16465
           1       0.51      0.92      0.66      5160

    accuracy                           0.77     21625
   macro avg       0.74      0.82      0.74     21625
weighted avg       0.86      0.77      0.79     21625

[[11972  4493]
 [  415  4745]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [18:36:03] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Tempo/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Tempo/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8728
Precision: 0.7203
Recall:    0.7636
F1-score:  0.7413
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14935  1530]
 [ 1220  3940]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Tempo/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Tempo/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8583
Precision: 0.6750
Recall:    0.7833
F1-score:  0.7252
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.68      0.78      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

[[14519  1946]
 [ 1118  4042]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Tempo/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Tempo/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9200
Precision: 0.8003
Recall:    0.8860
F1-score:  0.8410
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15324  1141]
 [  588  4572]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Tempo/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Tempo/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7871
Precision: 0.5294
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12022  4443]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Tempo/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Tempo/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9352, 'precision': 0.8272, 'recall': 0.9205, 'f1_score': 0.8714}
XGBoost: {'accuracy': 0.92, 'precision': 0.8003, 'recall': 0.886, 'f1_score': 0.841}
Decision Tree: {'accuracy': 0.8728, 'precision': 0.7203, 'recall': 0.7636, 'f1_score': 0.7413}
Random Forest: {'accuracy': 0.8583, 'precision': 0.675, 'recall': 0.7833, 'f1_score': 0.7252}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.773, 'precision': 0.5136, 'recall': 0.9196, 'f1_score': 0.6591}

##################################################
Running experiment without LENGTH embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4372, Test Loss: 0.3949, F1: 0.7442, AUC: 0.9386
Epoch [10/30] Train Loss: 0.1300, Test Loss: 0.1694, F1: 0.8678, AUC: 0.9847
Epoch [20/30] Train Loss: 0.0984, Test Loss: 0.1983, F1: 0.8548, AUC: 0.9854
Mejores resultados en la época:  24
f1-score 0.8958149563853769
AUC según el mejor F1-score 0.9851684039670714
Confusion Matrix:
 [[15992   473]
 [  590  4570]]
Matriz de confusión guardada en: outputs_hold_out/0/Length/confusion_matrix_param_160673.png
Accuracy:   0.9508
Precision:  0.9062
Recall:     0.8857
F1-score:   0.8958

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4125, Test Loss: 0.2453, F1: 0.7933, AUC: 0.9530
Epoch [10/30] Train Loss: 0.1186, Test Loss: 0.2775, F1: 0.8099, AUC: 0.9854
Epoch [20/30] Train Loss: 0.0931, Test Loss: 0.2135, F1: 0.8550, AUC: 0.9854
Mejores resultados en la época:  25
f1-score 0.8958611481975968
AUC según el mejor F1-score 0.9860454346429
Confusion Matrix:
 [[15836   629]
 [  463  4697]]
Matriz de confusión guardada en: outputs_hold_out/0/Length/confusion_matrix_param_160673.png
Accuracy:   0.9495
Precision:  0.8819
Recall:     0.9103
F1-score:   0.8959

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4106, Test Loss: 0.2456, F1: 0.7890, AUC: 0.9526
Epoch [10/30] Train Loss: 0.1161, Test Loss: 0.1356, F1: 0.8896, AUC: 0.9850
Epoch [20/30] Train Loss: 0.0938, Test Loss: 0.1371, F1: 0.8917, AUC: 0.9855
Mejores resultados en la época:  26
f1-score 0.8966807840047276
AUC según el mejor F1-score 0.9858848579439121
Confusion Matrix:
 [[16024   441]
 [  608  4552]]
Matriz de confusión guardada en: outputs_hold_out/0/Length/confusion_matrix_param_160673.png
Accuracy:   0.9515
Precision:  0.9117
Recall:     0.8822
F1-score:   0.8967
Tiempo total para red 1: 155.03 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3746, Test Loss: 0.4055, F1: 0.7287, AUC: 0.9704
Epoch [10/30] Train Loss: 0.1034, Test Loss: 0.1459, F1: 0.8859, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0540, Test Loss: 0.1599, F1: 0.8944, AUC: 0.9888
Mejores resultados en la época:  25
f1-score 0.9140873589817761
AUC según el mejor F1-score 0.9886970776629779
Confusion Matrix:
 [[15994   471]
 [  420  4740]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Matriz de confusión guardada en: outputs_hold_out/0/Length/confusion_matrix_param_652929.png
Accuracy:   0.9588
Precision:  0.9096
Recall:     0.9186
F1-score:   0.9141

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3739, Test Loss: 0.1901, F1: 0.8307, AUC: 0.9700
Epoch [10/30] Train Loss: 0.1079, Test Loss: 0.1355, F1: 0.8982, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0580, Test Loss: 0.1423, F1: 0.8995, AUC: 0.9868
Mejores resultados en la época:  27
f1-score 0.9086766660337954
AUC según el mejor F1-score 0.9890118574283717
Confusion Matrix:
 [[15877   588]
 [  374  4786]]
Matriz de confusión guardada en: outputs_hold_out/0/Length/confusion_matrix_param_652929.png
Accuracy:   0.9555
Precision:  0.8906
Recall:     0.9275
F1-score:   0.9087

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3801, Test Loss: 0.3845, F1: 0.7442, AUC: 0.9724
Epoch [10/30] Train Loss: 0.1004, Test Loss: 0.1351, F1: 0.8982, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0577, Test Loss: 0.1609, F1: 0.9003, AUC: 0.9878
Mejores resultados en la época:  29
f1-score 0.911545360143491
AUC según el mejor F1-score 0.9899103924933556
Confusion Matrix:
 [[15860   605]
 [  332  4828]]
Matriz de confusión guardada en: outputs_hold_out/0/Length/confusion_matrix_param_652929.png
Accuracy:   0.9567
Precision:  0.8886
Recall:     0.9357
F1-score:   0.9115
Tiempo total para red 3: 167.80 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3618, Test Loss: 0.1669, F1: 0.8527, AUC: 0.9755
Epoch [10/30] Train Loss: 0.0958, Test Loss: 0.1679, F1: 0.8766, AUC: 0.9874
Epoch [20/30] Train Loss: 0.0483, Test Loss: 0.3791, F1: 0.8233, AUC: 0.9868
Mejores resultados en la época:  19
f1-score 0.9161902916384072
AUC según el mejor F1-score 0.9899668547565071
Confusion Matrix:
 [[16032   433]
 [  432  4728]]
Matriz de confusión guardada en: outputs_hold_out/0/Length/confusion_matrix_param_2742785.png
Accuracy:   0.9600
Precision:  0.9161
Recall:     0.9163
F1-score:   0.9162

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3445, Test Loss: 0.1710, F1: 0.8644, AUC: 0.9760
Epoch [10/30] Train Loss: 0.0964, Test Loss: 0.1401, F1: 0.8921, AUC: 0.9886
Epoch [20/30] Train Loss: 0.0577, Test Loss: 0.1655, F1: 0.9037, AUC: 0.9899
Mejores resultados en la época:  16
f1-score 0.9105567675234753
AUC según el mejor F1-score 0.9894385200460454
Confusion Matrix:
 [[15882   583]
 [  360  4800]]
Matriz de confusión guardada en: outputs_hold_out/0/Length/confusion_matrix_param_2742785.png
Accuracy:   0.9564
Precision:  0.8917
Recall:     0.9302
F1-score:   0.9106

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3717, Test Loss: 0.4257, F1: 0.7006, AUC: 0.9729
Epoch [10/30] Train Loss: 0.0946, Test Loss: 0.1401, F1: 0.8962, AUC: 0.9877
Epoch [20/30] Train Loss: 0.0417, Test Loss: 0.1620, F1: 0.9001, AUC: 0.9879
Mejores resultados en la época:  21
f1-score 0.9165330566820711
AUC según el mejor F1-score 0.9901690748757641
Confusion Matrix:
 [[15881   584]
 [  301  4859]]
Matriz de confusión guardada en: outputs_hold_out/0/Length/confusion_matrix_param_2742785.png
Accuracy:   0.9591
Precision:  0.8927
Recall:     0.9417
F1-score:   0.9165
Tiempo total para red 5: 184.45 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3693, Test Loss: 0.1868, F1: 0.8385, AUC: 0.9714
Epoch [10/30] Train Loss: 0.0964, Test Loss: 0.1541, F1: 0.8865, AUC: 0.9849
Epoch [20/30] Train Loss: 0.0607, Test Loss: 0.3018, F1: 0.8302, AUC: 0.9871
Mejores resultados en la época:  26
f1-score 0.9182833627278072
AUC según el mejor F1-score 0.9894667570627853
Confusion Matrix:
 [[16105   360]
 [  474  4686]]
Matriz de confusión guardada en: outputs_hold_out/0/Length/confusion_matrix_param_5839873.png
Accuracy:   0.9614
Precision:  0.9287
Recall:     0.9081
F1-score:   0.9183

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3608, Test Loss: 0.2118, F1: 0.8243, AUC: 0.9684
Epoch [10/30] Train Loss: 0.0947, Test Loss: 0.1571, F1: 0.8938, AUC: 0.9852
Epoch [20/30] Train Loss: 0.0488, Test Loss: 0.1879, F1: 0.8849, AUC: 0.9898
Mejores resultados en la época:  26
f1-score 0.9121850319669048
AUC según el mejor F1-score 0.9901030668766494
Confusion Matrix:
 [[15840   625]
 [  309  4851]]
Matriz de confusión guardada en: outputs_hold_out/0/Length/confusion_matrix_param_5839873.png
Accuracy:   0.9568
Precision:  0.8859
Recall:     0.9401
F1-score:   0.9122

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3699, Test Loss: 0.2847, F1: 0.8216, AUC: 0.9744
Epoch [10/30] Train Loss: 0.1014, Test Loss: 0.1747, F1: 0.8831, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0492, Test Loss: 0.1492, F1: 0.9136, AUC: 0.9893
Mejores resultados en la época:  23
f1-score 0.9177380376993717
AUC según el mejor F1-score 0.9900246941480285
Confusion Matrix:
 [[16027   438]
 [  413  4747]]
Matriz de confusión guardada en: outputs_hold_out/0/Length/confusion_matrix_param_5839873.png
Accuracy:   0.9606
Precision:  0.9155
Recall:     0.9200
F1-score:   0.9177
Tiempo total para red 6: 214.10 segundos
Saved on: outputs_hold_out/0/Length

==============================
Model: Logistic Regression
Accuracy:  0.9355
Precision: 0.8276
Recall:    0.9217
F1-score:  0.8721
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15474   991]
 [  404  4756]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Length/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Length/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7657
Precision: 0.5051
Recall:    0.9025
F1-score:  0.6477
              precision    recall  f1-score   support

           0       0.96      0.72      0.82     16465
           1       0.51      0.90      0.65      5160

    accuracy                           0.77     21625
   macro avg       0.73      0.81      0.74     21625
weighted avg       0.85      0.77      0.78     21625

[[11902  4563]
 [  503  4657]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Length/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Length/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8725
Precision: 0.7191
Recall:    0.7641
F1-score:  0.7410
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14925  1540]
 [ 1217  3943]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Length/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Length/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8606
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [18:58:48] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Precision: 0.6799
Recall:    0.7859
F1-score:  0.7291
              precision    recall  f1-score   support

           0       0.93      0.88      0.91     16465
           1       0.68      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14556  1909]
 [ 1105  4055]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Length/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Length/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9192
Precision: 0.7998
Recall:    0.8820
F1-score:  0.8389
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15326  1139]
 [  609  4551]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Length/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Length/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7871
Precision: 0.5294
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12022  4443]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Length/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Length/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9355, 'precision': 0.8276, 'recall': 0.9217, 'f1_score': 0.8721}
XGBoost: {'accuracy': 0.9192, 'precision': 0.7998, 'recall': 0.882, 'f1_score': 0.8389}
Decision Tree: {'accuracy': 0.8725, 'precision': 0.7191, 'recall': 0.7641, 'f1_score': 0.741}
Random Forest: {'accuracy': 0.8606, 'precision': 0.6799, 'recall': 0.7859, 'f1_score': 0.7291}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7657, 'precision': 0.5051, 'recall': 0.9025, 'f1_score': 0.6477}

##################################################
Running experiment without LOUDNESS (DB) embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4275, Test Loss: 0.2995, F1: 0.7921, AUC: 0.9410
Epoch [10/30] Train Loss: 0.1303, Test Loss: 0.2641, F1: 0.8103, AUC: 0.9844
Epoch [20/30] Train Loss: 0.1057, Test Loss: 0.1729, F1: 0.8691, AUC: 0.9843
Mejores resultados en la época:  25
f1-score 0.8983925962006819
AUC según el mejor F1-score 0.9857811378140618
Confusion Matrix:
 [[15971   494]
 [  549  4611]]
Matriz de confusión guardada en: outputs_hold_out/0/Loudness (db)/confusion_matrix_param_160673.png
Accuracy:   0.9518
Precision:  0.9032
Recall:     0.8936
F1-score:   0.8984

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4231, Test Loss: 0.2921, F1: 0.7857, AUC: 0.9431
Epoch [10/30] Train Loss: 0.1317, Test Loss: 0.1758, F1: 0.8657, AUC: 0.9850
Epoch [20/30] Train Loss: 0.1032, Test Loss: 0.1450, F1: 0.8911, AUC: 0.9858
Mejores resultados en la época:  28
f1-score 0.8930782217220034
AUC según el mejor F1-score 0.9860873428955477
Confusion Matrix:
 [[15724   741]
 [  399  4761]]
Matriz de confusión guardada en: outputs_hold_out/0/Loudness (db)/confusion_matrix_param_160673.png
Accuracy:   0.9473
Precision:  0.8653
Recall:     0.9227
F1-score:   0.8931

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4276, Test Loss: 0.3546, F1: 0.7660, AUC: 0.9345
Epoch [10/30] Train Loss: 0.1272, Test Loss: 0.1341, F1: 0.8887, AUC: 0.9847
Epoch [20/30] Train Loss: 0.1121, Test Loss: 0.1303, F1: 0.8974, AUC: 0.9857
Mejores resultados en la época:  20
f1-score 0.8973779829126977
AUC según el mejor F1-score 0.9857087502972007
Confusion Matrix:
 [[16011   454]
 [  591  4569]]
Matriz de confusión guardada en: outputs_hold_out/0/Loudness (db)/confusion_matrix_param_160673.png
Accuracy:   0.9517
Precision:  0.9096
Recall:     0.8855
F1-score:   0.8974
Tiempo total para red 1: 156.08 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3902, Test Loss: 0.2058, F1: 0.8521, AUC: 0.9713
Epoch [10/30] Train Loss: 0.1254, Test Loss: 0.1688, F1: 0.8730, AUC: 0.9847
Epoch [20/30] Train Loss: 0.0757, Test Loss: 0.1300, F1: 0.9017, AUC: 0.9873
Mejores resultados en la época:  28
f1-score 0.9135898684851437
AUC según el mejor F1-score 0.9892276487357491
Confusion Matrix:
 [[16049   416]
 [  471  4689]]
Matriz de confusión guardada en: outputs_hold_out/0/Loudness (db)/confusion_matrix_param_652929.png
Accuracy:   0.9590
Precision:  0.9185
Recall:     0.9087
F1-score:   0.9136

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3902, Test Loss: 0.6222, F1: 0.6306, AUC: 0.9663
Epoch [10/30] Train Loss: 0.1264, Test Loss: 0.1273, F1: 0.8990, AUC: 0.9863
Epoch [20/30] Train Loss: 0.0795, Test Loss: 0.1381, F1: 0.9014, AUC: 0.9884
Mejores resultados en la época:  17
f1-score 0.9050162120923135
AUC según el mejor F1-score 0.9878289688957312
Confusion Matrix:
 [[15884   581]
 [  415  4745]]
Matriz de confusión guardada en: outputs_hold_out/0/Loudness (db)/confusion_matrix_param_652929.png
Accuracy:   0.9539
Precision:  0.8909
Recall:     0.9196
F1-score:   0.9050

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4076, Test Loss: 0.3013, F1: 0.7993, AUC: 0.9630
Epoch [10/30] Train Loss: 0.1186, Test Loss: 0.1501, F1: 0.8836, AUC: 0.9856
Epoch [20/30] Train Loss: 0.0758, Test Loss: 0.2186, F1: 0.8553, AUC: 0.9882
Mejores resultados en la época:  24
f1-score 0.9082058414464534
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [19:21:36] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
AUC según el mejor F1-score 0.9874600338514633
Confusion Matrix:
 [[16130   335]
 [  589  4571]]
Matriz de confusión guardada en: outputs_hold_out/0/Loudness (db)/confusion_matrix_param_652929.png
Accuracy:   0.9573
Precision:  0.9317
Recall:     0.8859
F1-score:   0.9082
Tiempo total para red 3: 167.76 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4021, Test Loss: 0.2222, F1: 0.8350, AUC: 0.9685
Epoch [10/30] Train Loss: 0.1223, Test Loss: 0.1759, F1: 0.8658, AUC: 0.9844
Epoch [20/30] Train Loss: 0.0675, Test Loss: 0.1969, F1: 0.8674, AUC: 0.9873
Mejores resultados en la época:  23
f1-score 0.9122271990851043
AUC según el mejor F1-score 0.9895249142531609
Confusion Matrix:
 [[15918   547]
 [  374  4786]]
Matriz de confusión guardada en: outputs_hold_out/0/Loudness (db)/confusion_matrix_param_2742785.png
Accuracy:   0.9574
Precision:  0.8974
Recall:     0.9275
F1-score:   0.9122

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3864, Test Loss: 0.8739, F1: 0.5063, AUC: 0.9688
Epoch [10/30] Train Loss: 0.1254, Test Loss: 0.1681, F1: 0.8803, AUC: 0.9853
Epoch [20/30] Train Loss: 0.0699, Test Loss: 0.1387, F1: 0.9103, AUC: 0.9887
Mejores resultados en la época:  29
f1-score 0.9157608695652174
AUC según el mejor F1-score 0.9895428051516371
Confusion Matrix:
 [[16039   426]
 [  442  4718]]
Matriz de confusión guardada en: outputs_hold_out/0/Loudness (db)/confusion_matrix_param_2742785.png
Accuracy:   0.9599
Precision:  0.9172
Recall:     0.9143
F1-score:   0.9158

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3964, Test Loss: 0.2732, F1: 0.8191, AUC: 0.9708
Epoch [10/30] Train Loss: 0.1217, Test Loss: 0.1411, F1: 0.8910, AUC: 0.9856
Epoch [20/30] Train Loss: 0.0752, Test Loss: 0.1603, F1: 0.8765, AUC: 0.9862
Mejores resultados en la época:  26
f1-score 0.9087941628264209
AUC según el mejor F1-score 0.988801027314223
Confusion Matrix:
 [[15942   523]
 [  427  4733]]
Matriz de confusión guardada en: outputs_hold_out/0/Loudness (db)/confusion_matrix_param_2742785.png
Accuracy:   0.9561
Precision:  0.9005
Recall:     0.9172
F1-score:   0.9088
Tiempo total para red 5: 185.35 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4172, Test Loss: 0.2045, F1: 0.8298, AUC: 0.9627
Epoch [10/30] Train Loss: 0.1179, Test Loss: 0.2620, F1: 0.8340, AUC: 0.9857
Epoch [20/30] Train Loss: 0.0778, Test Loss: 0.1232, F1: 0.8951, AUC: 0.9880
Mejores resultados en la época:  27
f1-score 0.9129000969932105
AUC según el mejor F1-score 0.9895502498840621
Confusion Matrix:
 [[16021   444]
 [  454  4706]]
Matriz de confusión guardada en: outputs_hold_out/0/Loudness (db)/confusion_matrix_param_5839873.png
Accuracy:   0.9585
Precision:  0.9138
Recall:     0.9120
F1-score:   0.9129

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4033, Test Loss: 0.2911, F1: 0.8124, AUC: 0.9580
Epoch [10/30] Train Loss: 0.1161, Test Loss: 0.1354, F1: 0.8915, AUC: 0.9856
Epoch [20/30] Train Loss: 0.0791, Test Loss: 0.1937, F1: 0.8598, AUC: 0.9880
Mejores resultados en la época:  27
f1-score 0.9126620900076278
AUC según el mejor F1-score 0.9893199516474929
Confusion Matrix:
 [[15923   542]
 [  374  4786]]
Matriz de confusión guardada en: outputs_hold_out/0/Loudness (db)/confusion_matrix_param_5839873.png
Accuracy:   0.9576
Precision:  0.8983
Recall:     0.9275
F1-score:   0.9127

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3968, Test Loss: 0.8662, F1: 0.4843, AUC: 0.9669
Epoch [10/30] Train Loss: 0.1278, Test Loss: 0.1541, F1: 0.8754, AUC: 0.9857
Epoch [20/30] Train Loss: 0.0808, Test Loss: 0.3034, F1: 0.8038, AUC: 0.9882
Mejores resultados en la época:  25
f1-score 0.911977186311787
AUC según el mejor F1-score 0.9890466622881049
Confusion Matrix:
 [[15902   563]
 [  363  4797]]
Matriz de confusión guardada en: outputs_hold_out/0/Loudness (db)/confusion_matrix_param_5839873.png
Accuracy:   0.9572
Precision:  0.8950
Recall:     0.9297
F1-score:   0.9120
Tiempo total para red 6: 214.99 segundos
Saved on: outputs_hold_out/0/Loudness (db)

==============================
Model: Logistic Regression
Accuracy:  0.9358
Precision: 0.8290
Recall:    0.9207
F1-score:  0.8725
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15485   980]
 [  409  4751]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Loudness (db)/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Loudness (db)/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7579
Precision: 0.4959
Recall:    0.9000
F1-score:  0.6395
              precision    recall  f1-score   support

           0       0.96      0.71      0.82     16465
           1       0.50      0.90      0.64      5160

    accuracy                           0.76     21625
   macro avg       0.73      0.81      0.73     21625
weighted avg       0.85      0.76      0.78     21625

[[11745  4720]
 [  516  4644]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Loudness (db)/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Loudness (db)/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8726
Precision: 0.7179
Recall:    0.7674
F1-score:  0.7419
              precision    recall  f1-score   support

           0       0.93      0.91      0.92     16465
           1       0.72      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14909  1556]
 [ 1200  3960]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Loudness (db)/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Loudness (db)/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8606
Precision: 0.6791
Recall:    0.7882
F1-score:  0.7296
              precision    recall  f1-score   support

           0       0.93      0.88      0.91     16465
           1       0.68      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.84      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14543  1922]
 [ 1093  4067]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Loudness (db)/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Loudness (db)/random_forest_model.pkl

==============================
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Model: XGBoost
Accuracy:  0.9201
Precision: 0.8012
Recall:    0.8849
F1-score:  0.8410
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15332  1133]
 [  594  4566]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Loudness (db)/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Loudness (db)/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7871
Precision: 0.5294
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12022  4443]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Loudness (db)/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Loudness (db)/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9358, 'precision': 0.829, 'recall': 0.9207, 'f1_score': 0.8725}
XGBoost: {'accuracy': 0.9201, 'precision': 0.8012, 'recall': 0.8849, 'f1_score': 0.841}
Decision Tree: {'accuracy': 0.8726, 'precision': 0.7179, 'recall': 0.7674, 'f1_score': 0.7419}
Random Forest: {'accuracy': 0.8606, 'precision': 0.6791, 'recall': 0.7882, 'f1_score': 0.7296}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7579, 'precision': 0.4959, 'recall': 0.9, 'f1_score': 0.6395}

##################################################
Running experiment without POPULARITY embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4328, Test Loss: 0.4557, F1: 0.7003, AUC: 0.9344
Epoch [10/30] Train Loss: 0.1312, Test Loss: 0.2846, F1: 0.8008, AUC: 0.9849
Epoch [20/30] Train Loss: 0.1032, Test Loss: 0.1690, F1: 0.8749, AUC: 0.9854
Mejores resultados en la época:  21
f1-score 0.8938385883714203
AUC según el mejor F1-score 0.985494759849999
Confusion Matrix:
 [[15889   576]
 [  525  4635]]
Matriz de confusión guardada en: outputs_hold_out/0/Popularity/confusion_matrix_param_160673.png
Accuracy:   0.9491
Precision:  0.8895
Recall:     0.8983
F1-score:   0.8938

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4262, Test Loss: 0.2701, F1: 0.7600, AUC: 0.9348
Epoch [10/30] Train Loss: 0.1326, Test Loss: 0.1417, F1: 0.8885, AUC: 0.9839
Epoch [20/30] Train Loss: 0.1061, Test Loss: 0.1447, F1: 0.8885, AUC: 0.9851
Mejores resultados en la época:  26
f1-score 0.8968820252174763
AUC según el mejor F1-score 0.9852830705019103
Confusion Matrix:
 [[15982   483]
 [  572  4588]]
Matriz de confusión guardada en: outputs_hold_out/0/Popularity/confusion_matrix_param_160673.png
Accuracy:   0.9512
Precision:  0.9048
Recall:     0.8891
F1-score:   0.8969

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4411, Test Loss: 0.4073, F1: 0.7321, AUC: 0.9291
Epoch [10/30] Train Loss: 0.1284, Test Loss: 0.2001, F1: 0.8491, AUC: 0.9844
Epoch [20/30] Train Loss: 0.1064, Test Loss: 0.2022, F1: 0.8550, AUC: 0.9853
Mejores resultados en la época:  25
f1-score 0.8976347421481194
AUC según el mejor F1-score 0.9854489968149494
Confusion Matrix:
 [[15939   526]
 [  530  4630]]
Matriz de confusión guardada en: outputs_hold_out/0/Popularity/confusion_matrix_param_160673.png
Accuracy:   0.9512
Precision:  0.8980
Recall:     0.8973
F1-score:   0.8976
Tiempo total para red 1: 159.39 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3711, Test Loss: 0.2026, F1: 0.8455, AUC: 0.9706
Epoch [10/30] Train Loss: 0.1175, Test Loss: 0.1777, F1: 0.8686, AUC: 0.9862
Epoch [20/30] Train Loss: 0.0751, Test Loss: 0.1911, F1: 0.8783, AUC: 0.9868
Mejores resultados en la época:  23
f1-score 0.9094066570188133
AUC según el mejor F1-score 0.9880448955618802
Confusion Matrix:
 [[15973   492]
 [  447  4713]]
Matriz de confusión guardada en: outputs_hold_out/0/Popularity/confusion_matrix_param_652929.png
Accuracy:   0.9566
Precision:  0.9055
Recall:     0.9134
F1-score:   0.9094

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4028, Test Loss: 0.2186, F1: 0.8422, AUC: 0.9722
Epoch [10/30] Train Loss: 0.1178, Test Loss: 0.1448, F1: 0.8876, AUC: 0.9857
Epoch [20/30] Train Loss: 0.0695, Test Loss: 0.3213, F1: 0.8090, AUC: 0.9871
Mejores resultados en la época:  26
f1-score 0.9094030135418654
AUC según el mejor F1-score 0.9890012994442052
Confusion Matrix:
 [[15907   558]
 [  392  4768]]
Matriz de confusión guardada en: outputs_hold_out/0/Popularity/confusion_matrix_param_652929.png
Accuracy:   0.9561
Precision:  0.8952
Recall:     0.9240
F1-score:   0.9094

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4048, Test Loss: 0.2658, F1: 0.8150, AUC: 0.9686
Epoch [10/30] Train Loss: 0.1180, Test Loss: 0.1374, F1: 0.8956, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0772, Test Loss: 0.1772, F1: 0.8834, AUC: 0.9865
Mejores resultados en la época:  18
f1-score 0.9065743944636678
AUC según el mejor F1-score 0.9874682966216805
Confusion Matrix:
 [[15937   528]
 [  444  4716]]
Matriz de confusión guardada en: outputs_hold_out/0/Popularity/confusion_matrix_param_652929.png
Accuracy:   0.9551
Precision:  0.8993
Recall:     0.9140
F1-score:   0.9066
Tiempo total para red 3: 169.61 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3874, Test Loss: 0.3286, F1: 0.7978, AUC: 0.9677
Epoch [10/30] Train Loss: 0.1158, Test Loss: 0.2092, F1: 0.8360, AUC: 0.9852
Epoch [20/30] Train Loss: 0.0665, Test Loss: 0.1754, F1: 0.8867, AUC: 0.9866
Mejores resultados en la época:  29
f1-score 0.911567199470349
AUC según el mejor F1-score 0.9894239071839019
Confusion Matrix:
 [[15871   594]
 [  341  4819]]
Matriz de confusión guardada en: outputs_hold_out/0/Popularity/confusion_matrix_param_2742785.png
Accuracy:   0.9568
Precision:  0.8903
Recall:     0.9339
F1-score:   0.9116

--- Iteración 2 de 3 para la red 5 ---
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [19:44:35] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [0/30] Train Loss: 0.3826, Test Loss: 0.3412, F1: 0.7716, AUC: 0.9672
Epoch [10/30] Train Loss: 0.1164, Test Loss: 0.2012, F1: 0.8460, AUC: 0.9852
Epoch [20/30] Train Loss: 0.0661, Test Loss: 0.1703, F1: 0.8979, AUC: 0.9874
Mejores resultados en la época:  29
f1-score 0.9114716766296612
AUC según el mejor F1-score 0.9896867150662552
Confusion Matrix:
 [[15889   576]
 [  357  4803]]
Matriz de confusión guardada en: outputs_hold_out/0/Popularity/confusion_matrix_param_2742785.png
Accuracy:   0.9569
Precision:  0.8929
Recall:     0.9308
F1-score:   0.9115

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3886, Test Loss: 0.2485, F1: 0.8174, AUC: 0.9665
Epoch [10/30] Train Loss: 0.1158, Test Loss: 0.1622, F1: 0.8783, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0746, Test Loss: 0.1515, F1: 0.9036, AUC: 0.9884
Mejores resultados en la época:  28
f1-score 0.9157170923379175
AUC según el mejor F1-score 0.9887463364854271
Confusion Matrix:
 [[16106   359]
 [  499  4661]]
Matriz de confusión guardada en: outputs_hold_out/0/Popularity/confusion_matrix_param_2742785.png
Accuracy:   0.9603
Precision:  0.9285
Recall:     0.9033
F1-score:   0.9157
Tiempo total para red 5: 187.63 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3865, Test Loss: 0.4248, F1: 0.7249, AUC: 0.9729
Epoch [10/30] Train Loss: 0.1217, Test Loss: 0.1397, F1: 0.8853, AUC: 0.9861
Epoch [20/30] Train Loss: 0.0751, Test Loss: 0.1760, F1: 0.8852, AUC: 0.9882
Mejores resultados en la época:  24
f1-score 0.9065161228590566
AUC según el mejor F1-score 0.9875298142406843
Confusion Matrix:
 [[15911   554]
 [  423  4737]]
Matriz de confusión guardada en: outputs_hold_out/0/Popularity/confusion_matrix_param_5839873.png
Accuracy:   0.9548
Precision:  0.8953
Recall:     0.9180
F1-score:   0.9065

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4140, Test Loss: 0.2806, F1: 0.8044, AUC: 0.9637
Epoch [10/30] Train Loss: 0.1129, Test Loss: 0.3190, F1: 0.8016, AUC: 0.9864
Epoch [20/30] Train Loss: 0.0701, Test Loss: 0.1815, F1: 0.8781, AUC: 0.9834
Mejores resultados en la época:  27
f1-score 0.9150058616647128
AUC según el mejor F1-score 0.9890868461877085
Confusion Matrix:
 [[16072   393]
 [  477  4683]]
Matriz de confusión guardada en: outputs_hold_out/0/Popularity/confusion_matrix_param_5839873.png
Accuracy:   0.9598
Precision:  0.9226
Recall:     0.9076
F1-score:   0.9150

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4146, Test Loss: 0.2094, F1: 0.8123, AUC: 0.9602
Epoch [10/30] Train Loss: 0.1193, Test Loss: 0.1351, F1: 0.8910, AUC: 0.9857
Epoch [20/30] Train Loss: 0.0713, Test Loss: 0.6240, F1: 0.7219, AUC: 0.9874
Mejores resultados en la época:  24
f1-score 0.908879410624273
AUC según el mejor F1-score 0.9881136225067503
Confusion Matrix:
 [[15997   468]
 [  472  4688]]
Matriz de confusión guardada en: outputs_hold_out/0/Popularity/confusion_matrix_param_5839873.png
Accuracy:   0.9565
Precision:  0.9092
Recall:     0.9085
F1-score:   0.9089
Tiempo total para red 6: 216.51 segundos
Saved on: outputs_hold_out/0/Popularity

==============================
Model: Logistic Regression
Accuracy:  0.9356
Precision: 0.8284
Recall:    0.9207
F1-score:  0.8721
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15481   984]
 [  409  4751]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Popularity/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Popularity/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8071
Precision: 0.5601
Recall:    0.8917
F1-score:  0.6881
              precision    recall  f1-score   support

           0       0.96      0.78      0.86     16465
           1       0.56      0.89      0.69      5160

    accuracy                           0.81     21625
   macro avg       0.76      0.84      0.77     21625
weighted avg       0.86      0.81      0.82     21625

[[12852  3613]
 [  559  4601]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Popularity/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Popularity/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8728
Precision: 0.7201
Recall:    0.7641
F1-score:  0.7414
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14932  1533]
 [ 1217  3943]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Popularity/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Popularity/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8593
Precision: 0.6751
Recall:    0.7909
F1-score:  0.7284
              precision    recall  f1-score   support

           0       0.93      0.88      0.91     16465
           1       0.68      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.84      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14501  1964]
 [ 1079  4081]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Popularity/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Popularity/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9200
Precision: 0.7997
Recall:    0.8872
F1-score:  0.8412
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15318  1147]
 [  582  4578]]
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Popularity/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Popularity/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7871
Precision: 0.5294
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12022  4443]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Popularity/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Popularity/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9356, 'precision': 0.8284, 'recall': 0.9207, 'f1_score': 0.8721}
XGBoost: {'accuracy': 0.92, 'precision': 0.7997, 'recall': 0.8872, 'f1_score': 0.8412}
Decision Tree: {'accuracy': 0.8728, 'precision': 0.7201, 'recall': 0.7641, 'f1_score': 0.7414}
Random Forest: {'accuracy': 0.8593, 'precision': 0.6751, 'recall': 0.7909, 'f1_score': 0.7284}
SVM: {'accuracy': 0.8071, 'precision': 0.5601, 'recall': 0.8917, 'f1_score': 0.6881}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}

##################################################
Running experiment without ENERGY embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4133, Test Loss: 0.2638, F1: 0.7891, AUC: 0.9419
Epoch [10/30] Train Loss: 0.1244, Test Loss: 0.1362, F1: 0.8887, AUC: 0.9841
Epoch [20/30] Train Loss: 0.0973, Test Loss: 0.1385, F1: 0.8940, AUC: 0.9856
Mejores resultados en la época:  25
f1-score 0.8995905634626633
AUC según el mejor F1-score 0.986169493899439
Confusion Matrix:
 [[15981   484]
 [  546  4614]]
Matriz de confusión guardada en: outputs_hold_out/0/Energy/confusion_matrix_param_160673.png
Accuracy:   0.9524
Precision:  0.9051
Recall:     0.8942
F1-score:   0.8996

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4212, Test Loss: 0.2681, F1: 0.7795, AUC: 0.9410
Epoch [10/30] Train Loss: 0.1322, Test Loss: 0.1721, F1: 0.8699, AUC: 0.9839
Epoch [20/30] Train Loss: 0.1073, Test Loss: 0.2207, F1: 0.8453, AUC: 0.9851
Mejores resultados en la época:  26
f1-score 0.8934019832189168
AUC según el mejor F1-score 0.9854209775492765
Confusion Matrix:
 [[15822   643]
 [  475  4685]]
Matriz de confusión guardada en: outputs_hold_out/0/Energy/confusion_matrix_param_160673.png
Accuracy:   0.9483
Precision:  0.8793
Recall:     0.9079
F1-score:   0.8934

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4178, Test Loss: 0.3079, F1: 0.7857, AUC: 0.9426
Epoch [10/30] Train Loss: 0.1303, Test Loss: 0.1412, F1: 0.8874, AUC: 0.9839
Epoch [20/30] Train Loss: 0.0997, Test Loss: 0.1422, F1: 0.8934, AUC: 0.9851
Mejores resultados en la época:  21
f1-score 0.8938925396211829
AUC según el mejor F1-score 0.9856204669524503
Confusion Matrix:
 [[15902   563]
 [  535  4625]]
Matriz de confusión guardada en: outputs_hold_out/0/Energy/confusion_matrix_param_160673.png
Accuracy:   0.9492
Precision:  0.8915
Recall:     0.8963
F1-score:   0.8939
Tiempo total para red 1: 157.86 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3919, Test Loss: 0.2338, F1: 0.8287, AUC: 0.9707
Epoch [10/30] Train Loss: 0.1137, Test Loss: 0.1463, F1: 0.8859, AUC: 0.9862
Epoch [20/30] Train Loss: 0.0684, Test Loss: 0.2223, F1: 0.8602, AUC: 0.9884
Mejores resultados en la época:  26
f1-score 0.9125824018343365
AUC según el mejor F1-score 0.9889350560385315
Confusion Matrix:
 [[15934   531]
 [  384  4776]]
Matriz de confusión guardada en: outputs_hold_out/0/Energy/confusion_matrix_param_652929.png
Accuracy:   0.9577
Precision:  0.8999
Recall:     0.9256
F1-score:   0.9126

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3935, Test Loss: 0.3384, F1: 0.7665, AUC: 0.9725
Epoch [10/30] Train Loss: 0.1210, Test Loss: 0.5139, F1: 0.7188, AUC: 0.9846
Epoch [20/30] Train Loss: 0.0739, Test Loss: 0.2598, F1: 0.8377, AUC: 0.9873
Mejores resultados en la época:  28
f1-score 0.9130602782071098
AUC según el mejor F1-score 0.9891914608624824
Confusion Matrix:
 [[15999   466]
 [  434  4726]]
Matriz de confusión guardada en: outputs_hold_out/0/Energy/confusion_matrix_param_652929.png
Accuracy:   0.9584
Precision:  0.9102
Recall:     0.9159
F1-score:   0.9131

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3944, Test Loss: 0.2447, F1: 0.7487, AUC: 0.9584
Epoch [10/30] Train Loss: 0.1220, Test Loss: 0.1320, F1: 0.8981, AUC: 0.9861
Epoch [20/30] Train Loss: 0.0711, Test Loss: 0.1759, F1: 0.8840, AUC: 0.9884
Mejores resultados en la época:  25
f1-score 0.9141801924385266
AUC según el mejor F1-score 0.9889591910959823
Confusion Matrix:
 [[16039   426]
 [  457  4703]]
Matriz de confusión guardada en: outputs_hold_out/0/Energy/confusion_matrix_param_652929.png
Accuracy:   0.9592
Precision:  0.9169
Recall:     0.9114
F1-score:   0.9142
Tiempo total para red 3: 169.16 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3866, Test Loss: 0.1826, F1: 0.8487, AUC: 0.9725
Epoch [10/30] Train Loss: 0.1100, Test Loss: 0.1514, F1: 0.8793, AUC: 0.9855
Epoch [20/30] Train Loss: 0.0611, Test Loss: 0.3676, F1: 0.8127, AUC: 0.9857
Mejores resultados en la época:  29
f1-score 0.9079789235980429
AUC según el mejor F1-score 0.9896212720428817
Confusion Matrix:
 [[15822   643]
 [  335  4825]]
Matriz de confusión guardada en: outputs_hold_out/0/Energy/confusion_matrix_param_2742785.png
Accuracy:   0.9548
Precision:  0.8824
Recall:     0.9351
F1-score:   0.9080

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3856, Test Loss: 0.2739, F1: 0.8024, AUC: 0.9719
Epoch [10/30] Train Loss: 0.1149, Test Loss: 0.2285, F1: 0.8320, AUC: 0.9849
Epoch [20/30] Train Loss: 0.0675, Test Loss: 0.1605, F1: 0.8896, AUC: 0.9874
Mejores resultados en la época:  28
f1-score 0.9154257350133639
AUC según el mejor F1-score 0.9902364953142325
Confusion Matrix:
 [[15944   521]
 [  365  4795]]
Matriz de confusión guardada en: outputs_hold_out/0/Energy/confusion_matrix_param_2742785.png
Accuracy:   0.9590
Precision:  0.9020
Recall:     0.9293
F1-score:   0.9154

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3922, Test Loss: 0.4957, F1: 0.6835, AUC: 0.9710
Epoch [10/30] Train Loss: 0.1187, Test Loss: 0.1360, F1: 0.8950, AUC: 0.9874
Epoch [20/30] Train Loss: 0.0607, Test Loss: 0.1448, F1: 0.8985, AUC: 0.9871
Mejores resultados en la época:  19
f1-score 0.9116124621545073
AUC según el mejor F1-score 0.9892457691556203
Confusion Matrix:
 [[16053   412]
 [  493  4667]]
Matriz de confusión guardada en: outputs_hold_out/0/Energy/confusion_matrix_param_2742785.png
Accuracy:   0.9582
Precision:  0.9189
Recall:     0.9045
F1-score:   0.9116
Tiempo total para red 5: 186.80 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4069, Test Loss: 0.2207, F1: 0.8358, AUC: 0.9682
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [20:10:09] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [10/30] Train Loss: 0.1144, Test Loss: 0.1808, F1: 0.8666, AUC: 0.9856
Epoch [20/30] Train Loss: 0.0690, Test Loss: 0.1721, F1: 0.8932, AUC: 0.9886
Mejores resultados en la época:  29
f1-score 0.9119683481701286
AUC según el mejor F1-score 0.9882664896409344
Confusion Matrix:
 [[16125   340]
 [  550  4610]]
Matriz de confusión guardada en: outputs_hold_out/0/Energy/confusion_matrix_param_5839873.png
Accuracy:   0.9588
Precision:  0.9313
Recall:     0.8934
F1-score:   0.9120

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4005, Test Loss: 0.2263, F1: 0.8358, AUC: 0.9680
Epoch [10/30] Train Loss: 0.1213, Test Loss: 0.3081, F1: 0.7951, AUC: 0.9869
Epoch [20/30] Train Loss: 0.0734, Test Loss: 0.1568, F1: 0.9047, AUC: 0.9878
Mejores resultados en la época:  18
f1-score 0.9077355019847032
AUC según el mejor F1-score 0.9879840017702572
Confusion Matrix:
 [[15984   481]
 [  472  4688]]
Matriz de confusión guardada en: outputs_hold_out/0/Energy/confusion_matrix_param_5839873.png
Accuracy:   0.9559
Precision:  0.9069
Recall:     0.9085
F1-score:   0.9077

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3916, Test Loss: 0.2420, F1: 0.8291, AUC: 0.9731
Epoch [10/30] Train Loss: 0.1133, Test Loss: 0.2459, F1: 0.8220, AUC: 0.9866
Epoch [20/30] Train Loss: 0.0711, Test Loss: 0.1874, F1: 0.8719, AUC: 0.9872
Mejores resultados en la época:  25
f1-score 0.9121615063573716
AUC según el mejor F1-score 0.9886085589116685
Confusion Matrix:
 [[16021   444]
 [  461  4699]]
Matriz de confusión guardada en: outputs_hold_out/0/Energy/confusion_matrix_param_5839873.png
Accuracy:   0.9582
Precision:  0.9137
Recall:     0.9107
F1-score:   0.9122
Tiempo total para red 6: 231.38 segundos
Saved on: outputs_hold_out/0/Energy

==============================
Model: Logistic Regression
Accuracy:  0.9356
Precision: 0.8285
Recall:    0.9209
F1-score:  0.8722
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15481   984]
 [  408  4752]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Energy/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Energy/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7702
Precision: 0.5103
Recall:    0.9209
F1-score:  0.6567
              precision    recall  f1-score   support

           0       0.97      0.72      0.83     16465
           1       0.51      0.92      0.66      5160

    accuracy                           0.77     21625
   macro avg       0.74      0.82      0.74     21625
weighted avg       0.86      0.77      0.79     21625

[[11904  4561]
 [  408  4752]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Energy/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Energy/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8774
Precision: 0.7391
Recall:    0.7516
F1-score:  0.7453
              precision    recall  f1-score   support

           0       0.92      0.92      0.92     16465
           1       0.74      0.75      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.83      0.83     21625
weighted avg       0.88      0.88      0.88     21625

[[15096  1369]
 [ 1282  3878]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Energy/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Energy/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8634
Precision: 0.6863
Recall:    0.7874
F1-score:  0.7334
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14608  1857]
 [ 1097  4063]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Energy/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Energy/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9189
Precision: 0.7991
Recall:    0.8820
F1-score:  0.8385
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15321  1144]
 [  609  4551]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Energy/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Energy/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7871
Precision: 0.5294
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12022  4443]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Energy/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Energy/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9356, 'precision': 0.8285, 'recall': 0.9209, 'f1_score': 0.8722}
XGBoost: {'accuracy': 0.9189, 'precision': 0.7991, 'recall': 0.882, 'f1_score': 0.8385}
Decision Tree: {'accuracy': 0.8774, 'precision': 0.7391, 'recall': 0.7516, 'f1_score': 0.7453}
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Random Forest: {'accuracy': 0.8634, 'precision': 0.6863, 'recall': 0.7874, 'f1_score': 0.7334}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7702, 'precision': 0.5103, 'recall': 0.9209, 'f1_score': 0.6567}

##################################################
Running experiment without DANCEABILITY embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4280, Test Loss: 0.3216, F1: 0.7905, AUC: 0.9453
Epoch [10/30] Train Loss: 0.1319, Test Loss: 0.1338, F1: 0.8849, AUC: 0.9847
Epoch [20/30] Train Loss: 0.1072, Test Loss: 0.1468, F1: 0.8876, AUC: 0.9854
Mejores resultados en la época:  21
f1-score 0.8931206380857428
AUC según el mejor F1-score 0.985349555199307
Confusion Matrix:
 [[16074   391]
 [  681  4479]]
Matriz de confusión guardada en: outputs_hold_out/0/Danceability/confusion_matrix_param_160673.png
Accuracy:   0.9504
Precision:  0.9197
Recall:     0.8680
F1-score:   0.8931

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4390, Test Loss: 0.3107, F1: 0.7783, AUC: 0.9412
Epoch [10/30] Train Loss: 0.1302, Test Loss: 0.2693, F1: 0.8108, AUC: 0.9846
Epoch [20/30] Train Loss: 0.1051, Test Loss: 0.1566, F1: 0.8783, AUC: 0.9845
Mejores resultados en la época:  24
f1-score 0.8916942190161307
AUC según el mejor F1-score 0.9844282622052415
Confusion Matrix:
 [[16094   371]
 [  710  4450]]
Matriz de confusión guardada en: outputs_hold_out/0/Danceability/confusion_matrix_param_160673.png
Accuracy:   0.9500
Precision:  0.9230
Recall:     0.8624
F1-score:   0.8917

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4293, Test Loss: 0.3624, F1: 0.7686, AUC: 0.9464
Epoch [10/30] Train Loss: 0.1252, Test Loss: 0.1395, F1: 0.8893, AUC: 0.9844
Epoch [20/30] Train Loss: 0.0998, Test Loss: 0.2441, F1: 0.8316, AUC: 0.9856
Mejores resultados en la época:  27
f1-score 0.8965780192545992
AUC según el mejor F1-score 0.9858783430673946
Confusion Matrix:
 [[15837   628]
 [  457  4703]]
Matriz de confusión guardada en: outputs_hold_out/0/Danceability/confusion_matrix_param_160673.png
Accuracy:   0.9498
Precision:  0.8822
Recall:     0.9114
F1-score:   0.8966
Tiempo total para red 1: 207.17 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3980, Test Loss: 0.2069, F1: 0.8256, AUC: 0.9624
Epoch [10/30] Train Loss: 0.1173, Test Loss: 0.1952, F1: 0.8537, AUC: 0.9855
Epoch [20/30] Train Loss: 0.0804, Test Loss: 0.1343, F1: 0.8943, AUC: 0.9871
Mejores resultados en la época:  24
f1-score 0.9053323029366306
AUC según el mejor F1-score 0.9878518739539122
Confusion Matrix:
 [[15959   506]
 [  474  4686]]
Matriz de confusión guardada en: outputs_hold_out/0/Danceability/confusion_matrix_param_652929.png
Accuracy:   0.9547
Precision:  0.9025
Recall:     0.9081
F1-score:   0.9053

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3825, Test Loss: 0.3037, F1: 0.7918, AUC: 0.9720
Epoch [10/30] Train Loss: 0.1221, Test Loss: 0.1958, F1: 0.8450, AUC: 0.9855
Epoch [20/30] Train Loss: 0.0723, Test Loss: 0.1445, F1: 0.8976, AUC: 0.9869
Mejores resultados en la época:  25
f1-score 0.9047384494138508
AUC según el mejor F1-score 0.9876923683547671
Confusion Matrix:
 [[16066   399]
 [  568  4592]]
Matriz de confusión guardada en: outputs_hold_out/0/Danceability/confusion_matrix_param_652929.png
Accuracy:   0.9553
Precision:  0.9201
Recall:     0.8899
F1-score:   0.9047

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4017, Test Loss: 0.2097, F1: 0.8435, AUC: 0.9664
Epoch [10/30] Train Loss: 0.1132, Test Loss: 0.1997, F1: 0.8558, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0796, Test Loss: 0.1240, F1: 0.9073, AUC: 0.9883
Mejores resultados en la época:  20
f1-score 0.9073310972688069
AUC según el mejor F1-score 0.9882553019442227
Confusion Matrix:
 [[15924   541]
 [  426  4734]]
Matriz de confusión guardada en: outputs_hold_out/0/Danceability/confusion_matrix_param_652929.png
Accuracy:   0.9553
Precision:  0.8974
Recall:     0.9174
F1-score:   0.9073
Tiempo total para red 3: 228.95 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3938, Test Loss: 0.1930, F1: 0.8320, AUC: 0.9661
Epoch [10/30] Train Loss: 0.1238, Test Loss: 0.1320, F1: 0.8752, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0729, Test Loss: 0.1691, F1: 0.8824, AUC: 0.9878
Mejores resultados en la época:  28
f1-score 0.9080919080919081
AUC según el mejor F1-score 0.9890076730767873
Confusion Matrix:
 [[16160   305]
 [  615  4545]]
Matriz de confusión guardada en: outputs_hold_out/0/Danceability/confusion_matrix_param_2742785.png
Accuracy:   0.9575
Precision:  0.9371
Recall:     0.8808
F1-score:   0.9081

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3981, Test Loss: 0.3854, F1: 0.7549, AUC: 0.9652
Epoch [10/30] Train Loss: 0.1096, Test Loss: 0.1914, F1: 0.8700, AUC: 0.9863
Epoch [20/30] Train Loss: 0.0691, Test Loss: 0.1313, F1: 0.9075, AUC: 0.9884
Mejores resultados en la época:  20
f1-score 0.907505032109652
AUC según el mejor F1-score 0.9884302384433034
Confusion Matrix:
 [[15926   539]
 [  426  4734]]
Matriz de confusión guardada en: outputs_hold_out/0/Danceability/confusion_matrix_param_2742785.png
Accuracy:   0.9554
Precision:  0.8978
Recall:     0.9174
F1-score:   0.9075

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3888, Test Loss: 0.2426, F1: 0.8331, AUC: 0.9719
Epoch [10/30] Train Loss: 0.1230, Test Loss: 0.1882, F1: 0.8645, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0683, Test Loss: 0.1577, F1: 0.8964, AUC: 0.9888
Mejores resultados en la época:  27
f1-score 0.9105737316263632
AUC según el mejor F1-score 0.9895259147310362
Confusion Matrix:
 [[15881   584]
 [  359  4801]]
Matriz de confusión guardada en: outputs_hold_out/0/Danceability/confusion_matrix_param_2742785.png
Accuracy:   0.9564
Precision:  0.8916
Recall:     0.9304
F1-score:   0.9106
Tiempo total para red 5: 208.20 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4076, Test Loss: 0.3068, F1: 0.7808, AUC: 0.9691
Epoch [10/30] Train Loss: 0.1194, Test Loss: 0.1330, F1: 0.8937, AUC: 0.9863
Epoch [20/30] Train Loss: 0.0788, Test Loss: 0.1570, F1: 0.8830, AUC: 0.9875
Mejores resultados en la época:  28
f1-score 0.9095657220234065
AUC según el mejor F1-score 0.9896266922789002
Confusion Matrix:
 [[15988   477]
 [  458  4702]]
Matriz de confusión guardada en: outputs_hold_out/0/Danceability/confusion_matrix_param_5839873.png
Accuracy:   0.9568
Precision:  0.9079
Recall:     0.9112
F1-score:   0.9096

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3900, Test Loss: 0.1875, F1: 0.8284, AUC: 0.9707
Epoch [10/30] Train Loss: 0.1123, Test Loss: 0.2367, F1: 0.8408, AUC: 0.9871
Epoch [20/30] Train Loss: 0.0619, Test Loss: 0.1568, F1: 0.9023, AUC: 0.9889
Mejores resultados en la época:  26
f1-score 0.9063590538968592
AUC según el mejor F1-score 0.988443179918879
Confusion Matrix:
 [[15984   481]
 [  485  4675]]
Matriz de confusión guardada en: outputs_hold_out/0/Danceability/confusion_matrix_param_5839873.png
Accuracy:   0.9553
Precision:  0.9067
Recall:     0.9060
F1-score:   0.9064

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4178, Test Loss: 0.3036, F1: 0.8050, AUC: 0.9715
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [20:39:22] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Epoch [10/30] Train Loss: 0.1158, Test Loss: 0.1348, F1: 0.8950, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0669, Test Loss: 0.2019, F1: 0.8855, AUC: 0.9891
Mejores resultados en la época:  26
f1-score 0.9049958290851794
AUC según el mejor F1-score 0.9897347733152542
Confusion Matrix:
 [[15718   747]
 [  278  4882]]
Matriz de confusión guardada en: outputs_hold_out/0/Danceability/confusion_matrix_param_5839873.png
Accuracy:   0.9526
Precision:  0.8673
Recall:     0.9461
F1-score:   0.9050
Tiempo total para red 6: 270.99 segundos
Saved on: outputs_hold_out/0/Danceability

==============================
Model: Logistic Regression
Accuracy:  0.9354
Precision: 0.8286
Recall:    0.9192
F1-score:  0.8716
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15484   981]
 [  417  4743]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Danceability/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Danceability/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8825
Precision: 0.7628
Recall:    0.7368
F1-score:  0.7496
              precision    recall  f1-score   support

           0       0.92      0.93      0.92     16465
           1       0.76      0.74      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.84      0.83      0.84     21625
weighted avg       0.88      0.88      0.88     21625

[[15283  1182]
 [ 1358  3802]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Danceability/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Danceability/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8755
Precision: 0.7292
Recall:    0.7605
F1-score:  0.7445
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.73      0.76      0.74      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.84      0.83     21625
weighted avg       0.88      0.88      0.88     21625

[[15008  1457]
 [ 1236  3924]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Danceability/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Danceability/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8628
Precision: 0.6855
Recall:    0.7853
F1-score:  0.7320
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14606  1859]
 [ 1108  4052]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Danceability/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Danceability/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9202
Precision: 0.8029
Recall:    0.8824
F1-score:  0.8407
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15347  1118]
 [  607  4553]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Danceability/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Danceability/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7871
Precision: 0.5294
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12022  4443]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Danceability/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Danceability/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.8286, 'recall': 0.9192, 'f1_score': 0.8716}
XGBoost: {'accuracy': 0.9202, 'precision': 0.8029, 'recall': 0.8824, 'f1_score': 0.8407}
SVM: {'accuracy': 0.8825, 'precision': 0.7628, 'recall': 0.7368, 'f1_score': 0.7496}
Decision Tree: {'accuracy': 0.8755, 'precision': 0.7292, 'recall': 0.7605, 'f1_score': 0.7445}
Random Forest: {'accuracy': 0.8628, 'precision': 0.6855, 'recall': 0.7853, 'f1_score': 0.732}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}

##################################################
Running experiment without POSITIVENESS embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4346, Test Loss: 0.2920, F1: 0.7776, AUC: 0.9426
Epoch [10/30] Train Loss: 0.1286, Test Loss: 0.1667, F1: 0.8733, AUC: 0.9847
Epoch [20/30] Train Loss: 0.1029, Test Loss: 0.1554, F1: 0.8803, AUC: 0.9846
Mejores resultados en la época:  17
f1-score 0.8931851135814403
AUC según el mejor F1-score 0.9854136858311148
Confusion Matrix:
 [[15900   565]
 [  540  4620]]
Matriz de confusión guardada en: outputs_hold_out/0/Positiveness/confusion_matrix_param_160673.png
Accuracy:   0.9489
Precision:  0.8910
Recall:     0.8953
F1-score:   0.8932

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4318, Test Loss: 0.2786, F1: 0.7879, AUC: 0.9472
Epoch [10/30] Train Loss: 0.1362, Test Loss: 0.1364, F1: 0.8910, AUC: 0.9847
Epoch [20/30] Train Loss: 0.1017, Test Loss: 0.1396, F1: 0.8918, AUC: 0.9856
Mejores resultados en la época:  18
f1-score 0.8956971411845058
AUC según el mejor F1-score 0.9853409334340874
Confusion Matrix:
 [[15966   499]
 [  570  4590]]
Matriz de confusión guardada en: outputs_hold_out/0/Positiveness/confusion_matrix_param_160673.png
Accuracy:   0.9506
Precision:  0.9019
Recall:     0.8895
F1-score:   0.8957

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4352, Test Loss: 0.3109, F1: 0.7881, AUC: 0.9484
Epoch [10/30] Train Loss: 0.1268, Test Loss: 0.2758, F1: 0.8052, AUC: 0.9844
Epoch [20/30] Train Loss: 0.1082, Test Loss: 0.1526, F1: 0.8824, AUC: 0.9846
Mejores resultados en la época:  26
f1-score 0.892357475968402
AUC según el mejor F1-score 0.9851615418658795
Confusion Matrix:
 [[15806   659]
 [  472  4688]]
Matriz de confusión guardada en: outputs_hold_out/0/Positiveness/confusion_matrix_param_160673.png
Accuracy:   0.9477
Precision:  0.8768
Recall:     0.9085
F1-score:   0.8924
Tiempo total para red 1: 232.38 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3965, Test Loss: 0.1924, F1: 0.8493, AUC: 0.9705
Epoch [10/30] Train Loss: 0.1263, Test Loss: 0.1540, F1: 0.8814, AUC: 0.9843
Epoch [20/30] Train Loss: 0.0777, Test Loss: 0.1800, F1: 0.8709, AUC: 0.9871
Mejores resultados en la época:  23
f1-score 0.907505032109652
AUC según el mejor F1-score 0.9879856790419894
Confusion Matrix:
 [[15926   539]
 [  426  4734]]
Matriz de confusión guardada en: outputs_hold_out/0/Positiveness/confusion_matrix_param_652929.png
Accuracy:   0.9554
Precision:  0.8978
Recall:     0.9174
F1-score:   0.9075

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4084, Test Loss: 0.1832, F1: 0.8378, AUC: 0.9712
Epoch [10/30] Train Loss: 0.1154, Test Loss: 0.1431, F1: 0.8876, AUC: 0.9863
Epoch [20/30] Train Loss: 0.0747, Test Loss: 0.2006, F1: 0.8453, AUC: 0.9870
Mejores resultados en la época:  15
f1-score 0.9008048094637836
AUC según el mejor F1-score 0.987165805078661
Confusion Matrix:
 [[15957   508]
 [  515  4645]]
Matriz de confusión guardada en: outputs_hold_out/0/Positiveness/confusion_matrix_param_652929.png
Accuracy:   0.9527
Precision:  0.9014
Recall:     0.9002
F1-score:   0.9008

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3966, Test Loss: 0.2531, F1: 0.8225, AUC: 0.9700
Epoch [10/30] Train Loss: 0.1218, Test Loss: 0.1405, F1: 0.8894, AUC: 0.9850
Epoch [20/30] Train Loss: 0.0757, Test Loss: 0.1352, F1: 0.9045, AUC: 0.9881
Mejores resultados en la época:  26
f1-score 0.9066999424073718
AUC según el mejor F1-score 0.9874087858435912
Confusion Matrix:
 [[15930   535]
 [  437  4723]]
Matriz de confusión guardada en: outputs_hold_out/0/Positiveness/confusion_matrix_param_652929.png
Accuracy:   0.9551
Precision:  0.8983
Recall:     0.9153
F1-score:   0.9067
Tiempo total para red 3: 244.00 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3886, Test Loss: 0.2022, F1: 0.8464, AUC: 0.9689
Epoch [10/30] Train Loss: 0.1180, Test Loss: 0.1435, F1: 0.8874, AUC: 0.9864
Epoch [20/30] Train Loss: 0.0716, Test Loss: 0.1701, F1: 0.8842, AUC: 0.9883
Mejores resultados en la época:  26
f1-score 0.9114760470313866
AUC según el mejor F1-score 0.9883290548191253
Confusion Matrix:
 [[16024   441]
 [  470  4690]]
Matriz de confusión guardada en: outputs_hold_out/0/Positiveness/confusion_matrix_param_2742785.png
Accuracy:   0.9579
Precision:  0.9141
Recall:     0.9089
F1-score:   0.9115

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3804, Test Loss: 0.1765, F1: 0.8564, AUC: 0.9748
Epoch [10/30] Train Loss: 0.1150, Test Loss: 0.1366, F1: 0.8960, AUC: 0.9866
Epoch [20/30] Train Loss: 0.0776, Test Loss: 0.1328, F1: 0.8973, AUC: 0.9888
Mejores resultados en la época:  24
f1-score 0.9110833413438432
AUC según el mejor F1-score 0.9885363479497267
Confusion Matrix:
 [[15961   504]
 [  421  4739]]
Matriz de confusión guardada en: outputs_hold_out/0/Positiveness/confusion_matrix_param_2742785.png
Accuracy:   0.9572
Precision:  0.9039
Recall:     0.9184
F1-score:   0.9111

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3815, Test Loss: 0.2047, F1: 0.8317, AUC: 0.9664
Epoch [10/30] Train Loss: 0.1214, Test Loss: 0.1497, F1: 0.8856, AUC: 0.9849
Epoch [20/30] Train Loss: 0.0668, Test Loss: 0.1361, F1: 0.9042, AUC: 0.9880
Mejores resultados en la época:  24
f1-score 0.9090909090909091
AUC según el mejor F1-score 0.9882161597186421
Confusion Matrix:
 [[15955   510]
 [  435  4725]]
Matriz de confusión guardada en: outputs_hold_out/0/Positiveness/confusion_matrix_param_2742785.png
Accuracy:   0.9563
Precision:  0.9026
Recall:     0.9157
F1-score:   0.9091
Tiempo total para red 5: 277.06 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3741, Test Loss: 0.2279, F1: 0.8393, AUC: 0.9723
Epoch [10/30] Train Loss: 0.1163, Test Loss: 0.2054, F1: 0.8491, AUC: 0.9863
Epoch [20/30] Train Loss: 0.0678, Test Loss: 0.1292, F1: 0.9132, AUC: 0.9890
Mejores resultados en la época:  20
f1-score 0.9132111861137898
AUC según el mejor F1-score 0.9890319434930097
Confusion Matrix:
 [[15990   475]
 [  425  4735]]
Matriz de confusión guardada en: outputs_hold_out/0/Positiveness/confusion_matrix_param_5839873.png
Accuracy:   0.9584
Precision:  0.9088
Recall:     0.9176
F1-score:   0.9132

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3896, Test Loss: 0.5085, F1: 0.6720, AUC: 0.9701
Epoch [10/30] Train Loss: 0.1192, Test Loss: 0.1981, F1: 0.8571, AUC: 0.9873
Epoch [20/30] Train Loss: 0.0703, Test Loss: 0.1921, F1: 0.8803, AUC: 0.9877
Mejores resultados en la época:  28
f1-score 0.9154380291068202
AUC según el mejor F1-score 0.989927830234206
Confusion Matrix:
 [[15924   541]
 [  348  4812]]
Matriz de confusión guardada en: outputs_hold_out/0/Positiveness/confusion_matrix_param_5839873.png
Accuracy:   0.9589
Precision:  0.8989
Recall:     0.9326
F1-score:   0.9154

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4039, Test Loss: 0.1861, F1: 0.8436, AUC: 0.9701
Epoch [10/30] Train Loss: 0.1201, Test Loss: 0.1747, F1: 0.8842, AUC: 0.9870
Epoch [20/30] Train Loss: 0.0696, Test Loss: 0.2550, F1: 0.8263, AUC: 0.9862
Mejores resultados en la época:  28
f1-score 0.9172233517281441
AUC según el mejor F1-score 0.9897191187790874
Confusion Matrix:
 [[16033   432]
 [  423  4737]]
Matriz de confusión guardada en: outputs_hold_out/0/Positiveness/confusion_matrix_param_5839873.png
Accuracy:   0.9605
Precision:  0.9164
Recall:     0.9180
F1-score:   0.9172
Tiempo total para red 6: 308.99 segundos
Saved on: outputs_hold_out/0/Positiveness

==============================
Model: Logistic Regression
Accuracy:  0.9357
Precision: 0.8287
Recall:    0.9207
F1-score:  0.8723
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15483   982]
 [  409  4751]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [21:07:48] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Positiveness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Positiveness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8050
Precision: 0.5566
Recall:    0.8983
F1-score:  0.6873
              precision    recall  f1-score   support

           0       0.96      0.78      0.86     16465
           1       0.56      0.90      0.69      5160

    accuracy                           0.80     21625
   macro avg       0.76      0.84      0.77     21625
weighted avg       0.86      0.80      0.82     21625

[[12773  3692]
 [  525  4635]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Positiveness/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Positiveness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8714
Precision: 0.7156
Recall:    0.7647
F1-score:  0.7394
              precision    recall  f1-score   support

           0       0.92      0.90      0.91     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14897  1568]
 [ 1214  3946]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Positiveness/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Positiveness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8613
Precision: 0.6817
Recall:    0.7855
F1-score:  0.7299
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.68      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14573  1892]
 [ 1107  4053]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Positiveness/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Positiveness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9192
Precision: 0.7989
Recall:    0.8837
F1-score:  0.8392
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15317  1148]
 [  600  4560]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Positiveness/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Positiveness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7871
Precision: 0.5294
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12022  4443]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Positiveness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Positiveness/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9357, 'precision': 0.8287, 'recall': 0.9207, 'f1_score': 0.8723}
XGBoost: {'accuracy': 0.9192, 'precision': 0.7989, 'recall': 0.8837, 'f1_score': 0.8392}
Decision Tree: {'accuracy': 0.8714, 'precision': 0.7156, 'recall': 0.7647, 'f1_score': 0.7394}
Random Forest: {'accuracy': 0.8613, 'precision': 0.6817, 'recall': 0.7855, 'f1_score': 0.7299}
SVM: {'accuracy': 0.805, 'precision': 0.5566, 'recall': 0.8983, 'f1_score': 0.6873}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}

##################################################
Running experiment without SPEECHINESS embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4737, Test Loss: 0.3109, F1: 0.7833, AUC: 0.9381
Epoch [10/30] Train Loss: 0.1393, Test Loss: 0.1494, F1: 0.8796, AUC: 0.9842
Epoch [20/30] Train Loss: 0.1143, Test Loss: 0.1974, F1: 0.8606, AUC: 0.9849
Mejores resultados en la época:  17
f1-score 0.8894448186279227
AUC según el mejor F1-score 0.9845237725313504
Confusion Matrix:
 [[15854   611]
 [  538  4622]]
Matriz de confusión guardada en: outputs_hold_out/0/Speechiness/confusion_matrix_param_160673.png
Accuracy:   0.9469
Precision:  0.8832
Recall:     0.8957
F1-score:   0.8894

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.5011, Test Loss: 0.4417, F1: 0.6787, AUC: 0.9280
Epoch [10/30] Train Loss: 0.1309, Test Loss: 0.1417, F1: 0.8820, AUC: 0.9826
Epoch [20/30] Train Loss: 0.1196, Test Loss: 0.1430, F1: 0.8882, AUC: 0.9851
Mejores resultados en la época:  19
f1-score 0.8913508260447036
AUC según el mejor F1-score 0.9847279994915218
Confusion Matrix:
 [[15921   544]
 [  574  4586]]
Matriz de confusión guardada en: outputs_hold_out/0/Speechiness/confusion_matrix_param_160673.png
Accuracy:   0.9483
Precision:  0.8940
Recall:     0.8888
F1-score:   0.8914

--- Iteración 3 de 3 para la red 1 ---
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Epoch [0/30] Train Loss: 0.4953, Test Loss: 0.3457, F1: 0.7694, AUC: 0.9330
Epoch [10/30] Train Loss: 0.1355, Test Loss: 0.3615, F1: 0.7618, AUC: 0.9837
Epoch [20/30] Train Loss: 0.1087, Test Loss: 0.2207, F1: 0.8439, AUC: 0.9844
Mejores resultados en la época:  28
f1-score 0.8934434384638344
AUC según el mejor F1-score 0.9851686393736302
Confusion Matrix:
 [[16064   401]
 [  670  4490]]
Matriz de confusión guardada en: outputs_hold_out/0/Speechiness/confusion_matrix_param_160673.png
Accuracy:   0.9505
Precision:  0.9180
Recall:     0.8702
F1-score:   0.8934
Tiempo total para red 1: 155.59 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4288, Test Loss: 0.1888, F1: 0.8367, AUC: 0.9681
Epoch [10/30] Train Loss: 0.1223, Test Loss: 0.2030, F1: 0.8508, AUC: 0.9855
Epoch [20/30] Train Loss: 0.0832, Test Loss: 0.1425, F1: 0.9009, AUC: 0.9868
Mejores resultados en la época:  21
f1-score 0.9046487308679533
AUC según el mejor F1-score 0.9872823842917912
Confusion Matrix:
 [[15864   601]
 [  402  4758]]
Matriz de confusión guardada en: outputs_hold_out/0/Speechiness/confusion_matrix_param_652929.png
Accuracy:   0.9536
Precision:  0.8879
Recall:     0.9221
F1-score:   0.9046

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4232, Test Loss: 0.2491, F1: 0.8249, AUC: 0.9698
Epoch [10/30] Train Loss: 0.1245, Test Loss: 0.1426, F1: 0.8855, AUC: 0.9855
Epoch [20/30] Train Loss: 0.0713, Test Loss: 0.2008, F1: 0.8676, AUC: 0.9870
Mejores resultados en la época:  17
f1-score 0.9023456668956717
AUC según el mejor F1-score 0.9869143261369548
Confusion Matrix:
 [[16033   432]
 [  563  4597]]
Matriz de confusión guardada en: outputs_hold_out/0/Speechiness/confusion_matrix_param_652929.png
Accuracy:   0.9540
Precision:  0.9141
Recall:     0.8909
F1-score:   0.9023

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4041, Test Loss: 0.4177, F1: 0.7146, AUC: 0.9715
Epoch [10/30] Train Loss: 0.1220, Test Loss: 0.1833, F1: 0.8698, AUC: 0.9842
Epoch [20/30] Train Loss: 0.0737, Test Loss: 0.3221, F1: 0.8195, AUC: 0.9862
Mejores resultados en la época:  23
f1-score 0.9067396639607325
AUC según el mejor F1-score 0.9885981539417651
Confusion Matrix:
 [[15834   631]
 [  357  4803]]
Matriz de confusión guardada en: outputs_hold_out/0/Speechiness/confusion_matrix_param_652929.png
Accuracy:   0.9543
Precision:  0.8839
Recall:     0.9308
F1-score:   0.9067
Tiempo total para red 3: 167.41 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4103, Test Loss: 0.2512, F1: 0.8113, AUC: 0.9667
Epoch [10/30] Train Loss: 0.1297, Test Loss: 0.3029, F1: 0.7919, AUC: 0.9854
Epoch [20/30] Train Loss: 0.0755, Test Loss: 0.1504, F1: 0.9008, AUC: 0.9868
Mejores resultados en la época:  16
f1-score 0.9052994069255788
AUC según el mejor F1-score 0.9877113185827584
Confusion Matrix:
 [[15903   562]
 [  428  4732]]
Matriz de confusión guardada en: outputs_hold_out/0/Speechiness/confusion_matrix_param_2742785.png
Accuracy:   0.9542
Precision:  0.8938
Recall:     0.9171
F1-score:   0.9053

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4037, Test Loss: 0.1817, F1: 0.8491, AUC: 0.9692
Epoch [10/30] Train Loss: 0.1183, Test Loss: 0.1931, F1: 0.8551, AUC: 0.9857
Epoch [20/30] Train Loss: 0.0695, Test Loss: 0.2232, F1: 0.8547, AUC: 0.9858
Mejores resultados en la época:  26
f1-score 0.9108497723823976
AUC según el mejor F1-score 0.9892745064112975
Confusion Matrix:
 [[15883   582]
 [  358  4802]]
Matriz de confusión guardada en: outputs_hold_out/0/Speechiness/confusion_matrix_param_2742785.png
Accuracy:   0.9565
Precision:  0.8919
Recall:     0.9306
F1-score:   0.9108

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4243, Test Loss: 0.3579, F1: 0.7452, AUC: 0.9634
Epoch [10/30] Train Loss: 0.1270, Test Loss: 0.2133, F1: 0.8536, AUC: 0.9864
Epoch [20/30] Train Loss: 0.0755, Test Loss: 0.1577, F1: 0.8919, AUC: 0.9879
Mejores resultados en la época:  25
f1-score 0.9119418682474424
AUC según el mejor F1-score 0.9892691038307709
Confusion Matrix:
 [[15935   530]
 [  391  4769]]
Matriz de confusión guardada en: outputs_hold_out/0/Speechiness/confusion_matrix_param_2742785.png
Accuracy:   0.9574
Precision:  0.9000
Recall:     0.9242
F1-score:   0.9119
Tiempo total para red 5: 184.59 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4307, Test Loss: 0.2653, F1: 0.7043, AUC: 0.9601
Epoch [10/30] Train Loss: 0.1171, Test Loss: 0.2559, F1: 0.8213, AUC: 0.9862
Epoch [20/30] Train Loss: 0.0781, Test Loss: 0.2015, F1: 0.8544, AUC: 0.9868
Mejores resultados en la época:  28
f1-score 0.9113826614067161
AUC según el mejor F1-score 0.9887778044571878
Confusion Matrix:
 [[15968   497]
 [  424  4736]]
Matriz de confusión guardada en: outputs_hold_out/0/Speechiness/confusion_matrix_param_5839873.png
Accuracy:   0.9574
Precision:  0.9050
Recall:     0.9178
F1-score:   0.9114

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4178, Test Loss: 0.3970, F1: 0.7489, AUC: 0.9725
Epoch [10/30] Train Loss: 0.1266, Test Loss: 0.4713, F1: 0.7328, AUC: 0.9832
Epoch [20/30] Train Loss: 0.0772, Test Loss: 0.1392, F1: 0.9049, AUC: 0.9878
Mejores resultados en la época:  27
f1-score 0.912408043158411
AUC según el mejor F1-score 0.9885362714425949
Confusion Matrix:
 [[16081   384]
 [  509  4651]]
Matriz de confusión guardada en: outputs_hold_out/0/Speechiness/confusion_matrix_param_5839873.png
Accuracy:   0.9587
Precision:  0.9237
Recall:     0.9014
F1-score:   0.9124

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4254, Test Loss: 0.3028, F1: 0.8067, AUC: 0.9707
Epoch [10/30] Train Loss: 0.1261, Test Loss: 0.1372, F1: 0.8926, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0734, Test Loss: 0.1568, F1: 0.8942, AUC: 0.9889
Mejores resultados en la época:  27
f1-score 0.9089677856244558
AUC según el mejor F1-score 0.9878394739134222
Confusion Matrix:
 [[15986   479]
 [  462  4698]]
Matriz de confusión guardada en: outputs_hold_out/0/Speechiness/confusion_matrix_param_5839873.png
Accuracy:   0.9565
Precision:  0.9075
Recall:     0.9105
F1-score:   0.9090
Tiempo total para red 6: 214.09 segundos
Saved on: outputs_hold_out/0/Speechiness

==============================
Model: Logistic Regression
Accuracy:  0.9325
Precision: 0.8196
Recall:    0.9194
F1-score:  0.8666
              precision    recall  f1-score   support

           0       0.97      0.94      0.95     16465
           1       0.82      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.93     21625

[[15421  1044]
 [  416  4744]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Speechiness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Speechiness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7822
Precision: 0.5255
Recall:    0.8955
F1-score:  0.6624
              precision    recall  f1-score   support

           0       0.96      0.75      0.84     16465
           1       0.53      0.90      0.66      5160

    accuracy                           0.78     21625
   macro avg       0.74      0.82      0.75     21625
weighted avg       0.85      0.78      0.80     21625

[[12293  4172]
 [  539  4621]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Speechiness/conf_matrix_svm.png
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [21:30:21] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Modelo guardado como: outputs_hold_out/0/Speechiness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8718
Precision: 0.7468
Recall:    0.7002
F1-score:  0.7227
              precision    recall  f1-score   support

           0       0.91      0.93      0.92     16465
           1       0.75      0.70      0.72      5160

    accuracy                           0.87     21625
   macro avg       0.83      0.81      0.82     21625
weighted avg       0.87      0.87      0.87     21625

[[15240  1225]
 [ 1547  3613]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Speechiness/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Speechiness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8442
Precision: 0.6440
Recall:    0.7758
F1-score:  0.7038
              precision    recall  f1-score   support

           0       0.92      0.87      0.89     16465
           1       0.64      0.78      0.70      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.82      0.80     21625
weighted avg       0.86      0.84      0.85     21625

[[14252  2213]
 [ 1157  4003]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Speechiness/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Speechiness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9215
Precision: 0.8089
Recall:    0.8787
F1-score:  0.8424
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.81      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.92      0.92      0.92     21625

[[15394  1071]
 [  626  4534]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Speechiness/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Speechiness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7871
Precision: 0.5294
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12022  4443]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Speechiness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Speechiness/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9325, 'precision': 0.8196, 'recall': 0.9194, 'f1_score': 0.8666}
XGBoost: {'accuracy': 0.9215, 'precision': 0.8089, 'recall': 0.8787, 'f1_score': 0.8424}
Decision Tree: {'accuracy': 0.8718, 'precision': 0.7468, 'recall': 0.7002, 'f1_score': 0.7227}
Random Forest: {'accuracy': 0.8442, 'precision': 0.644, 'recall': 0.7758, 'f1_score': 0.7038}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7822, 'precision': 0.5255, 'recall': 0.8955, 'f1_score': 0.6624}

##################################################
Running experiment without LIVENESS embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4188, Test Loss: 0.4072, F1: 0.7405, AUC: 0.9442
Epoch [10/30] Train Loss: 0.1296, Test Loss: 0.1392, F1: 0.8890, AUC: 0.9845
Epoch [20/30] Train Loss: 0.1013, Test Loss: 0.1468, F1: 0.8875, AUC: 0.9848
Mejores resultados en la época:  23
f1-score 0.8932768252745136
AUC según el mejor F1-score 0.9852088291583979
Confusion Matrix:
 [[15880   585]
 [  523  4637]]
Matriz de confusión guardada en: outputs_hold_out/0/Liveness/confusion_matrix_param_160673.png
Accuracy:   0.9488
Precision:  0.8880
Recall:     0.8986
F1-score:   0.8933

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4221, Test Loss: 0.3107, F1: 0.7804, AUC: 0.9412
Epoch [10/30] Train Loss: 0.1266, Test Loss: 0.1491, F1: 0.8747, AUC: 0.9840
Epoch [20/30] Train Loss: 0.1022, Test Loss: 0.1385, F1: 0.8919, AUC: 0.9859
Mejores resultados en la época:  28
f1-score 0.8946337641989816
AUC según el mejor F1-score 0.9858681735040502
Confusion Matrix:
 [[15981   484]
 [  592  4568]]
Matriz de confusión guardada en: outputs_hold_out/0/Liveness/confusion_matrix_param_160673.png
Accuracy:   0.9502
Precision:  0.9042
Recall:     0.8853
F1-score:   0.8946

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4258, Test Loss: 0.3153, F1: 0.7703, AUC: 0.9325
Epoch [10/30] Train Loss: 0.1280, Test Loss: 0.1608, F1: 0.8755, AUC: 0.9847
Epoch [20/30] Train Loss: 0.1089, Test Loss: 0.1436, F1: 0.8851, AUC: 0.9834
Mejores resultados en la época:  19
f1-score 0.8897489332142503
AUC según el mejor F1-score 0.9846555001565455
Confusion Matrix:
 [[16031   434]
 [  677  4483]]
Matriz de confusión guardada en: outputs_hold_out/0/Liveness/confusion_matrix_param_160673.png
Accuracy:   0.9486
Precision:  0.9117
Recall:     0.8688
F1-score:   0.8897
Tiempo total para red 1: 156.03 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4076, Test Loss: 0.5002, F1: 0.6710, AUC: 0.9632
Epoch [10/30] Train Loss: 0.1162, Test Loss: 0.1330, F1: 0.8933, AUC: 0.9851
Epoch [20/30] Train Loss: 0.0839, Test Loss: 0.1650, F1: 0.8788, AUC: 0.9872
Mejores resultados en la época:  14
f1-score 0.8995290423861853
AUC según el mejor F1-score 0.985917279312236
Confusion Matrix:
 [[16017   448]
 [  576  4584]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Matriz de confusión guardada en: outputs_hold_out/0/Liveness/confusion_matrix_param_652929.png
Accuracy:   0.9526
Precision:  0.9110
Recall:     0.8884
F1-score:   0.8995

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3940, Test Loss: 0.2177, F1: 0.8235, AUC: 0.9621
Epoch [10/30] Train Loss: 0.1158, Test Loss: 0.1628, F1: 0.8627, AUC: 0.9842
Epoch [20/30] Train Loss: 0.0730, Test Loss: 0.1755, F1: 0.8812, AUC: 0.9883
Mejores resultados en la época:  26
f1-score 0.906120880176804
AUC según el mejor F1-score 0.9875435796392159
Confusion Matrix:
 [[15933   532]
 [  445  4715]]
Matriz de confusión guardada en: outputs_hold_out/0/Liveness/confusion_matrix_param_652929.png
Accuracy:   0.9548
Precision:  0.8986
Recall:     0.9138
F1-score:   0.9061

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3784, Test Loss: 0.2335, F1: 0.8346, AUC: 0.9696
Epoch [10/30] Train Loss: 0.1199, Test Loss: 0.2099, F1: 0.8491, AUC: 0.9861
Epoch [20/30] Train Loss: 0.0769, Test Loss: 0.1628, F1: 0.8916, AUC: 0.9881
Mejores resultados en la época:  24
f1-score 0.9085318985395849
AUC según el mejor F1-score 0.9887752267553678
Confusion Matrix:
 [[15945   520]
 [  432  4728]]
Matriz de confusión guardada en: outputs_hold_out/0/Liveness/confusion_matrix_param_652929.png
Accuracy:   0.9560
Precision:  0.9009
Recall:     0.9163
F1-score:   0.9085
Tiempo total para red 3: 167.64 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3946, Test Loss: 0.2356, F1: 0.8280, AUC: 0.9703
Epoch [10/30] Train Loss: 0.1177, Test Loss: 0.1534, F1: 0.8775, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0724, Test Loss: 0.1961, F1: 0.8763, AUC: 0.9887
Mejores resultados en la época:  29
f1-score 0.9138257575757576
AUC según el mejor F1-score 0.9899140824911663
Confusion Matrix:
 [[15890   575]
 [  335  4825]]
Matriz de confusión guardada en: outputs_hold_out/0/Liveness/confusion_matrix_param_2742785.png
Accuracy:   0.9579
Precision:  0.8935
Recall:     0.9351
F1-score:   0.9138

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3974, Test Loss: 0.3158, F1: 0.7770, AUC: 0.9628
Epoch [10/30] Train Loss: 0.1180, Test Loss: 0.1970, F1: 0.8529, AUC: 0.9839
Epoch [20/30] Train Loss: 0.0754, Test Loss: 0.1580, F1: 0.9033, AUC: 0.9892
Mejores resultados en la época:  29
f1-score 0.9129371944339978
AUC según el mejor F1-score 0.9894135022139986
Confusion Matrix:
 [[15844   621]
 [  305  4855]]
Matriz de confusión guardada en: outputs_hold_out/0/Liveness/confusion_matrix_param_2742785.png
Accuracy:   0.9572
Precision:  0.8866
Recall:     0.9409
F1-score:   0.9129

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3741, Test Loss: 0.4305, F1: 0.7166, AUC: 0.9759
Epoch [10/30] Train Loss: 0.1218, Test Loss: 0.2001, F1: 0.8458, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0778, Test Loss: 0.1356, F1: 0.9003, AUC: 0.9873
Mejores resultados en la época:  27
f1-score 0.9088572558323448
AUC según el mejor F1-score 0.9890842743710526
Confusion Matrix:
 [[16106   359]
 [  563  4597]]
Matriz de confusión guardada en: outputs_hold_out/0/Liveness/confusion_matrix_param_2742785.png
Accuracy:   0.9574
Precision:  0.9276
Recall:     0.8909
F1-score:   0.9089
Tiempo total para red 5: 184.68 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3868, Test Loss: 0.4637, F1: 0.7128, AUC: 0.9609
Epoch [10/30] Train Loss: 0.1219, Test Loss: 0.1783, F1: 0.8645, AUC: 0.9863
Epoch [20/30] Train Loss: 0.0873, Test Loss: 0.1297, F1: 0.9010, AUC: 0.9858
Mejores resultados en la época:  22
f1-score 0.9067017082785808
AUC según el mejor F1-score 0.9892075273601273
Confusion Matrix:
 [[15801   664]
 [  330  4830]]
Matriz de confusión guardada en: outputs_hold_out/0/Liveness/confusion_matrix_param_5839873.png
Accuracy:   0.9540
Precision:  0.8791
Recall:     0.9360
F1-score:   0.9067

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3883, Test Loss: 0.2146, F1: 0.8266, AUC: 0.9675
Epoch [10/30] Train Loss: 0.1198, Test Loss: 0.1342, F1: 0.8906, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0766, Test Loss: 0.1416, F1: 0.9042, AUC: 0.9891
Mejores resultados en la época:  26
f1-score 0.9073113438387002
AUC según el mejor F1-score 0.9880855738152575
Confusion Matrix:
 [[16043   422]
 [  525  4635]]
Matriz de confusión guardada en: outputs_hold_out/0/Liveness/confusion_matrix_param_5839873.png
Accuracy:   0.9562
Precision:  0.9166
Recall:     0.8983
F1-score:   0.9073

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4074, Test Loss: 0.4381, F1: 0.7373, AUC: 0.9665
Epoch [10/30] Train Loss: 0.1181, Test Loss: 0.1347, F1: 0.8901, AUC: 0.9853
Epoch [20/30] Train Loss: 0.0797, Test Loss: 0.1635, F1: 0.8792, AUC: 0.9892
Mejores resultados en la época:  27
f1-score 0.9175516508978567
AUC según el mejor F1-score 0.98974971574658
Confusion Matrix:
 [[16019   446]
 [  408  4752]]
Matriz de confusión guardada en: outputs_hold_out/0/Liveness/confusion_matrix_param_5839873.png
Accuracy:   0.9605
Precision:  0.9142
Recall:     0.9209
F1-score:   0.9176
Tiempo total para red 6: 214.78 segundos
Saved on: outputs_hold_out/0/Liveness

==============================
Model: Logistic Regression
Accuracy:  0.9354
Precision: 0.8276
Recall:    0.9213
F1-score:  0.8720
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15475   990]
 [  406  4754]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Liveness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Liveness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7592
Precision: 0.4975
Recall:    0.9169
F1-score:  0.6450
              precision    recall  f1-score   support

           0       0.96      0.71      0.82     16465
           1       0.50      0.92      0.65      5160

    accuracy                           0.76     21625
   macro avg       0.73      0.81      0.73     21625
weighted avg       0.85      0.76      0.78     21625

[[11687  4778]
 [  429  4731]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Liveness/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Liveness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8727
Precision: 0.7204
Recall:    0.7624
F1-score:  0.7408
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14938  1527]
 [ 1226  3934]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Liveness/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Liveness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8602
Precision: 0.6801
Recall:    0.7820
F1-score:  0.7275
              precision    recall  f1-score   support

           0       0.93      0.88      0.91     16465
           1       0.68      0.78      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.82     21625
weighted avg       0.87      0.86      0.86     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [21:53:03] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[14567  1898]
 [ 1125  4035]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Liveness/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Liveness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9211
Precision: 0.8033
Recall:    0.8864
F1-score:  0.8428
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15345  1120]
 [  586  4574]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Liveness/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Liveness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7871
Precision: 0.5294
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12022  4443]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Liveness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Liveness/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.8276, 'recall': 0.9213, 'f1_score': 0.872}
XGBoost: {'accuracy': 0.9211, 'precision': 0.8033, 'recall': 0.8864, 'f1_score': 0.8428}
Decision Tree: {'accuracy': 0.8727, 'precision': 0.7204, 'recall': 0.7624, 'f1_score': 0.7408}
Random Forest: {'accuracy': 0.8602, 'precision': 0.6801, 'recall': 0.782, 'f1_score': 0.7275}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7592, 'precision': 0.4975, 'recall': 0.9169, 'f1_score': 0.645}

##################################################
Running experiment without ACOUSTICNESS embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4408, Test Loss: 0.3077, F1: 0.7579, AUC: 0.9273
Epoch [10/30] Train Loss: 0.1347, Test Loss: 0.1327, F1: 0.8875, AUC: 0.9846
Epoch [20/30] Train Loss: 0.1053, Test Loss: 0.1485, F1: 0.8866, AUC: 0.9854
Mejores resultados en la época:  29
f1-score 0.8938504475026465
AUC según el mejor F1-score 0.98540099741759
Confusion Matrix:
 [[15878   587]
 [  516  4644]]
Matriz de confusión guardada en: outputs_hold_out/0/Acousticness/confusion_matrix_param_160673.png
Accuracy:   0.9490
Precision:  0.8878
Recall:     0.9000
F1-score:   0.8939

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4316, Test Loss: 0.3666, F1: 0.7515, AUC: 0.9345
Epoch [10/30] Train Loss: 0.1318, Test Loss: 0.1592, F1: 0.8672, AUC: 0.9834
Epoch [20/30] Train Loss: 0.1044, Test Loss: 0.1520, F1: 0.8818, AUC: 0.9851
Mejores resultados en la época:  23
f1-score 0.8957927541877678
AUC según el mejor F1-score 0.985162289281704
Confusion Matrix:
 [[15956   509]
 [  561  4599]]
Matriz de confusión guardada en: outputs_hold_out/0/Acousticness/confusion_matrix_param_160673.png
Accuracy:   0.9505
Precision:  0.9004
Recall:     0.8913
F1-score:   0.8958

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4360, Test Loss: 0.2762, F1: 0.7670, AUC: 0.9344
Epoch [10/30] Train Loss: 0.1286, Test Loss: 0.2157, F1: 0.8439, AUC: 0.9848
Epoch [20/30] Train Loss: 0.1101, Test Loss: 0.2421, F1: 0.8302, AUC: 0.9855
Mejores resultados en la época:  29
f1-score 0.8971482889733841
AUC según el mejor F1-score 0.9858177847301183
Confusion Matrix:
 [[15824   641]
 [  441  4719]]
Matriz de confusión guardada en: outputs_hold_out/0/Acousticness/confusion_matrix_param_160673.png
Accuracy:   0.9500
Precision:  0.8804
Recall:     0.9145
F1-score:   0.8971
Tiempo total para red 1: 156.57 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3991, Test Loss: 0.1974, F1: 0.8505, AUC: 0.9720
Epoch [10/30] Train Loss: 0.1190, Test Loss: 0.1777, F1: 0.8662, AUC: 0.9853
Epoch [20/30] Train Loss: 0.0803, Test Loss: 0.1520, F1: 0.8874, AUC: 0.9865
Mejores resultados en la época:  21
f1-score 0.9024460988107899
AUC según el mejor F1-score 0.987373133520246
Confusion Matrix:
 [[15949   516]
 [  493  4667]]
Matriz de confusión guardada en: outputs_hold_out/0/Acousticness/confusion_matrix_param_652929.png
Accuracy:   0.9533
Precision:  0.9004
Recall:     0.9045
F1-score:   0.9024

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3861, Test Loss: 0.1864, F1: 0.8539, AUC: 0.9742
Epoch [10/30] Train Loss: 0.1208, Test Loss: 0.2463, F1: 0.8235, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0876, Test Loss: 0.1514, F1: 0.8866, AUC: 0.9886
Mejores resultados en la época:  23
f1-score 0.9103782735208535
AUC según el mejor F1-score 0.9882372874573031
Confusion Matrix:
 [[16008   457]
 [  467  4693]]
Matriz de confusión guardada en: outputs_hold_out/0/Acousticness/confusion_matrix_param_652929.png
Accuracy:   0.9573
Precision:  0.9113
Recall:     0.9095
F1-score:   0.9104

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3778, Test Loss: 0.2206, F1: 0.8435, AUC: 0.9750
Epoch [10/30] Train Loss: 0.1226, Test Loss: 0.1384, F1: 0.8907, AUC: 0.9861
Epoch [20/30] Train Loss: 0.0810, Test Loss: 0.1448, F1: 0.8967, AUC: 0.9873
Mejores resultados en la época:  24
f1-score 0.9005185327443825
AUC según el mejor F1-score 0.9871094134374773
Confusion Matrix:
 [[15900   565]
 [  471  4689]]
Matriz de confusión guardada en: outputs_hold_out/0/Acousticness/confusion_matrix_param_652929.png
Accuracy:   0.9521
Precision:  0.8925
Recall:     0.9087
F1-score:   0.9005
Tiempo total para red 3: 167.99 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [22:15:51] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3929, Test Loss: 0.2233, F1: 0.8357, AUC: 0.9701
Epoch [10/30] Train Loss: 0.1129, Test Loss: 0.1302, F1: 0.8945, AUC: 0.9863
Epoch [20/30] Train Loss: 0.0749, Test Loss: 0.1606, F1: 0.8898, AUC: 0.9873
Mejores resultados en la época:  27
f1-score 0.9114214654677635
AUC según el mejor F1-score 0.9892355289703083
Confusion Matrix:
 [[15935   530]
 [  396  4764]]
Matriz de confusión guardada en: outputs_hold_out/0/Acousticness/confusion_matrix_param_2742785.png
Accuracy:   0.9572
Precision:  0.8999
Recall:     0.9233
F1-score:   0.9114

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3937, Test Loss: 0.2019, F1: 0.8356, AUC: 0.9670
Epoch [10/30] Train Loss: 0.1198, Test Loss: 0.1348, F1: 0.8953, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0666, Test Loss: 0.2158, F1: 0.8679, AUC: 0.9886
Mejores resultados en la época:  25
f1-score 0.9106044839655661
AUC según el mejor F1-score 0.9897334726940161
Confusion Matrix:
 [[15867   598]
 [  347  4813]]
Matriz de confusión guardada en: outputs_hold_out/0/Acousticness/confusion_matrix_param_2742785.png
Accuracy:   0.9563
Precision:  0.8895
Recall:     0.9328
F1-score:   0.9106

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3903, Test Loss: 0.2913, F1: 0.8159, AUC: 0.9712
Epoch [10/30] Train Loss: 0.1170, Test Loss: 0.1653, F1: 0.8691, AUC: 0.9863
Epoch [20/30] Train Loss: 0.0780, Test Loss: 0.1242, F1: 0.8992, AUC: 0.9883
Mejores resultados en la época:  19
f1-score 0.9065245286743658
AUC según el mejor F1-score 0.9877834589227323
Confusion Matrix:
 [[16086   379]
 [  568  4592]]
Matriz de confusión guardada en: outputs_hold_out/0/Acousticness/confusion_matrix_param_2742785.png
Accuracy:   0.9562
Precision:  0.9238
Recall:     0.8899
F1-score:   0.9065
Tiempo total para red 5: 185.53 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3964, Test Loss: 0.2135, F1: 0.8355, AUC: 0.9639
Epoch [10/30] Train Loss: 0.1249, Test Loss: 0.1423, F1: 0.8964, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0754, Test Loss: 0.1945, F1: 0.8761, AUC: 0.9867
Mejores resultados en la época:  22
f1-score 0.903615651312531
AUC según el mejor F1-score 0.9875777783270597
Confusion Matrix:
 [[16091   374]
 [  599  4561]]
Matriz de confusión guardada en: outputs_hold_out/0/Acousticness/confusion_matrix_param_5839873.png
Accuracy:   0.9550
Precision:  0.9242
Recall:     0.8839
F1-score:   0.9036

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4079, Test Loss: 0.4175, F1: 0.7145, AUC: 0.9649
Epoch [10/30] Train Loss: 0.1281, Test Loss: 0.1463, F1: 0.8819, AUC: 0.9861
Epoch [20/30] Train Loss: 0.0794, Test Loss: 0.3345, F1: 0.7955, AUC: 0.9883
Mejores resultados en la época:  16
f1-score 0.9075019638648861
AUC según el mejor F1-score 0.9880003625261007
Confusion Matrix:
 [[16062   403]
 [  539  4621]]
Matriz de confusión guardada en: outputs_hold_out/0/Acousticness/confusion_matrix_param_5839873.png
Accuracy:   0.9564
Precision:  0.9198
Recall:     0.8955
F1-score:   0.9075

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4089, Test Loss: 0.2280, F1: 0.8210, AUC: 0.9638
Epoch [10/30] Train Loss: 0.1200, Test Loss: 0.1389, F1: 0.8903, AUC: 0.9851
Epoch [20/30] Train Loss: 0.0731, Test Loss: 0.1431, F1: 0.9119, AUC: 0.9887
Mejores resultados en la época:  20
f1-score 0.9119241192411924
AUC según el mejor F1-score 0.9886515206086671
Confusion Matrix:
 [[16004   461]
 [  449  4711]]
Matriz de confusión guardada en: outputs_hold_out/0/Acousticness/confusion_matrix_param_5839873.png
Accuracy:   0.9579
Precision:  0.9109
Recall:     0.9130
F1-score:   0.9119
Tiempo total para red 6: 215.22 segundos
Saved on: outputs_hold_out/0/Acousticness

==============================
Model: Logistic Regression
Accuracy:  0.9359
Precision: 0.8292
Recall:    0.9209
F1-score:  0.8726
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15486   979]
 [  408  4752]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Acousticness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Acousticness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8034
Precision: 0.5547
Recall:    0.8926
F1-score:  0.6842
              precision    recall  f1-score   support

           0       0.96      0.78      0.86     16465
           1       0.55      0.89      0.68      5160

    accuracy                           0.80     21625
   macro avg       0.76      0.83      0.77     21625
weighted avg       0.86      0.80      0.82     21625

[[12767  3698]
 [  554  4606]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Acousticness/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Acousticness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8743
Precision: 0.7267
Recall:    0.7585
F1-score:  0.7423
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.73      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.83     21625
weighted avg       0.88      0.87      0.88     21625

[[14993  1472]
 [ 1246  3914]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Acousticness/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Acousticness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8664
Precision: 0.6945
Recall:    0.7855
F1-score:  0.7372
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.87      0.87     21625

[[14682  1783]
 [ 1107  4053]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Acousticness/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Acousticness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9207
Precision: 0.8028
Recall:    0.8853
F1-score:  0.8420
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[15343  1122]
 [  592  4568]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Acousticness/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Acousticness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7871
Precision: 0.5294
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12022  4443]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Acousticness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Acousticness/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9359, 'precision': 0.8292, 'recall': 0.9209, 'f1_score': 0.8726}
XGBoost: {'accuracy': 0.9207, 'precision': 0.8028, 'recall': 0.8853, 'f1_score': 0.842}
Decision Tree: {'accuracy': 0.8743, 'precision': 0.7267, 'recall': 0.7585, 'f1_score': 0.7423}
Random Forest: {'accuracy': 0.8664, 'precision': 0.6945, 'recall': 0.7855, 'f1_score': 0.7372}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.8034, 'precision': 0.5547, 'recall': 0.8926, 'f1_score': 0.6842}

##################################################
Running experiment without INSTRUMENTALNESS embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4279, Test Loss: 0.2800, F1: 0.7795, AUC: 0.9380
Epoch [10/30] Train Loss: 0.1315, Test Loss: 0.1354, F1: 0.8876, AUC: 0.9841
Epoch [20/30] Train Loss: 0.1032, Test Loss: 0.1611, F1: 0.8788, AUC: 0.9851
Mejores resultados en la época:  28
f1-score 0.8923107100823853
AUC según el mejor F1-score 0.9850175319034739
Confusion Matrix:
 [[15978   487]
 [  611  4549]]
Matriz de confusión guardada en: outputs_hold_out/0/Instrumentalness/confusion_matrix_param_160673.png
Accuracy:   0.9492
Precision:  0.9033
Recall:     0.8816
F1-score:   0.8923

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4275, Test Loss: 0.3400, F1: 0.7622, AUC: 0.9395
Epoch [10/30] Train Loss: 0.1340, Test Loss: 0.2029, F1: 0.8502, AUC: 0.9847
Epoch [20/30] Train Loss: 0.1067, Test Loss: 0.1638, F1: 0.8749, AUC: 0.9851
Mejores resultados en la época:  27
f1-score 0.8925473186119873
AUC según el mejor F1-score 0.9850747121566301
Confusion Matrix:
 [[16008   457]
 [  633  4527]]
Matriz de confusión guardada en: outputs_hold_out/0/Instrumentalness/confusion_matrix_param_160673.png
Accuracy:   0.9496
Precision:  0.9083
Recall:     0.8773
F1-score:   0.8925

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4393, Test Loss: 0.3046, F1: 0.7687, AUC: 0.9321
Epoch [10/30] Train Loss: 0.1361, Test Loss: 0.1544, F1: 0.8814, AUC: 0.9846
Epoch [20/30] Train Loss: 0.1083, Test Loss: 0.1389, F1: 0.8880, AUC: 0.9846
Mejores resultados en la época:  17
f1-score 0.8917185256103399
AUC según el mejor F1-score 0.9849606812195003
Confusion Matrix:
 [[15837   628]
 [  503  4657]]
Matriz de confusión guardada en: outputs_hold_out/0/Instrumentalness/confusion_matrix_param_160673.png
Accuracy:   0.9477
Precision:  0.8812
Recall:     0.9025
F1-score:   0.8917
Tiempo total para red 1: 155.59 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4095, Test Loss: 0.1939, F1: 0.8311, AUC: 0.9677
Epoch [10/30] Train Loss: 0.1216, Test Loss: 0.2121, F1: 0.8441, AUC: 0.9849
Epoch [20/30] Train Loss: 0.0879, Test Loss: 0.1584, F1: 0.8869, AUC: 0.9860
Mejores resultados en la época:  28
f1-score 0.9067074921060185
AUC según el mejor F1-score 0.9881617454925528
Confusion Matrix:
 [[15912   553]
 [  422  4738]]
Matriz de confusión guardada en: outputs_hold_out/0/Instrumentalness/confusion_matrix_param_652929.png
Accuracy:   0.9549
Precision:  0.8955
Recall:     0.9182
F1-score:   0.9067

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3931, Test Loss: 0.1880, F1: 0.8381, AUC: 0.9708
Epoch [10/30] Train Loss: 0.1198, Test Loss: 0.2464, F1: 0.8211, AUC: 0.9857
Epoch [20/30] Train Loss: 0.0859, Test Loss: 0.1307, F1: 0.9058, AUC: 0.9883
Mejores resultados en la época:  25
f1-score 0.9068952084144917
AUC según el mejor F1-score 0.9883420198353567
Confusion Matrix:
 [[16013   452]
 [  504  4656]]
Matriz de confusión guardada en: outputs_hold_out/0/Instrumentalness/confusion_matrix_param_652929.png
Accuracy:   0.9558
Precision:  0.9115
Recall:     0.9023
F1-score:   0.9069

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3998, Test Loss: 0.2028, F1: 0.8160, AUC: 0.9668
Epoch [10/30] Train Loss: 0.1272, Test Loss: 0.1723, F1: 0.8711, AUC: 0.9846
Epoch [20/30] Train Loss: 0.0839, Test Loss: 0.1273, F1: 0.8999, AUC: 0.9873
Mejores resultados en la época:  24
f1-score 0.9012818060072699
AUC según el mejor F1-score 0.9872438246974439
Confusion Matrix:
 [[15882   583]
 [  449  4711]]
Matriz de confusión guardada en: outputs_hold_out/0/Instrumentalness/confusion_matrix_param_652929.png
Accuracy:   0.9523
Precision:  0.8899
Recall:     0.9130
F1-score:   0.9013
Tiempo total para red 3: 167.57 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4037, Test Loss: 0.1997, F1: 0.8293, AUC: 0.9673
Epoch [10/30] Train Loss: 0.1222, Test Loss: 0.1403, F1: 0.8889, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0709, Test Loss: 0.1537, F1: 0.8922, AUC: 0.9879
Mejores resultados en la época:  23
f1-score 0.8985105005088352
AUC según el mejor F1-score 0.9885460996664289
Confusion Matrix:
 [[15672   793]
 [  304  4856]]
Matriz de confusión guardada en: outputs_hold_out/0/Instrumentalness/confusion_matrix_param_2742785.png
Accuracy:   0.9493
Precision:  0.8596
Recall:     0.9411
F1-score:   0.8985

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3748, Test Loss: 0.1939, F1: 0.8437, AUC: 0.9681
Epoch [10/30] Train Loss: 0.1205, Test Loss: 0.1821, F1: 0.8684, AUC: 0.9864
Epoch [20/30] Train Loss: 0.0722, Test Loss: 0.1557, F1: 0.8906, AUC: 0.9889
Mejores resultados en la época:  24
f1-score 0.910985753896166
AUC según el mejor F1-score 0.9890635350532138
Confusion Matrix:
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [22:38:35] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
 [[15930   535]
 [  396  4764]]
Matriz de confusión guardada en: outputs_hold_out/0/Instrumentalness/confusion_matrix_param_2742785.png
Accuracy:   0.9569
Precision:  0.8990
Recall:     0.9233
F1-score:   0.9110

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4084, Test Loss: 0.2502, F1: 0.8179, AUC: 0.9697
Epoch [10/30] Train Loss: 0.1228, Test Loss: 0.1379, F1: 0.8949, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0739, Test Loss: 0.2370, F1: 0.8589, AUC: 0.9890
Mejores resultados en la época:  27
f1-score 0.9098992294013041
AUC según el mejor F1-score 0.988430126625188
Confusion Matrix:
 [[16108   357]
 [  555  4605]]
Matriz de confusión guardada en: outputs_hold_out/0/Instrumentalness/confusion_matrix_param_2742785.png
Accuracy:   0.9578
Precision:  0.9281
Recall:     0.8924
F1-score:   0.9099
Tiempo total para red 5: 184.91 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4133, Test Loss: 0.4883, F1: 0.6707, AUC: 0.9706
Epoch [10/30] Train Loss: 0.1200, Test Loss: 0.3214, F1: 0.8088, AUC: 0.9846
Epoch [20/30] Train Loss: 0.0720, Test Loss: 0.1273, F1: 0.9017, AUC: 0.9883
Mejores resultados en la época:  28
f1-score 0.9098069349726251
AUC según el mejor F1-score 0.9891894245957481
Confusion Matrix:
 [[15950   515]
 [  424  4736]]
Matriz de confusión guardada en: outputs_hold_out/0/Instrumentalness/confusion_matrix_param_5839873.png
Accuracy:   0.9566
Precision:  0.9019
Recall:     0.9178
F1-score:   0.9098

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4013, Test Loss: 0.2027, F1: 0.8407, AUC: 0.9669
Epoch [10/30] Train Loss: 0.1254, Test Loss: 0.1310, F1: 0.8960, AUC: 0.9866
Epoch [20/30] Train Loss: 0.0766, Test Loss: 0.1470, F1: 0.8902, AUC: 0.9873
Mejores resultados en la época:  26
f1-score 0.9095293209876543
AUC según el mejor F1-score 0.9887898631581674
Confusion Matrix:
 [[15972   493]
 [  445  4715]]
Matriz de confusión guardada en: outputs_hold_out/0/Instrumentalness/confusion_matrix_param_5839873.png
Accuracy:   0.9566
Precision:  0.9053
Recall:     0.9138
F1-score:   0.9095

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4174, Test Loss: 0.3959, F1: 0.7314, AUC: 0.9638
Epoch [10/30] Train Loss: 0.1196, Test Loss: 0.1460, F1: 0.8870, AUC: 0.9862
Epoch [20/30] Train Loss: 0.0679, Test Loss: 0.1498, F1: 0.8995, AUC: 0.9877
Mejores resultados en la época:  28
f1-score 0.9034408391334183
AUC según el mejor F1-score 0.9871806180363798
Confusion Matrix:
 [[16032   433]
 [  552  4608]]
Matriz de confusión guardada en: outputs_hold_out/0/Instrumentalness/confusion_matrix_param_5839873.png
Accuracy:   0.9545
Precision:  0.9141
Recall:     0.8930
F1-score:   0.9034
Tiempo total para red 6: 214.72 segundos
Saved on: outputs_hold_out/0/Instrumentalness

==============================
Model: Logistic Regression
Accuracy:  0.9347
Precision: 0.8259
Recall:    0.9202
F1-score:  0.8705
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.94     21625

[[15464  1001]
 [  412  4748]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Instrumentalness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Instrumentalness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7654
Precision: 0.5046
Recall:    0.9256
F1-score:  0.6531
              precision    recall  f1-score   support

           0       0.97      0.72      0.82     16465
           1       0.50      0.93      0.65      5160

    accuracy                           0.77     21625
   macro avg       0.74      0.82      0.74     21625
weighted avg       0.86      0.77      0.78     21625

[[11776  4689]
 [  384  4776]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Instrumentalness/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Instrumentalness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8720
Precision: 0.7176
Recall:    0.7641
F1-score:  0.7401
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14913  1552]
 [ 1217  3943]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Instrumentalness/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Instrumentalness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8617
Precision: 0.6844
Recall:    0.7800
F1-score:  0.7291
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.68      0.78      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.83      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14609  1856]
 [ 1135  4025]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Instrumentalness/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Instrumentalness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9194
Precision: 0.7991
Recall:    0.8843
F1-score:  0.8396
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15318  1147]
 [  597  4563]]
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Instrumentalness/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Instrumentalness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7871
Precision: 0.5294
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12022  4443]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Instrumentalness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Instrumentalness/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9347, 'precision': 0.8259, 'recall': 0.9202, 'f1_score': 0.8705}
XGBoost: {'accuracy': 0.9194, 'precision': 0.7991, 'recall': 0.8843, 'f1_score': 0.8396}
Decision Tree: {'accuracy': 0.872, 'precision': 0.7176, 'recall': 0.7641, 'f1_score': 0.7401}
Random Forest: {'accuracy': 0.8617, 'precision': 0.6844, 'recall': 0.78, 'f1_score': 0.7291}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7654, 'precision': 0.5046, 'recall': 0.9256, 'f1_score': 0.6531}

##################################################
Running experiment without GOOD FOR PARTY embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4355, Test Loss: 0.3318, F1: 0.7636, AUC: 0.9312
Epoch [10/30] Train Loss: 0.1307, Test Loss: 0.2394, F1: 0.8274, AUC: 0.9846
Epoch [20/30] Train Loss: 0.0998, Test Loss: 0.2054, F1: 0.8535, AUC: 0.9858
Mejores resultados en la época:  27
f1-score 0.8983278877570632
AUC según el mejor F1-score 0.9863826663088487
Confusion Matrix:
 [[15893   572]
 [  486  4674]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Party/confusion_matrix_param_160673.png
Accuracy:   0.9511
Precision:  0.8910
Recall:     0.9058
F1-score:   0.8983

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4395, Test Loss: 0.3615, F1: 0.7468, AUC: 0.9300
Epoch [10/30] Train Loss: 0.1347, Test Loss: 0.1795, F1: 0.8642, AUC: 0.9847
Epoch [20/30] Train Loss: 0.1081, Test Loss: 0.1480, F1: 0.8828, AUC: 0.9842
Mejores resultados en la época:  28
f1-score 0.8952904387377997
AUC según el mejor F1-score 0.9856567313328484
Confusion Matrix:
 [[15796   669]
 [  436  4724]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Party/confusion_matrix_param_160673.png
Accuracy:   0.9489
Precision:  0.8760
Recall:     0.9155
F1-score:   0.8953

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4244, Test Loss: 0.3056, F1: 0.7873, AUC: 0.9441
Epoch [10/30] Train Loss: 0.1338, Test Loss: 0.2476, F1: 0.8237, AUC: 0.9845
Epoch [20/30] Train Loss: 0.1004, Test Loss: 0.3012, F1: 0.8019, AUC: 0.9848
Mejores resultados en la época:  18
f1-score 0.8929468599033816
AUC según el mejor F1-score 0.985095663340372
Confusion Matrix:
 [[15896   569]
 [  539  4621]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Party/confusion_matrix_param_160673.png
Accuracy:   0.9488
Precision:  0.8904
Recall:     0.8955
F1-score:   0.8929
Tiempo total para red 1: 156.61 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3963, Test Loss: 0.2138, F1: 0.8004, AUC: 0.9623
Epoch [10/30] Train Loss: 0.1193, Test Loss: 0.2115, F1: 0.8386, AUC: 0.9849
Epoch [20/30] Train Loss: 0.0746, Test Loss: 0.1714, F1: 0.8861, AUC: 0.9869
Mejores resultados en la época:  17
f1-score 0.9061895551257253
AUC según el mejor F1-score 0.9879490497814251
Confusion Matrix:
 [[15970   495]
 [  475  4685]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Party/confusion_matrix_param_652929.png
Accuracy:   0.9551
Precision:  0.9044
Recall:     0.9079
F1-score:   0.9062

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3905, Test Loss: 0.1928, F1: 0.8399, AUC: 0.9700
Epoch [10/30] Train Loss: 0.1248, Test Loss: 0.1377, F1: 0.8904, AUC: 0.9843
Epoch [20/30] Train Loss: 0.0797, Test Loss: 0.1318, F1: 0.9079, AUC: 0.9878
Mejores resultados en la época:  20
f1-score 0.9079163050814772
AUC según el mejor F1-score 0.9878248316254586
Confusion Matrix:
 [[15962   503]
 [  452  4708]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Party/confusion_matrix_param_652929.png
Accuracy:   0.9558
Precision:  0.9035
Recall:     0.9124
F1-score:   0.9079

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3954, Test Loss: 0.2243, F1: 0.8356, AUC: 0.9665
Epoch [10/30] Train Loss: 0.1279, Test Loss: 0.1841, F1: 0.8602, AUC: 0.9856
Epoch [20/30] Train Loss: 0.0843, Test Loss: 0.1452, F1: 0.8985, AUC: 0.9865
Mejores resultados en la época:  23
f1-score 0.9066744845108962
AUC según el mejor F1-score 0.9880440069021202
Confusion Matrix:
 [[16031   434]
 [  521  4639]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Party/confusion_matrix_param_652929.png
Accuracy:   0.9558
Precision:  0.9144
Recall:     0.8990
F1-score:   0.9067
Tiempo total para red 3: 168.57 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3954, Test Loss: 0.2947, F1: 0.7998, AUC: 0.9667
Epoch [10/30] Train Loss: 0.1155, Test Loss: 0.1579, F1: 0.8845, AUC: 0.9871
Epoch [20/30] Train Loss: 0.0748, Test Loss: 0.1306, F1: 0.9053, AUC: 0.9883
Mejores resultados en la época:  25
f1-score 0.9108987353991698
AUC según el mejor F1-score 0.9876083811797166
Confusion Matrix:
 [[15984   481]
 [  442  4718]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Party/confusion_matrix_param_2742785.png
Accuracy:   0.9573
Precision:  0.9075
Recall:     0.9143
F1-score:   0.9109

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3879, Test Loss: 0.1889, F1: 0.8428, AUC: 0.9696
Epoch [10/30] Train Loss: 0.1292, Test Loss: 0.1588, F1: 0.8838, AUC: 0.9850
Epoch [20/30] Train Loss: 0.0707, Test Loss: 0.2010, F1: 0.8704, AUC: 0.9865
Mejores resultados en la época:  24
f1-score 0.9071753986332574
AUC según el mejor F1-score 0.9887827009136128
Confusion Matrix:
 [[15868   597]
 [  381  4779]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Party/confusion_matrix_param_2742785.png
Accuracy:   0.9548
Precision:  0.8890
Recall:     0.9262
F1-score:   0.9072

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3854, Test Loss: 0.2960, F1: 0.7902, AUC: 0.9709
Epoch [10/30] Train Loss: 0.1175, Test Loss: 0.1325, F1: 0.8955, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0806, Test Loss: 0.1454, F1: 0.9013, AUC: 0.9892
Mejores resultados en la época:  25
f1-score 0.9130988537278338
AUC según el mejor F1-score 0.9889713263040935
Confusion Matrix:
 [[16078   387]
 [  500  4660]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Party/confusion_matrix_param_2742785.png
Accuracy:   0.9590
Precision:  0.9233
Recall:     0.9031
F1-score:   0.9131
Tiempo total para red 5: 185.75 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [23:01:13] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [0/30] Train Loss: 0.4124, Test Loss: 0.2066, F1: 0.8238, AUC: 0.9623
Epoch [10/30] Train Loss: 0.1282, Test Loss: 0.1411, F1: 0.8873, AUC: 0.9855
Epoch [20/30] Train Loss: 0.0827, Test Loss: 0.1365, F1: 0.9038, AUC: 0.9878
Mejores resultados en la época:  25
f1-score 0.912222330952148
AUC según el mejor F1-score 0.9881164356151291
Confusion Matrix:
 [[16067   398]
 [  499  4661]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Party/confusion_matrix_param_5839873.png
Accuracy:   0.9585
Precision:  0.9213
Recall:     0.9033
F1-score:   0.9122

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4025, Test Loss: 0.2412, F1: 0.8188, AUC: 0.9687
Epoch [10/30] Train Loss: 0.1142, Test Loss: 0.1333, F1: 0.8869, AUC: 0.9850
Epoch [20/30] Train Loss: 0.0727, Test Loss: 0.1717, F1: 0.9029, AUC: 0.9882
Mejores resultados en la época:  28
f1-score 0.9068346338253267
AUC según el mejor F1-score 0.9894703940941202
Confusion Matrix:
 [[15811   654]
 [  337  4823]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Party/confusion_matrix_param_5839873.png
Accuracy:   0.9542
Precision:  0.8806
Recall:     0.9347
F1-score:   0.9068

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4012, Test Loss: 0.1863, F1: 0.8427, AUC: 0.9688
Epoch [10/30] Train Loss: 0.1184, Test Loss: 0.2134, F1: 0.8483, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0687, Test Loss: 0.1919, F1: 0.8828, AUC: 0.9885
Mejores resultados en la época:  27
f1-score 0.9177178342712983
AUC según el mejor F1-score 0.9897299298253048
Confusion Matrix:
 [[16048   417]
 [  431  4729]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Party/confusion_matrix_param_5839873.png
Accuracy:   0.9608
Precision:  0.9190
Recall:     0.9165
F1-score:   0.9177
Tiempo total para red 6: 215.19 segundos
Saved on: outputs_hold_out/0/Good for Party

==============================
Model: Logistic Regression
Accuracy:  0.9356
Precision: 0.8281
Recall:    0.9213
F1-score:  0.8722
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15478   987]
 [  406  4754]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Party/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Good for Party/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7924
Precision: 0.5373
Recall:    0.9343
F1-score:  0.6823
              precision    recall  f1-score   support

           0       0.97      0.75      0.85     16465
           1       0.54      0.93      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.84      0.76     21625
weighted avg       0.87      0.79      0.81     21625

[[12314  4151]
 [  339  4821]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Party/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Good for Party/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8724
Precision: 0.7192
Recall:    0.7634
F1-score:  0.7406
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14927  1538]
 [ 1221  3939]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Good for Party/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Good for Party/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8615
Precision: 0.6834
Recall:    0.7818
F1-score:  0.7293
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.68      0.78      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.83      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14596  1869]
 [ 1126  4034]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Party/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Good for Party/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9188
Precision: 0.7976
Recall:    0.8843
F1-score:  0.8387
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15307  1158]
 [  597  4563]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Party/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Good for Party/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7871
Precision: 0.5294
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12022  4443]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Good for Party/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Good for Party/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9356, 'precision': 0.8281, 'recall': 0.9213, 'f1_score': 0.8722}
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
XGBoost: {'accuracy': 0.9188, 'precision': 0.7976, 'recall': 0.8843, 'f1_score': 0.8387}
Decision Tree: {'accuracy': 0.8724, 'precision': 0.7192, 'recall': 0.7634, 'f1_score': 0.7406}
Random Forest: {'accuracy': 0.8615, 'precision': 0.6834, 'recall': 0.7818, 'f1_score': 0.7293}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7924, 'precision': 0.5373, 'recall': 0.9343, 'f1_score': 0.6823}

##################################################
Running experiment without GOOD FOR WORK/STUDY embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4346, Test Loss: 0.3915, F1: 0.7416, AUC: 0.9354
Epoch [10/30] Train Loss: 0.1296, Test Loss: 0.1827, F1: 0.8598, AUC: 0.9839
Epoch [20/30] Train Loss: 0.1169, Test Loss: 0.1595, F1: 0.8725, AUC: 0.9849
Mejores resultados en la época:  28
f1-score 0.89937106918239
AUC según el mejor F1-score 0.986516948095208
Confusion Matrix:
 [[16025   440]
 [  584  4576]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Work/Study/confusion_matrix_param_160673.png
Accuracy:   0.9526
Precision:  0.9123
Recall:     0.8868
F1-score:   0.8994

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4257, Test Loss: 0.3216, F1: 0.7790, AUC: 0.9421
Epoch [10/30] Train Loss: 0.1338, Test Loss: 0.1463, F1: 0.8858, AUC: 0.9850
Epoch [20/30] Train Loss: 0.1091, Test Loss: 0.1487, F1: 0.8816, AUC: 0.9849
Mejores resultados en la época:  13
f1-score 0.8918996829057365
AUC según el mejor F1-score 0.9852229712074239
Confusion Matrix:
 [[15859   606]
 [  519  4641]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Work/Study/confusion_matrix_param_160673.png
Accuracy:   0.9480
Precision:  0.8845
Recall:     0.8994
F1-score:   0.8919

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4234, Test Loss: 0.3484, F1: 0.7674, AUC: 0.9410
Epoch [10/30] Train Loss: 0.1319, Test Loss: 0.2024, F1: 0.8498, AUC: 0.9852
Epoch [20/30] Train Loss: 0.1016, Test Loss: 0.1352, F1: 0.8933, AUC: 0.9853
Mejores resultados en la época:  28
f1-score 0.8979120561916675
AUC según el mejor F1-score 0.9857904304879742
Confusion Matrix:
 [[15898   567]
 [  494  4666]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Work/Study/confusion_matrix_param_160673.png
Accuracy:   0.9509
Precision:  0.8916
Recall:     0.9043
F1-score:   0.8979
Tiempo total para red 1: 156.41 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4186, Test Loss: 0.2077, F1: 0.8150, AUC: 0.9617
Epoch [10/30] Train Loss: 0.1232, Test Loss: 0.1735, F1: 0.8700, AUC: 0.9867
Epoch [20/30] Train Loss: 0.0738, Test Loss: 0.1916, F1: 0.8718, AUC: 0.9881
Mejores resultados en la época:  22
f1-score 0.9071213640922768
AUC según el mejor F1-score 0.9883436205999572
Confusion Matrix:
 [[16177   288]
 [  638  4522]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Work/Study/confusion_matrix_param_652929.png
Accuracy:   0.9572
Precision:  0.9401
Recall:     0.8764
F1-score:   0.9071

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3955, Test Loss: 0.2440, F1: 0.8238, AUC: 0.9707
Epoch [10/30] Train Loss: 0.1248, Test Loss: 0.3192, F1: 0.7835, AUC: 0.9854
Epoch [20/30] Train Loss: 0.0823, Test Loss: 0.1994, F1: 0.8681, AUC: 0.9874
Mejores resultados en la época:  27
f1-score 0.9115307198313045
AUC según el mejor F1-score 0.9888021808063616
Confusion Matrix:
 [[15947   518]
 [  405  4755]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Work/Study/confusion_matrix_param_652929.png
Accuracy:   0.9573
Precision:  0.9018
Recall:     0.9215
F1-score:   0.9115

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3935, Test Loss: 0.3493, F1: 0.7678, AUC: 0.9682
Epoch [10/30] Train Loss: 0.1228, Test Loss: 0.1394, F1: 0.8901, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0798, Test Loss: 0.1387, F1: 0.8974, AUC: 0.9882
Mejores resultados en la época:  29
f1-score 0.9136484940907358
AUC según el mejor F1-score 0.9891961572233325
Confusion Matrix:
 [[15926   539]
 [  367  4793]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Work/Study/confusion_matrix_param_652929.png
Accuracy:   0.9581
Precision:  0.8989
Recall:     0.9289
F1-score:   0.9136
Tiempo total para red 3: 168.42 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4001, Test Loss: 0.2090, F1: 0.8346, AUC: 0.9658
Epoch [10/30] Train Loss: 0.1219, Test Loss: 0.2036, F1: 0.8394, AUC: 0.9840
Epoch [20/30] Train Loss: 0.0741, Test Loss: 0.1601, F1: 0.8951, AUC: 0.9878
Mejores resultados en la época:  26
f1-score 0.9145914396887159
AUC según el mejor F1-score 0.9893778440054897
Confusion Matrix:
 [[16046   419]
 [  459  4701]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Work/Study/confusion_matrix_param_2742785.png
Accuracy:   0.9594
Precision:  0.9182
Recall:     0.9110
F1-score:   0.9146

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4054, Test Loss: 0.3044, F1: 0.8028, AUC: 0.9600
Epoch [10/30] Train Loss: 0.1248, Test Loss: 0.1606, F1: 0.8726, AUC: 0.9870
Epoch [20/30] Train Loss: 0.0806, Test Loss: 0.1590, F1: 0.8954, AUC: 0.9869
Mejores resultados en la época:  29
f1-score 0.9056785370548605
AUC según el mejor F1-score 0.9860904561472892
Confusion Matrix:
 [[15940   525]
 [  455  4705]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Work/Study/confusion_matrix_param_2742785.png
Accuracy:   0.9547
Precision:  0.8996
Recall:     0.9118
F1-score:   0.9057

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3964, Test Loss: 0.2162, F1: 0.8083, AUC: 0.9615
Epoch [10/30] Train Loss: 0.1206, Test Loss: 0.1440, F1: 0.8956, AUC: 0.9870
Epoch [20/30] Train Loss: 0.0714, Test Loss: 0.1501, F1: 0.9041, AUC: 0.9882
Mejores resultados en la época:  28
f1-score 0.9082854180412857
AUC según el mejor F1-score 0.9884519311577059
Confusion Matrix:
 [[15834   631]
 [  342  4818]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Work/Study/confusion_matrix_param_2742785.png
Accuracy:   0.9550
Precision:  0.8842
Recall:     0.9337
F1-score:   0.9083
Tiempo total para red 5: 185.51 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4002, Test Loss: 0.2241, F1: 0.7859, AUC: 0.9570
Epoch [10/30] Train Loss: 0.1213, Test Loss: 0.1273, F1: 0.8863, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0820, Test Loss: 0.1482, F1: 0.8957, AUC: 0.9884
Mejores resultados en la época:  21
f1-score 0.9106719367588932
AUC según el mejor F1-score 0.9892659964641936
Confusion Matrix:
 [[16113   352]
 [  552  4608]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Work/Study/confusion_matrix_param_5839873.png
Accuracy:   0.9582
Precision:  0.9290
Recall:     0.8930
F1-score:   0.9107

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4196, Test Loss: 0.5536, F1: 0.6405, AUC: 0.9656
Epoch [10/30] Train Loss: 0.1203, Test Loss: 0.1400, F1: 0.8877, AUC: 0.9870
Epoch [20/30] Train Loss: 0.0735, Test Loss: 0.1384, F1: 0.9027, AUC: 0.9884
Mejores resultados en la época:  26
f1-score 0.9124831698403539
AUC según el mejor F1-score 0.9890574968749779
Confusion Matrix:
 [[15971   494]
 [  416  4744]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [23:23:53] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Matriz de confusión guardada en: outputs_hold_out/0/Good for Work/Study/confusion_matrix_param_5839873.png
Accuracy:   0.9579
Precision:  0.9057
Recall:     0.9194
F1-score:   0.9125

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4075, Test Loss: 0.2458, F1: 0.8254, AUC: 0.9620
Epoch [10/30] Train Loss: 0.1313, Test Loss: 0.3335, F1: 0.7659, AUC: 0.9862
Epoch [20/30] Train Loss: 0.0730, Test Loss: 0.2469, F1: 0.8220, AUC: 0.9874
Mejores resultados en la época:  24
f1-score 0.9021739130434783
AUC según el mejor F1-score 0.9894505610915331
Confusion Matrix:
 [[15666   799]
 [  263  4897]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Work/Study/confusion_matrix_param_5839873.png
Accuracy:   0.9509
Precision:  0.8597
Recall:     0.9490
F1-score:   0.9022
Tiempo total para red 6: 214.82 segundos
Saved on: outputs_hold_out/0/Good for Work/Study

==============================
Model: Logistic Regression
Accuracy:  0.9354
Precision: 0.8280
Recall:    0.9207
F1-score:  0.8719
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15478   987]
 [  409  4751]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Work/Study/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Good for Work/Study/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7954
Precision: 0.5430
Recall:    0.9004
F1-score:  0.6775
              precision    recall  f1-score   support

           0       0.96      0.76      0.85     16465
           1       0.54      0.90      0.68      5160

    accuracy                           0.80     21625
   macro avg       0.75      0.83      0.76     21625
weighted avg       0.86      0.80      0.81     21625

[[12555  3910]
 [  514  4646]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Work/Study/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Good for Work/Study/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8723
Precision: 0.7188
Recall:    0.7634
F1-score:  0.7404
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14924  1541]
 [ 1221  3939]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Good for Work/Study/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Good for Work/Study/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8641
Precision: 0.6904
Recall:    0.7802
F1-score:  0.7326
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.78      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14660  1805]
 [ 1134  4026]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Work/Study/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Good for Work/Study/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9195
Precision: 0.8000
Recall:    0.8837
F1-score:  0.8398
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15325  1140]
 [  600  4560]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Work/Study/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Good for Work/Study/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7870
Precision: 0.5293
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12021  4444]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Good for Work/Study/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Good for Work/Study/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.828, 'recall': 0.9207, 'f1_score': 0.8719}
XGBoost: {'accuracy': 0.9195, 'precision': 0.8, 'recall': 0.8837, 'f1_score': 0.8398}
Decision Tree: {'accuracy': 0.8723, 'precision': 0.7188, 'recall': 0.7634, 'f1_score': 0.7404}
Random Forest: {'accuracy': 0.8641, 'precision': 0.6904, 'recall': 0.7802, 'f1_score': 0.7326}
Naive Bayes: {'accuracy': 0.787, 'precision': 0.5293, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7954, 'precision': 0.543, 'recall': 0.9004, 'f1_score': 0.6775}

##################################################
Running experiment without GOOD FOR RELAXATION/MEDITATION embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4301, Test Loss: 0.4398, F1: 0.7144, AUC: 0.9340
Epoch [10/30] Train Loss: 0.1294, Test Loss: 0.1889, F1: 0.8575, AUC: 0.9842
Epoch [20/30] Train Loss: 0.1083, Test Loss: 0.1561, F1: 0.8791, AUC: 0.9852
Mejores resultados en la época:  27
f1-score 0.8924021203385102
AUC según el mejor F1-score 0.9863585135959058
Confusion Matrix:
 [[15670   795]
 [  362  4798]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Relaxation/Meditation/confusion_matrix_param_160673.png
Accuracy:   0.9465
Precision:  0.8579
Recall:     0.9298
F1-score:   0.8924

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4304, Test Loss: 0.3808, F1: 0.7386, AUC: 0.9362
Epoch [10/30] Train Loss: 0.1350, Test Loss: 0.1421, F1: 0.8794, AUC: 0.9839
Epoch [20/30] Train Loss: 0.1040, Test Loss: 0.1743, F1: 0.8692, AUC: 0.9857
Mejores resultados en la época:  18
f1-score 0.89380096276648
AUC según el mejor F1-score 0.9857263351671504
Confusion Matrix:
 [[15995   470]
 [  611  4549]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Relaxation/Meditation/confusion_matrix_param_160673.png
Accuracy:   0.9500
Precision:  0.9064
Recall:     0.8816
F1-score:   0.8938

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4479, Test Loss: 0.3149, F1: 0.7470, AUC: 0.9204
Epoch [10/30] Train Loss: 0.1367, Test Loss: 0.1490, F1: 0.8820, AUC: 0.9847
Epoch [20/30] Train Loss: 0.1089, Test Loss: 0.3258, F1: 0.7871, AUC: 0.9844
Mejores resultados en la época:  29
f1-score 0.8926596043873629
AUC según el mejor F1-score 0.9860318399141237
Confusion Matrix:
 [[15719   746]
 [  399  4761]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Relaxation/Meditation/confusion_matrix_param_160673.png
Accuracy:   0.9471
Precision:  0.8645
Recall:     0.9227
F1-score:   0.8927
Tiempo total para red 1: 156.57 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4006, Test Loss: 0.3609, F1: 0.7569, AUC: 0.9650
Epoch [10/30] Train Loss: 0.1259, Test Loss: 0.2677, F1: 0.8151, AUC: 0.9846
Epoch [20/30] Train Loss: 0.0784, Test Loss: 0.2976, F1: 0.8272, AUC: 0.9883
Mejores resultados en la época:  21
f1-score 0.911265810373566
AUC según el mejor F1-score 0.9886501317099696
Confusion Matrix:
 [[16073   392]
 [  513  4647]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Relaxation/Meditation/confusion_matrix_param_652929.png
Accuracy:   0.9582
Precision:  0.9222
Recall:     0.9006
F1-score:   0.9113

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3952, Test Loss: 0.2237, F1: 0.8334, AUC: 0.9708
Epoch [10/30] Train Loss: 0.1156, Test Loss: 0.1949, F1: 0.8582, AUC: 0.9850
Epoch [20/30] Train Loss: 0.0770, Test Loss: 0.2376, F1: 0.8548, AUC: 0.9878
Mejores resultados en la época:  26
f1-score 0.9112641040351884
AUC según el mejor F1-score 0.9888315948558958
Confusion Matrix:
 [[15932   533]
 [  395  4765]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Relaxation/Meditation/confusion_matrix_param_652929.png
Accuracy:   0.9571
Precision:  0.8994
Recall:     0.9234
F1-score:   0.9113

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3891, Test Loss: 0.2301, F1: 0.8075, AUC: 0.9566
Epoch [10/30] Train Loss: 0.1289, Test Loss: 0.1343, F1: 0.8947, AUC: 0.9855
Epoch [20/30] Train Loss: 0.0770, Test Loss: 0.2229, F1: 0.8585, AUC: 0.9871
Mejores resultados en la época:  25
f1-score 0.9050069265782703
AUC según el mejor F1-score 0.9867582045070941
Confusion Matrix:
 [[16092   373]
 [  587  4573]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Relaxation/Meditation/confusion_matrix_param_652929.png
Accuracy:   0.9556
Precision:  0.9246
Recall:     0.8862
F1-score:   0.9050
Tiempo total para red 3: 168.91 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3969, Test Loss: 0.2007, F1: 0.8424, AUC: 0.9698
Epoch [10/30] Train Loss: 0.1222, Test Loss: 0.1338, F1: 0.8955, AUC: 0.9857
Epoch [20/30] Train Loss: 0.0757, Test Loss: 0.1641, F1: 0.8938, AUC: 0.9879
Mejores resultados en la época:  26
f1-score 0.9116743471582182
AUC según el mejor F1-score 0.9892538436005904
Confusion Matrix:
 [[15957   508]
 [  412  4748]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Relaxation/Meditation/confusion_matrix_param_2742785.png
Accuracy:   0.9575
Precision:  0.9033
Recall:     0.9202
F1-score:   0.9117

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4023, Test Loss: 0.2153, F1: 0.8141, AUC: 0.9591
Epoch [10/30] Train Loss: 0.1195, Test Loss: 0.3093, F1: 0.7741, AUC: 0.9867
Epoch [20/30] Train Loss: 0.0692, Test Loss: 0.1531, F1: 0.8900, AUC: 0.9888
Mejores resultados en la época:  28
f1-score 0.9145450957173871
AUC según el mejor F1-score 0.9893176270077237
Confusion Matrix:
 [[16125   340]
 [  526  4634]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Relaxation/Meditation/confusion_matrix_param_2742785.png
Accuracy:   0.9600
Precision:  0.9316
Recall:     0.8981
F1-score:   0.9145

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4036, Test Loss: 0.2121, F1: 0.8183, AUC: 0.9627
Epoch [10/30] Train Loss: 0.1194, Test Loss: 0.1784, F1: 0.8691, AUC: 0.9852
Epoch [20/30] Train Loss: 0.0709, Test Loss: 0.1320, F1: 0.9023, AUC: 0.9873
Mejores resultados en la época:  27
f1-score 0.9129519801033097
AUC según el mejor F1-score 0.9893263900168786
Confusion Matrix:
 [[15943   522]
 [  388  4772]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Relaxation/Meditation/confusion_matrix_param_2742785.png
Accuracy:   0.9579
Precision:  0.9014
Recall:     0.9248
F1-score:   0.9130
Tiempo total para red 5: 187.15 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3932, Test Loss: 0.1996, F1: 0.8240, AUC: 0.9680
Epoch [10/30] Train Loss: 0.1252, Test Loss: 0.1644, F1: 0.8777, AUC: 0.9856
Epoch [20/30] Train Loss: 0.0745, Test Loss: 0.2432, F1: 0.8498, AUC: 0.9866
Mejores resultados en la época:  23
f1-score 0.9119081779053084
AUC según el mejor F1-score 0.989268268137487
Confusion Matrix:
 [[15937   528]
 [  393  4767]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Relaxation/Meditation/confusion_matrix_param_5839873.png
Accuracy:   0.9574
Precision:  0.9003
Recall:     0.9238
F1-score:   0.9119

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4093, Test Loss: 0.2215, F1: 0.8259, AUC: 0.9641
Epoch [10/30] Train Loss: 0.1228, Test Loss: 0.1436, F1: 0.8854, AUC: 0.9869
Epoch [20/30] Train Loss: 0.0790, Test Loss: 0.1578, F1: 0.8892, AUC: 0.9871
Mejores resultados en la época:  25
f1-score 0.9109370467072817
AUC según el mejor F1-score 0.9885519318639255
Confusion Matrix:
 [[15994   471]
 [  450  4710]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Relaxation/Meditation/confusion_matrix_param_5839873.png
Accuracy:   0.9574
Precision:  0.9091
Recall:     0.9128
F1-score:   0.9109

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4016, Test Loss: 0.3711, F1: 0.7583, AUC: 0.9677
Epoch [10/30] Train Loss: 0.1163, Test Loss: 0.3581, F1: 0.7878, AUC: 0.9851
Epoch [20/30] Train Loss: 0.0775, Test Loss: 0.1603, F1: 0.9028, AUC: 0.9881
Mejores resultados en la época:  26
f1-score 0.9155547083492185
AUC según el mejor F1-score 0.9896465252814873
Confusion Matrix:
 [[15936   529]
 [  357  4803]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Relaxation/Meditation/confusion_matrix_param_5839873.png
Accuracy:   0.9590
Precision:  0.9008
Recall:     0.9308
F1-score:   0.9156
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [23:46:32] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Tiempo total para red 6: 216.45 segundos
Saved on: outputs_hold_out/0/Good for Relaxation/Meditation

==============================
Model: Logistic Regression
Accuracy:  0.9354
Precision: 0.8280
Recall:    0.9207
F1-score:  0.8719
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15478   987]
 [  409  4751]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Relaxation/Meditation/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Good for Relaxation/Meditation/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8334
Precision: 0.6040
Recall:    0.8767
F1-score:  0.7153
              precision    recall  f1-score   support

           0       0.96      0.82      0.88     16465
           1       0.60      0.88      0.72      5160

    accuracy                           0.83     21625
   macro avg       0.78      0.85      0.80     21625
weighted avg       0.87      0.83      0.84     21625

[[13499  2966]
 [  636  4524]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Relaxation/Meditation/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Good for Relaxation/Meditation/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8728
Precision: 0.7202
Recall:    0.7638
F1-score:  0.7413
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14934  1531]
 [ 1219  3941]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Good for Relaxation/Meditation/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Good for Relaxation/Meditation/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8623
Precision: 0.6855
Recall:    0.7818
F1-score:  0.7305
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.78      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.83      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14614  1851]
 [ 1126  4034]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Relaxation/Meditation/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Good for Relaxation/Meditation/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9212
Precision: 0.8029
Recall:    0.8880
F1-score:  0.8433
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15340  1125]
 [  578  4582]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Relaxation/Meditation/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Good for Relaxation/Meditation/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7870
Precision: 0.5293
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12021  4444]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Good for Relaxation/Meditation/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Good for Relaxation/Meditation/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.828, 'recall': 0.9207, 'f1_score': 0.8719}
XGBoost: {'accuracy': 0.9212, 'precision': 0.8029, 'recall': 0.888, 'f1_score': 0.8433}
Decision Tree: {'accuracy': 0.8728, 'precision': 0.7202, 'recall': 0.7638, 'f1_score': 0.7413}
Random Forest: {'accuracy': 0.8623, 'precision': 0.6855, 'recall': 0.7818, 'f1_score': 0.7305}
SVM: {'accuracy': 0.8334, 'precision': 0.604, 'recall': 0.8767, 'f1_score': 0.7153}
Naive Bayes: {'accuracy': 0.787, 'precision': 0.5293, 'recall': 0.9686, 'f1_score': 0.6846}

##################################################
Running experiment without GOOD FOR EXERCISE embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4075, Test Loss: 0.3182, F1: 0.7910, AUC: 0.9519
Epoch [10/30] Train Loss: 0.1264, Test Loss: 0.1822, F1: 0.8585, AUC: 0.9837
Epoch [20/30] Train Loss: 0.1015, Test Loss: 0.1374, F1: 0.8942, AUC: 0.9855
Mejores resultados en la época:  25
f1-score 0.8950845152516029
AUC según el mejor F1-score 0.9858005411996789
Confusion Matrix:
 [[15938   527]
 [  553  4607]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Exercise/confusion_matrix_param_160673.png
Accuracy:   0.9501
Precision:  0.8974
Recall:     0.8928
F1-score:   0.8951

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4637, Test Loss: 0.4016, F1: 0.7218, AUC: 0.9116
Epoch [10/30] Train Loss: 0.1349, Test Loss: 0.1425, F1: 0.8777, AUC: 0.9822
Epoch [20/30] Train Loss: 0.1126, Test Loss: 0.1532, F1: 0.8835, AUC: 0.9854
Mejores resultados en la época:  27
f1-score 0.8952032363706415
AUC según el mejor F1-score 0.985475974406599
Confusion Matrix:
 [[15890   575]
 [  513  4647]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Exercise/confusion_matrix_param_160673.png
Accuracy:   0.9497
Precision:  0.8899
Recall:     0.9006
F1-score:   0.8952

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4244, Test Loss: 0.3152, F1: 0.7829, AUC: 0.9449
Epoch [10/30] Train Loss: 0.1294, Test Loss: 0.1434, F1: 0.8862, AUC: 0.9851
Epoch [20/30] Train Loss: 0.1008, Test Loss: 0.1483, F1: 0.8836, AUC: 0.9857
Mejores resultados en la época:  22
f1-score 0.8947568389057751
AUC según el mejor F1-score 0.986262391212744
Confusion Matrix:
 [[15807   658]
 [  450  4710]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Exercise/confusion_matrix_param_160673.png
Accuracy:   0.9488
Precision:  0.8774
Recall:     0.9128
F1-score:   0.8948
Tiempo total para red 1: 157.72 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4004, Test Loss: 0.2175, F1: 0.8292, AUC: 0.9653
Epoch [10/30] Train Loss: 0.1239, Test Loss: 0.1791, F1: 0.8667, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0775, Test Loss: 0.1328, F1: 0.8975, AUC: 0.9871
Mejores resultados en la época:  25
f1-score 0.9059047619047619
AUC según el mejor F1-score 0.9879363966788842
Confusion Matrix:
 [[15881   584]
 [  404  4756]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Exercise/confusion_matrix_param_652929.png
Accuracy:   0.9543
Precision:  0.8906
Recall:     0.9217
F1-score:   0.9059

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3984, Test Loss: 0.1983, F1: 0.8482, AUC: 0.9692
Epoch [10/30] Train Loss: 0.1166, Test Loss: 0.1610, F1: 0.8762, AUC: 0.9851
Epoch [20/30] Train Loss: 0.0829, Test Loss: 0.1771, F1: 0.8796, AUC: 0.9879
Mejores resultados en la época:  18
f1-score 0.9008830241095347
AUC según el mejor F1-score 0.9876106999343216
Confusion Matrix:
 [[16086   379]
 [  620  4540]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Exercise/confusion_matrix_param_652929.png
Accuracy:   0.9538
Precision:  0.9230
Recall:     0.8798
F1-score:   0.9009

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3845, Test Loss: 0.2380, F1: 0.8302, AUC: 0.9720
Epoch [10/30] Train Loss: 0.1186, Test Loss: 0.1404, F1: 0.8903, AUC: 0.9856
Epoch [20/30] Train Loss: 0.0785, Test Loss: 0.2061, F1: 0.8479, AUC: 0.9853
Mejores resultados en la época:  25
f1-score 0.9081967213114754
AUC según el mejor F1-score 0.9883430968203635
Confusion Matrix:
 [[15964   501]
 [  451  4709]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Exercise/confusion_matrix_param_652929.png
Accuracy:   0.9560
Precision:  0.9038
Recall:     0.9126
F1-score:   0.9082
Tiempo total para red 3: 168.92 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3894, Test Loss: 0.2575, F1: 0.8241, AUC: 0.9624
Epoch [10/30] Train Loss: 0.1236, Test Loss: 0.1319, F1: 0.8979, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0751, Test Loss: 0.2569, F1: 0.8196, AUC: 0.9860
Mejores resultados en la época:  24
f1-score 0.9117875145405195
AUC según el mejor F1-score 0.9892289258163312
Confusion Matrix:
 [[16012   453]
 [  457  4703]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Exercise/confusion_matrix_param_2742785.png
Accuracy:   0.9579
Precision:  0.9121
Recall:     0.9114
F1-score:   0.9118

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3862, Test Loss: 0.1955, F1: 0.8401, AUC: 0.9687
Epoch [10/30] Train Loss: 0.1165, Test Loss: 0.3372, F1: 0.7684, AUC: 0.9843
Epoch [20/30] Train Loss: 0.0784, Test Loss: 0.1416, F1: 0.9128, AUC: 0.9890
Mejores resultados en la época:  20
f1-score 0.9127529776314516
AUC según el mejor F1-score 0.9889802305571838
Confusion Matrix:
 [[16011   454]
 [  447  4713]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Exercise/confusion_matrix_param_2742785.png
Accuracy:   0.9583
Precision:  0.9121
Recall:     0.9134
F1-score:   0.9128

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4024, Test Loss: 0.3999, F1: 0.7671, AUC: 0.9642
Epoch [10/30] Train Loss: 0.1232, Test Loss: 0.3601, F1: 0.7521, AUC: 0.9836
Epoch [20/30] Train Loss: 0.0701, Test Loss: 0.1468, F1: 0.9031, AUC: 0.9884
Mejores resultados en la época:  26
f1-score 0.9114528608272041
AUC según el mejor F1-score 0.9887823713444304
Confusion Matrix:
 [[15927   538]
 [  389  4771]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Exercise/confusion_matrix_param_2742785.png
Accuracy:   0.9571
Precision:  0.8987
Recall:     0.9246
F1-score:   0.9115
Tiempo total para red 5: 186.32 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3957, Test Loss: 0.1990, F1: 0.8167, AUC: 0.9654
Epoch [10/30] Train Loss: 0.1231, Test Loss: 0.1402, F1: 0.8839, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0782, Test Loss: 0.1601, F1: 0.9028, AUC: 0.9878
Mejores resultados en la época:  24
f1-score 0.9031588873173032
AUC según el mejor F1-score 0.9882385056862455
Confusion Matrix:
 [[15809   656]
 [  371  4789]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Exercise/confusion_matrix_param_5839873.png
Accuracy:   0.9525
Precision:  0.8795
Recall:     0.9281
F1-score:   0.9032

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4076, Test Loss: 0.4138, F1: 0.7479, AUC: 0.9660
Epoch [10/30] Train Loss: 0.1183, Test Loss: 0.2003, F1: 0.8476, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0764, Test Loss: 0.2682, F1: 0.8375, AUC: 0.9883
Mejores resultados en la época:  25
f1-score 0.9169325983947394
AUC según el mejor F1-score 0.9891884888546765
Confusion Matrix:
 [[16025   440]
 [  419  4741]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Exercise/confusion_matrix_param_5839873.png
Accuracy:   0.9603
Precision:  0.9151
Recall:     0.9188
F1-score:   0.9169

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3930, Test Loss: 0.2346, F1: 0.8317, AUC: 0.9664
Epoch [10/30] Train Loss: 0.1231, Test Loss: 0.1907, F1: 0.8505, AUC: 0.9873
Epoch [20/30] Train Loss: 0.0720, Test Loss: 0.2445, F1: 0.8391, AUC: 0.9891
Mejores resultados en la época:  24
f1-score 0.908988974023547
AUC según el mejor F1-score 0.9896360084934687
Confusion Matrix:
 [[15787   678]
 [  296  4864]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Exercise/confusion_matrix_param_5839873.png
Accuracy:   0.9550
Precision:  0.8777
Recall:     0.9426
F1-score:   0.9090
Tiempo total para red 6: 215.75 segundos
Saved on: outputs_hold_out/0/Good for Exercise

==============================
Model: Logistic Regression
Accuracy:  0.9354
Precision: 0.8278
Recall:    0.9205
F1-score:  0.8717
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15477   988]
 [  410  4750]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [00:09:26] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Exercise/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Good for Exercise/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7280
Precision: 0.4649
Recall:    0.9287
F1-score:  0.6196
              precision    recall  f1-score   support

           0       0.97      0.67      0.79     16465
           1       0.46      0.93      0.62      5160

    accuracy                           0.73     21625
   macro avg       0.72      0.80      0.70     21625
weighted avg       0.85      0.73      0.75     21625

[[10950  5515]
 [  368  4792]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Exercise/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Good for Exercise/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8725
Precision: 0.7197
Recall:    0.7624
F1-score:  0.7404
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14933  1532]
 [ 1226  3934]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Good for Exercise/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Good for Exercise/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8646
Precision: 0.6908
Recall:    0.7829
F1-score:  0.7340
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.78      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14657  1808]
 [ 1120  4040]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Exercise/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Good for Exercise/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9207
Precision: 0.8027
Recall:    0.8855
F1-score:  0.8421
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15342  1123]
 [  591  4569]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Exercise/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Good for Exercise/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7867
Precision: 0.5289
Recall:    0.9686
F1-score:  0.6842
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12014  4451]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Good for Exercise/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Good for Exercise/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.8278, 'recall': 0.9205, 'f1_score': 0.8717}
XGBoost: {'accuracy': 0.9207, 'precision': 0.8027, 'recall': 0.8855, 'f1_score': 0.8421}
Decision Tree: {'accuracy': 0.8725, 'precision': 0.7197, 'recall': 0.7624, 'f1_score': 0.7404}
Random Forest: {'accuracy': 0.8646, 'precision': 0.6908, 'recall': 0.7829, 'f1_score': 0.734}
Naive Bayes: {'accuracy': 0.7867, 'precision': 0.5289, 'recall': 0.9686, 'f1_score': 0.6842}
SVM: {'accuracy': 0.728, 'precision': 0.4649, 'recall': 0.9287, 'f1_score': 0.6196}

##################################################
Running experiment without GOOD FOR RUNNING embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4208, Test Loss: 0.3190, F1: 0.7749, AUC: 0.9354
Epoch [10/30] Train Loss: 0.1255, Test Loss: 0.1320, F1: 0.8878, AUC: 0.9848
Epoch [20/30] Train Loss: 0.1096, Test Loss: 0.2108, F1: 0.8481, AUC: 0.9851
Mejores resultados en la época:  28
f1-score 0.8995038427862633
AUC según el mejor F1-score 0.9862437705539352
Confusion Matrix:
 [[15969   496]
 [  537  4623]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Running/confusion_matrix_param_160673.png
Accuracy:   0.9522
Precision:  0.9031
Recall:     0.8959
F1-score:   0.8995

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4387, Test Loss: 0.3313, F1: 0.7670, AUC: 0.9280
Epoch [10/30] Train Loss: 0.1328, Test Loss: 0.1468, F1: 0.8731, AUC: 0.9844
Epoch [20/30] Train Loss: 0.1039, Test Loss: 0.1566, F1: 0.8829, AUC: 0.9858
Mejores resultados en la época:  23
f1-score 0.8950807071483474
AUC según el mejor F1-score 0.9857251228233723
Confusion Matrix:
 [[15875   590]
 [  502  4658]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Running/confusion_matrix_param_160673.png
Accuracy:   0.9495
Precision:  0.8876
Recall:     0.9027
F1-score:   0.8951

/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4350, Test Loss: 0.4091, F1: 0.7311, AUC: 0.9324
Epoch [10/30] Train Loss: 0.1266, Test Loss: 0.1354, F1: 0.8881, AUC: 0.9845
Epoch [20/30] Train Loss: 0.1052, Test Loss: 0.1358, F1: 0.8911, AUC: 0.9852
Mejores resultados en la época:  22
f1-score 0.8967459932005828
AUC según el mejor F1-score 0.9859272075838577
Confusion Matrix:
 [[15946   519]
 [  544  4616]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Running/confusion_matrix_param_160673.png
Accuracy:   0.9508
Precision:  0.8989
Recall:     0.8946
F1-score:   0.8967
Tiempo total para red 1: 157.62 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4142, Test Loss: 0.2034, F1: 0.8305, AUC: 0.9637
Epoch [10/30] Train Loss: 0.1159, Test Loss: 0.1274, F1: 0.8919, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0746, Test Loss: 0.1408, F1: 0.8806, AUC: 0.9864
Mejores resultados en la época:  29
f1-score 0.9099600039020583
AUC según el mejor F1-score 0.9889412413458664
Confusion Matrix:
 [[16038   427]
 [  496  4664]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Running/confusion_matrix_param_652929.png
Accuracy:   0.9573
Precision:  0.9161
Recall:     0.9039
F1-score:   0.9100

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4094, Test Loss: 0.3874, F1: 0.7424, AUC: 0.9659
Epoch [10/30] Train Loss: 0.1203, Test Loss: 0.1777, F1: 0.8631, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0697, Test Loss: 0.2053, F1: 0.8656, AUC: 0.9861
Mejores resultados en la época:  24
f1-score 0.911565958280286
AUC según el mejor F1-score 0.9887753209179915
Confusion Matrix:
 [[16068   397]
 [  506  4654]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Running/confusion_matrix_param_652929.png
Accuracy:   0.9582
Precision:  0.9214
Recall:     0.9019
F1-score:   0.9116

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3921, Test Loss: 0.2154, F1: 0.8459, AUC: 0.9695
Epoch [10/30] Train Loss: 0.1185, Test Loss: 0.1397, F1: 0.8898, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0781, Test Loss: 0.1703, F1: 0.8815, AUC: 0.9878
Mejores resultados en la época:  23
f1-score 0.9034863108134725
AUC según el mejor F1-score 0.9875472519815348
Confusion Matrix:
 [[16058   407]
 [  573  4587]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Running/confusion_matrix_param_652929.png
Accuracy:   0.9547
Precision:  0.9185
Recall:     0.8890
F1-score:   0.9035
Tiempo total para red 3: 169.28 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4027, Test Loss: 0.2798, F1: 0.8154, AUC: 0.9670
Epoch [10/30] Train Loss: 0.1214, Test Loss: 0.1613, F1: 0.8726, AUC: 0.9863
Epoch [20/30] Train Loss: 0.0698, Test Loss: 0.2273, F1: 0.8504, AUC: 0.9878
Mejores resultados en la época:  29
f1-score 0.9081662591687042
AUC según el mejor F1-score 0.987701584521548
Confusion Matrix:
 [[16043   422]
 [  517  4643]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Running/confusion_matrix_param_2742785.png
Accuracy:   0.9566
Precision:  0.9167
Recall:     0.8998
F1-score:   0.9082

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3864, Test Loss: 0.1975, F1: 0.8319, AUC: 0.9666
Epoch [10/30] Train Loss: 0.1208, Test Loss: 0.1246, F1: 0.8931, AUC: 0.9866
Epoch [20/30] Train Loss: 0.0751, Test Loss: 0.1985, F1: 0.8677, AUC: 0.9881
Mejores resultados en la época:  28
f1-score 0.9146095475938911
AUC según el mejor F1-score 0.9895212419108422
Confusion Matrix:
 [[15975   490]
 [  399  4761]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Running/confusion_matrix_param_2742785.png
Accuracy:   0.9589
Precision:  0.9067
Recall:     0.9227
F1-score:   0.9146

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4038, Test Loss: 0.1971, F1: 0.8284, AUC: 0.9637
Epoch [10/30] Train Loss: 0.1193, Test Loss: 0.1474, F1: 0.8855, AUC: 0.9856
Epoch [20/30] Train Loss: 0.0675, Test Loss: 0.2138, F1: 0.8406, AUC: 0.9886
Mejores resultados en la época:  21
f1-score 0.9136506127101739
AUC según el mejor F1-score 0.9894785215055661
Confusion Matrix:
 [[15907   558]
 [  351  4809]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Running/confusion_matrix_param_2742785.png
Accuracy:   0.9580
Precision:  0.8960
Recall:     0.9320
F1-score:   0.9137
Tiempo total para red 5: 186.77 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3890, Test Loss: 0.2137, F1: 0.8327, AUC: 0.9675
Epoch [10/30] Train Loss: 0.1250, Test Loss: 0.2637, F1: 0.8201, AUC: 0.9851
Epoch [20/30] Train Loss: 0.0812, Test Loss: 0.4934, F1: 0.7835, AUC: 0.9871
Mejores resultados en la época:  19
f1-score 0.9072084391482711
AUC según el mejor F1-score 0.9880784645371788
Confusion Matrix:
 [[16031   434]
 [  516  4644]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Running/confusion_matrix_param_5839873.png
Accuracy:   0.9561
Precision:  0.9145
Recall:     0.9000
F1-score:   0.9072

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3938, Test Loss: 0.1872, F1: 0.8440, AUC: 0.9706
Epoch [10/30] Train Loss: 0.1223, Test Loss: 0.2122, F1: 0.8554, AUC: 0.9864
Epoch [20/30] Train Loss: 0.0744, Test Loss: 0.1521, F1: 0.8987, AUC: 0.9890
Mejores resultados en la época:  25
f1-score 0.9140236912495223
AUC según el mejor F1-score 0.9894844949469982
Confusion Matrix:
 [[15941   524]
 [  376  4784]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Running/confusion_matrix_param_5839873.png
Accuracy:   0.9584
Precision:  0.9013
Recall:     0.9271
F1-score:   0.9140

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4039, Test Loss: 0.2136, F1: 0.8297, AUC: 0.9664
Epoch [10/30] Train Loss: 0.1258, Test Loss: 0.2601, F1: 0.8252, AUC: 0.9853
Epoch [20/30] Train Loss: 0.0752, Test Loss: 0.1376, F1: 0.9014, AUC: 0.9883
Mejores resultados en la época:  27
f1-score 0.9176169878096736
AUC según el mejor F1-score 0.9899061139791477
Confusion Matrix:
 [[16120   345]
 [  493  4667]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Running/confusion_matrix_param_5839873.png
Accuracy:   0.9612
Precision:  0.9312
Recall:     0.9045
F1-score:   0.9176
Tiempo total para red 6: 216.29 segundos
Saved on: outputs_hold_out/0/Good for Running

==============================
Model: Logistic Regression
Accuracy:  0.9356
Precision: 0.8289
Recall:    0.9202
F1-score:  0.8722
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15485   980]
 [  412  4748]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Running/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Good for Running/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7621
Precision: 0.5009
Recall:    0.8742
F1-score:  0.6369
              precision    recall  f1-score   support

           0       0.95      0.73      0.82     16465
           1       0.50      0.87      0.64      5160

    accuracy                           0.76     21625
   macro avg       0.72      0.80      0.73     21625
weighted avg       0.84      0.76      0.78     21625

[[11970  4495]
 [  649  4511]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [00:32:20] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Running/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Good for Running/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8739
Precision: 0.7240
Recall:    0.7620
F1-score:  0.7425
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14966  1499]
 [ 1228  3932]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Good for Running/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Good for Running/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8630
Precision: 0.6869
Recall:    0.7824
F1-score:  0.7315
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.78      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14625  1840]
 [ 1123  4037]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Running/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Good for Running/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9207
Precision: 0.8014
Recall:    0.8876
F1-score:  0.8423
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15330  1135]
 [  580  4580]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Running/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Good for Running/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7871
Precision: 0.5294
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12022  4443]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Good for Running/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Good for Running/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9356, 'precision': 0.8289, 'recall': 0.9202, 'f1_score': 0.8722}
XGBoost: {'accuracy': 0.9207, 'precision': 0.8014, 'recall': 0.8876, 'f1_score': 0.8423}
Decision Tree: {'accuracy': 0.8739, 'precision': 0.724, 'recall': 0.762, 'f1_score': 0.7425}
Random Forest: {'accuracy': 0.863, 'precision': 0.6869, 'recall': 0.7824, 'f1_score': 0.7315}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7621, 'precision': 0.5009, 'recall': 0.8742, 'f1_score': 0.6369}

##################################################
Running experiment without GOOD FOR YOGA/STRETCHING embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4250, Test Loss: 0.2949, F1: 0.7817, AUC: 0.9429
Epoch [10/30] Train Loss: 0.1289, Test Loss: 0.2109, F1: 0.8396, AUC: 0.9838
Epoch [20/30] Train Loss: 0.1031, Test Loss: 0.2180, F1: 0.8463, AUC: 0.9861
Mejores resultados en la época:  29
f1-score 0.8977047322187589
AUC según el mejor F1-score 0.9860186512616614
Confusion Matrix:
 [[15790   675]
 [  408  4752]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Yoga/Stretching/confusion_matrix_param_160673.png
Accuracy:   0.9499
Precision:  0.8756
Recall:     0.9209
F1-score:   0.8977

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4317, Test Loss: 0.3995, F1: 0.7393, AUC: 0.9342
Epoch [10/30] Train Loss: 0.1298, Test Loss: 0.1342, F1: 0.8878, AUC: 0.9843
Epoch [20/30] Train Loss: 0.1056, Test Loss: 0.1404, F1: 0.8895, AUC: 0.9849
Mejores resultados en la época:  29
f1-score 0.9025312772766948
AUC según el mejor F1-score 0.9864596148277882
Confusion Matrix:
 [[15967   498]
 [  507  4653]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Yoga/Stretching/confusion_matrix_param_160673.png
Accuracy:   0.9535
Precision:  0.9033
Recall:     0.9017
F1-score:   0.9025

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4302, Test Loss: 0.2934, F1: 0.7676, AUC: 0.9326
Epoch [10/30] Train Loss: 0.1300, Test Loss: 0.1406, F1: 0.8890, AUC: 0.9847
Epoch [20/30] Train Loss: 0.1040, Test Loss: 0.1712, F1: 0.8718, AUC: 0.9856
Mejores resultados en la época:  22
f1-score 0.8958069851152836
AUC según el mejor F1-score 0.9856351092404138
Confusion Matrix:
 [[15950   515]
 [  556  4604]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Yoga/Stretching/confusion_matrix_param_160673.png
Accuracy:   0.9505
Precision:  0.8994
Recall:     0.8922
F1-score:   0.8958
Tiempo total para red 1: 157.74 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3991, Test Loss: 0.2299, F1: 0.8223, AUC: 0.9631
Epoch [10/30] Train Loss: 0.1203, Test Loss: 0.1384, F1: 0.8879, AUC: 0.9841
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Epoch [20/30] Train Loss: 0.0838, Test Loss: 0.1262, F1: 0.8918, AUC: 0.9873
Mejores resultados en la época:  23
f1-score 0.9091804570527975
AUC según el mejor F1-score 0.9885747839556306
Confusion Matrix:
 [[16088   377]
 [  545  4615]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Yoga/Stretching/confusion_matrix_param_652929.png
Accuracy:   0.9574
Precision:  0.9245
Recall:     0.8944
F1-score:   0.9092

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4184, Test Loss: 0.2286, F1: 0.7796, AUC: 0.9592
Epoch [10/30] Train Loss: 0.1232, Test Loss: 0.2681, F1: 0.8167, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0781, Test Loss: 0.1583, F1: 0.8950, AUC: 0.9879
Mejores resultados en la época:  29
f1-score 0.9081862561021404
AUC según el mejor F1-score 0.9895247141575857
Confusion Matrix:
 [[15810   655]
 [  323  4837]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Yoga/Stretching/confusion_matrix_param_652929.png
Accuracy:   0.9548
Precision:  0.8807
Recall:     0.9374
F1-score:   0.9082

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3949, Test Loss: 0.2434, F1: 0.8284, AUC: 0.9709
Epoch [10/30] Train Loss: 0.1187, Test Loss: 0.1370, F1: 0.8849, AUC: 0.9845
Epoch [20/30] Train Loss: 0.0750, Test Loss: 0.2009, F1: 0.8719, AUC: 0.9872
Mejores resultados en la época:  22
f1-score 0.9068240220805177
AUC según el mejor F1-score 0.9884975706043123
Confusion Matrix:
 [[15882   583]
 [  396  4764]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Yoga/Stretching/confusion_matrix_param_652929.png
Accuracy:   0.9547
Precision:  0.8910
Recall:     0.9233
F1-score:   0.9068
Tiempo total para red 3: 169.87 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4099, Test Loss: 0.2795, F1: 0.8167, AUC: 0.9697
Epoch [10/30] Train Loss: 0.1192, Test Loss: 0.1663, F1: 0.8767, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0749, Test Loss: 0.1217, F1: 0.9069, AUC: 0.9887
Mejores resultados en la época:  26
f1-score 0.9136332585146872
AUC según el mejor F1-score 0.9893431980451838
Confusion Matrix:
 [[16059   406]
 [  479  4681]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Yoga/Stretching/confusion_matrix_param_2742785.png
Accuracy:   0.9591
Precision:  0.9202
Recall:     0.9072
F1-score:   0.9136

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3938, Test Loss: 0.2720, F1: 0.8152, AUC: 0.9634
Epoch [10/30] Train Loss: 0.1213, Test Loss: 0.1857, F1: 0.8693, AUC: 0.9871
Epoch [20/30] Train Loss: 0.0765, Test Loss: 0.1315, F1: 0.9046, AUC: 0.9879
Mejores resultados en la época:  22
f1-score 0.9098204050439435
AUC según el mejor F1-score 0.9885430746921472
Confusion Matrix:
 [[15919   546]
 [  398  4762]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Yoga/Stretching/confusion_matrix_param_2742785.png
Accuracy:   0.9563
Precision:  0.8971
Recall:     0.9229
F1-score:   0.9098

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4014, Test Loss: 0.2091, F1: 0.8021, AUC: 0.9620
Epoch [10/30] Train Loss: 0.1160, Test Loss: 0.3103, F1: 0.7926, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0803, Test Loss: 0.1971, F1: 0.8640, AUC: 0.9879
Mejores resultados en la época:  27
f1-score 0.9081081081081082
AUC según el mejor F1-score 0.9887815179956544
Confusion Matrix:
 [[16171   294]
 [  624  4536]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Yoga/Stretching/confusion_matrix_param_2742785.png
Accuracy:   0.9575
Precision:  0.9391
Recall:     0.8791
F1-score:   0.9081
Tiempo total para red 5: 187.44 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3991, Test Loss: 0.4536, F1: 0.6879, AUC: 0.9659
Epoch [10/30] Train Loss: 0.1222, Test Loss: 0.1320, F1: 0.8945, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0770, Test Loss: 0.2420, F1: 0.8536, AUC: 0.9876
Mejores resultados en la época:  22
f1-score 0.9005988023952096
AUC según el mejor F1-score 0.987194636496962
Confusion Matrix:
 [[16117   348]
 [  648  4512]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Yoga/Stretching/confusion_matrix_param_5839873.png
Accuracy:   0.9539
Precision:  0.9284
Recall:     0.8744
F1-score:   0.9006

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3885, Test Loss: 0.3711, F1: 0.7815, AUC: 0.9700
Epoch [10/30] Train Loss: 0.1189, Test Loss: 0.1513, F1: 0.8832, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0814, Test Loss: 0.1711, F1: 0.8992, AUC: 0.9876
Mejores resultados en la época:  22
f1-score 0.9114167548331249
AUC según el mejor F1-score 0.9890056897765287
Confusion Matrix:
 [[15966   499]
 [  422  4738]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Yoga/Stretching/confusion_matrix_param_5839873.png
Accuracy:   0.9574
Precision:  0.9047
Recall:     0.9182
F1-score:   0.9114

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4048, Test Loss: 0.2001, F1: 0.8351, AUC: 0.9653
Epoch [10/30] Train Loss: 0.1204, Test Loss: 0.1306, F1: 0.8994, AUC: 0.9862
Epoch [20/30] Train Loss: 0.0752, Test Loss: 0.1945, F1: 0.8565, AUC: 0.9881
Mejores resultados en la época:  28
f1-score 0.9076693968726731
AUC según el mejor F1-score 0.9901217699277537
Confusion Matrix:
 [[15757   708]
 [  284  4876]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Yoga/Stretching/confusion_matrix_param_5839873.png
Accuracy:   0.9541
Precision:  0.8732
Recall:     0.9450
F1-score:   0.9077
Tiempo total para red 6: 216.95 segundos
Saved on: outputs_hold_out/0/Good for Yoga/Stretching

==============================
Model: Logistic Regression
Accuracy:  0.9354
Precision: 0.8277
Recall:    0.9207
F1-score:  0.8717
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15476   989]
 [  409  4751]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Yoga/Stretching/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Good for Yoga/Stretching/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7742
Precision: 0.5154
Recall:    0.8986
F1-score:  0.6551
              precision    recall  f1-score   support

           0       0.96      0.74      0.83     16465
           1       0.52      0.90      0.66      5160

    accuracy                           0.77     21625
   macro avg       0.74      0.82      0.74     21625
weighted avg       0.85      0.77      0.79     21625

[[12105  4360]
 [  523  4637]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Yoga/Stretching/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Good for Yoga/Stretching/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8732
Precision: 0.7215
Recall:    0.7630
F1-score:  0.7416
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14945  1520]
 [ 1223  3937]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [00:55:04] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Good for Yoga/Stretching/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Good for Yoga/Stretching/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8605
Precision: 0.6821
Recall:    0.7781
F1-score:  0.7270
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.68      0.78      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14594  1871]
 [ 1145  4015]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Yoga/Stretching/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Good for Yoga/Stretching/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9207
Precision: 0.8028
Recall:    0.8853
F1-score:  0.8420
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15343  1122]
 [  592  4568]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Yoga/Stretching/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Good for Yoga/Stretching/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7870
Precision: 0.5293
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12021  4444]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Good for Yoga/Stretching/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Good for Yoga/Stretching/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.8277, 'recall': 0.9207, 'f1_score': 0.8717}
XGBoost: {'accuracy': 0.9207, 'precision': 0.8028, 'recall': 0.8853, 'f1_score': 0.842}
Decision Tree: {'accuracy': 0.8732, 'precision': 0.7215, 'recall': 0.763, 'f1_score': 0.7416}
Random Forest: {'accuracy': 0.8605, 'precision': 0.6821, 'recall': 0.7781, 'f1_score': 0.727}
Naive Bayes: {'accuracy': 0.787, 'precision': 0.5293, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7742, 'precision': 0.5154, 'recall': 0.8986, 'f1_score': 0.6551}

##################################################
Running experiment without GOOD FOR DRIVING embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4526, Test Loss: 0.3263, F1: 0.7567, AUC: 0.9276
Epoch [10/30] Train Loss: 0.1323, Test Loss: 0.1366, F1: 0.8897, AUC: 0.9845
Epoch [20/30] Train Loss: 0.1081, Test Loss: 0.1382, F1: 0.8912, AUC: 0.9850
Mejores resultados en la época:  29
f1-score 0.891516125981273
AUC según el mejor F1-score 0.9851919681636171
Confusion Matrix:
 [[15765   700]
 [  447  4713]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Driving/confusion_matrix_param_160673.png
Accuracy:   0.9470
Precision:  0.8707
Recall:     0.9134
F1-score:   0.8915

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4230, Test Loss: 0.3615, F1: 0.7583, AUC: 0.9400
Epoch [10/30] Train Loss: 0.1293, Test Loss: 0.1408, F1: 0.8870, AUC: 0.9847
Epoch [20/30] Train Loss: 0.1054, Test Loss: 0.1519, F1: 0.8837, AUC: 0.9856
Mejores resultados en la época:  28
f1-score 0.8982402448355011
AUC según el mejor F1-score 0.9862110725829043
Confusion Matrix:
 [[15865   600]
 [  464  4696]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Driving/confusion_matrix_param_160673.png
Accuracy:   0.9508
Precision:  0.8867
Recall:     0.9101
F1-score:   0.8982

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4345, Test Loss: 0.2985, F1: 0.7671, AUC: 0.9291
Epoch [10/30] Train Loss: 0.1338, Test Loss: 0.1413, F1: 0.8861, AUC: 0.9840
Epoch [20/30] Train Loss: 0.1073, Test Loss: 0.2672, F1: 0.8173, AUC: 0.9840
Mejores resultados en la época:  29
f1-score 0.8935064935064935
AUC según el mejor F1-score 0.9854377914627459
Confusion Matrix:
 [[15874   591]
 [  516  4644]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Driving/confusion_matrix_param_160673.png
Accuracy:   0.9488
Precision:  0.8871
Recall:     0.9000
F1-score:   0.8935
Tiempo total para red 1: 159.78 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3986, Test Loss: 0.4320, F1: 0.7196, AUC: 0.9692
Epoch [10/30] Train Loss: 0.1207, Test Loss: 0.1318, F1: 0.8991, AUC: 0.9862
Epoch [20/30] Train Loss: 0.0811, Test Loss: 0.1323, F1: 0.9032, AUC: 0.9888
Mejores resultados en la época:  19
f1-score 0.9040877367896311
AUC según el mejor F1-score 0.9883785431629696
Confusion Matrix:
 [[16129   336]
 [  626  4534]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Driving/confusion_matrix_param_652929.png
Accuracy:   0.9555
Precision:  0.9310
Recall:     0.8787
F1-score:   0.9041

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4094, Test Loss: 0.2141, F1: 0.8278, AUC: 0.9637
Epoch [10/30] Train Loss: 0.1219, Test Loss: 0.1478, F1: 0.8851, AUC: 0.9870
Epoch [20/30] Train Loss: 0.0717, Test Loss: 0.2649, F1: 0.8305, AUC: 0.9881
Mejores resultados en la época:  21
f1-score 0.9105830709037122
AUC según el mejor F1-score 0.9887770335007076
Confusion Matrix:
 [[15917   548]
 [  389  4771]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Driving/confusion_matrix_param_652929.png
Accuracy:   0.9567
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Precision:  0.8970
Recall:     0.9246
F1-score:   0.9106

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4086, Test Loss: 0.3098, F1: 0.7842, AUC: 0.9481
Epoch [10/30] Train Loss: 0.1233, Test Loss: 0.3072, F1: 0.7728, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0812, Test Loss: 0.1905, F1: 0.8683, AUC: 0.9876
Mejores resultados en la época:  25
f1-score 0.9100222286653136
AUC según el mejor F1-score 0.9886405742036786
Confusion Matrix:
 [[15986   479]
 [  452  4708]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Driving/confusion_matrix_param_652929.png
Accuracy:   0.9569
Precision:  0.9077
Recall:     0.9124
F1-score:   0.9100
Tiempo total para red 3: 170.36 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3843, Test Loss: 0.2156, F1: 0.7951, AUC: 0.9677
Epoch [10/30] Train Loss: 0.1233, Test Loss: 0.2722, F1: 0.8230, AUC: 0.9851
Epoch [20/30] Train Loss: 0.0748, Test Loss: 0.1630, F1: 0.8822, AUC: 0.9883
Mejores resultados en la época:  18
f1-score 0.9094335879377046
AUC según el mejor F1-score 0.9889608095160747
Confusion Matrix:
 [[16128   337]
 [  576  4584]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Driving/confusion_matrix_param_2742785.png
Accuracy:   0.9578
Precision:  0.9315
Recall:     0.8884
F1-score:   0.9094

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3914, Test Loss: 0.5432, F1: 0.6600, AUC: 0.9661
Epoch [10/30] Train Loss: 0.1210, Test Loss: 0.2130, F1: 0.8327, AUC: 0.9867
Epoch [20/30] Train Loss: 0.0782, Test Loss: 0.1252, F1: 0.9103, AUC: 0.9893
Mejores resultados en la época:  26
f1-score 0.9105904404873477
AUC según el mejor F1-score 0.9894703117018249
Confusion Matrix:
 [[15813   652]
 [  302  4858]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Driving/confusion_matrix_param_2742785.png
Accuracy:   0.9559
Precision:  0.8817
Recall:     0.9415
F1-score:   0.9106

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4053, Test Loss: 0.2210, F1: 0.8298, AUC: 0.9672
Epoch [10/30] Train Loss: 0.1209, Test Loss: 0.1341, F1: 0.8959, AUC: 0.9863
Epoch [20/30] Train Loss: 0.0716, Test Loss: 0.1481, F1: 0.8999, AUC: 0.9881
Mejores resultados en la época:  23
f1-score 0.9120419865876178
AUC según el mejor F1-score 0.989029530575781
Confusion Matrix:
 [[16028   437]
 [  468  4692]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Driving/confusion_matrix_param_2742785.png
Accuracy:   0.9582
Precision:  0.9148
Recall:     0.9093
F1-score:   0.9120
Tiempo total para red 5: 188.13 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3951, Test Loss: 0.2676, F1: 0.8317, AUC: 0.9721
Epoch [10/30] Train Loss: 0.1252, Test Loss: 0.1248, F1: 0.8937, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0729, Test Loss: 0.1393, F1: 0.8936, AUC: 0.9871
Mejores resultados en la época:  21
f1-score 0.9101605344500947
AUC según el mejor F1-score 0.9892409138953431
Confusion Matrix:
 [[16160   305]
 [  596  4564]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Driving/confusion_matrix_param_5839873.png
Accuracy:   0.9583
Precision:  0.9374
Recall:     0.8845
F1-score:   0.9102

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4015, Test Loss: 0.5027, F1: 0.6705, AUC: 0.9650
Epoch [10/30] Train Loss: 0.1229, Test Loss: 0.1413, F1: 0.8949, AUC: 0.9869
Epoch [20/30] Train Loss: 0.0784, Test Loss: 0.1395, F1: 0.8938, AUC: 0.9879
Mejores resultados en la época:  29
f1-score 0.9117675358414318
AUC según el mejor F1-score 0.9891359343404026
Confusion Matrix:
 [[15970   495]
 [  422  4738]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Driving/confusion_matrix_param_5839873.png
Accuracy:   0.9576
Precision:  0.9054
Recall:     0.9182
F1-score:   0.9118

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3919, Test Loss: 0.2047, F1: 0.8370, AUC: 0.9659
Epoch [10/30] Train Loss: 0.1201, Test Loss: 0.1555, F1: 0.8826, AUC: 0.9856
Epoch [20/30] Train Loss: 0.0819, Test Loss: 0.1888, F1: 0.8720, AUC: 0.9877
Mejores resultados en la época:  28
f1-score 0.9104592821304516
AUC según el mejor F1-score 0.9886577235714942
Confusion Matrix:
 [[15979   486]
 [  442  4718]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Driving/confusion_matrix_param_5839873.png
Accuracy:   0.9571
Precision:  0.9066
Recall:     0.9143
F1-score:   0.9105
Tiempo total para red 6: 218.14 segundos
Saved on: outputs_hold_out/0/Good for Driving

==============================
Model: Logistic Regression
Accuracy:  0.9353
Precision: 0.8276
Recall:    0.9203
F1-score:  0.8715
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15476   989]
 [  411  4749]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Driving/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Good for Driving/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7939
Precision: 0.5406
Recall:    0.9062
F1-score:  0.6772
              precision    recall  f1-score   support

           0       0.96      0.76      0.85     16465
           1       0.54      0.91      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.75      0.83      0.76     21625
weighted avg       0.86      0.79      0.81     21625

[[12492  3973]
 [  484  4676]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Driving/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Good for Driving/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8726
Precision: 0.7198
Recall:    0.7634
F1-score:  0.7410
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14932  1533]
 [ 1221  3939]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Good for Driving/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Good for Driving/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8566
Precision: 0.6718
Recall:    0.7800
F1-score:  0.7219
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.78      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

[[14499  1966]
 [ 1135  4025]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [01:18:01] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Driving/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Good for Driving/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9207
Precision: 0.8028
Recall:    0.8851
F1-score:  0.8419
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15343  1122]
 [  593  4567]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Driving/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Good for Driving/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7871
Precision: 0.5294
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12022  4443]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Good for Driving/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Good for Driving/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9353, 'precision': 0.8276, 'recall': 0.9203, 'f1_score': 0.8715}
XGBoost: {'accuracy': 0.9207, 'precision': 0.8028, 'recall': 0.8851, 'f1_score': 0.8419}
Decision Tree: {'accuracy': 0.8726, 'precision': 0.7198, 'recall': 0.7634, 'f1_score': 0.741}
Random Forest: {'accuracy': 0.8566, 'precision': 0.6718, 'recall': 0.78, 'f1_score': 0.7219}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7939, 'precision': 0.5406, 'recall': 0.9062, 'f1_score': 0.6772}

##################################################
Running experiment without GOOD FOR SOCIAL GATHERINGS embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4277, Test Loss: 0.3774, F1: 0.7530, AUC: 0.9398
Epoch [10/30] Train Loss: 0.1296, Test Loss: 0.1346, F1: 0.8874, AUC: 0.9846
Epoch [20/30] Train Loss: 0.1003, Test Loss: 0.1458, F1: 0.8860, AUC: 0.9851
Mejores resultados en la época:  25
f1-score 0.8973303990048799
AUC según el mejor F1-score 0.9862248732924197
Confusion Matrix:
 [[15863   602]
 [  471  4689]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Social Gatherings/confusion_matrix_param_160673.png
Accuracy:   0.9504
Precision:  0.8862
Recall:     0.9087
F1-score:   0.8973

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4451, Test Loss: 0.3505, F1: 0.7462, AUC: 0.9224
Epoch [10/30] Train Loss: 0.1341, Test Loss: 0.1768, F1: 0.8631, AUC: 0.9840
Epoch [20/30] Train Loss: 0.1016, Test Loss: 0.2468, F1: 0.8304, AUC: 0.9857
Mejores resultados en la época:  27
f1-score 0.8946384653221839
AUC según el mejor F1-score 0.985346553765681
Confusion Matrix:
 [[16007   458]
 [  613  4547]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Social Gatherings/confusion_matrix_param_160673.png
Accuracy:   0.9505
Precision:  0.9085
Recall:     0.8812
F1-score:   0.8946

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4389, Test Loss: 0.3223, F1: 0.7663, AUC: 0.9282
Epoch [10/30] Train Loss: 0.1333, Test Loss: 0.1573, F1: 0.8781, AUC: 0.9840
Epoch [20/30] Train Loss: 0.1117, Test Loss: 0.1352, F1: 0.8894, AUC: 0.9845
Mejores resultados en la época:  21
f1-score 0.8927342256214149
AUC según el mejor F1-score 0.9851727472180831
Confusion Matrix:
 [[15834   631]
 [  491  4669]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Social Gatherings/confusion_matrix_param_160673.png
Accuracy:   0.9481
Precision:  0.8809
Recall:     0.9048
F1-score:   0.8927
Tiempo total para red 1: 159.82 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3999, Test Loss: 0.2005, F1: 0.8359, AUC: 0.9664
Epoch [10/30] Train Loss: 0.1191, Test Loss: 0.1926, F1: 0.8621, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0782, Test Loss: 0.1608, F1: 0.8915, AUC: 0.9882
Mejores resultados en la época:  26
f1-score 0.9094262692942433
AUC según el mejor F1-score 0.9881967033665492
Confusion Matrix:
 [[16008   457]
 [  476  4684]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Social Gatherings/confusion_matrix_param_652929.png
Accuracy:   0.9569
Precision:  0.9111
Recall:     0.9078
F1-score:   0.9094

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4103, Test Loss: 0.2417, F1: 0.8245, AUC: 0.9655
Epoch [10/30] Train Loss: 0.1208, Test Loss: 0.1410, F1: 0.8874, AUC: 0.9854
Epoch [20/30] Train Loss: 0.0887, Test Loss: 0.1303, F1: 0.8998, AUC: 0.9875
Mejores resultados en la época:  28
f1-score 0.9079466466278414
AUC según el mejor F1-score 0.9896907287480843
Confusion Matrix:
 [[15812   653]
 [  327  4833]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Social Gatherings/confusion_matrix_param_652929.png
Accuracy:   0.9547
Precision:  0.8810
Recall:     0.9366
F1-score:   0.9079

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3950, Test Loss: 0.3475, F1: 0.7559, AUC: 0.9681
Epoch [10/30] Train Loss: 0.1240, Test Loss: 0.1535, F1: 0.8845, AUC: 0.9841
Epoch [20/30] Train Loss: 0.0745, Test Loss: 0.1872, F1: 0.8745, AUC: 0.9879
Mejores resultados en la época:  28
f1-score 0.9105268226909371
AUC según el mejor F1-score 0.9892964404174227
Confusion Matrix:
 [[15969   496]
 [  433  4727]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Social Gatherings/confusion_matrix_param_652929.png
Accuracy:   0.9570
Precision:  0.9050
Recall:     0.9161
F1-score:   0.9105
Tiempo total para red 3: 170.55 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3897, Test Loss: 0.3032, F1: 0.7875, AUC: 0.9730
Epoch [10/30] Train Loss: 0.1222, Test Loss: 0.1328, F1: 0.8937, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0720, Test Loss: 0.1229, F1: 0.9010, AUC: 0.9874
Mejores resultados en la época:  21
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [01:40:56] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
f1-score 0.9111090029408975
AUC según el mejor F1-score 0.9892296791173196
Confusion Matrix:
 [[15886   579]
 [  358  4802]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Social Gatherings/confusion_matrix_param_2742785.png
Accuracy:   0.9567
Precision:  0.8924
Recall:     0.9306
F1-score:   0.9111

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4149, Test Loss: 0.2364, F1: 0.8184, AUC: 0.9645
Epoch [10/30] Train Loss: 0.1258, Test Loss: 0.1910, F1: 0.8654, AUC: 0.9844
Epoch [20/30] Train Loss: 0.0745, Test Loss: 0.1435, F1: 0.9070, AUC: 0.9887
Mejores resultados en la época:  18
f1-score 0.9107710651828299
AUC según el mejor F1-score 0.9889668123833266
Confusion Matrix:
 [[16144   321]
 [  577  4583]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Social Gatherings/confusion_matrix_param_2742785.png
Accuracy:   0.9585
Precision:  0.9345
Recall:     0.8882
F1-score:   0.9108

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4070, Test Loss: 0.2303, F1: 0.7876, AUC: 0.9549
Epoch [10/30] Train Loss: 0.1184, Test Loss: 0.1380, F1: 0.8911, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0732, Test Loss: 0.1706, F1: 0.8810, AUC: 0.9876
Mejores resultados en la época:  16
f1-score 0.9073690440958864
AUC según el mejor F1-score 0.9883773249340273
Confusion Matrix:
 [[16087   378]
 [  561  4599]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Social Gatherings/confusion_matrix_param_2742785.png
Accuracy:   0.9566
Precision:  0.9241
Recall:     0.8913
F1-score:   0.9074
Tiempo total para red 5: 188.02 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4307, Test Loss: 0.2281, F1: 0.7923, AUC: 0.9591
Epoch [10/30] Train Loss: 0.1216, Test Loss: 0.1383, F1: 0.8925, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0789, Test Loss: 0.1770, F1: 0.8852, AUC: 0.9885
Mejores resultados en la época:  28
f1-score 0.9113681996366766
AUC según el mejor F1-score 0.9892270661045157
Confusion Matrix:
 [[15932   533]
 [  394  4766]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Social Gatherings/confusion_matrix_param_5839873.png
Accuracy:   0.9571
Precision:  0.8994
Recall:     0.9236
F1-score:   0.9114

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4051, Test Loss: 0.1876, F1: 0.8411, AUC: 0.9685
Epoch [10/30] Train Loss: 0.1231, Test Loss: 0.1961, F1: 0.8684, AUC: 0.9869
Epoch [20/30] Train Loss: 0.0740, Test Loss: 0.1910, F1: 0.8715, AUC: 0.9884
Mejores resultados en la época:  28
f1-score 0.9134699427017577
AUC según el mejor F1-score 0.9890608690739342
Confusion Matrix:
 [[16031   434]
 [  457  4703]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Social Gatherings/confusion_matrix_param_5839873.png
Accuracy:   0.9588
Precision:  0.9155
Recall:     0.9114
F1-score:   0.9135

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4031, Test Loss: 0.2487, F1: 0.7602, AUC: 0.9572
Epoch [10/30] Train Loss: 0.1235, Test Loss: 0.1539, F1: 0.8880, AUC: 0.9866
Epoch [20/30] Train Loss: 0.0770, Test Loss: 0.1623, F1: 0.8952, AUC: 0.9862
Mejores resultados en la época:  27
f1-score 0.9106342393896042
AUC según el mejor F1-score 0.9897509457458504
Confusion Matrix:
 [[15914   551]
 [  386  4774]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Social Gatherings/confusion_matrix_param_5839873.png
Accuracy:   0.9567
Precision:  0.8965
Recall:     0.9252
F1-score:   0.9106
Tiempo total para red 6: 217.88 segundos
Saved on: outputs_hold_out/0/Good for Social Gatherings

==============================
Model: Logistic Regression
Accuracy:  0.9357
Precision: 0.8289
Recall:    0.9203
F1-score:  0.8723
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15485   980]
 [  411  4749]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Social Gatherings/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Good for Social Gatherings/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7047
Precision: 0.4444
Recall:    0.9484
F1-score:  0.6052
              precision    recall  f1-score   support

           0       0.97      0.63      0.76     16465
           1       0.44      0.95      0.61      5160

    accuracy                           0.70     21625
   macro avg       0.71      0.79      0.68     21625
weighted avg       0.85      0.70      0.73     21625

[[10346  6119]
 [  266  4894]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Social Gatherings/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Good for Social Gatherings/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8725
Precision: 0.7195
Recall:    0.7630
F1-score:  0.7406
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14930  1535]
 [ 1223  3937]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Good for Social Gatherings/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Good for Social Gatherings/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8576
Precision: 0.6744
Recall:    0.7797
F1-score:  0.7232
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.78      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

[[14523  1942]
 [ 1137  4023]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Social Gatherings/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Good for Social Gatherings/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9201
Precision: 0.8011
Recall:    0.8849
F1-score:  0.8409
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15331  1134]
 [  594  4566]]
/home/marcelino.maita/musica_ia_workshop/models_comparison/baseline_clean/No_text/main_complete_hold_out.py:277: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Social Gatherings/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Good for Social Gatherings/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7871
Precision: 0.5294
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12022  4443]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Good for Social Gatherings/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Good for Social Gatherings/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9357, 'precision': 0.8289, 'recall': 0.9203, 'f1_score': 0.8723}
XGBoost: {'accuracy': 0.9201, 'precision': 0.8011, 'recall': 0.8849, 'f1_score': 0.8409}
Decision Tree: {'accuracy': 0.8725, 'precision': 0.7195, 'recall': 0.763, 'f1_score': 0.7406}
Random Forest: {'accuracy': 0.8576, 'precision': 0.6744, 'recall': 0.7797, 'f1_score': 0.7232}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7047, 'precision': 0.4444, 'recall': 0.9484, 'f1_score': 0.6052}

##################################################
Running experiment without GOOD FOR MORNING ROUTINE embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5019)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5019)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5019, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4256, Test Loss: 0.3533, F1: 0.7457, AUC: 0.9318
Epoch [10/30] Train Loss: 0.1303, Test Loss: 0.2493, F1: 0.8177, AUC: 0.9851
Epoch [20/30] Train Loss: 0.1064, Test Loss: 0.1683, F1: 0.8731, AUC: 0.9856
Mejores resultados en la época:  29
f1-score 0.8927647340680402
AUC según el mejor F1-score 0.9849596277751491
Confusion Matrix:
 [[15848   617]
 [  502  4658]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Morning Routine/confusion_matrix_param_160673.png
Accuracy:   0.9483
Precision:  0.8830
Recall:     0.9027
F1-score:   0.8928

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4252, Test Loss: 0.3248, F1: 0.7697, AUC: 0.9356
Epoch [10/30] Train Loss: 0.1269, Test Loss: 0.1632, F1: 0.8756, AUC: 0.9847
Epoch [20/30] Train Loss: 0.1004, Test Loss: 0.1727, F1: 0.8611, AUC: 0.9841
Mejores resultados en la época:  29
f1-score 0.8964721714889936
AUC según el mejor F1-score 0.9857745993968884
Confusion Matrix:
 [[15885   580]
 [  497  4663]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Morning Routine/confusion_matrix_param_160673.png
Accuracy:   0.9502
Precision:  0.8894
Recall:     0.9037
F1-score:   0.8965

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4430, Test Loss: 0.3516, F1: 0.7526, AUC: 0.9248
Epoch [10/30] Train Loss: 0.1336, Test Loss: 0.1542, F1: 0.8815, AUC: 0.9848
Epoch [20/30] Train Loss: 0.1034, Test Loss: 0.1480, F1: 0.8854, AUC: 0.9851
Mejores resultados en la época:  26
f1-score 0.8960801309833382
AUC según el mejor F1-score 0.9855204427055745
Confusion Matrix:
 [[15894   571]
 [  508  4652]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Morning Routine/confusion_matrix_param_160673.png
Accuracy:   0.9501
Precision:  0.8907
Recall:     0.9016
F1-score:   0.8961
Tiempo total para red 1: 159.64 segundos

Entrenando red 3 con capas [5019, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3872, Test Loss: 0.2678, F1: 0.8158, AUC: 0.9704
Epoch [10/30] Train Loss: 0.1217, Test Loss: 0.1628, F1: 0.8651, AUC: 0.9847
Epoch [20/30] Train Loss: 0.0778, Test Loss: 0.2739, F1: 0.8473, AUC: 0.9878
Mejores resultados en la época:  28
f1-score 0.9058086240763259
AUC según el mejor F1-score 0.9882916075207687
Confusion Matrix:
 [[15776   689]
 [  318  4842]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Morning Routine/confusion_matrix_param_652929.png
Accuracy:   0.9534
Precision:  0.8754
Recall:     0.9384
F1-score:   0.9058

--- Iteración 2 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.3971, Test Loss: 0.1974, F1: 0.8421, AUC: 0.9677
Epoch [10/30] Train Loss: 0.1211, Test Loss: 0.1770, F1: 0.8600, AUC: 0.9834
Epoch [20/30] Train Loss: 0.0750, Test Loss: 0.1303, F1: 0.9037, AUC: 0.9876
Mejores resultados en la época:  28
f1-score 0.9130606599486267
AUC según el mejor F1-score 0.9895665105921182
Confusion Matrix:
 [[16124   341]
 [  539  4621]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Morning Routine/confusion_matrix_param_652929.png
Accuracy:   0.9593
Precision:  0.9313
Recall:     0.8955
F1-score:   0.9131

--- Iteración 3 de 3 para la red 3 ---
Epoch [0/30] Train Loss: 0.4006, Test Loss: 0.1950, F1: 0.8409, AUC: 0.9668
Epoch [10/30] Train Loss: 0.1201, Test Loss: 0.2251, F1: 0.8402, AUC: 0.9863
Epoch [20/30] Train Loss: 0.0789, Test Loss: 0.1793, F1: 0.8666, AUC: 0.9865
Mejores resultados en la época:  25
f1-score 0.9119141186619381
AUC según el mejor F1-score 0.9888221491677202
Confusion Matrix:
 [[15949   516]
 [  403  4757]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Morning Routine/confusion_matrix_param_652929.png
Accuracy:   0.9575
Precision:  0.9021
Recall:     0.9219
F1-score:   0.9119
Tiempo total para red 3: 171.48 segundos

Entrenando red 5 con capas [5019, 512, 256, 128, 64, 1]

--- Iteración 1 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3950, Test Loss: 0.2557, F1: 0.8263, AUC: 0.9676
Epoch [10/30] Train Loss: 0.1180, Test Loss: 0.1629, F1: 0.8837, AUC: 0.9857
Epoch [20/30] Train Loss: 0.0757, Test Loss: 0.5517, F1: 0.7590, AUC: 0.9889
Mejores resultados en la época:  27
f1-score 0.9152249134948097
AUC según el mejor F1-score 0.9896917056853037
Confusion Matrix:
 [[15982   483]
 [  399  4761]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Morning Routine/confusion_matrix_param_2742785.png
Accuracy:   0.9592
Precision:  0.9079
Recall:     0.9227
F1-score:   0.9152

--- Iteración 2 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.4041, Test Loss: 0.7176, F1: 0.4805, AUC: 0.9619
Epoch [10/30] Train Loss: 0.1255, Test Loss: 0.1325, F1: 0.8987, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0716, Test Loss: 0.1372, F1: 0.9087, AUC: 0.9876
Mejores resultados en la época:  26
f1-score 0.9108873266197985
AUC según el mejor F1-score 0.9889941842809624
Confusion Matrix:
 [[15893   572]
 [  366  4794]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Morning Routine/confusion_matrix_param_2742785.png
Accuracy:   0.9566
Precision:  0.8934
Recall:     0.9291
F1-score:   0.9109

--- Iteración 3 de 3 para la red 5 ---
Epoch [0/30] Train Loss: 0.3833, Test Loss: 0.1918, F1: 0.8558, AUC: 0.9745
Epoch [10/30] Train Loss: 0.1244, Test Loss: 0.3335, F1: 0.7962, AUC: 0.9840
Epoch [20/30] Train Loss: 0.0771, Test Loss: 0.1362, F1: 0.8968, AUC: 0.9884
Mejores resultados en la época:  21
f1-score 0.9102332222751071
AUC según el mejor F1-score 0.9888650167020955
Confusion Matrix:
 [[15901   564]
 [  379  4781]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Morning Routine/confusion_matrix_param_2742785.png
Accuracy:   0.9564
Precision:  0.8945
Recall:     0.9266
F1-score:   0.9102
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [02:03:51] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Tiempo total para red 5: 188.33 segundos

Entrenando red 6 con capas [5019, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4036, Test Loss: 0.4005, F1: 0.7556, AUC: 0.9661
Epoch [10/30] Train Loss: 0.1257, Test Loss: 0.1722, F1: 0.8817, AUC: 0.9853
Epoch [20/30] Train Loss: 0.0850, Test Loss: 0.1828, F1: 0.8543, AUC: 0.9882
Mejores resultados en la época:  29
f1-score 0.9074964639321075
AUC según el mejor F1-score 0.9891081740219445
Confusion Matrix:
 [[15832   633]
 [  348  4812]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Morning Routine/confusion_matrix_param_5839873.png
Accuracy:   0.9546
Precision:  0.8837
Recall:     0.9326
F1-score:   0.9075

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3849, Test Loss: 0.2183, F1: 0.8462, AUC: 0.9709
Epoch [10/30] Train Loss: 0.1193, Test Loss: 0.1780, F1: 0.8567, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0746, Test Loss: 0.1266, F1: 0.8971, AUC: 0.9878
Mejores resultados en la época:  27
f1-score 0.9103578154425612
AUC según el mejor F1-score 0.9895482606986397
Confusion Matrix:
 [[15839   626]
 [  326  4834]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Morning Routine/confusion_matrix_param_5839873.png
Accuracy:   0.9560
Precision:  0.8853
Recall:     0.9368
F1-score:   0.9104

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4098, Test Loss: 0.2417, F1: 0.8241, AUC: 0.9612
Epoch [10/30] Train Loss: 0.1190, Test Loss: 0.1525, F1: 0.8782, AUC: 0.9861
Epoch [20/30] Train Loss: 0.0738, Test Loss: 0.2068, F1: 0.8657, AUC: 0.9887
Mejores resultados en la época:  25
f1-score 0.9081671779141104
AUC según el mejor F1-score 0.9880601911030444
Confusion Matrix:
 [[15930   535]
 [  423  4737]]
Matriz de confusión guardada en: outputs_hold_out/0/Good for Morning Routine/confusion_matrix_param_5839873.png
Accuracy:   0.9557
Precision:  0.8985
Recall:     0.9180
F1-score:   0.9082
Tiempo total para red 6: 218.48 segundos
Saved on: outputs_hold_out/0/Good for Morning Routine

==============================
Model: Logistic Regression
Accuracy:  0.9354
Precision: 0.8277
Recall:    0.9209
F1-score:  0.8718
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15476   989]
 [  408  4752]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Morning Routine/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_hold_out/0/Good for Morning Routine/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7639
Precision: 0.5030
Recall:    0.8882
F1-score:  0.6422
              precision    recall  f1-score   support

           0       0.95      0.72      0.82     16465
           1       0.50      0.89      0.64      5160

    accuracy                           0.76     21625
   macro avg       0.73      0.81      0.73     21625
weighted avg       0.85      0.76      0.78     21625

[[11936  4529]
 [  577  4583]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Morning Routine/conf_matrix_svm.png
Modelo guardado como: outputs_hold_out/0/Good for Morning Routine/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8727
Precision: 0.7201
Recall:    0.7632
F1-score:  0.7410
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     16465
           1       0.72      0.76      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14934  1531]
 [ 1222  3938]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_hold_out/0/Good for Morning Routine/conf_matrix_decision_tree.png
Modelo guardado como: outputs_hold_out/0/Good for Morning Routine/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8595
Precision: 0.6789
Recall:    0.7802
F1-score:  0.7261
              precision    recall  f1-score   support

           0       0.93      0.88      0.91     16465
           1       0.68      0.78      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14561  1904]
 [ 1134  4026]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Morning Routine/conf_matrix_random_forest.png
Modelo guardado como: outputs_hold_out/0/Good for Morning Routine/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9211
Precision: 0.8037
Recall:    0.8857
F1-score:  0.8427
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.92      0.92      0.92     21625

[[15349  1116]
 [  590  4570]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_hold_out/0/Good for Morning Routine/conf_matrix_xgboost.png
Modelo guardado como: outputs_hold_out/0/Good for Morning Routine/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7871
Precision: 0.5294
Recall:    0.9686
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12022  4443]
 [  162  4998]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_hold_out/0/Good for Morning Routine/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_hold_out/0/Good for Morning Routine/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.8277, 'recall': 0.9209, 'f1_score': 0.8718}
XGBoost: {'accuracy': 0.9211, 'precision': 0.8037, 'recall': 0.8857, 'f1_score': 0.8427}
Decision Tree: {'accuracy': 0.8727, 'precision': 0.7201, 'recall': 0.7632, 'f1_score': 0.741}
Random Forest: {'accuracy': 0.8595, 'precision': 0.6789, 'recall': 0.7802, 'f1_score': 0.7261}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7639, 'precision': 0.503, 'recall': 0.8882, 'f1_score': 0.6422}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: EMOTION
MLP_653057: {'accuracy': 0.9565317919075145, 'precision': 0.8919019316493314, 'recall': 0.9306201550387597, 'f1_score': 0.9108497723823976}
MLP_5840897: {'accuracy': 0.9541734104046242, 'precision': 0.879206840094597, 'recall': 0.9366279069767441, 'f1_score': 0.9070094773388383}
MLP_2743297: {'accuracy': 0.9547745664739884, 'precision': 0.9019607843137255, 'recall': 0.9093023255813953, 'f1_score': 0.9056166763173132}
MLP_160705: {'accuracy': 0.9511676300578035, 'precision': 0.9110576923076923, 'recall': 0.8813953488372093, 'f1_score': 0.8959810874704491}
Logistic Regression: {'accuracy': 0.9355, 'precision': 0.8287, 'recall': 0.9198, 'f1_score': 0.8719}
XGBoost: {'accuracy': 0.9209, 'precision': 0.8032, 'recall': 0.8857, 'f1_score': 0.8424}
Decision Tree: {'accuracy': 0.8779, 'precision': 0.7395, 'recall': 0.7539, 'f1_score': 0.7466}
Random Forest: {'accuracy': 0.8627, 'precision': 0.687, 'recall': 0.7798, 'f1_score': 0.7305}
Naive Bayes: {'accuracy': 0.786, 'precision': 0.5281, 'recall': 0.968, 'f1_score': 0.6834}
SVM: {'accuracy': 0.8068, 'precision': 0.5612, 'recall': 0.8723, 'f1_score': 0.683}


EMBEDDINGS TYPE: TIME SIGNATURE
MLP_5840897: {'accuracy': 0.9592138728323699, 'precision': 0.93125, 'recall': 0.8951550387596899, 'f1_score': 0.9128458498023715}
MLP_2743297: {'accuracy': 0.9580578034682081, 'precision': 0.9131532931804935, 'recall': 0.9108527131782945, 'f1_score': 0.9120015523430678}
MLP_653057: {'accuracy': 0.9566705202312139, 'precision': 0.8981708466905525, 'recall': 0.923062015503876, 'f1_score': 0.91044633470324}
MLP_160705: {'accuracy': 0.9485780346820809, 'precision': 0.8748148148148148, 'recall': 0.9155038759689923, 'f1_score': 0.8946969696969697}
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.8278, 'recall': 0.9205, 'f1_score': 0.8717}
XGBoost: {'accuracy': 0.9198, 'precision': 0.8001, 'recall': 0.8851, 'f1_score': 0.8404}
Decision Tree: {'accuracy': 0.8726, 'precision': 0.7197, 'recall': 0.7638, 'f1_score': 0.7411}
Random Forest: {'accuracy': 0.8566, 'precision': 0.6702, 'recall': 0.7853, 'f1_score': 0.7232}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7568, 'precision': 0.4948, 'recall': 0.9291, 'f1_score': 0.6457}


EMBEDDINGS TYPE: ARTIST(S)
MLP_5840897: {'accuracy': 0.9568092485549133, 'precision': 0.886996336996337, 'recall': 0.9385658914728682, 'f1_score': 0.9120527306967985}
MLP_2743297: {'accuracy': 0.9567630057803468, 'precision': 0.9149479473580829, 'recall': 0.9027131782945736, 'f1_score': 0.9087893864013267}
MLP_653057: {'accuracy': 0.9545895953757225, 'precision': 0.8816222141030325, 'recall': 0.9352713178294574, 'f1_score': 0.9076546924957682}
MLP_160705: {'accuracy': 0.949456647398844, 'precision': 0.8800224257148197, 'recall': 0.9125968992248062, 'f1_score': 0.8960136999334031}
Logistic Regression: {'accuracy': 0.9351, 'precision': 0.8266, 'recall': 0.9213, 'f1_score': 0.8714}
XGBoost: {'accuracy': 0.9202, 'precision': 0.8005, 'recall': 0.8866, 'f1_score': 0.8414}
Decision Tree: {'accuracy': 0.8713, 'precision': 0.716, 'recall': 0.7632, 'f1_score': 0.7388}
Random Forest: {'accuracy': 0.8658, 'precision': 0.695, 'recall': 0.7795, 'f1_score': 0.7348}
SVM: {'accuracy': 0.8225, 'precision': 0.5862, 'recall': 0.8713, 'f1_score': 0.7009}
Naive Bayes: {'accuracy': 0.7754, 'precision': 0.5156, 'recall': 0.9713, 'f1_score': 0.6736}


EMBEDDINGS TYPE: SONG
MLP_2743297: {'accuracy': 0.9591676300578035, 'precision': 0.9242213846459036, 'recall': 0.9029069767441861, 'f1_score': 0.9134398588373689}
MLP_5840897: {'accuracy': 0.9580115606936416, 'precision': 0.9023467070401211, 'recall': 0.924031007751938, 'f1_score': 0.9130601302183071}
MLP_653057: {'accuracy': 0.9579190751445087, 'precision': 0.9038388445458001, 'recall': 0.9217054263565891, 'f1_score': 0.9126847054308194}
MLP_160705: {'accuracy': 0.9524624277456647, 'precision': 0.8887843432442605, 'recall': 0.9153100775193799, 'f1_score': 0.9018522054611419}
Logistic Regression: {'accuracy': 0.9379, 'precision': 0.8325, 'recall': 0.926, 'f1_score': 0.8768}
XGBoost: {'accuracy': 0.9207, 'precision': 0.8039, 'recall': 0.8833, 'f1_score': 0.8417}
Random Forest: {'accuracy': 0.8722, 'precision': 0.7085, 'recall': 0.7891, 'f1_score': 0.7467}
Decision Tree: {'accuracy': 0.8738, 'precision': 0.7223, 'recall': 0.7653, 'f1_score': 0.7432}
SVM: {'accuracy': 0.8436, 'precision': 0.6253, 'recall': 0.8595, 'f1_score': 0.7239}
Naive Bayes: {'accuracy': 0.7993, 'precision': 0.5446, 'recall': 0.9694, 'f1_score': 0.6974}


EMBEDDINGS TYPE: GENRE
MLP_5840897: {'accuracy': 0.958936416184971, 'precision': 0.9160498636540709, 'recall': 0.9114341085271318, 'f1_score': 0.9137361569846513}
MLP_653057: {'accuracy': 0.9578265895953757, 'precision': 0.9251401120896717, 'recall': 0.8957364341085271, 'f1_score': 0.9102008664828672}
MLP_2743297: {'accuracy': 0.9565317919075145, 'precision': 0.9173259493670886, 'recall': 0.8988372093023256, 'f1_score': 0.9079874706342992}
MLP_160705: {'accuracy': 0.9494104046242775, 'precision': 0.8878290728729492, 'recall': 0.9019379844961241, 'f1_score': 0.8948279177081331}
Logistic Regression: {'accuracy': 0.934, 'precision': 0.8249, 'recall': 0.9184, 'f1_score': 0.8691}
XGBoost: {'accuracy': 0.9134, 'precision': 0.7824, 'recall': 0.8824, 'f1_score': 0.8294}
Random Forest: {'accuracy': 0.8833, 'precision': 0.7433, 'recall': 0.7806, 'f1_score': 0.7615}
SVM: {'accuracy': 0.8521, 'precision': 0.6555, 'recall': 0.801, 'f1_score': 0.721}
Decision Tree: {'accuracy': 0.8243, 'precision': 0.5994, 'recall': 0.7946, 'f1_score': 0.6833}
Naive Bayes: {'accuracy': 0.7802, 'precision': 0.5212, 'recall': 0.9694, 'f1_score': 0.6779}


EMBEDDINGS TYPE: ALBUM
MLP_5840897: {'accuracy': 0.9572716763005781, 'precision': 0.8862144420131292, 'recall': 0.9418604651162791, 'f1_score': 0.9131905298759865}
MLP_653057: {'accuracy': 0.958242774566474, 'precision': 0.9094056549336411, 'recall': 0.9162790697674419, 'f1_score': 0.9128294236895453}
MLP_2743297: {'accuracy': 0.9544971098265896, 'precision': 0.8741935483870967, 'recall': 0.9453488372093023, 'f1_score': 0.9083798882681564}
MLP_160705: {'accuracy': 0.9506589595375723, 'precision': 0.889142422513786, 'recall': 0.9062015503875969, 'f1_score': 0.897590939629523}
Logistic Regression: {'accuracy': 0.936, 'precision': 0.8299, 'recall': 0.9207, 'f1_score': 0.8729}
XGBoost: {'accuracy': 0.9199, 'precision': 0.802, 'recall': 0.8818, 'f1_score': 0.84}
Decision Tree: {'accuracy': 0.8716, 'precision': 0.7166, 'recall': 0.7643, 'f1_score': 0.7397}
Random Forest: {'accuracy': 0.8608, 'precision': 0.6818, 'recall': 0.7812, 'f1_score': 0.7281}
Naive Bayes: {'accuracy': 0.7942, 'precision': 0.5382, 'recall': 0.969, 'f1_score': 0.692}
SVM: {'accuracy': 0.744, 'precision': 0.4808, 'recall': 0.9101, 'f1_score': 0.6292}


EMBEDDINGS TYPE: RELEASE DATE
MLP_653057: {'accuracy': 0.9569017341040462, 'precision': 0.8982667671439337, 'recall': 0.924031007751938, 'f1_score': 0.9109667558272831}
MLP_5840897: {'accuracy': 0.9565780346820809, 'precision': 0.8929435859244088, 'recall': 0.9294573643410853, 'f1_score': 0.9108346785680372}
MLP_2743297: {'accuracy': 0.9561156069364162, 'precision': 0.893624976631146, 'recall': 0.9263565891472868, 'f1_score': 0.9096964506613379}
MLP_160705: {'accuracy': 0.9502890173410404, 'precision': 0.9007259172061998, 'recall': 0.8897286821705427, 'f1_score': 0.8951935263722336}
Logistic Regression: {'accuracy': 0.9364, 'precision': 0.8298, 'recall': 0.9225, 'f1_score': 0.8737}
XGBoost: {'accuracy': 0.9201, 'precision': 0.8014, 'recall': 0.8843, 'f1_score': 0.8408}
Decision Tree: {'accuracy': 0.8732, 'precision': 0.7204, 'recall': 0.7661, 'f1_score': 0.7426}
Random Forest: {'accuracy': 0.8647, 'precision': 0.6909, 'recall': 0.7839, 'f1_score': 0.7345}
Naive Bayes: {'accuracy': 0.7877, 'precision': 0.5302, 'recall': 0.9684, 'f1_score': 0.6853}
SVM: {'accuracy': 0.7982, 'precision': 0.5464, 'recall': 0.9064, 'f1_score': 0.6818}


EMBEDDINGS TYPE: KEY
MLP_5840897: {'accuracy': 0.9557919075144509, 'precision': 0.8930441286462228, 'recall': 0.9255813953488372, 'f1_score': 0.9090216977540921}
MLP_2743297: {'accuracy': 0.9553757225433526, 'precision': 0.8880666049953746, 'recall': 0.9302325581395349, 'f1_score': 0.9086606720302887}
MLP_653057: {'accuracy': 0.9558843930635839, 'precision': 0.933429513602638, 'recall': 0.8777131782945736, 'f1_score': 0.9047143427886536}
MLP_160705: {'accuracy': 0.9505202312138729, 'precision': 0.9044699367088608, 'recall': 0.8862403100775194, 'f1_score': 0.8952623335943618}
Logistic Regression: {'accuracy': 0.935, 'precision': 0.8269, 'recall': 0.9203, 'f1_score': 0.8711}
XGBoost: {'accuracy': 0.9208, 'precision': 0.8025, 'recall': 0.8864, 'f1_score': 0.8424}
Decision Tree: {'accuracy': 0.8726, 'precision': 0.7198, 'recall': 0.7636, 'f1_score': 0.741}
Random Forest: {'accuracy': 0.8634, 'precision': 0.6874, 'recall': 0.7843, 'f1_score': 0.7327}
Naive Bayes: {'accuracy': 0.7852, 'precision': 0.5271, 'recall': 0.969, 'f1_score': 0.6828}
SVM: {'accuracy': 0.797, 'precision': 0.5453, 'recall': 0.8994, 'f1_score': 0.679}


EMBEDDINGS TYPE: SIMILAR ARTIST 1
MLP_5840897: {'accuracy': 0.9507976878612717, 'precision': 0.8911382734912147, 'recall': 0.9042635658914728, 'f1_score': 0.8976529434397845}
MLP_653057: {'accuracy': 0.9496416184971098, 'precision': 0.9020343669761012, 'recall': 0.8850775193798449, 'f1_score': 0.8934754964296194}
MLP_2743297: {'accuracy': 0.9462658959537572, 'precision': 0.8833908707326429, 'recall': 0.8926356589147287, 'f1_score': 0.8879892037786775}
MLP_160705: {'accuracy': 0.9440924855491329, 'precision': 0.899171549808042, 'recall': 0.8624031007751938, 'f1_score': 0.8804036007518053}
Logistic Regression: {'accuracy': 0.9252, 'precision': 0.8045, 'recall': 0.9068, 'f1_score': 0.8526}
XGBoost: {'accuracy': 0.9157, 'precision': 0.7917, 'recall': 0.8775, 'f1_score': 0.8324}
Decision Tree: {'accuracy': 0.8728, 'precision': 0.7227, 'recall': 0.7576, 'f1_score': 0.7397}
Random Forest: {'accuracy': 0.8564, 'precision': 0.6717, 'recall': 0.7787, 'f1_score': 0.7212}
Naive Bayes: {'accuracy': 0.7488, 'precision': 0.4867, 'recall': 0.9692, 'f1_score': 0.648}
SVM: {'accuracy': 0.7303, 'precision': 0.4648, 'recall': 0.8595, 'f1_score': 0.6033}


EMBEDDINGS TYPE: SIMILAR SONG 1
MLP_653057: {'accuracy': 0.9559768786127167, 'precision': 0.9247073072264836, 'recall': 0.8877906976744186, 'f1_score': 0.9058730472612221}
MLP_2743297: {'accuracy': 0.9531560693641619, 'precision': 0.8877875444174304, 'recall': 0.9199612403100775, 'f1_score': 0.9035880841343866}
MLP_5840897: {'accuracy': 0.9534797687861272, 'precision': 0.9042428960685092, 'recall': 0.9003875968992248, 'f1_score': 0.9023111283744416}
MLP_160705: {'accuracy': 0.9490404624277456, 'precision': 0.8994094488188976, 'recall': 0.8854651162790698, 'f1_score': 0.8923828125}
Logistic Regression: {'accuracy': 0.9351, 'precision': 0.8268, 'recall': 0.9207, 'f1_score': 0.8713}
XGBoost: {'accuracy': 0.919, 'precision': 0.7988, 'recall': 0.8831, 'f1_score': 0.8388}
Decision Tree: {'accuracy': 0.8731, 'precision': 0.721, 'recall': 0.7636, 'f1_score': 0.7416}
Random Forest: {'accuracy': 0.8663, 'precision': 0.6941, 'recall': 0.7859, 'f1_score': 0.7371}
Naive Bayes: {'accuracy': 0.7908, 'precision': 0.5339, 'recall': 0.9707, 'f1_score': 0.6889}
SVM: {'accuracy': 0.7696, 'precision': 0.5095, 'recall': 0.9264, 'f1_score': 0.6574}


EMBEDDINGS TYPE: SIMILAR ARTIST 2
MLP_5840897: {'accuracy': 0.9522774566473988, 'precision': 0.9, 'recall': 0.9, 'f1_score': 0.9}
MLP_2743297: {'accuracy': 0.9498265895953757, 'precision': 0.8881691750809678, 'recall': 0.9034883720930232, 'f1_score': 0.8957632817753867}
MLP_653057: {'accuracy': 0.947514450867052, 'precision': 0.8730305838739574, 'recall': 0.9127906976744186, 'f1_score': 0.8924680246328754}
MLP_160705: {'accuracy': 0.9408092485549133, 'precision': 0.8777258566978193, 'recall': 0.8736434108527131, 'f1_score': 0.8756798756798757}
Logistic Regression: {'accuracy': 0.9238, 'precision': 0.8013, 'recall': 0.9052, 'f1_score': 0.8501}
XGBoost: {'accuracy': 0.9134, 'precision': 0.7862, 'recall': 0.8752, 'f1_score': 0.8283}
Decision Tree: {'accuracy': 0.872, 'precision': 0.7215, 'recall': 0.755, 'f1_score': 0.7379}
Random Forest: {'accuracy': 0.8567, 'precision': 0.6718, 'recall': 0.781, 'f1_score': 0.7223}
SVM: {'accuracy': 0.7905, 'precision': 0.5377, 'recall': 0.8711, 'f1_score': 0.6649}
Naive Bayes: {'accuracy': 0.749, 'precision': 0.4869, 'recall': 0.9645, 'f1_score': 0.6472}


EMBEDDINGS TYPE: SIMILAR SONG 2
MLP_653057: {'accuracy': 0.9567630057803468, 'precision': 0.9133242027000587, 'recall': 0.9046511627906977, 'f1_score': 0.9089669944503943}
MLP_5840897: {'accuracy': 0.9543583815028902, 'precision': 0.8861743475846752, 'recall': 0.9279069767441861, 'f1_score': 0.9065606361829026}
MLP_2743297: {'accuracy': 0.9523236994219653, 'precision': 0.879015972094731, 'recall': 0.9279069767441861, 'f1_score': 0.9028000377109456}
MLP_160705: {'accuracy': 0.9494104046242775, 'precision': 0.8912625096227867, 'recall': 0.8974806201550387, 'f1_score': 0.8943607570490537}
Logistic Regression: {'accuracy': 0.9339, 'precision': 0.8234, 'recall': 0.9205, 'f1_score': 0.8692}
XGBoost: {'accuracy': 0.919, 'precision': 0.7977, 'recall': 0.8851, 'f1_score': 0.8391}
Decision Tree: {'accuracy': 0.8756, 'precision': 0.7324, 'recall': 0.7543, 'f1_score': 0.7432}
Random Forest: {'accuracy': 0.8617, 'precision': 0.6813, 'recall': 0.7897, 'f1_score': 0.7315}
Naive Bayes: {'accuracy': 0.7916, 'precision': 0.535, 'recall': 0.9671, 'f1_score': 0.6889}
SVM: {'accuracy': 0.8077, 'precision': 0.5638, 'recall': 0.8578, 'f1_score': 0.6804}


EMBEDDINGS TYPE: SIMILAR ARTIST 3
MLP_5840897: {'accuracy': 0.9470520231213873, 'precision': 0.8684162231602128, 'recall': 0.9170542635658915, 'f1_score': 0.8920727684041851}
MLP_2743297: {'accuracy': 0.9472832369942197, 'precision': 0.8868360277136259, 'recall': 0.8930232558139535, 'f1_score': 0.8899188876013905}
MLP_653057: {'accuracy': 0.945664739884393, 'precision': 0.9067156562563788, 'recall': 0.8608527131782946, 'f1_score': 0.8831891838154886}
MLP_160705: {'accuracy': 0.9421040462427746, 'precision': 0.8846456692913386, 'recall': 0.8709302325581395, 'f1_score': 0.877734375}
Logistic Regression: {'accuracy': 0.9214, 'precision': 0.7956, 'recall': 0.9023, 'f1_score': 0.8456}
XGBoost: {'accuracy': 0.9129, 'precision': 0.7865, 'recall': 0.8715, 'f1_score': 0.8268}
Decision Tree: {'accuracy': 0.872, 'precision': 0.7191, 'recall': 0.7607, 'f1_score': 0.7393}
Random Forest: {'accuracy': 0.8503, 'precision': 0.6571, 'recall': 0.7789, 'f1_score': 0.7128}
Naive Bayes: {'accuracy': 0.7454, 'precision': 0.4832, 'recall': 0.9643, 'f1_score': 0.6438}
SVM: {'accuracy': 0.7459, 'precision': 0.4824, 'recall': 0.8897, 'f1_score': 0.6256}


EMBEDDINGS TYPE: SIMILAR SONG 3
MLP_2743297: {'accuracy': 0.9542196531791908, 'precision': 0.8959361944549943, 'recall': 0.9143410852713179, 'f1_score': 0.9050450796086706}
MLP_5840897: {'accuracy': 0.9554682080924856, 'precision': 0.9354637891678772, 'recall': 0.8736434108527131, 'f1_score': 0.9034973444232889}
MLP_653057: {'accuracy': 0.9519075144508671, 'precision': 0.8841850055949273, 'recall': 0.9187984496124031, 'f1_score': 0.9011594753849078}
MLP_160705: {'accuracy': 0.9482543352601156, 'precision': 0.8978145304193739, 'recall': 0.8837209302325582, 'f1_score': 0.8907119835921476}
Logistic Regression: {'accuracy': 0.9342, 'precision': 0.8252, 'recall': 0.9192, 'f1_score': 0.8696}
XGBoost: {'accuracy': 0.9202, 'precision': 0.8009, 'recall': 0.8859, 'f1_score': 0.8413}
Decision Tree: {'accuracy': 0.8721, 'precision': 0.7198, 'recall': 0.7597, 'f1_score': 0.7392}
Random Forest: {'accuracy': 0.8594, 'precision': 0.6756, 'recall': 0.7899, 'f1_score': 0.7283}
Naive Bayes: {'accuracy': 0.7916, 'precision': 0.535, 'recall': 0.9686, 'f1_score': 0.6893}
SVM: {'accuracy': 0.7951, 'precision': 0.5444, 'recall': 0.8657, 'f1_score': 0.6684}


EMBEDDINGS TYPE: SONG_NORMALIZED
MLP_2743297: {'accuracy': 0.9611098265895954, 'precision': 0.9230166503428012, 'recall': 0.9131782945736434, 'f1_score': 0.9180711154408183}
MLP_5840897: {'accuracy': 0.9606936416184971, 'precision': 0.9200779727095516, 'recall': 0.9147286821705426, 'f1_score': 0.9173955296404276}
MLP_653057: {'accuracy': 0.959028901734104, 'precision': 0.8994392523364486, 'recall': 0.9325581395348838, 'f1_score': 0.9156993339676499}
MLP_160705: {'accuracy': 0.9533410404624277, 'precision': 0.9262682275621278, 'recall': 0.874031007751938, 'f1_score': 0.8993917638847343}
Logistic Regression: {'accuracy': 0.9376, 'precision': 0.8311, 'recall': 0.9267, 'f1_score': 0.8763}
XGBoost: {'accuracy': 0.92, 'precision': 0.8018, 'recall': 0.8833, 'f1_score': 0.8406}
Decision Tree: {'accuracy': 0.8734, 'precision': 0.7218, 'recall': 0.764, 'f1_score': 0.7423}
Random Forest: {'accuracy': 0.8679, 'precision': 0.6979, 'recall': 0.787, 'f1_score': 0.7398}
Naive Bayes: {'accuracy': 0.8017, 'precision': 0.5476, 'recall': 0.9702, 'f1_score': 0.7001}
SVM: {'accuracy': 0.7662, 'precision': 0.5056, 'recall': 0.9039, 'f1_score': 0.6485}


EMBEDDINGS TYPE: ARTIST_NORMALIZED
MLP_2743297: {'accuracy': 0.9600462427745665, 'precision': 0.9323671497584541, 'recall': 0.8976744186046511, 'f1_score': 0.9146919431279621}
MLP_5840897: {'accuracy': 0.9585202312138729, 'precision': 0.9251944943147815, 'recall': 0.8988372093023256, 'f1_score': 0.9118254202300207}
MLP_653057: {'accuracy': 0.9564393063583815, 'precision': 0.908879410624273, 'recall': 0.9085271317829458, 'f1_score': 0.9087032370614461}
MLP_160705: {'accuracy': 0.9496878612716763, 'precision': 0.8982785602503912, 'recall': 0.889922480620155, 'f1_score': 0.8940809968847352}
Logistic Regression: {'accuracy': 0.9351, 'precision': 0.8266, 'recall': 0.9211, 'f1_score': 0.8713}
XGBoost: {'accuracy': 0.9225, 'precision': 0.807, 'recall': 0.8876, 'f1_score': 0.8454}
Decision Tree: {'accuracy': 0.8713, 'precision': 0.7172, 'recall': 0.7607, 'f1_score': 0.7383}
Random Forest: {'accuracy': 0.8561, 'precision': 0.6696, 'recall': 0.7833, 'f1_score': 0.722}
SVM: {'accuracy': 0.8147, 'precision': 0.5714, 'recall': 0.8938, 'f1_score': 0.6972}
Naive Bayes: {'accuracy': 0.7755, 'precision': 0.5158, 'recall': 0.9709, 'f1_score': 0.6737}


EMBEDDINGS TYPE: TEMPO
MLP_652929: {'accuracy': 0.9603699421965318, 'precision': 0.9152673229106351, 'recall': 0.9189922480620155, 'f1_score': 0.9171260032878832}
MLP_5839873: {'accuracy': 0.9575953757225434, 'precision': 0.9186895598973752, 'recall': 0.9021317829457365, 'f1_score': 0.9103353867214237}
MLP_2742785: {'accuracy': 0.9562080924855492, 'precision': 0.8917612051329737, 'recall': 0.9292635658914729, 'f1_score': 0.910126221884787}
MLP_160673: {'accuracy': 0.950335260115607, 'precision': 0.8921305182341651, 'recall': 0.9007751937984496, 'f1_score': 0.8964320154291224}
Logistic Regression: {'accuracy': 0.9352, 'precision': 0.8272, 'recall': 0.9205, 'f1_score': 0.8714}
XGBoost: {'accuracy': 0.92, 'precision': 0.8003, 'recall': 0.886, 'f1_score': 0.841}
Decision Tree: {'accuracy': 0.8728, 'precision': 0.7203, 'recall': 0.7636, 'f1_score': 0.7413}
Random Forest: {'accuracy': 0.8583, 'precision': 0.675, 'recall': 0.7833, 'f1_score': 0.7252}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.773, 'precision': 0.5136, 'recall': 0.9196, 'f1_score': 0.6591}


EMBEDDINGS TYPE: LENGTH
MLP_5839873: {'accuracy': 0.9606473988439307, 'precision': 0.9155255544840887, 'recall': 0.9199612403100775, 'f1_score': 0.9177380376993717}
MLP_2742785: {'accuracy': 0.9590751445086705, 'precision': 0.8927062281829873, 'recall': 0.9416666666666667, 'f1_score': 0.9165330566820711}
MLP_652929: {'accuracy': 0.9566705202312139, 'precision': 0.8886434750598197, 'recall': 0.9356589147286821, 'f1_score': 0.911545360143491}
MLP_160673: {'accuracy': 0.9514913294797688, 'precision': 0.9116763468856399, 'recall': 0.8821705426356589, 'f1_score': 0.8966807840047276}
Logistic Regression: {'accuracy': 0.9355, 'precision': 0.8276, 'recall': 0.9217, 'f1_score': 0.8721}
XGBoost: {'accuracy': 0.9192, 'precision': 0.7998, 'recall': 0.882, 'f1_score': 0.8389}
Decision Tree: {'accuracy': 0.8725, 'precision': 0.7191, 'recall': 0.7641, 'f1_score': 0.741}
Random Forest: {'accuracy': 0.8606, 'precision': 0.6799, 'recall': 0.7859, 'f1_score': 0.7291}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7657, 'precision': 0.5051, 'recall': 0.9025, 'f1_score': 0.6477}


EMBEDDINGS TYPE: LOUDNESS (DB)
MLP_5839873: {'accuracy': 0.9571791907514451, 'precision': 0.8949626865671642, 'recall': 0.9296511627906977, 'f1_score': 0.911977186311787}
MLP_2742785: {'accuracy': 0.9560693641618497, 'precision': 0.9004946727549468, 'recall': 0.9172480620155039, 'f1_score': 0.9087941628264209}
MLP_652929: {'accuracy': 0.9572716763005781, 'precision': 0.9317162657969833, 'recall': 0.8858527131782946, 'f1_score': 0.9082058414464534}
MLP_160673: {'accuracy': 0.9516763005780347, 'precision': 0.9096157674696397, 'recall': 0.8854651162790698, 'f1_score': 0.8973779829126977}
Logistic Regression: {'accuracy': 0.9358, 'precision': 0.829, 'recall': 0.9207, 'f1_score': 0.8725}
XGBoost: {'accuracy': 0.9201, 'precision': 0.8012, 'recall': 0.8849, 'f1_score': 0.841}
Decision Tree: {'accuracy': 0.8726, 'precision': 0.7179, 'recall': 0.7674, 'f1_score': 0.7419}
Random Forest: {'accuracy': 0.8606, 'precision': 0.6791, 'recall': 0.7882, 'f1_score': 0.7296}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7579, 'precision': 0.4959, 'recall': 0.9, 'f1_score': 0.6395}


EMBEDDINGS TYPE: POPULARITY
MLP_2742785: {'accuracy': 0.9603236994219653, 'precision': 0.9284860557768925, 'recall': 0.9032945736434108, 'f1_score': 0.9157170923379175}
MLP_5839873: {'accuracy': 0.9565317919075145, 'precision': 0.9092319627618308, 'recall': 0.9085271317829458, 'f1_score': 0.908879410624273}
MLP_652929: {'accuracy': 0.9550520231213873, 'precision': 0.8993135011441648, 'recall': 0.913953488372093, 'f1_score': 0.9065743944636678}
MLP_160673: {'accuracy': 0.9511676300578035, 'precision': 0.8979829325058185, 'recall': 0.8972868217054264, 'f1_score': 0.8976347421481194}
Logistic Regression: {'accuracy': 0.9356, 'precision': 0.8284, 'recall': 0.9207, 'f1_score': 0.8721}
XGBoost: {'accuracy': 0.92, 'precision': 0.7997, 'recall': 0.8872, 'f1_score': 0.8412}
Decision Tree: {'accuracy': 0.8728, 'precision': 0.7201, 'recall': 0.7641, 'f1_score': 0.7414}
Random Forest: {'accuracy': 0.8593, 'precision': 0.6751, 'recall': 0.7909, 'f1_score': 0.7284}
SVM: {'accuracy': 0.8071, 'precision': 0.5601, 'recall': 0.8917, 'f1_score': 0.6881}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}


EMBEDDINGS TYPE: ENERGY
MLP_652929: {'accuracy': 0.9591676300578035, 'precision': 0.9169428738545525, 'recall': 0.9114341085271318, 'f1_score': 0.9141801924385266}
MLP_5839873: {'accuracy': 0.958150289017341, 'precision': 0.9136690647482014, 'recall': 0.9106589147286822, 'f1_score': 0.9121615063573716}
MLP_2742785: {'accuracy': 0.958150289017341, 'precision': 0.918881669620004, 'recall': 0.9044573643410853, 'f1_score': 0.9116124621545073}
MLP_160673: {'accuracy': 0.9492254335260115, 'precision': 0.8914803392444102, 'recall': 0.8963178294573644, 'f1_score': 0.8938925396211829}
Logistic Regression: {'accuracy': 0.9356, 'precision': 0.8285, 'recall': 0.9209, 'f1_score': 0.8722}
XGBoost: {'accuracy': 0.9189, 'precision': 0.7991, 'recall': 0.882, 'f1_score': 0.8385}
Decision Tree: {'accuracy': 0.8774, 'precision': 0.7391, 'recall': 0.7516, 'f1_score': 0.7453}
Random Forest: {'accuracy': 0.8634, 'precision': 0.6863, 'recall': 0.7874, 'f1_score': 0.7334}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7702, 'precision': 0.5103, 'recall': 0.9209, 'f1_score': 0.6567}


EMBEDDINGS TYPE: DANCEABILITY
MLP_2742785: {'accuracy': 0.956393063583815, 'precision': 0.8915506035283194, 'recall': 0.9304263565891473, 'f1_score': 0.9105737316263632}
MLP_652929: {'accuracy': 0.9552832369942197, 'precision': 0.8974407582938388, 'recall': 0.9174418604651163, 'f1_score': 0.9073310972688069}
MLP_5839873: {'accuracy': 0.9526011560693641, 'precision': 0.8672943684491029, 'recall': 0.9461240310077519, 'f1_score': 0.9049958290851794}
MLP_160673: {'accuracy': 0.9498265895953757, 'precision': 0.8821984618270493, 'recall': 0.9114341085271318, 'f1_score': 0.8965780192545992}
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.8286, 'recall': 0.9192, 'f1_score': 0.8716}
XGBoost: {'accuracy': 0.9202, 'precision': 0.8029, 'recall': 0.8824, 'f1_score': 0.8407}
SVM: {'accuracy': 0.8825, 'precision': 0.7628, 'recall': 0.7368, 'f1_score': 0.7496}
Decision Tree: {'accuracy': 0.8755, 'precision': 0.7292, 'recall': 0.7605, 'f1_score': 0.7445}
Random Forest: {'accuracy': 0.8628, 'precision': 0.6855, 'recall': 0.7853, 'f1_score': 0.732}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}


EMBEDDINGS TYPE: POSITIVENESS
MLP_5839873: {'accuracy': 0.9604624277456647, 'precision': 0.9164248403946604, 'recall': 0.9180232558139535, 'f1_score': 0.9172233517281441}
MLP_2742785: {'accuracy': 0.956300578034682, 'precision': 0.9025787965616046, 'recall': 0.9156976744186046, 'f1_score': 0.9090909090909091}
MLP_652929: {'accuracy': 0.9550520231213873, 'precision': 0.898250285279574, 'recall': 0.9153100775193799, 'f1_score': 0.9066999424073718}
MLP_160673: {'accuracy': 0.9476994219653179, 'precision': 0.8767533196184777, 'recall': 0.9085271317829458, 'f1_score': 0.892357475968402}
Logistic Regression: {'accuracy': 0.9357, 'precision': 0.8287, 'recall': 0.9207, 'f1_score': 0.8723}
XGBoost: {'accuracy': 0.9192, 'precision': 0.7989, 'recall': 0.8837, 'f1_score': 0.8392}
Decision Tree: {'accuracy': 0.8714, 'precision': 0.7156, 'recall': 0.7647, 'f1_score': 0.7394}
Random Forest: {'accuracy': 0.8613, 'precision': 0.6817, 'recall': 0.7855, 'f1_score': 0.7299}
SVM: {'accuracy': 0.805, 'precision': 0.5566, 'recall': 0.8983, 'f1_score': 0.6873}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}


EMBEDDINGS TYPE: SPEECHINESS
MLP_2742785: {'accuracy': 0.9574104046242775, 'precision': 0.8999811285148142, 'recall': 0.9242248062015503, 'f1_score': 0.9119418682474424}
MLP_5839873: {'accuracy': 0.956485549132948, 'precision': 0.9074753718369712, 'recall': 0.9104651162790698, 'f1_score': 0.9089677856244558}
MLP_652929: {'accuracy': 0.9543121387283237, 'precision': 0.8838792786161207, 'recall': 0.9308139534883721, 'f1_score': 0.9067396639607325}
MLP_160673: {'accuracy': 0.9504739884393063, 'precision': 0.9180126763443058, 'recall': 0.8701550387596899, 'f1_score': 0.8934434384638344}
Logistic Regression: {'accuracy': 0.9325, 'precision': 0.8196, 'recall': 0.9194, 'f1_score': 0.8666}
XGBoost: {'accuracy': 0.9215, 'precision': 0.8089, 'recall': 0.8787, 'f1_score': 0.8424}
Decision Tree: {'accuracy': 0.8718, 'precision': 0.7468, 'recall': 0.7002, 'f1_score': 0.7227}
Random Forest: {'accuracy': 0.8442, 'precision': 0.644, 'recall': 0.7758, 'f1_score': 0.7038}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7822, 'precision': 0.5255, 'recall': 0.8955, 'f1_score': 0.6624}


EMBEDDINGS TYPE: LIVENESS
MLP_5839873: {'accuracy': 0.9605086705202313, 'precision': 0.9141977683724509, 'recall': 0.9209302325581395, 'f1_score': 0.9175516508978567}
MLP_2742785: {'accuracy': 0.957364161849711, 'precision': 0.9275625504439063, 'recall': 0.890891472868217, 'f1_score': 0.9088572558323448}
MLP_652929: {'accuracy': 0.9559768786127167, 'precision': 0.9009146341463414, 'recall': 0.9162790697674419, 'f1_score': 0.9085318985395849}
MLP_160673: {'accuracy': 0.9486242774566473, 'precision': 0.9117347976408379, 'recall': 0.868798449612403, 'f1_score': 0.8897489332142503}
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.8276, 'recall': 0.9213, 'f1_score': 0.872}
XGBoost: {'accuracy': 0.9211, 'precision': 0.8033, 'recall': 0.8864, 'f1_score': 0.8428}
Decision Tree: {'accuracy': 0.8727, 'precision': 0.7204, 'recall': 0.7624, 'f1_score': 0.7408}
Random Forest: {'accuracy': 0.8602, 'precision': 0.6801, 'recall': 0.782, 'f1_score': 0.7275}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7592, 'precision': 0.4975, 'recall': 0.9169, 'f1_score': 0.645}


EMBEDDINGS TYPE: ACOUSTICNESS
MLP_5839873: {'accuracy': 0.9579190751445087, 'precision': 0.9108662026295437, 'recall': 0.912984496124031, 'f1_score': 0.9119241192411924}
MLP_2742785: {'accuracy': 0.9562080924855492, 'precision': 0.9237577952122309, 'recall': 0.889922480620155, 'f1_score': 0.9065245286743658}
MLP_652929: {'accuracy': 0.952092485549133, 'precision': 0.8924628854206319, 'recall': 0.9087209302325582, 'f1_score': 0.9005185327443825}
MLP_160673: {'accuracy': 0.9499653179190751, 'precision': 0.880410447761194, 'recall': 0.9145348837209303, 'f1_score': 0.8971482889733841}
Logistic Regression: {'accuracy': 0.9359, 'precision': 0.8292, 'recall': 0.9209, 'f1_score': 0.8726}
XGBoost: {'accuracy': 0.9207, 'precision': 0.8028, 'recall': 0.8853, 'f1_score': 0.842}
Decision Tree: {'accuracy': 0.8743, 'precision': 0.7267, 'recall': 0.7585, 'f1_score': 0.7423}
Random Forest: {'accuracy': 0.8664, 'precision': 0.6945, 'recall': 0.7855, 'f1_score': 0.7372}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.8034, 'precision': 0.5547, 'recall': 0.8926, 'f1_score': 0.6842}


EMBEDDINGS TYPE: INSTRUMENTALNESS
MLP_2742785: {'accuracy': 0.9578265895953757, 'precision': 0.9280532043530835, 'recall': 0.8924418604651163, 'f1_score': 0.9098992294013041}
MLP_5839873: {'accuracy': 0.9544508670520231, 'precision': 0.9141043443761159, 'recall': 0.8930232558139535, 'f1_score': 0.9034408391334183}
MLP_652929: {'accuracy': 0.9522774566473988, 'precision': 0.8898753305629014, 'recall': 0.912984496124031, 'f1_score': 0.9012818060072699}
MLP_160673: {'accuracy': 0.9476994219653179, 'precision': 0.8811731315042574, 'recall': 0.9025193798449612, 'f1_score': 0.8917185256103399}
Logistic Regression: {'accuracy': 0.9347, 'precision': 0.8259, 'recall': 0.9202, 'f1_score': 0.8705}
XGBoost: {'accuracy': 0.9194, 'precision': 0.7991, 'recall': 0.8843, 'f1_score': 0.8396}
Decision Tree: {'accuracy': 0.872, 'precision': 0.7176, 'recall': 0.7641, 'f1_score': 0.7401}
Random Forest: {'accuracy': 0.8617, 'precision': 0.6844, 'recall': 0.78, 'f1_score': 0.7291}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7654, 'precision': 0.5046, 'recall': 0.9256, 'f1_score': 0.6531}


EMBEDDINGS TYPE: GOOD FOR PARTY
MLP_5839873: {'accuracy': 0.96078612716763, 'precision': 0.918966187329965, 'recall': 0.9164728682170543, 'f1_score': 0.9177178342712983}
MLP_2742785: {'accuracy': 0.9589826589595376, 'precision': 0.9233207846245294, 'recall': 0.9031007751937985, 'f1_score': 0.9130988537278338}
MLP_652929: {'accuracy': 0.9558381502890173, 'precision': 0.9144490439582101, 'recall': 0.899031007751938, 'f1_score': 0.9066744845108962}
MLP_160673: {'accuracy': 0.9487630057803468, 'precision': 0.8903660886319846, 'recall': 0.8955426356589147, 'f1_score': 0.8929468599033816}
Logistic Regression: {'accuracy': 0.9356, 'precision': 0.8281, 'recall': 0.9213, 'f1_score': 0.8722}
XGBoost: {'accuracy': 0.9188, 'precision': 0.7976, 'recall': 0.8843, 'f1_score': 0.8387}
Decision Tree: {'accuracy': 0.8724, 'precision': 0.7192, 'recall': 0.7634, 'f1_score': 0.7406}
Random Forest: {'accuracy': 0.8615, 'precision': 0.6834, 'recall': 0.7818, 'f1_score': 0.7293}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7924, 'precision': 0.5373, 'recall': 0.9343, 'f1_score': 0.6823}


EMBEDDINGS TYPE: GOOD FOR WORK/STUDY
MLP_652929: {'accuracy': 0.9581040462427746, 'precision': 0.8989122280570142, 'recall': 0.9288759689922481, 'f1_score': 0.9136484940907358}
MLP_2742785: {'accuracy': 0.9550057803468208, 'precision': 0.8841989355845109, 'recall': 0.9337209302325581, 'f1_score': 0.9082854180412857}
MLP_5839873: {'accuracy': 0.9508901734104046, 'precision': 0.8597261235955056, 'recall': 0.9490310077519379, 'f1_score': 0.9021739130434783}
MLP_160673: {'accuracy': 0.950936416184971, 'precision': 0.8916491496273647, 'recall': 0.9042635658914728, 'f1_score': 0.8979120561916675}
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.828, 'recall': 0.9207, 'f1_score': 0.8719}
XGBoost: {'accuracy': 0.9195, 'precision': 0.8, 'recall': 0.8837, 'f1_score': 0.8398}
Decision Tree: {'accuracy': 0.8723, 'precision': 0.7188, 'recall': 0.7634, 'f1_score': 0.7404}
Random Forest: {'accuracy': 0.8641, 'precision': 0.6904, 'recall': 0.7802, 'f1_score': 0.7326}
Naive Bayes: {'accuracy': 0.787, 'precision': 0.5293, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7954, 'precision': 0.543, 'recall': 0.9004, 'f1_score': 0.6775}


EMBEDDINGS TYPE: GOOD FOR RELAXATION/MEDITATION
MLP_5839873: {'accuracy': 0.959028901734104, 'precision': 0.9007876969242311, 'recall': 0.9308139534883721, 'f1_score': 0.9155547083492185}
MLP_2742785: {'accuracy': 0.9579190751445087, 'precision': 0.9013978088401965, 'recall': 0.9248062015503876, 'f1_score': 0.9129519801033097}
MLP_652929: {'accuracy': 0.955606936416185, 'precision': 0.9245855236554792, 'recall': 0.8862403100775194, 'f1_score': 0.9050069265782703}
MLP_160673: {'accuracy': 0.9470520231213873, 'precision': 0.8645360450335936, 'recall': 0.9226744186046512, 'f1_score': 0.8926596043873629}
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.828, 'recall': 0.9207, 'f1_score': 0.8719}
XGBoost: {'accuracy': 0.9212, 'precision': 0.8029, 'recall': 0.888, 'f1_score': 0.8433}
Decision Tree: {'accuracy': 0.8728, 'precision': 0.7202, 'recall': 0.7638, 'f1_score': 0.7413}
Random Forest: {'accuracy': 0.8623, 'precision': 0.6855, 'recall': 0.7818, 'f1_score': 0.7305}
SVM: {'accuracy': 0.8334, 'precision': 0.604, 'recall': 0.8767, 'f1_score': 0.7153}
Naive Bayes: {'accuracy': 0.787, 'precision': 0.5293, 'recall': 0.9686, 'f1_score': 0.6846}


EMBEDDINGS TYPE: GOOD FOR EXERCISE
MLP_2742785: {'accuracy': 0.9571329479768786, 'precision': 0.8986626483330195, 'recall': 0.9246124031007752, 'f1_score': 0.9114528608272041}
MLP_5839873: {'accuracy': 0.9549595375722544, 'precision': 0.877661494045471, 'recall': 0.9426356589147287, 'f1_score': 0.908988974023547}
MLP_652929: {'accuracy': 0.9559768786127167, 'precision': 0.9038387715930902, 'recall': 0.9125968992248062, 'f1_score': 0.9081967213114754}
MLP_160673: {'accuracy': 0.9487630057803468, 'precision': 0.8774217585692996, 'recall': 0.9127906976744186, 'f1_score': 0.8947568389057751}
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.8278, 'recall': 0.9205, 'f1_score': 0.8717}
XGBoost: {'accuracy': 0.9207, 'precision': 0.8027, 'recall': 0.8855, 'f1_score': 0.8421}
Decision Tree: {'accuracy': 0.8725, 'precision': 0.7197, 'recall': 0.7624, 'f1_score': 0.7404}
Random Forest: {'accuracy': 0.8646, 'precision': 0.6908, 'recall': 0.7829, 'f1_score': 0.734}
Naive Bayes: {'accuracy': 0.7867, 'precision': 0.5289, 'recall': 0.9686, 'f1_score': 0.6842}
SVM: {'accuracy': 0.728, 'precision': 0.4649, 'recall': 0.9287, 'f1_score': 0.6196}


EMBEDDINGS TYPE: GOOD FOR RUNNING
MLP_5839873: {'accuracy': 0.9612485549132948, 'precision': 0.9311652035115723, 'recall': 0.9044573643410853, 'f1_score': 0.9176169878096736}
MLP_2742785: {'accuracy': 0.9579653179190751, 'precision': 0.8960313024035774, 'recall': 0.9319767441860465, 'f1_score': 0.9136506127101739}
MLP_652929: {'accuracy': 0.9546820809248555, 'precision': 0.9185022026431718, 'recall': 0.888953488372093, 'f1_score': 0.9034863108134725}
MLP_160673: {'accuracy': 0.9508439306358382, 'precision': 0.8989289191820837, 'recall': 0.8945736434108527, 'f1_score': 0.8967459932005828}
Logistic Regression: {'accuracy': 0.9356, 'precision': 0.8289, 'recall': 0.9202, 'f1_score': 0.8722}
XGBoost: {'accuracy': 0.9207, 'precision': 0.8014, 'recall': 0.8876, 'f1_score': 0.8423}
Decision Tree: {'accuracy': 0.8739, 'precision': 0.724, 'recall': 0.762, 'f1_score': 0.7425}
Random Forest: {'accuracy': 0.863, 'precision': 0.6869, 'recall': 0.7824, 'f1_score': 0.7315}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7621, 'precision': 0.5009, 'recall': 0.8742, 'f1_score': 0.6369}


EMBEDDINGS TYPE: GOOD FOR YOGA/STRETCHING
MLP_2742785: {'accuracy': 0.9575491329479768, 'precision': 0.9391304347826087, 'recall': 0.8790697674418605, 'f1_score': 0.9081081081081082}
MLP_5839873: {'accuracy': 0.9541271676300578, 'precision': 0.8732091690544412, 'recall': 0.9449612403100776, 'f1_score': 0.9076693968726731}
MLP_652929: {'accuracy': 0.954728323699422, 'precision': 0.8909668973256032, 'recall': 0.9232558139534883, 'f1_score': 0.9068240220805177}
MLP_160673: {'accuracy': 0.9504739884393063, 'precision': 0.8993944129712834, 'recall': 0.8922480620155039, 'f1_score': 0.8958069851152836}
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.8277, 'recall': 0.9207, 'f1_score': 0.8717}
XGBoost: {'accuracy': 0.9207, 'precision': 0.8028, 'recall': 0.8853, 'f1_score': 0.842}
Decision Tree: {'accuracy': 0.8732, 'precision': 0.7215, 'recall': 0.763, 'f1_score': 0.7416}
Random Forest: {'accuracy': 0.8605, 'precision': 0.6821, 'recall': 0.7781, 'f1_score': 0.727}
Naive Bayes: {'accuracy': 0.787, 'precision': 0.5293, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7742, 'precision': 0.5154, 'recall': 0.8986, 'f1_score': 0.6551}


EMBEDDINGS TYPE: GOOD FOR DRIVING
MLP_2742785: {'accuracy': 0.958150289017341, 'precision': 0.9147982062780269, 'recall': 0.9093023255813953, 'f1_score': 0.9120419865876178}
MLP_5839873: {'accuracy': 0.9570867052023121, 'precision': 0.9066102997694081, 'recall': 0.9143410852713179, 'f1_score': 0.9104592821304516}
MLP_652929: {'accuracy': 0.9569479768786128, 'precision': 0.9076537497590129, 'recall': 0.9124031007751938, 'f1_score': 0.9100222286653136}
MLP_160673: {'accuracy': 0.9488092485549133, 'precision': 0.8871060171919771, 'recall': 0.9, 'f1_score': 0.8935064935064935}
Logistic Regression: {'accuracy': 0.9353, 'precision': 0.8276, 'recall': 0.9203, 'f1_score': 0.8715}
XGBoost: {'accuracy': 0.9207, 'precision': 0.8028, 'recall': 0.8851, 'f1_score': 0.8419}
Decision Tree: {'accuracy': 0.8726, 'precision': 0.7198, 'recall': 0.7634, 'f1_score': 0.741}
Random Forest: {'accuracy': 0.8566, 'precision': 0.6718, 'recall': 0.78, 'f1_score': 0.7219}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7939, 'precision': 0.5406, 'recall': 0.9062, 'f1_score': 0.6772}


EMBEDDINGS TYPE: GOOD FOR SOCIAL GATHERINGS
MLP_5839873: {'accuracy': 0.9566705202312139, 'precision': 0.8965258215962442, 'recall': 0.9251937984496124, 'f1_score': 0.9106342393896042}
MLP_652929: {'accuracy': 0.9570404624277457, 'precision': 0.9050354202565576, 'recall': 0.9160852713178295, 'f1_score': 0.9105268226909371}
MLP_2742785: {'accuracy': 0.9565780346820809, 'precision': 0.9240506329113924, 'recall': 0.8912790697674419, 'f1_score': 0.9073690440958864}
MLP_160673: {'accuracy': 0.9481156069364162, 'precision': 0.880943396226415, 'recall': 0.9048449612403101, 'f1_score': 0.8927342256214149}
Logistic Regression: {'accuracy': 0.9357, 'precision': 0.8289, 'recall': 0.9203, 'f1_score': 0.8723}
XGBoost: {'accuracy': 0.9201, 'precision': 0.8011, 'recall': 0.8849, 'f1_score': 0.8409}
Decision Tree: {'accuracy': 0.8725, 'precision': 0.7195, 'recall': 0.763, 'f1_score': 0.7406}
Random Forest: {'accuracy': 0.8576, 'precision': 0.6744, 'recall': 0.7797, 'f1_score': 0.7232}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7047, 'precision': 0.4444, 'recall': 0.9484, 'f1_score': 0.6052}


EMBEDDINGS TYPE: GOOD FOR MORNING ROUTINE
MLP_652929: {'accuracy': 0.9575028901734104, 'precision': 0.9021429926038308, 'recall': 0.9218992248062016, 'f1_score': 0.9119141186619381}
MLP_2742785: {'accuracy': 0.956393063583815, 'precision': 0.8944808231992516, 'recall': 0.9265503875968992, 'f1_score': 0.9102332222751071}
MLP_5839873: {'accuracy': 0.9556994219653179, 'precision': 0.8985204855842185, 'recall': 0.9180232558139535, 'f1_score': 0.9081671779141104}
MLP_160673: {'accuracy': 0.9501040462427746, 'precision': 0.890675856787287, 'recall': 0.9015503875968992, 'f1_score': 0.8960801309833382}
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.8277, 'recall': 0.9209, 'f1_score': 0.8718}
XGBoost: {'accuracy': 0.9211, 'precision': 0.8037, 'recall': 0.8857, 'f1_score': 0.8427}
Decision Tree: {'accuracy': 0.8727, 'precision': 0.7201, 'recall': 0.7632, 'f1_score': 0.741}
Random Forest: {'accuracy': 0.8595, 'precision': 0.6789, 'recall': 0.7802, 'f1_score': 0.7261}
Naive Bayes: {'accuracy': 0.7871, 'precision': 0.5294, 'recall': 0.9686, 'f1_score': 0.6846}
SVM: {'accuracy': 0.7639, 'precision': 0.503, 'recall': 0.8882, 'f1_score': 0.6422}

 Mejores modelos por embedding:
                         embedding   best_model  ...    recall  f1_score
0                          emotion   MLP_653057  ...  0.930620  0.910850
1                   Time signature  MLP_5840897  ...  0.895155  0.912846
2                        Artist(s)  MLP_5840897  ...  0.938566  0.912053
3                             song  MLP_2743297  ...  0.902907  0.913440
4                            Genre  MLP_5840897  ...  0.911434  0.913736
5                            Album  MLP_5840897  ...  0.941860  0.913191
6                     Release Date   MLP_653057  ...  0.924031  0.910967
7                              Key  MLP_5840897  ...  0.925581  0.909022
8                 Similar Artist 1  MLP_5840897  ...  0.904264  0.897653
9                   Similar Song 1   MLP_653057  ...  0.887791  0.905873
10                Similar Artist 2  MLP_5840897  ...  0.900000  0.900000
11                  Similar Song 2   MLP_653057  ...  0.904651  0.908967
12                Similar Artist 3  MLP_5840897  ...  0.917054  0.892073
13                  Similar Song 3  MLP_2743297  ...  0.914341  0.905045
14                 song_normalized  MLP_2743297  ...  0.913178  0.918071
15               artist_normalized  MLP_2743297  ...  0.897674  0.914692
16                           Tempo   MLP_652929  ...  0.918992  0.917126
17                          Length  MLP_5839873  ...  0.919961  0.917738
18                   Loudness (db)  MLP_5839873  ...  0.929651  0.911977
19                      Popularity  MLP_2742785  ...  0.903295  0.915717
20                          Energy   MLP_652929  ...  0.911434  0.914180
21                    Danceability  MLP_2742785  ...  0.930426  0.910574
22                    Positiveness  MLP_5839873  ...  0.918023  0.917223
23                     Speechiness  MLP_2742785  ...  0.924225  0.911942
24                        Liveness  MLP_5839873  ...  0.920930  0.917552
25                    Acousticness  MLP_5839873  ...  0.912984  0.911924
26                Instrumentalness  MLP_2742785  ...  0.892442  0.909899
27                  Good for Party  MLP_5839873  ...  0.916473  0.917718
28             Good for Work/Study   MLP_652929  ...  0.928876  0.913648
29  Good for Relaxation/Meditation  MLP_5839873  ...  0.930814  0.915555
30               Good for Exercise  MLP_2742785  ...  0.924612  0.911453
31                Good for Running  MLP_5839873  ...  0.904457  0.917617
32        Good for Yoga/Stretching  MLP_2742785  ...  0.879070  0.908108
33                Good for Driving  MLP_2742785  ...  0.909302  0.912042
34      Good for Social Gatherings  MLP_5839873  ...  0.925194  0.910634
35        Good for Morning Routine   MLP_652929  ...  0.921899  0.911914

[36 rows x 7 columns]

 Mejor global: song_normalized con MLP_2743297 (f1=0.9181)

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['emotion', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Release Date', 'Key', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
====================================

