2025-10-23 01:35:57.638387: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-23 01:35:57.638384: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_numerical_categorical_metadata.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_numerical_categorical_metadata.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 15 ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']

CAT_LOW 3 ['emotion', 'Key', 'Time signature']
CAT_HIGH 12 ['Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
--> PaTH:  ../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy
Contaning the categorical cols
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 5023)
Shape of X_test after concatenation:  (21625, 5023)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.2968, Test Loss: 0.1784, F1: 0.8674, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0855, Test Loss: 0.1916, F1: 0.8691, AUC: 0.9827
Epoch [20/30] Train Loss: 0.0523, Test Loss: 0.2372, F1: 0.8665, AUC: 0.9813
Mejores resultados en la época:  2
f1-score 0.879653520088463
AUC según el mejor F1-score 0.9841003232132054
Confusion Matrix:
 [[15546   919]
 [  387  4773]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_160801.png
Accuracy:   0.9396
Precision:  0.8385
Recall:     0.9250
F1-score:   0.8797

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.2928, Test Loss: 0.1837, F1: 0.8649, AUC: 0.9798
Epoch [10/30] Train Loss: 0.0858, Test Loss: 0.1876, F1: 0.8735, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0467, Test Loss: 0.2502, F1: 0.8651, AUC: 0.9819
Mejores resultados en la época:  18
f1-score 0.8798449612403101
AUC según el mejor F1-score 0.9820272388929301
Confusion Matrix:
 [[15556   909]
 [  393  4767]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_160801.png
Accuracy:   0.9398
Precision:  0.8399
Recall:     0.9238
F1-score:   0.8798

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.2935, Test Loss: 0.1957, F1: 0.8572, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0765, Test Loss: 0.1960, F1: 0.8709, AUC: 0.9833
Epoch [20/30] Train Loss: 0.0493, Test Loss: 0.2391, F1: 0.8711, AUC: 0.9811
Mejores resultados en la época:  1
f1-score 0.8796132750766943
AUC según el mejor F1-score 0.9833626179092602
Confusion Matrix:
 [[15599   866]
 [  429  4731]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_160801.png
Accuracy:   0.9401
Precision:  0.8453
Recall:     0.9169
F1-score:   0.8796

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.2951, Test Loss: 0.2014, F1: 0.8572, AUC: 0.9800
Epoch [10/30] Train Loss: 0.0885, Test Loss: 0.2026, F1: 0.8664, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0758, Test Loss: 0.2314, F1: 0.8671, AUC: 0.9803
Mejores resultados en la época:  2
f1-score 0.8866635470285447
AUC según el mejor F1-score 0.9841148831088733
Confusion Matrix:
 [[15677   788]
 [  423  4737]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_160801.png
Accuracy:   0.9440
Precision:  0.8574
Recall:     0.9180
F1-score:   0.8867
Tiempo total para red 1: 1475.47 segundos

Entrenando red 2 con capas [5023, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.2525, Test Loss: 0.1948, F1: 0.8545, AUC: 0.9824
Epoch [10/30] Train Loss: 0.0189, Test Loss: 0.2783, F1: 0.8717, AUC: 0.9833
Epoch [20/30] Train Loss: 0.0025, Test Loss: 0.4030, F1: 0.8894, AUC: 0.9835
Mejores resultados en la época:  20
f1-score 0.889362490733877
AUC según el mejor F1-score 0.9834794619547689
Confusion Matrix:
 [[15632   833]
 [  361  4799]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_323649.png
Accuracy:   0.9448
Precision:  0.8521
Recall:     0.9300
F1-score:   0.8894

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.2432, Test Loss: 0.1790, F1: 0.8661, AUC: 0.9824
Epoch [10/30] Train Loss: 0.0074, Test Loss: 0.3353, F1: 0.8745, AUC: 0.9840
Epoch [20/30] Train Loss: 0.0002, Test Loss: 0.5319, F1: 0.8782, AUC: 0.9823
Mejores resultados en la época:  14
f1-score 0.8895688456189151
AUC según el mejor F1-score 0.9829983556851861
Confusion Matrix:
 [[15637   828]
 [  363  4797]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_323649.png
Accuracy:   0.9449
Precision:  0.8528
Recall:     0.9297
F1-score:   0.8896

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.2455, Test Loss: 0.1526, F1: 0.8824, AUC: 0.9823
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.3424, F1: 0.8814, AUC: 0.9846
Epoch [20/30] Train Loss: 0.0001, Test Loss: 0.5304, F1: 0.8811, AUC: 0.9832
Mejores resultados en la época:  3
f1-score 0.892983293556086
AUC según el mejor F1-score 0.9848538654934003
Confusion Matrix:
 [[15827   638]
 [  483  4677]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_323649.png
Accuracy:   0.9482
Precision:  0.8800
Recall:     0.9064
F1-score:   0.8930

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.2416, Test Loss: 0.1654, F1: 0.8755, AUC: 0.9826
Epoch [10/30] Train Loss: 0.0108, Test Loss: 0.3021, F1: 0.8791, AUC: 0.9833
Epoch [20/30] Train Loss: 0.0004, Test Loss: 0.4923, F1: 0.8835, AUC: 0.9829
Mejores resultados en la época:  14
f1-score 0.8892174073731868
AUC según el mejor F1-score 0.9833352930929361
Confusion Matrix:
 [[15614   851]
 [  348  4812]]
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 15 ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']

CAT_LOW 3 ['emotion', 'Key', 'Time signature']
CAT_HIGH 12 ['Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
--> PaTH:  ../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy
Contaning the categorical cols
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 5023)
Shape of X_test after concatenation:  (21625, 5023)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.2952, Test Loss: 0.2022, F1: 0.8543, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0882, Test Loss: 0.1919, F1: 0.8722, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0747, Test Loss: 0.2604, F1: 0.8581, AUC: 0.9809
Mejores resultados en la época:  4
f1-score 0.8846261813418171
AUC según el mejor F1-score 0.9839676951579224
Confusion Matrix:
 [[15665   800]
 [  433  4727]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_160801.png
Accuracy:   0.9430
Precision:  0.8553
Recall:     0.9161
F1-score:   0.8846

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.2974, Test Loss: 0.1898, F1: 0.8607, AUC: 0.9791
Epoch [10/30] Train Loss: 0.0867, Test Loss: 0.1888, F1: 0.8699, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0586, Test Loss: 0.2344, F1: 0.8680, AUC: 0.9814
Mejores resultados en la época:  3
f1-score 0.8806149856441604
AUC según el mejor F1-score 0.9841332389353032
Confusion Matrix:
 [[15582   883]
 [  406  4754]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_160801.png
Accuracy:   0.9404
Precision:  0.8434
Recall:     0.9213
F1-score:   0.8806

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.2944, Test Loss: 0.1959, F1: 0.8579, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0768, Test Loss: 0.1895, F1: 0.8696, AUC: 0.9834
Epoch [20/30] Train Loss: 0.0364, Test Loss: 0.2454, F1: 0.8688, AUC: 0.9824
Mejores resultados en la época:  2
f1-score 0.8844859813084112
AUC según el mejor F1-score 0.9840612162986085
Confusion Matrix:
 [[15657   808]
 [  428  4732]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_160801.png
Accuracy:   0.9428
Precision:  0.8542
Recall:     0.9171
F1-score:   0.8845

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.2910, Test Loss: 0.1768, F1: 0.8696, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0882, Test Loss: 0.2114, F1: 0.8599, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0678, Test Loss: 0.2495, F1: 0.8622, AUC: 0.9810
Mejores resultados en la época:  3
f1-score 0.8798599594619495
AUC según el mejor F1-score 0.9844225182852044
Confusion Matrix:
 [[15546   919]
 [  385  4775]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_160801.png
Accuracy:   0.9397
Precision:  0.8386
Recall:     0.9254
F1-score:   0.8799
Tiempo total para red 1: 1487.40 segundos

Entrenando red 2 con capas [5023, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.2424, Test Loss: 0.1907, F1: 0.8581, AUC: 0.9823
Epoch [10/30] Train Loss: 0.0080, Test Loss: 0.3349, F1: 0.8699, AUC: 0.9837
Epoch [20/30] Train Loss: 0.0026, Test Loss: 0.4826, F1: 0.8831, AUC: 0.9830
Mejores resultados en la época:  12
f1-score 0.8906627065156859
AUC según el mejor F1-score 0.9837118141135649
Confusion Matrix:
 [[15649   816]
 [  362  4798]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_323649.png
Accuracy:   0.9455
Precision:  0.8546
Recall:     0.9298
F1-score:   0.8907

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.2454, Test Loss: 0.1638, F1: 0.8747, AUC: 0.9823
Epoch [10/30] Train Loss: 0.0183, Test Loss: 0.2770, F1: 0.8759, AUC: 0.9831
Epoch [20/30] Train Loss: 0.0012, Test Loss: 0.4741, F1: 0.8855, AUC: 0.9825
Mejores resultados en la época:  17
f1-score 0.8892687559354227
AUC según el mejor F1-score 0.9823341207682729
Confusion Matrix:
 [[15777   688]
 [  478  4682]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_323649.png
Accuracy:   0.9461
Precision:  0.8719
Recall:     0.9074
F1-score:   0.8893

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.2415, Test Loss: 0.1883, F1: 0.8553, AUC: 0.9819
Epoch [10/30] Train Loss: 0.0058, Test Loss: 0.3025, F1: 0.8898, AUC: 0.9840
Epoch [20/30] Train Loss: 0.0019, Test Loss: 0.5313, F1: 0.8726, AUC: 0.9831
Mejores resultados en la época:  15
f1-score 0.8907048008171604
AUC según el mejor F1-score 0.9838833077917216
Confusion Matrix:
 [[15652   813]
 [  364  4796]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_323649.png
Accuracy:   0.9456
Precision:  0.8551
Recall:     0.9295
F1-score:   0.8907

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.2457, Test Loss: 0.1676, F1: 0.8743, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0047, Test Loss: 0.3201, F1: 0.8869, AUC: 0.9844
Epoch [20/30] Train Loss: 0.0002, Test Loss: 0.4920, F1: 0.8831, AUC: 0.9836
Mejores resultados en la época:  12
f1-score 0.891566265060241
AUC según el mejor F1-score 0.9838865858280542
Confusion Matrix:
 [[15645   820]
 [  350  4810]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_323649.png
Accuracy:   0.9459
Precision:  0.8544
Recall:     0.9322
F1-score:   0.8916
Tiempo total para red 2: 1515.70 segundos

Entrenando red 3 con capas [5023, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.2323, Test Loss: 0.1552, F1: 0.8791, AUC: 0.9822
Epoch [10/30] Train Loss: 0.0025, Test Loss: 0.4642, F1: 0.8741, AUC: 0.9834
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5388, F1: 0.8881, AUC: 0.9839
Mejores resultados en la época:  11
f1-score 0.8916820702402958
AUC según el mejor F1-score 0.9833371822305713
Confusion Matrix:
 [[15629   836]
 [  336  4824]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_653441.png
Accuracy:   0.9458
Precision:  0.8523
Recall:     0.9349
F1-score:   0.8917

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.2308, Test Loss: 0.2473, F1: 0.8195, AUC: 0.9826
Epoch [10/30] Train Loss: 0.0042, Test Loss: 0.4477, F1: 0.8674, AUC: 0.9836
Epoch [20/30] Train Loss: 0.0011, Test Loss: 0.4598, F1: 0.8858, AUC: 0.9842
Mejores resultados en la época:  19
f1-score 0.8958177744585512
AUC según el mejor F1-score 0.9840603629498326
Confusion Matrix:
 [[15711   754]
 [  362  4798]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_653441.png
Accuracy:   0.9484
Precision:  0.8642
Recall:     0.9298
F1-score:   0.8958

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.2346, Test Loss: 0.1909, F1: 0.8573, AUC: 0.9822
Epoch [10/30] Train Loss: 0.0025, Test Loss: 0.4897, F1: 0.8923, AUC: 0.9839
Epoch [20/30] Train Loss: 0.0004, Test Loss: 0.4668, F1: 0.8915, AUC: 0.9845
Mejores resultados en la época:  6
f1-score 0.8960414354421014
AUC según el mejor F1-score 0.9850308971108552
Confusion Matrix:
 [[15657   808]
 [  316  4844]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_653441.png
Accuracy:   0.9480
Precision:  0.8570
Recall:     0.9388
F1-score:   0.8960

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.2354, Test Loss: 0.1864, F1: 0.8573, AUC: 0.9821
Epoch [10/30] Train Loss: 0.0048, Test Loss: 0.4001, F1: 0.8922, AUC: 0.9837
Epoch [20/30] Train Loss: 0.0002, Test Loss: 0.5475, F1: 0.8867, AUC: 0.9833
Mejores resultados en la época:  13
f1-score 0.8925604578179804
AUC según el mejor F1-score 0.9842264010809869
Confusion Matrix:
 [[15626   839]
 [  325  4835]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_653441.png
Accuracy:   0.9462
Precision:  0.8521
Recall:     0.9370
F1-score:   0.8926
Tiempo total para red 3: 1413.72 segundos

Entrenando red 4 con capas [5023, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.2272, Test Loss: 0.1495, F1: 0.8842, AUC: 0.9822
Epoch [10/30] Train Loss: 0.0033, Test Loss: 0.2877, F1: 0.9027, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0013, Test Loss: 0.3553, F1: 0.8992, AUC: 0.9869
Mejores resultados en la época:  12
f1-score 0.9089875935221139
AUC según el mejor F1-score 0.9868833054376559
Confusion Matrix:
 [[15865   600]
 [  361  4799]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_1329409.png
Accuracy:   0.9556
Precision:  0.8889
Recall:     0.9300
F1-score:   0.9090

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.2293, Test Loss: 0.1465, F1: 0.8822, AUC: 0.9818
Epoch [10/30] Train Loss: 0.0017, Test Loss: 0.4804, F1: 0.8746, AUC: 0.9853
Epoch [20/30] Train Loss: 0.0001, Test Loss: 0.4615, F1: 0.8944, AUC: 0.9867
Mejores resultados en la época:  15
f1-score 0.9031594690596373
AUC según el mejor F1-score 0.9873387053110073
Confusion Matrix:
 [[15758   707]
 [  329  4831]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_1329409.png
Accuracy:   0.9521
Precision:  0.8723
Recall:     0.9362
F1-score:   0.9032

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.2316, Test Loss: 0.1584, F1: 0.8758, AUC: 0.9820
Epoch [10/30] Train Loss: 0.0014, Test Loss: 0.4513, F1: 0.8945, AUC: 0.9854
Epoch [20/30] Train Loss: 0.0002, Test Loss: 0.5488, F1: 0.8712, AUC: 0.9852
Mejores resultados en la época:  21
f1-score 0.9072086617912433
AUC según el mejor F1-score 0.9866121700482817
Confusion Matrix:
 [[15872   593]
 [  384  4776]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_1329409.png
Accuracy:   0.9548
Precision:  0.8896
Recall:     0.9256
F1-score:   0.9072

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.2316, Test Loss: 0.2000, F1: 0.8533, AUC: 0.9827
Epoch [10/30] Train Loss: 0.0035, Test Loss: 0.3575, F1: 0.9026, AUC: 0.9857
Epoch [20/30] Train Loss: 0.0014, Test Loss: 0.4235, F1: 0.8826, AUC: 0.9866
Mejores resultados en la época:  10
f1-score 0.9026096822995462
AUC según el mejor F1-score 0.985719802635141
Confusion Matrix:
 [[15822   643]
 [  387  4773]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_1329409.png
Accuracy:   0.9524
Precision:  0.8813
Recall:     0.9250
F1-score:   0.9026
Tiempo total para red 4: 1425.68 segundos

Entrenando red 5 con capas [5023, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.2233, Test Loss: 0.1827, F1: 0.8640, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.2687, F1: 0.8850, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5224, F1: 0.8980, AUC: 0.9865
Mejores resultados en la época:  15
f1-score 0.9032678521552928
AUC según el mejor F1-score 0.9878031506813842
Confusion Matrix:
 [[15735   730]
 [  309  4851]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_2744833.png
Accuracy:   0.9520
Precision:  0.8692
Recall:     0.9401
F1-score:   0.9033

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.2225, Test Loss: 0.1882, F1: 0.8589, AUC: 0.9832
Epoch [10/30] Train Loss: 0.0043, Test Loss: 0.2933, F1: 0.9028, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6080, F1: 0.9024, AUC: 0.9849
Mejores resultados en la época:  17
f1-score 0.909628217349857
AUC según el mejor F1-score 0.9860765083086745
Confusion Matrix:
 [[15906   559]
 [  389  4771]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_2744833.png
Accuracy:   0.9562
Precision:  0.8951
Recall:     0.9246
F1-score:   0.9096

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.2228, Test Loss: 0.1614, F1: 0.8720, AUC: 0.9832
Epoch [10/30] Train Loss: 0.0037, Test Loss: 0.2765, F1: 0.8961, AUC: 0.9874
Epoch [20/30] Train Loss: 0.0007, Test Loss: 0.3795, F1: 0.9116, AUC: 0.9873
Mejores resultados en la época:  20
f1-score 0.911608497723824
AUC según el mejor F1-score 0.9872544944997258
Confusion Matrix:
 [[15887   578]
 [  354  4806]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_2744833.png
Accuracy:   0.9569
Precision:  0.8926
Recall:     0.9314
F1-score:   0.9116

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.2238, Test Loss: 0.1730, F1: 0.8657, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.3529, F1: 0.8825, AUC: 0.9879
Epoch [20/30] Train Loss: 0.0013, Test Loss: 0.3481, F1: 0.8880, AUC: 0.9885
Mejores resultados en la época:  21
f1-score 0.9104236078058068
AUC según el mejor F1-score 0.9884247005040054
Confusion Matrix:
 [[15902   563]
 [  378  4782]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_2744833.png
Accuracy:   0.9565
Precision:  0.8947
Recall:     0.9267
F1-score:   0.9104
Tiempo total para red 5: 1519.94 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.2252, Test Loss: 0.1701, F1: 0.8768, AUC: 0.9826
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.5420, F1: 0.9061, AUC: 0.9862
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_323649.png
Accuracy:   0.9446
Precision:  0.8497
Recall:     0.9326
F1-score:   0.8892
Tiempo total para red 2: 1511.04 segundos

Entrenando red 3 con capas [5023, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.2345, Test Loss: 0.1647, F1: 0.8744, AUC: 0.9827
Epoch [10/30] Train Loss: 0.0048, Test Loss: 0.3336, F1: 0.8957, AUC: 0.9845
Epoch [20/30] Train Loss: 0.0009, Test Loss: 0.5240, F1: 0.8883, AUC: 0.9837
Mejores resultados en la época:  16
f1-score 0.8982136211388165
AUC según el mejor F1-score 0.9847520403863492
Confusion Matrix:
 [[15704   761]
 [  333  4827]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_653441.png
Accuracy:   0.9494
Precision:  0.8638
Recall:     0.9355
F1-score:   0.8982

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.2329, Test Loss: 0.1572, F1: 0.8794, AUC: 0.9826
Epoch [10/30] Train Loss: 0.0050, Test Loss: 0.4289, F1: 0.8856, AUC: 0.9843
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.4834, F1: 0.8912, AUC: 0.9848
Mejores resultados en la época:  8
f1-score 0.9033652942282384
AUC según el mejor F1-score 0.9844889970974373
Confusion Matrix:
 [[15792   673]
 [  355  4805]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_653441.png
Accuracy:   0.9525
Precision:  0.8771
Recall:     0.9312
F1-score:   0.9034

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.2340, Test Loss: 0.1710, F1: 0.8716, AUC: 0.9825
Epoch [10/30] Train Loss: 0.0058, Test Loss: 0.4925, F1: 0.8767, AUC: 0.9829
Epoch [20/30] Train Loss: 0.0004, Test Loss: 0.4723, F1: 0.8897, AUC: 0.9837
Mejores resultados en la época:  25
f1-score 0.9029657356752088
AUC según el mejor F1-score 0.9839350736940232
Confusion Matrix:
 [[15910   555]
 [  456  4704]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_653441.png
Accuracy:   0.9532
Precision:  0.8945
Recall:     0.9116
F1-score:   0.9030

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.2337, Test Loss: 0.2349, F1: 0.8265, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0044, Test Loss: 0.3642, F1: 0.8934, AUC: 0.9838
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5628, F1: 0.8876, AUC: 0.9836
Mejores resultados en la época:  8
f1-score 0.8941966784847919
AUC según el mejor F1-score 0.9840154297229029
Confusion Matrix:
 [[15699   766]
 [  368  4792]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_653441.png
Accuracy:   0.9476
Precision:  0.8622
Recall:     0.9287
F1-score:   0.8942
Tiempo total para red 3: 1422.47 segundos

Entrenando red 4 con capas [5023, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.2289, Test Loss: 0.1667, F1: 0.8711, AUC: 0.9819
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.3777, F1: 0.8818, AUC: 0.9855
Epoch [20/30] Train Loss: 0.0021, Test Loss: 0.4941, F1: 0.8871, AUC: 0.9854
Mejores resultados en la época:  6
f1-score 0.905337265721625
AUC según el mejor F1-score 0.9858817505773346
Confusion Matrix:
 [[15872   593]
 [  402  4758]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_1329409.png
Accuracy:   0.9540
Precision:  0.8892
Recall:     0.9221
F1-score:   0.9053

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.2296, Test Loss: 0.2275, F1: 0.8351, AUC: 0.9824
Epoch [10/30] Train Loss: 0.0019, Test Loss: 0.3206, F1: 0.9043, AUC: 0.9857
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.9471, F1: 0.8906, AUC: 0.9792
Mejores resultados en la época:  10
f1-score 0.904304381245196
AUC según el mejor F1-score 0.9857310903796402
Confusion Matrix:
 [[15923   542]
 [  454  4706]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_1329409.png
Accuracy:   0.9539
Precision:  0.8967
Recall:     0.9120
F1-score:   0.9043

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.2307, Test Loss: 0.1982, F1: 0.8469, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0036, Test Loss: 0.4028, F1: 0.8869, AUC: 0.9862
Epoch [20/30] Train Loss: 0.0019, Test Loss: 0.3028, F1: 0.9051, AUC: 0.9871
Mejores resultados en la época:  16
f1-score 0.9070821529745042
AUC según el mejor F1-score 0.9872652349239757
Confusion Matrix:
 [[15838   627]
 [  357  4803]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_1329409.png
Accuracy:   0.9545
Precision:  0.8845
Recall:     0.9308
F1-score:   0.9071

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.2320, Test Loss: 0.1613, F1: 0.8788, AUC: 0.9827
Epoch [10/30] Train Loss: 0.0040, Test Loss: 0.4049, F1: 0.8883, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0009, Test Loss: 0.3732, F1: 0.8948, AUC: 0.9869
Mejores resultados en la época:  17
f1-score 0.9099195190536217
AUC según el mejor F1-score 0.9872261338945427
Confusion Matrix:
 [[16004   461]
 [  468  4692]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_1329409.png
Accuracy:   0.9570
Precision:  0.9105
Recall:     0.9093
F1-score:   0.9099
Tiempo total para red 4: 1438.41 segundos

Entrenando red 5 con capas [5023, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.2213, Test Loss: 0.1737, F1: 0.8681, AUC: 0.9834
Epoch [10/30] Train Loss: 0.0028, Test Loss: 0.3937, F1: 0.8751, AUC: 0.9874
Epoch [20/30] Train Loss: 0.0005, Test Loss: 0.3577, F1: 0.9091, AUC: 0.9876
Mejores resultados en la época:  20
f1-score 0.9091253671941628
AUC según el mejor F1-score 0.9876276256659062
Confusion Matrix:
 [[15869   596]
 [  363  4797]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_2744833.png
Accuracy:   0.9557
Precision:  0.8895
Recall:     0.9297
F1-score:   0.9091

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.2238, Test Loss: 0.1623, F1: 0.8707, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0024, Test Loss: 0.5613, F1: 0.8845, AUC: 0.9849
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8625, F1: 0.8970, AUC: 0.9799
Mejores resultados en la época:  7
f1-score 0.9110197840007646
AUC según el mejor F1-score 0.9870651334637485
Confusion Matrix:
 [[15928   537]
 [  394  4766]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_2744833.png
Accuracy:   0.9569
Precision:  0.8987
Recall:     0.9236
F1-score:   0.9110

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.2226, Test Loss: 0.1535, F1: 0.8768, AUC: 0.9827
Epoch [10/30] Train Loss: 0.0025, Test Loss: 0.4219, F1: 0.8862, AUC: 0.9870
Epoch [20/30] Train Loss: 0.0033, Test Loss: 0.3097, F1: 0.8999, AUC: 0.9880
Mejores resultados en la época:  9
f1-score 0.9090735980196134
AUC según el mejor F1-score 0.9865812023154589
Confusion Matrix:
 [[15896   569]
 [  386  4774]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_2744833.png
Accuracy:   0.9558
Precision:  0.8935
Recall:     0.9252
F1-score:   0.9091

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.2230, Test Loss: 0.1874, F1: 0.8617, AUC: 0.9825
Epoch [10/30] Train Loss: 0.0020, Test Loss: 0.3533, F1: 0.9078, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0029, Test Loss: 0.4087, F1: 0.9063, AUC: 0.9866
Mejores resultados en la época:  9
f1-score 0.9094495585196318
AUC según el mejor F1-score 0.9877341647892993
Confusion Matrix:
 [[15820   645]
 [  319  4841]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_2744833.png
Accuracy:   0.9554
Precision:  0.8824
Recall:     0.9382
F1-score:   0.9094
Tiempo total para red 5: 1545.32 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.2253, Test Loss: 0.1777, F1: 0.8690, AUC: 0.9834
Epoch [10/30] Train Loss: 0.0039, Test Loss: 0.6786, F1: 0.8647, AUC: 0.9827
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:31:38] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:31:47] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [20/30] Train Loss: 0.0016, Test Loss: 0.6705, F1: 0.8945, AUC: 0.9848
Mejores resultados en la época:  21
f1-score 0.9151846785225718
AUC según el mejor F1-score 0.9880232499287894
Confusion Matrix:
 [[16074   391]
 [  477  4683]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_5843969.png
Accuracy:   0.9599
Precision:  0.9229
Recall:     0.9076
F1-score:   0.9152

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.2275, Test Loss: 0.1754, F1: 0.8643, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0037, Test Loss: 0.3802, F1: 0.8857, AUC: 0.9870
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5538, F1: 0.9094, AUC: 0.9860
Mejores resultados en la época:  18
f1-score 0.9119366396379408
AUC según el mejor F1-score 0.9862725548909244
Confusion Matrix:
 [[15855   610]
 [  324  4836]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_5843969.png
Accuracy:   0.9568
Precision:  0.8880
Recall:     0.9372
F1-score:   0.9119

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.2223, Test Loss: 0.1719, F1: 0.8761, AUC: 0.9834
Epoch [10/30] Train Loss: 0.0030, Test Loss: 0.3684, F1: 0.9082, AUC: 0.9883
Epoch [20/30] Train Loss: 0.0020, Test Loss: 0.5515, F1: 0.8912, AUC: 0.9863
Mejores resultados en la época:  12
f1-score 0.9116042881324055
AUC según el mejor F1-score 0.9884742830104734
Confusion Matrix:
 [[15838   627]
 [  313  4847]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_5843969.png
Accuracy:   0.9565
Precision:  0.8855
Recall:     0.9393
F1-score:   0.9116

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.2240, Test Loss: 0.1633, F1: 0.8747, AUC: 0.9833
Epoch [10/30] Train Loss: 0.0043, Test Loss: 0.3527, F1: 0.9045, AUC: 0.9882
Epoch [20/30] Train Loss: 0.0015, Test Loss: 0.4820, F1: 0.8849, AUC: 0.9869
Mejores resultados en la época:  22
f1-score 0.9164733178654292
AUC según el mejor F1-score 0.9879381033764364
Confusion Matrix:
 [[16021   444]
 [  420  4740]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_5843969.png
Accuracy:   0.9600
Precision:  0.9144
Recall:     0.9186
F1-score:   0.9165
Tiempo total para red 6: 1845.06 segundos
Saved on: outputs_numerical_categorical_metadata/2/tfidf

==============================
Model: Logistic Regression
Accuracy:  0.9359
Precision: 0.8261
Recall:    0.9262
F1-score:  0.8733
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15459  1006]
 [  381  4779]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/tfidf/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/tfidf/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7706
Precision: 0.5108
Recall:    0.9079
F1-score:  0.6538
              precision    recall  f1-score   support

           0       0.96      0.73      0.83     16465
           1       0.51      0.91      0.65      5160

    accuracy                           0.77     21625
   macro avg       0.74      0.82      0.74     21625
weighted avg       0.85      0.77      0.79     21625

[[11979  4486]
 [  475  4685]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/tfidf/conf_matrix_svm.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/tfidf/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8709
Precision: 0.7116
Recall:    0.7721
F1-score:  0.7406
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14850  1615]
 [ 1176  3984]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/tfidf/conf_matrix_decision_tree.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8635
Precision: 0.6869
Recall:    0.7866
F1-score:  0.7334
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14615  1850]
 [ 1101  4059]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/tfidf/conf_matrix_random_forest.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/tfidf/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9213
Precision: 0.8015
Recall:    0.8911
F1-score:  0.8439
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15326  1139]
 [  562  4598]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/tfidf/conf_matrix_xgboost.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.9206
Precision: 0.8092
Recall:    0.8729
F1-score:  0.8398
              precision    recall  f1-score   support

           0       0.96      0.94      0.95     16465
           1       0.81      0.87      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.90      0.89     21625
weighted avg       0.92      0.92      0.92     21625
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_numerical_categorical_metadata.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Epoch [20/30] Train Loss: 0.0021, Test Loss: 0.3395, F1: 0.8982, AUC: 0.9882
Mejores resultados en la época:  22
f1-score 0.9158067316038191
AUC según el mejor F1-score 0.9872124038069949
Confusion Matrix:
 [[16004   461]
 [  412  4748]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_5843969.png
Accuracy:   0.9596
Precision:  0.9115
Recall:     0.9202
F1-score:   0.9158

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.2218, Test Loss: 0.1724, F1: 0.8779, AUC: 0.9836
Epoch [10/30] Train Loss: 0.0042, Test Loss: 0.2006, F1: 0.9138, AUC: 0.9880
Epoch [20/30] Train Loss: 0.0009, Test Loss: 0.4679, F1: 0.9010, AUC: 0.9879
Mejores resultados en la época:  8
f1-score 0.9140893470790378
AUC según el mejor F1-score 0.9879002088056178
Confusion Matrix:
 [[15937   528]
 [  372  4788]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_5843969.png
Accuracy:   0.9584
Precision:  0.9007
Recall:     0.9279
F1-score:   0.9141

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.2202, Test Loss: 0.2077, F1: 0.8476, AUC: 0.9832
Epoch [10/30] Train Loss: 0.0030, Test Loss: 0.4464, F1: 0.8916, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0019, Test Loss: 0.3934, F1: 0.8920, AUC: 0.9877
Mejores resultados en la época:  21
f1-score 0.9123395853899309
AUC según el mejor F1-score 0.9877735953879148
Confusion Matrix:
 [[16116   349]
 [  539  4621]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_5843969.png
Accuracy:   0.9589
Precision:  0.9298
Recall:     0.8955
F1-score:   0.9123

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.2280, Test Loss: 0.1645, F1: 0.8710, AUC: 0.9824
Epoch [10/30] Train Loss: 0.0036, Test Loss: 0.4460, F1: 0.8963, AUC: 0.9874
Epoch [20/30] Train Loss: 0.0015, Test Loss: 0.4894, F1: 0.9017, AUC: 0.9863
Mejores resultados en la época:  25
f1-score 0.9089720847726637
AUC según el mejor F1-score 0.9876302445638742
Confusion Matrix:
 [[15782   683]
 [  292  4868]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/tfidf/confusion_matrix_param_5843969.png
Accuracy:   0.9549
Precision:  0.8770
Recall:     0.9434
F1-score:   0.9090
Tiempo total para red 6: 1840.90 segundos
Saved on: outputs_numerical_categorical_metadata/2/tfidf

==============================
Model: Logistic Regression
Accuracy:  0.9359
Precision: 0.8261
Recall:    0.9262
F1-score:  0.8733
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15459  1006]
 [  381  4779]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/tfidf/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/tfidf/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7706
Precision: 0.5108
Recall:    0.9079
F1-score:  0.6538
              precision    recall  f1-score   support

           0       0.96      0.73      0.83     16465
           1       0.51      0.91      0.65      5160

    accuracy                           0.77     21625
   macro avg       0.74      0.82      0.74     21625
weighted avg       0.85      0.77      0.79     21625

[[11979  4486]
 [  475  4685]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/tfidf/conf_matrix_svm.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/tfidf/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8709
Precision: 0.7116
Recall:    0.7721
F1-score:  0.7406
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14850  1615]
 [ 1176  3984]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/tfidf/conf_matrix_decision_tree.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8635
Precision: 0.6869
Recall:    0.7866
F1-score:  0.7334
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14615  1850]
 [ 1101  4059]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/tfidf/conf_matrix_random_forest.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/tfidf/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9213
Precision: 0.8015
Recall:    0.8911
F1-score:  0.8439
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15326  1139]
 [  562  4598]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/tfidf/conf_matrix_xgboost.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.9206
Precision: 0.8092
Recall:    0.8729
F1-score:  0.8398
              precision    recall  f1-score   support

           0       0.96      0.94      0.95     16465
           1       0.81      0.87      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.90      0.89     21625
weighted avg       0.92      0.92      0.92     21625
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_numerical_categorical_metadata.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[15403  1062]
 [  656  4504]]
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/tfidf/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/tfidf/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9359, 'precision': 0.8261, 'recall': 0.9262, 'f1_score': 0.8733}
XGBoost: {'accuracy': 0.9213, 'precision': 0.8015, 'recall': 0.8911, 'f1_score': 0.8439}
Naive Bayes: {'accuracy': 0.9206, 'precision': 0.8092, 'recall': 0.8729, 'f1_score': 0.8398}
Decision Tree: {'accuracy': 0.8709, 'precision': 0.7116, 'recall': 0.7721, 'f1_score': 0.7406}
Random Forest: {'accuracy': 0.8635, 'precision': 0.6869, 'recall': 0.7866, 'f1_score': 0.7334}
SVM: {'accuracy': 0.7706, 'precision': 0.5108, 'recall': 0.9079, 'f1_score': 0.6538}
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 323)
Shape of X_test after concatenation:  (21625, 323)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 323)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 323)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [323, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5420, Test Loss: 0.4100, F1: 0.6893, AUC: 0.8892
Epoch [10/30] Train Loss: 0.3591, Test Loss: 0.3268, F1: 0.7494, AUC: 0.9243
Epoch [20/30] Train Loss: 0.3460, Test Loss: 0.3060, F1: 0.7615, AUC: 0.9296
Mejores resultados en la época:  22
f1-score 0.7672227674190383
AUC según el mejor F1-score 0.930140914366156
Confusion Matrix:
 [[15344  1121]
 [ 1251  3909]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_10401.png
Accuracy:   0.8903
Precision:  0.7771
Recall:     0.7576
F1-score:   0.7672

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5186, Test Loss: 0.4402, F1: 0.6688, AUC: 0.8937
Epoch [10/30] Train Loss: 0.3546, Test Loss: 0.3862, F1: 0.7197, AUC: 0.9261
Epoch [20/30] Train Loss: 0.3458, Test Loss: 0.3243, F1: 0.7509, AUC: 0.9297
Mejores resultados en la época:  25
f1-score 0.7651400454201362
AUC según el mejor F1-score 0.9308045313408522
Confusion Matrix:
 [[15100  1365]
 [ 1117  4043]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_10401.png
Accuracy:   0.8852
Precision:  0.7476
Recall:     0.7835
F1-score:   0.7651

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5615, Test Loss: 0.4924, F1: 0.6325, AUC: 0.8843
Epoch [10/30] Train Loss: 0.3603, Test Loss: 0.3703, F1: 0.7236, AUC: 0.9228
Epoch [20/30] Train Loss: 0.3450, Test Loss: 0.2842, F1: 0.7611, AUC: 0.9284
Mejores resultados en la época:  26
f1-score 0.7648187224247075
AUC según el mejor F1-score 0.9308720459419441
Confusion Matrix:
 [[15186  1279]
 [ 1173  3987]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_10401.png
Accuracy:   0.8866
Precision:  0.7571
Recall:     0.7727
F1-score:   0.7648

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5308, Test Loss: 0.4110, F1: 0.6860, AUC: 0.8902
Epoch [10/30] Train Loss: 0.3564, Test Loss: 0.3788, F1: 0.7206, AUC: 0.9255
Epoch [20/30] Train Loss: 0.3451, Test Loss: 0.2786, F1: 0.7656, AUC: 0.9301
Mejores resultados en la época:  23
f1-score 0.7689597808004697
AUC según el mejor F1-score 0.9310787682116398
Confusion Matrix:
 [[15335  1130]
 [ 1231  3929]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_10401.png
Accuracy:   0.8908
Precision:  0.7766
Recall:     0.7614
F1-score:   0.7690
Tiempo total para red 1: 770.37 segundos

Entrenando red 2 con capas [323, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4837, Test Loss: 0.4211, F1: 0.6912, AUC: 0.9054
Epoch [10/30] Train Loss: 0.3548, Test Loss: 0.2848, F1: 0.7578, AUC: 0.9289
Epoch [20/30] Train Loss: 0.3321, Test Loss: 0.3072, F1: 0.7509, AUC: 0.9346
Mejores resultados en la época:  27
f1-score 0.7683499155246856
AUC según el mejor F1-score 0.9360434042613296
Confusion Matrix:
 [[15064  1401]
 [ 1067  4093]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_22849.png
Accuracy:   0.8859
Precision:  0.7450
Recall:     0.7932
F1-score:   0.7683

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4889, Test Loss: 0.3962, F1: 0.6965, AUC: 0.9046
Epoch [10/30] Train Loss: 0.3540, Test Loss: 0.4780, F1: 0.6623, AUC: 0.9274
Epoch [20/30] Train Loss: 0.3321, Test Loss: 0.2782, F1: 0.7644, AUC: 0.9326
Mejores resultados en la época:  29
f1-score 0.7837510105092966
AUC según el mejor F1-score 0.9390478216654075
Confusion Matrix:
 [[15607   858]
 [ 1282  3878]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_22849.png
Accuracy:   0.9010
Precision:  0.8188
Recall:     0.7516
F1-score:   0.7838

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4862, Test Loss: 0.4316, F1: 0.6790, AUC: 0.9041
Epoch [10/30] Train Loss: 0.3497, Test Loss: 0.3672, F1: 0.7301, AUC: 0.9290
Epoch [20/30] Train Loss: 0.3328, Test Loss: 0.2942, F1: 0.7729, AUC: 0.9354
Mejores resultados en la época:  29
f1-score 0.7799746761468783
AUC según el mejor F1-score 0.9378660454287577
Confusion Matrix:
 [[15362  1103]
 [ 1156  4004]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_22849.png
Accuracy:   0.8955
Precision:  0.7840
Recall:     0.7760
F1-score:   0.7800

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4901, Test Loss: 0.3770, F1: 0.7121, AUC: 0.9037
Epoch [10/30] Train Loss: 0.3540, Test Loss: 0.4393, F1: 0.6847, AUC: 0.9289
Epoch [20/30] Train Loss: 0.3310, Test Loss: 0.3542, F1: 0.7317, AUC: 0.9365
Mejores resultados en la época:  29
f1-score 0.7665727016459034
AUC según el mejor F1-score 0.9388724732048485
Confusion Matrix:
 [[14843  1622]
 [  945  4215]]

[[15403  1062]
 [  656  4504]]
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/tfidf/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/tfidf/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9359, 'precision': 0.8261, 'recall': 0.9262, 'f1_score': 0.8733}
XGBoost: {'accuracy': 0.9213, 'precision': 0.8015, 'recall': 0.8911, 'f1_score': 0.8439}
Naive Bayes: {'accuracy': 0.9206, 'precision': 0.8092, 'recall': 0.8729, 'f1_score': 0.8398}
Decision Tree: {'accuracy': 0.8709, 'precision': 0.7116, 'recall': 0.7721, 'f1_score': 0.7406}
Random Forest: {'accuracy': 0.8635, 'precision': 0.6869, 'recall': 0.7866, 'f1_score': 0.7334}
SVM: {'accuracy': 0.7706, 'precision': 0.5108, 'recall': 0.9079, 'f1_score': 0.6538}
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 323)
Shape of X_test after concatenation:  (21625, 323)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 323)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 323)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [323, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5089, Test Loss: 0.3757, F1: 0.7096, AUC: 0.8957
Epoch [10/30] Train Loss: 0.3564, Test Loss: 0.4274, F1: 0.6950, AUC: 0.9261
Epoch [20/30] Train Loss: 0.3446, Test Loss: 0.3643, F1: 0.7325, AUC: 0.9301
Mejores resultados en la época:  18
f1-score 0.7653488609663909
AUC según el mejor F1-score 0.9295892802915275
Confusion Matrix:
 [[15582   883]
 [ 1414  3746]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_10401.png
Accuracy:   0.8938
Precision:  0.8092
Recall:     0.7260
F1-score:   0.7653

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5270, Test Loss: 0.3890, F1: 0.7013, AUC: 0.8901
Epoch [10/30] Train Loss: 0.3581, Test Loss: 0.3084, F1: 0.7546, AUC: 0.9246
Epoch [20/30] Train Loss: 0.3456, Test Loss: 0.3667, F1: 0.7288, AUC: 0.9290
Mejores resultados en la época:  16
f1-score 0.7609427609427609
AUC según el mejor F1-score 0.9280867920442
Confusion Matrix:
 [[15553   912]
 [ 1431  3729]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_10401.png
Accuracy:   0.8917
Precision:  0.8035
Recall:     0.7227
F1-score:   0.7609

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5178, Test Loss: 0.3432, F1: 0.7038, AUC: 0.8941
Epoch [10/30] Train Loss: 0.3568, Test Loss: 0.4095, F1: 0.6993, AUC: 0.9251
Epoch [20/30] Train Loss: 0.3418, Test Loss: 0.3308, F1: 0.7433, AUC: 0.9315
Mejores resultados en la época:  17
f1-score 0.7597685983964275
AUC según el mejor F1-score 0.9297389812074944
Confusion Matrix:
 [[15515   950]
 [ 1417  3743]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_10401.png
Accuracy:   0.8905
Precision:  0.7976
Recall:     0.7254
F1-score:   0.7598

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5223, Test Loss: 0.3588, F1: 0.7098, AUC: 0.8927
Epoch [10/30] Train Loss: 0.3580, Test Loss: 0.4550, F1: 0.6710, AUC: 0.9252
Epoch [20/30] Train Loss: 0.3459, Test Loss: 0.3777, F1: 0.7149, AUC: 0.9300
Mejores resultados en la época:  28
f1-score 0.7741612418627942
AUC según el mejor F1-score 0.9347019635261078
Confusion Matrix:
 [[15505   960]
 [ 1295  3865]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_10401.png
Accuracy:   0.8957
Precision:  0.8010
Recall:     0.7490
F1-score:   0.7742
Tiempo total para red 1: 775.56 segundos

Entrenando red 2 con capas [323, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4941, Test Loss: 0.4952, F1: 0.6397, AUC: 0.9018
Epoch [10/30] Train Loss: 0.3522, Test Loss: 0.2873, F1: 0.7621, AUC: 0.9273
Epoch [20/30] Train Loss: 0.3370, Test Loss: 0.3436, F1: 0.7386, AUC: 0.9334
Mejores resultados en la época:  18
f1-score 0.7622456669178599
AUC según el mejor F1-score 0.9322206018404087
Confusion Matrix:
 [[15055  1410]
 [ 1114  4046]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_22849.png
Accuracy:   0.8833
Precision:  0.7416
Recall:     0.7841
F1-score:   0.7622

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4809, Test Loss: 0.3765, F1: 0.7133, AUC: 0.9064
Epoch [10/30] Train Loss: 0.3574, Test Loss: 0.3557, F1: 0.7337, AUC: 0.9294
Epoch [20/30] Train Loss: 0.3343, Test Loss: 0.3476, F1: 0.7357, AUC: 0.9347
Mejores resultados en la época:  27
f1-score 0.7731436367179887
AUC según el mejor F1-score 0.9360951289674831
Confusion Matrix:
 [[15330  1135]
 [ 1193  3967]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_22849.png
Accuracy:   0.8923
Precision:  0.7775
Recall:     0.7688
F1-score:   0.7731

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4833, Test Loss: 0.4770, F1: 0.6494, AUC: 0.9028
Epoch [10/30] Train Loss: 0.3493, Test Loss: 0.3606, F1: 0.7273, AUC: 0.9298
Epoch [20/30] Train Loss: 0.3319, Test Loss: 0.3012, F1: 0.7587, AUC: 0.9348
Mejores resultados en la época:  29
f1-score 0.7710178044368275
AUC según el mejor F1-score 0.9385769673514643
Confusion Matrix:
 [[15171  1294]
 [ 1111  4049]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_22849.png
Accuracy:   0.8888
Precision:  0.7578
Recall:     0.7847
F1-score:   0.7710

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4809, Test Loss: 0.3407, F1: 0.7293, AUC: 0.9069
Epoch [10/30] Train Loss: 0.3542, Test Loss: 0.2774, F1: 0.7526, AUC: 0.9282
Epoch [20/30] Train Loss: 0.3334, Test Loss: 0.3369, F1: 0.7375, AUC: 0.9329
Mejores resultados en la época:  26
f1-score 0.7717011429111174
AUC según el mejor F1-score 0.9369848362865086
Confusion Matrix:
 [[15123  1342]
 [ 1075  4085]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_22849.png
Accuracy:   0.8813
Precision:  0.7221
Recall:     0.8169
F1-score:   0.7666
Tiempo total para red 2: 784.07 segundos

Entrenando red 3 con capas [323, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4807, Test Loss: 0.3573, F1: 0.7274, AUC: 0.9077
Epoch [10/30] Train Loss: 0.3478, Test Loss: 0.3981, F1: 0.7069, AUC: 0.9299
Epoch [20/30] Train Loss: 0.3327, Test Loss: 0.3385, F1: 0.7424, AUC: 0.9361
Mejores resultados en la época:  23
f1-score 0.7690052765292164
AUC según el mejor F1-score 0.9370309936275445
Confusion Matrix:
 [[15326  1139]
 [ 1225  3935]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_51841.png
Accuracy:   0.8907
Precision:  0.7755
Recall:     0.7626
F1-score:   0.7690

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4776, Test Loss: 0.3218, F1: 0.7300, AUC: 0.9067
Epoch [10/30] Train Loss: 0.3543, Test Loss: 0.3594, F1: 0.7298, AUC: 0.9303
Epoch [20/30] Train Loss: 0.3351, Test Loss: 0.2727, F1: 0.7708, AUC: 0.9343
Mejores resultados en la época:  28
f1-score 0.7716732416049263
AUC según el mejor F1-score 0.9373039593029142
Confusion Matrix:
 [[15242  1223]
 [ 1150  4010]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_51841.png
Accuracy:   0.8903
Precision:  0.7663
Recall:     0.7771
F1-score:   0.7717

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4631, Test Loss: 0.3789, F1: 0.7199, AUC: 0.9095
Epoch [10/30] Train Loss: 0.3546, Test Loss: 0.3632, F1: 0.7277, AUC: 0.9289
Epoch [20/30] Train Loss: 0.3345, Test Loss: 0.2852, F1: 0.7637, AUC: 0.9345
Mejores resultados en la época:  27
f1-score 0.7715500579822188
AUC según el mejor F1-score 0.9369491427670158
Confusion Matrix:
 [[15269  1196]
 [ 1168  3992]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_51841.png
Accuracy:   0.8907
Precision:  0.7695
Recall:     0.7736
F1-score:   0.7716

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4691, Test Loss: 0.4884, F1: 0.6542, AUC: 0.9087
Epoch [10/30] Train Loss: 0.3540, Test Loss: 0.3015, F1: 0.7590, AUC: 0.9302
Epoch [20/30] Train Loss: 0.3388, Test Loss: 0.2926, F1: 0.7603, AUC: 0.9340
Mejores resultados en la época:  22
f1-score 0.7666187050359712
AUC según el mejor F1-score 0.9345257970277567
Confusion Matrix:
 [[15196  1269]
 [ 1164  3996]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_51841.png
Accuracy:   0.8875
Precision:  0.7590
Recall:     0.7744
F1-score:   0.7666
Tiempo total para red 3: 799.11 segundos

Entrenando red 4 con capas [323, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4877, Test Loss: 0.3215, F1: 0.7322, AUC: 0.9048
Epoch [10/30] Train Loss: 0.3487, Test Loss: 0.3741, F1: 0.7191, AUC: 0.9296
Epoch [20/30] Train Loss: 0.3313, Test Loss: 0.2855, F1: 0.7681, AUC: 0.9352
Mejores resultados en la época:  29
f1-score 0.7722489016041688
AUC según el mejor F1-score 0.9361704649514944
Confusion Matrix:
 [[15617   848]
 [ 1381  3779]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_126209.png
Accuracy:   0.8969
Precision:  0.8167
Recall:     0.7324
F1-score:   0.7722

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4605, Test Loss: 0.3540, F1: 0.7359, AUC: 0.9099
Epoch [10/30] Train Loss: 0.3518, Test Loss: 0.3246, F1: 0.7575, AUC: 0.9317
Epoch [20/30] Train Loss: 0.3315, Test Loss: 0.2651, F1: 0.7690, AUC: 0.9359
Mejores resultados en la época:  25
f1-score 0.770166270783848
AUC según el mejor F1-score 0.9359372300181027
Confusion Matrix:
 [[15153  1312]
 [ 1107  4053]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_126209.png
Accuracy:   0.8881
Precision:  0.7555
Recall:     0.7855
F1-score:   0.7702

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4752, Test Loss: 0.5877, F1: 0.6047, AUC: 0.9069
Epoch [10/30] Train Loss: 0.3490, Test Loss: 0.3106, F1: 0.7562, AUC: 0.9313
Epoch [20/30] Train Loss: 0.3305, Test Loss: 0.3990, F1: 0.7020, AUC: 0.9354
Mejores resultados en la época:  26
f1-score 0.7702916783151617
AUC según el mejor F1-score 0.9391039249335565
Confusion Matrix:
 [[15027  1438]
 [ 1027  4133]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_126209.png
Accuracy:   0.8860
Precision:  0.7419
Recall:     0.8010
F1-score:   0.7703

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4634, Test Loss: 0.3478, F1: 0.7329, AUC: 0.9096
Epoch [10/30] Train Loss: 0.3505, Test Loss: 0.2899, F1: 0.7611, AUC: 0.9312
Epoch [20/30] Train Loss: 0.3316, Test Loss: 0.3509, F1: 0.7389, AUC: 0.9368
Mejores resultados en la época:  27
f1-score 0.7786499215070644
AUC según el mejor F1-score 0.9373922367625006
Confusion Matrix:
 [[15401  1064]
 [ 1192  3968]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_126209.png
Accuracy:   0.8957
Precision:  0.7886
Recall:     0.7690
F1-score:   0.7786
Tiempo total para red 4: 816.68 segundos

Entrenando red 5 con capas [323, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4626, Test Loss: 0.4243, F1: 0.7021, AUC: 0.9091
Epoch [10/30] Train Loss: 0.3524, Test Loss: 0.3075, F1: 0.7520, AUC: 0.9307
Epoch [20/30] Train Loss: 0.3273, Test Loss: 0.2760, F1: 0.7683, AUC: 0.9370
Mejores resultados en la época:  24
f1-score 0.7733103580317825
AUC según el mejor F1-score 0.9379331951496832
Confusion Matrix:
 [[15218  1247]
 [ 1121  4039]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_338433.png
Accuracy:   0.8905
Precision:  0.7641
Recall:     0.7828
F1-score:   0.7733

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4685, Test Loss: 0.3352, F1: 0.7364, AUC: 0.9104
Epoch [10/30] Train Loss: 0.3470, Test Loss: 0.3138, F1: 0.7593, AUC: 0.9319
Epoch [20/30] Train Loss: 0.3281, Test Loss: 0.3038, F1: 0.7563, AUC: 0.9361
Mejores resultados en la época:  23
f1-score 0.7727227869337807
AUC según el mejor F1-score 0.9371714783767306
Confusion Matrix:
 [[15407  1058]
 [ 1245  3915]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_338433.png
Accuracy:   0.8935
Precision:  0.7873
Recall:     0.7587
F1-score:   0.7727

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4574, Test Loss: 0.3183, F1: 0.7316, AUC: 0.9125
Epoch [10/30] Train Loss: 0.3498, Test Loss: 0.3047, F1: 0.7551, AUC: 0.9324
Epoch [20/30] Train Loss: 0.3261, Test Loss: 0.3148, F1: 0.7562, AUC: 0.9373
Mejores resultados en la época:  27
f1-score 0.7813223306833048
AUC según el mejor F1-score 0.9387196649223042
Confusion Matrix:
 [[15570   895]
 [ 1278  3882]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_338433.png
Accuracy:   0.8995
Precision:  0.8126
Recall:     0.7523
F1-score:   0.7813

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4655, Test Loss: 0.3428, F1: 0.7397, AUC: 0.9096
Epoch [10/30] Train Loss: 0.3508, Test Loss: 0.4470, F1: 0.6987, AUC: 0.9311
Epoch [20/30] Train Loss: 0.3298, Test Loss: 0.3663, F1: 0.7265, AUC: 0.9363
Mejores resultados en la época:  23
f1-score 0.7805498483514334
AUC según el mejor F1-score 0.9375784374654247
Confusion Matrix:
 [[15393  1072]
 [ 1171  3989]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_338433.png
Accuracy:   0.8963
Precision:  0.7882
Recall:     0.7731
F1-score:   0.7805
Tiempo total para red 5: 833.08 segundos

Entrenando red 6 con capas [323, 1024, 512, 256, 128, 64, 32, 1]

Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_22849.png
Accuracy:   0.8882
Precision:  0.7527
Recall:     0.7917
F1-score:   0.7717
Tiempo total para red 2: 785.56 segundos

Entrenando red 3 con capas [323, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4886, Test Loss: 0.4413, F1: 0.6756, AUC: 0.9063
Epoch [10/30] Train Loss: 0.3495, Test Loss: 0.3004, F1: 0.7576, AUC: 0.9293
Epoch [20/30] Train Loss: 0.3397, Test Loss: 0.3285, F1: 0.7377, AUC: 0.9346
Mejores resultados en la época:  28
f1-score 0.7708679097920531
AUC según el mejor F1-score 0.9364902059101171
Confusion Matrix:
 [[15330  1135]
 [ 1212  3948]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_51841.png
Accuracy:   0.8915
Precision:  0.7767
Recall:     0.7651
F1-score:   0.7709

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4680, Test Loss: 0.4096, F1: 0.6983, AUC: 0.9081
Epoch [10/30] Train Loss: 0.3522, Test Loss: 0.3941, F1: 0.7118, AUC: 0.9299
Epoch [20/30] Train Loss: 0.3315, Test Loss: 0.3298, F1: 0.7554, AUC: 0.9364
Mejores resultados en la época:  29
f1-score 0.7746703955253695
AUC según el mejor F1-score 0.9371787583245644
Confusion Matrix:
 [[15491   974]
 [ 1282  3878]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_51841.png
Accuracy:   0.8957
Precision:  0.7993
Recall:     0.7516
F1-score:   0.7747

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4675, Test Loss: 0.6062, F1: 0.6023, AUC: 0.9087
Epoch [10/30] Train Loss: 0.3479, Test Loss: 0.3645, F1: 0.7273, AUC: 0.9312
Epoch [20/30] Train Loss: 0.3317, Test Loss: 0.2685, F1: 0.7701, AUC: 0.9360
Mejores resultados en la época:  28
f1-score 0.7789578163771712
AUC según el mejor F1-score 0.9388474553728015
Confusion Matrix:
 [[15474   991]
 [ 1236  3924]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_51841.png
Accuracy:   0.8970
Precision:  0.7984
Recall:     0.7605
F1-score:   0.7790

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4775, Test Loss: 0.3857, F1: 0.7088, AUC: 0.9082
Epoch [10/30] Train Loss: 0.3539, Test Loss: 0.3394, F1: 0.7348, AUC: 0.9303
Epoch [20/30] Train Loss: 0.3311, Test Loss: 0.3308, F1: 0.7434, AUC: 0.9351
Mejores resultados en la época:  26
f1-score 0.7679671457905544
AUC según el mejor F1-score 0.937442543144137
Confusion Matrix:
 [[15025  1440]
 [ 1046  4114]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_51841.png
Accuracy:   0.8850
Precision:  0.7407
Recall:     0.7973
F1-score:   0.7680
Tiempo total para red 3: 800.21 segundos

Entrenando red 4 con capas [323, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4693, Test Loss: 0.3219, F1: 0.7354, AUC: 0.9090
Epoch [10/30] Train Loss: 0.3526, Test Loss: 0.3742, F1: 0.7190, AUC: 0.9305
Epoch [20/30] Train Loss: 0.3295, Test Loss: 0.2984, F1: 0.7579, AUC: 0.9351
Mejores resultados en la época:  27
f1-score 0.7757575757575758
AUC según el mejor F1-score 0.9390172894347183
Confusion Matrix:
 [[15363  1102]
 [ 1192  3968]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_126209.png
Accuracy:   0.8939
Precision:  0.7826
Recall:     0.7690
F1-score:   0.7758

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4742, Test Loss: 0.6013, F1: 0.6113, AUC: 0.9082
Epoch [10/30] Train Loss: 0.3505, Test Loss: 0.3582, F1: 0.7323, AUC: 0.9300
Epoch [20/30] Train Loss: 0.3296, Test Loss: 0.3380, F1: 0.7387, AUC: 0.9367
Mejores resultados en la época:  28
f1-score 0.7708200018283207
AUC según el mejor F1-score 0.9388044877906387
Confusion Matrix:
 [[14902  1563]
 [  944  4216]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_126209.png
Accuracy:   0.8841
Precision:  0.7295
Recall:     0.8171
F1-score:   0.7708

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4658, Test Loss: 0.3576, F1: 0.7321, AUC: 0.9099
Epoch [10/30] Train Loss: 0.3516, Test Loss: 0.3273, F1: 0.7534, AUC: 0.9304
Epoch [20/30] Train Loss: 0.3298, Test Loss: 0.2696, F1: 0.7734, AUC: 0.9355
Mejores resultados en la época:  20
f1-score 0.7734413595494516
AUC según el mejor F1-score 0.9354603375259242
Confusion Matrix:
 [[15418  1047]
 [ 1246  3914]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_126209.png
Accuracy:   0.8940
Precision:  0.7890
Recall:     0.7585
F1-score:   0.7734

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4674, Test Loss: 0.3168, F1: 0.6699, AUC: 0.9095
Epoch [10/30] Train Loss: 0.3493, Test Loss: 0.4144, F1: 0.7007, AUC: 0.9295
Epoch [20/30] Train Loss: 0.3303, Test Loss: 0.3440, F1: 0.7243, AUC: 0.9373
Mejores resultados en la época:  26
f1-score 0.779260978912749
AUC según el mejor F1-score 0.9387285574050663
Confusion Matrix:
 [[15315  1150]
 [ 1132  4028]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_126209.png
Accuracy:   0.8945
Precision:  0.7779
Recall:     0.7806
F1-score:   0.7793
Tiempo total para red 4: 817.33 segundos

Entrenando red 5 con capas [323, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4686, Test Loss: 0.4562, F1: 0.6767, AUC: 0.9097
Epoch [10/30] Train Loss: 0.3530, Test Loss: 0.3464, F1: 0.7332, AUC: 0.9314
Epoch [20/30] Train Loss: 0.3289, Test Loss: 0.3550, F1: 0.7236, AUC: 0.9368
Mejores resultados en la época:  29
f1-score 0.7749676942957356
AUC según el mejor F1-score 0.9407666897365095
Confusion Matrix:
 [[14989  1476]
 [  962  4198]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_338433.png
Accuracy:   0.8873
Precision:  0.7399
Recall:     0.8136
F1-score:   0.7750

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4649, Test Loss: 0.4966, F1: 0.6591, AUC: 0.9089
Epoch [10/30] Train Loss: 0.3535, Test Loss: 0.3142, F1: 0.7557, AUC: 0.9319
Epoch [20/30] Train Loss: 0.3315, Test Loss: 0.2711, F1: 0.7621, AUC: 0.9333
Mejores resultados en la época:  24
f1-score 0.7794688152790212
AUC según el mejor F1-score 0.9395324766888655
Confusion Matrix:
 [[15490   975]
 [ 1242  3918]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_338433.png
Accuracy:   0.8975
Precision:  0.8007
Recall:     0.7593
F1-score:   0.7795

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4709, Test Loss: 0.3637, F1: 0.7266, AUC: 0.9076
Epoch [10/30] Train Loss: 0.3480, Test Loss: 0.2706, F1: 0.7638, AUC: 0.9305
Epoch [20/30] Train Loss: 0.3323, Test Loss: 0.3247, F1: 0.7512, AUC: 0.9370
Mejores resultados en la época:  10
f1-score 0.7637698898408812
AUC según el mejor F1-score 0.930480041054904
Confusion Matrix:
 [[15565   900]
 [ 1416  3744]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_338433.png
Accuracy:   0.8929
Precision:  0.8062
Recall:     0.7256
F1-score:   0.7638

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4676, Test Loss: 0.4838, F1: 0.6647, AUC: 0.9111
Epoch [10/30] Train Loss: 0.3524, Test Loss: 0.3430, F1: 0.7383, AUC: 0.9306
Epoch [20/30] Train Loss: 0.3338, Test Loss: 0.4017, F1: 0.7072, AUC: 0.9352
Mejores resultados en la época:  27
f1-score 0.7699893647877791
AUC según el mejor F1-score 0.9377720122788061
Confusion Matrix:
 [[15264  1201]
 [ 1178  3982]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_338433.png
Accuracy:   0.8900
Precision:  0.7683
Recall:     0.7717
F1-score:   0.7700
Tiempo total para red 5: 835.09 segundos

Entrenando red 6 con capas [323, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4710, Test Loss: 0.4195, F1: 0.6954, AUC: 0.9098
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:03:48] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:04:51] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4728, Test Loss: 0.3293, F1: 0.7355, AUC: 0.9120
Epoch [10/30] Train Loss: 0.3497, Test Loss: 0.2813, F1: 0.7627, AUC: 0.9308
Epoch [20/30] Train Loss: 0.3286, Test Loss: 0.2758, F1: 0.7674, AUC: 0.9363
Mejores resultados en la época:  28
f1-score 0.7777450113044333
AUC según el mejor F1-score 0.9403148209615416
Confusion Matrix:
 [[15408  1057]
 [ 1204  3956]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_1031169.png
Accuracy:   0.8954
Precision:  0.7891
Recall:     0.7667
F1-score:   0.7777

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4771, Test Loss: 0.3487, F1: 0.5846, AUC: 0.9108
Epoch [10/30] Train Loss: 0.3538, Test Loss: 0.2791, F1: 0.7616, AUC: 0.9301
Epoch [20/30] Train Loss: 0.3346, Test Loss: 0.3824, F1: 0.7181, AUC: 0.9349
Mejores resultados en la época:  29
f1-score 0.7772282028901174
AUC según el mejor F1-score 0.9380432418308038
Confusion Matrix:
 [[15321  1144]
 [ 1153  4007]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_1031169.png
Accuracy:   0.8938
Precision:  0.7779
Recall:     0.7766
F1-score:   0.7772

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4948, Test Loss: 0.3202, F1: 0.7376, AUC: 0.9084
Epoch [10/30] Train Loss: 0.3486, Test Loss: 0.3879, F1: 0.7027, AUC: 0.9311
Epoch [20/30] Train Loss: 0.3275, Test Loss: 0.2747, F1: 0.7682, AUC: 0.9344
Mejores resultados en la época:  24
f1-score 0.7717834455295345
AUC según el mejor F1-score 0.9386788218843354
Confusion Matrix:
 [[15148  1317]
 [ 1090  4070]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_1031169.png
Accuracy:   0.8887
Precision:  0.7555
Recall:     0.7888
F1-score:   0.7718

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4856, Test Loss: 0.3665, F1: 0.7240, AUC: 0.9100
Epoch [10/30] Train Loss: 0.3525, Test Loss: 0.2947, F1: 0.7629, AUC: 0.9314
Epoch [20/30] Train Loss: 0.3330, Test Loss: 0.2975, F1: 0.7593, AUC: 0.9348
Mejores resultados en la época:  27
f1-score 0.7711880261927034
AUC según el mejor F1-score 0.9383024067966581
Confusion Matrix:
 [[15057  1408]
 [ 1038  4122]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_1031169.png
Accuracy:   0.8869
Precision:  0.7454
Recall:     0.7988
F1-score:   0.7712
Tiempo total para red 6: 880.51 segundos
Saved on: outputs_numerical_categorical_metadata/2/lyrics_bert

==============================
Model: Logistic Regression
Accuracy:  0.8571
Precision: 0.6574
Recall:    0.8372
F1-score:  0.7365
              precision    recall  f1-score   support

           0       0.94      0.86      0.90     16465
           1       0.66      0.84      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.85      0.82     21625
weighted avg       0.88      0.86      0.86     21625

[[14214  2251]
 [  840  4320]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.5669
Precision: 0.3270
Recall:    0.7705
F1-score:  0.4592
              precision    recall  f1-score   support

           0       0.87      0.50      0.64     16465
           1       0.33      0.77      0.46      5160

    accuracy                           0.57     21625
   macro avg       0.60      0.64      0.55     21625
weighted avg       0.74      0.57      0.60     21625

[[8283 8182]
 [1184 3976]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_svm.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8268
Precision: 0.6083
Recall:    0.7702
F1-score:  0.6797
              precision    recall  f1-score   support

           0       0.92      0.84      0.88     16465
           1       0.61      0.77      0.68      5160

    accuracy                           0.83     21625
   macro avg       0.76      0.81      0.78     21625
weighted avg       0.85      0.83      0.83     21625

[[13906  2559]
 [ 1186  3974]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8791
Precision: 0.7167
Recall:    0.8155
F1-score:  0.7629
              precision    recall  f1-score   support

           0       0.94      0.90      0.92     16465
           1       0.72      0.82      0.76      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.86      0.84     21625
weighted avg       0.89      0.88      0.88     21625

[[14802  1663]
 [  952  4208]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_random_forest.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8918
Precision: 0.7315
Recall:    0.8636
F1-score:  0.7920
              precision    recall  f1-score   support

           0       0.95      0.90      0.93     16465
           1       0.73      0.86      0.79      5160

    accuracy                           0.89     21625
   macro avg       0.84      0.88      0.86     21625
weighted avg       0.90      0.89      0.89     21625

[[14829  1636]
 [  704  4456]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_numerical_categorical_metadata.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Epoch [10/30] Train Loss: 0.3546, Test Loss: 0.3271, F1: 0.7534, AUC: 0.9310
Epoch [20/30] Train Loss: 0.3269, Test Loss: 0.2865, F1: 0.7607, AUC: 0.9362
Mejores resultados en la época:  27
f1-score 0.7707728337236534
AUC según el mejor F1-score 0.9394481128633205
Confusion Matrix:
 [[15064  1401]
 [ 1046  4114]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_1031169.png
Accuracy:   0.8868
Precision:  0.7460
Recall:     0.7973
F1-score:   0.7708

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4650, Test Loss: 0.4647, F1: 0.6951, AUC: 0.9130
Epoch [10/30] Train Loss: 0.3500, Test Loss: 0.3080, F1: 0.7529, AUC: 0.9325
Epoch [20/30] Train Loss: 0.3271, Test Loss: 0.3222, F1: 0.7529, AUC: 0.9381
Mejores resultados en la época:  24
f1-score 0.7778439547349613
AUC según el mejor F1-score 0.9390398413830606
Confusion Matrix:
 [[15469   996]
 [ 1242  3918]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_1031169.png
Accuracy:   0.8965
Precision:  0.7973
Recall:     0.7593
F1-score:   0.7778

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4764, Test Loss: 0.3722, F1: 0.7213, AUC: 0.9117
Epoch [10/30] Train Loss: 0.3430, Test Loss: 0.4187, F1: 0.7185, AUC: 0.9314
Epoch [20/30] Train Loss: 0.3255, Test Loss: 0.3073, F1: 0.7567, AUC: 0.9374
Mejores resultados en la época:  29
f1-score 0.7702100572883513
AUC según el mejor F1-score 0.9401326045146268
Confusion Matrix:
 [[14863  1602]
 [  925  4235]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_1031169.png
Accuracy:   0.8831
Precision:  0.7255
Recall:     0.8207
F1-score:   0.7702

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4608, Test Loss: 0.4680, F1: 0.6817, AUC: 0.9122
Epoch [10/30] Train Loss: 0.3506, Test Loss: 0.3324, F1: 0.7380, AUC: 0.9319
Epoch [20/30] Train Loss: 0.3276, Test Loss: 0.3494, F1: 0.7352, AUC: 0.9346
Mejores resultados en la época:  26
f1-score 0.7797820005787596
AUC según el mejor F1-score 0.9390929667582399
Confusion Matrix:
 [[15300  1165]
 [ 1118  4042]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_1031169.png
Accuracy:   0.8944
Precision:  0.7763
Recall:     0.7833
F1-score:   0.7798
Tiempo total para red 6: 873.79 segundos
Saved on: outputs_numerical_categorical_metadata/2/lyrics_bert

==============================
Model: Logistic Regression
Accuracy:  0.8571
Precision: 0.6574
Recall:    0.8372
F1-score:  0.7365
              precision    recall  f1-score   support

           0       0.94      0.86      0.90     16465
           1       0.66      0.84      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.85      0.82     21625
weighted avg       0.88      0.86      0.86     21625

[[14214  2251]
 [  840  4320]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.5669
Precision: 0.3270
Recall:    0.7705
F1-score:  0.4592
              precision    recall  f1-score   support

           0       0.87      0.50      0.64     16465
           1       0.33      0.77      0.46      5160

    accuracy                           0.57     21625
   macro avg       0.60      0.64      0.55     21625
weighted avg       0.74      0.57      0.60     21625

[[8283 8182]
 [1184 3976]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_svm.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8268
Precision: 0.6083
Recall:    0.7702
F1-score:  0.6797
              precision    recall  f1-score   support

           0       0.92      0.84      0.88     16465
           1       0.61      0.77      0.68      5160

    accuracy                           0.83     21625
   macro avg       0.76      0.81      0.78     21625
weighted avg       0.85      0.83      0.83     21625

[[13906  2559]
 [ 1186  3974]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8791
Precision: 0.7167
Recall:    0.8155
F1-score:  0.7629
              precision    recall  f1-score   support

           0       0.94      0.90      0.92     16465
           1       0.72      0.82      0.76      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.86      0.84     21625
weighted avg       0.89      0.88      0.88     21625

[[14802  1663]
 [  952  4208]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_random_forest.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8918
Precision: 0.7315
Recall:    0.8636
F1-score:  0.7920
              precision    recall  f1-score   support

           0       0.95      0.90      0.93     16465
           1       0.73      0.86      0.79      5160

    accuracy                           0.89     21625
   macro avg       0.84      0.88      0.86     21625
weighted avg       0.90      0.89      0.89     21625

[[14829  1636]
 [  704  4456]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_xgboost.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/xgboost_model.pkl

/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_numerical_categorical_metadata.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_xgboost.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7637
Precision: 0.5030
Recall:    0.8141
F1-score:  0.6218
              precision    recall  f1-score   support

           0       0.93      0.75      0.83     16465
           1       0.50      0.81      0.62      5160

    accuracy                           0.76     21625
   macro avg       0.72      0.78      0.72     21625
weighted avg       0.83      0.76      0.78     21625

[[12314  4151]
 [  959  4201]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.8918, 'precision': 0.7315, 'recall': 0.8636, 'f1_score': 0.792}
Random Forest: {'accuracy': 0.8791, 'precision': 0.7167, 'recall': 0.8155, 'f1_score': 0.7629}
Logistic Regression: {'accuracy': 0.8571, 'precision': 0.6574, 'recall': 0.8372, 'f1_score': 0.7365}
Decision Tree: {'accuracy': 0.8268, 'precision': 0.6083, 'recall': 0.7702, 'f1_score': 0.6797}
Naive Bayes: {'accuracy': 0.7637, 'precision': 0.503, 'recall': 0.8141, 'f1_score': 0.6218}
SVM: {'accuracy': 0.5669, 'precision': 0.327, 'recall': 0.7705, 'f1_score': 0.4592}
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 1559)
Shape of X_test after concatenation:  (21625, 1559)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1559)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1559)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1559, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.3917, Test Loss: 0.3069, F1: 0.7656, AUC: 0.9415
Epoch [10/30] Train Loss: 0.2620, Test Loss: 0.3340, F1: 0.7625, AUC: 0.9606
Epoch [20/30] Train Loss: 0.2500, Test Loss: 0.2015, F1: 0.8324, AUC: 0.9630
Mejores resultados en la época:  20
f1-score 0.8324088748019017
AUC según el mejor F1-score 0.962951598057425
Confusion Matrix:
 [[15731   734]
 [  958  4202]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_49953.png
Accuracy:   0.9218
Precision:  0.8513
Recall:     0.8143
F1-score:   0.8324

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.3948, Test Loss: 0.4217, F1: 0.6992, AUC: 0.9430
Epoch [10/30] Train Loss: 0.2594, Test Loss: 0.2095, F1: 0.8287, AUC: 0.9607
Epoch [20/30] Train Loss: 0.2504, Test Loss: 0.2455, F1: 0.8122, AUC: 0.9631
Mejores resultados en la época:  28
f1-score 0.8326473245658067
AUC según el mejor F1-score 0.9638538643163678
Confusion Matrix:
 [[15811   654]
 [ 1013  4147]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_49953.png
Accuracy:   0.9229
Precision:  0.8638
Recall:     0.8037
F1-score:   0.8326

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5510, Test Loss: 0.3939, F1: 0.7539, AUC: 0.9248
Epoch [10/30] Train Loss: 0.2735, Test Loss: 0.2591, F1: 0.8012, AUC: 0.9577
Epoch [20/30] Train Loss: 0.2528, Test Loss: 0.2550, F1: 0.8056, AUC: 0.9609
Mejores resultados en la época:  25
f1-score 0.8313454903893543
AUC según el mejor F1-score 0.9620932704327008
Confusion Matrix:
 [[15697   768]
 [  943  4217]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_49953.png
Accuracy:   0.9209
Precision:  0.8459
Recall:     0.8172
F1-score:   0.8313

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4197, Test Loss: 0.3724, F1: 0.7251, AUC: 0.9373
Epoch [10/30] Train Loss: 0.2610, Test Loss: 0.2113, F1: 0.8249, AUC: 0.9594
Epoch [20/30] Train Loss: 0.2496, Test Loss: 0.2103, F1: 0.8280, AUC: 0.9622
Mejores resultados en la época:  17
f1-score 0.8303571428571429
AUC según el mejor F1-score 0.9615584679270334
Confusion Matrix:
 [[15730   735]
 [  975  4185]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_49953.png
Accuracy:   0.9209
Precision:  0.8506
Recall:     0.8110
F1-score:   0.8304
Tiempo total para red 1: 798.72 segundos

Entrenando red 2 con capas [1559, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3951, Test Loss: 0.2808, F1: 0.7826, AUC: 0.9443
Epoch [10/30] Train Loss: 0.2610, Test Loss: 0.2354, F1: 0.8188, AUC: 0.9610
Epoch [20/30] Train Loss: 0.2497, Test Loss: 0.2252, F1: 0.8244, AUC: 0.9631
Mejores resultados en la época:  27
f1-score 0.8329551060680809
AUC según el mejor F1-score 0.964447159466757
Confusion Matrix:
 [[15711   754]
 [  939  4221]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_101953.png
Accuracy:   0.9217
Precision:  0.8484
Recall:     0.8180
F1-score:   0.8330

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3732, Test Loss: 0.3425, F1: 0.7482, AUC: 0.9471
Epoch [10/30] Train Loss: 0.2643, Test Loss: 0.3823, F1: 0.7290, AUC: 0.9609
Epoch [20/30] Train Loss: 0.2450, Test Loss: 0.2537, F1: 0.8090, AUC: 0.9645
Mejores resultados en la época:  27
f1-score 0.8353563038371182
AUC según el mejor F1-score 0.9656926837995562
Confusion Matrix:
 [[15676   789]
 [  893  4267]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_101953.png
Accuracy:   0.9222
Precision:  0.8439
Recall:     0.8269
F1-score:   0.8354

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3789, Test Loss: 0.2459, F1: 0.8041, AUC: 0.9478
Epoch [10/30] Train Loss: 0.2651, Test Loss: 0.3024, F1: 0.7818, AUC: 0.9613
Epoch [20/30] Train Loss: 0.2475, Test Loss: 0.2234, F1: 0.8228, AUC: 0.9646
Mejores resultados en la época:  23
f1-score 0.830672748004561
AUC según el mejor F1-score 0.9655634514838853
Confusion Matrix:
 [[15472   993]
 [  789  4371]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_101953.png
Accuracy:   0.9176
Precision:  0.8149
==============================
Model: Naive Bayes
Accuracy:  0.7637
Precision: 0.5030
Recall:    0.8141
F1-score:  0.6218
              precision    recall  f1-score   support

           0       0.93      0.75      0.83     16465
           1       0.50      0.81      0.62      5160

    accuracy                           0.76     21625
   macro avg       0.72      0.78      0.72     21625
weighted avg       0.83      0.76      0.78     21625

[[12314  4151]
 [  959  4201]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.8918, 'precision': 0.7315, 'recall': 0.8636, 'f1_score': 0.792}
Random Forest: {'accuracy': 0.8791, 'precision': 0.7167, 'recall': 0.8155, 'f1_score': 0.7629}
Logistic Regression: {'accuracy': 0.8571, 'precision': 0.6574, 'recall': 0.8372, 'f1_score': 0.7365}
Decision Tree: {'accuracy': 0.8268, 'precision': 0.6083, 'recall': 0.7702, 'f1_score': 0.6797}
Naive Bayes: {'accuracy': 0.7637, 'precision': 0.503, 'recall': 0.8141, 'f1_score': 0.6218}
SVM: {'accuracy': 0.5669, 'precision': 0.327, 'recall': 0.7705, 'f1_score': 0.4592}
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 1559)
Shape of X_test after concatenation:  (21625, 1559)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1559)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1559)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1559, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4223, Test Loss: 0.3502, F1: 0.7377, AUC: 0.9370
Epoch [10/30] Train Loss: 0.2632, Test Loss: 0.2383, F1: 0.8149, AUC: 0.9594
Epoch [20/30] Train Loss: 0.2491, Test Loss: 0.2060, F1: 0.8303, AUC: 0.9622
Mejores resultados en la época:  20
f1-score 0.8303432137285491
AUC según el mejor F1-score 0.9622443425918732
Confusion Matrix:
 [[15627   838]
 [  902  4258]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_49953.png
Accuracy:   0.9195
Precision:  0.8356
Recall:     0.8252
F1-score:   0.8303

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4011, Test Loss: 0.2887, F1: 0.7768, AUC: 0.9407
Epoch [10/30] Train Loss: 0.2599, Test Loss: 0.2854, F1: 0.7870, AUC: 0.9604
Epoch [20/30] Train Loss: 0.2480, Test Loss: 0.2243, F1: 0.8225, AUC: 0.9629
Mejores resultados en la época:  28
f1-score 0.8336134453781513
AUC según el mejor F1-score 0.9640064077665332
Confusion Matrix:
 [[15726   739]
 [  944  4216]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_49953.png
Accuracy:   0.9222
Precision:  0.8509
Recall:     0.8171
F1-score:   0.8336

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4990, Test Loss: 0.3823, F1: 0.7182, AUC: 0.9307
Epoch [10/30] Train Loss: 0.2659, Test Loss: 0.3101, F1: 0.7737, AUC: 0.9578
Epoch [20/30] Train Loss: 0.2525, Test Loss: 0.2748, F1: 0.7954, AUC: 0.9608
Mejores resultados en la época:  23
f1-score 0.8299881936245572
AUC según el mejor F1-score 0.9614055125153897
Confusion Matrix:
 [[15679   786]
 [  942  4218]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_49953.png
Accuracy:   0.9201
Precision:  0.8429
Recall:     0.8174
F1-score:   0.8300

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5402, Test Loss: 0.5137, F1: 0.7034, AUC: 0.9188
Epoch [10/30] Train Loss: 0.2713, Test Loss: 0.3078, F1: 0.7823, AUC: 0.9569
Epoch [20/30] Train Loss: 0.2550, Test Loss: 0.2139, F1: 0.8295, AUC: 0.9582
Mejores resultados en la época:  28
f1-score 0.831651513637726
AUC según el mejor F1-score 0.9600154544405916
Confusion Matrix:
 [[15778   687]
 [  998  4162]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_49953.png
Accuracy:   0.9221
Precision:  0.8583
Recall:     0.8066
F1-score:   0.8317
Tiempo total para red 1: 796.91 segundos

Entrenando red 2 con capas [1559, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3772, Test Loss: 0.2952, F1: 0.7784, AUC: 0.9476
Epoch [10/30] Train Loss: 0.2610, Test Loss: 0.2968, F1: 0.7813, AUC: 0.9614
Epoch [20/30] Train Loss: 0.2474, Test Loss: 0.1990, F1: 0.8165, AUC: 0.9645
Mejores resultados en la época:  28
f1-score 0.8340879364121139
AUC según el mejor F1-score 0.9660120363373563
Confusion Matrix:
 [[15831   634]
 [ 1015  4145]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_101953.png
Accuracy:   0.9237
Precision:  0.8673
Recall:     0.8033
F1-score:   0.8341

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3694, Test Loss: 0.2489, F1: 0.8051, AUC: 0.9482
Epoch [10/30] Train Loss: 0.2634, Test Loss: 0.2481, F1: 0.8116, AUC: 0.9612
Epoch [20/30] Train Loss: 0.2463, Test Loss: 0.1983, F1: 0.8343, AUC: 0.9648
Mejores resultados en la época:  25
f1-score 0.8346578794339988
AUC según el mejor F1-score 0.9661568761078823
Confusion Matrix:
 [[15613   852]
 [  854  4306]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_101953.png
Accuracy:   0.9211
Precision:  0.8348
Recall:     0.8345
F1-score:   0.8347

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3818, Test Loss: 0.3827, F1: 0.7248, AUC: 0.9468
Epoch [10/30] Train Loss: 0.2582, Test Loss: 0.2564, F1: 0.8085, AUC: 0.9615
Epoch [20/30] Train Loss: 0.2484, Test Loss: 0.2809, F1: 0.7929, AUC: 0.9644
Mejores resultados en la época:  18
f1-score 0.8343643983236879
AUC según el mejor F1-score 0.9636463122385516
Confusion Matrix:
 [[15784   681]
 [  979  4181]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_101953.png
Accuracy:   0.9232
Precision:  0.8599
Recall:     0.8103
F1-score:   0.8344

--- Iteración 4 de 4 para la red 2 ---
Recall:     0.8471
F1-score:   0.8307

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3931, Test Loss: 0.2582, F1: 0.7972, AUC: 0.9444
Epoch [10/30] Train Loss: 0.2666, Test Loss: 0.3385, F1: 0.7570, AUC: 0.9609
Epoch [20/30] Train Loss: 0.2491, Test Loss: 0.2313, F1: 0.8197, AUC: 0.9635
Mejores resultados en la época:  22
f1-score 0.8314146341463414
AUC según el mejor F1-score 0.9638355261454294
Confusion Matrix:
 [[15636   829]
 [  899  4261]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_101953.png
Accuracy:   0.9201
Precision:  0.8371
Recall:     0.8258
F1-score:   0.8314
Tiempo total para red 2: 805.43 segundos

Entrenando red 3 con capas [1559, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3766, Test Loss: 0.2602, F1: 0.7946, AUC: 0.9478
Epoch [10/30] Train Loss: 0.2598, Test Loss: 0.3195, F1: 0.7719, AUC: 0.9615
Epoch [20/30] Train Loss: 0.2444, Test Loss: 0.2070, F1: 0.8291, AUC: 0.9654
Mejores resultados en la época:  26
f1-score 0.8338989624745466
AUC según el mejor F1-score 0.9666661840832208
Confusion Matrix:
 [[15612   853]
 [  860  4300]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_210049.png
Accuracy:   0.9208
Precision:  0.8345
Recall:     0.8333
F1-score:   0.8339

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3809, Test Loss: 0.2855, F1: 0.7827, AUC: 0.9464
Epoch [10/30] Train Loss: 0.2612, Test Loss: 0.2637, F1: 0.8075, AUC: 0.9618
Epoch [20/30] Train Loss: 0.2457, Test Loss: 0.2064, F1: 0.8148, AUC: 0.9645
Mejores resultados en la época:  17
f1-score 0.833071792859945
AUC según el mejor F1-score 0.9641656720739552
Confusion Matrix:
 [[15676   789]
 [  913  4247]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_210049.png
Accuracy:   0.9213
Precision:  0.8433
Recall:     0.8231
F1-score:   0.8331

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3760, Test Loss: 0.2598, F1: 0.7989, AUC: 0.9493
Epoch [10/30] Train Loss: 0.2645, Test Loss: 0.3706, F1: 0.7375, AUC: 0.9616
Epoch [20/30] Train Loss: 0.2484, Test Loss: 0.3127, F1: 0.7645, AUC: 0.9647
Mejores resultados en la época:  29
f1-score 0.8345993163691606
AUC según el mejor F1-score 0.9672437011090003
Confusion Matrix:
 [[15488   977]
 [  765  4395]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_210049.png
Accuracy:   0.9194
Precision:  0.8181
Recall:     0.8517
F1-score:   0.8346

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3711, Test Loss: 0.2351, F1: 0.8066, AUC: 0.9482
Epoch [10/30] Train Loss: 0.2595, Test Loss: 0.2588, F1: 0.8030, AUC: 0.9625
Epoch [20/30] Train Loss: 0.2438, Test Loss: 0.2225, F1: 0.8255, AUC: 0.9657
Mejores resultados en la época:  27
f1-score 0.8351318944844125
AUC según el mejor F1-score 0.9667357349510471
Confusion Matrix:
 [[15796   669]
 [  981  4179]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_210049.png
Accuracy:   0.9237
Precision:  0.8620
Recall:     0.8099
F1-score:   0.8351
Tiempo total para red 3: 815.88 segundos

Entrenando red 4 con capas [1559, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3750, Test Loss: 0.4371, F1: 0.7044, AUC: 0.9490
Epoch [10/30] Train Loss: 0.2604, Test Loss: 0.2255, F1: 0.8187, AUC: 0.9626
Epoch [20/30] Train Loss: 0.2461, Test Loss: 0.2142, F1: 0.8306, AUC: 0.9655
Mejores resultados en la época:  29
f1-score 0.8347021618049656
AUC según el mejor F1-score 0.9678721483437972
Confusion Matrix:
 [[15453  1012]
 [  739  4421]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_442625.png
Accuracy:   0.9190
Precision:  0.8137
Recall:     0.8568
F1-score:   0.8347

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3739, Test Loss: 0.2550, F1: 0.8025, AUC: 0.9482
Epoch [10/30] Train Loss: 0.2620, Test Loss: 0.2469, F1: 0.8143, AUC: 0.9625
Epoch [20/30] Train Loss: 0.2471, Test Loss: 0.2472, F1: 0.8048, AUC: 0.9658
Mejores resultados en la época:  29
f1-score 0.8360823781567421
AUC según el mejor F1-score 0.967860225001589
Confusion Matrix:
 [[15380  1085]
 [  674  4486]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_442625.png
Accuracy:   0.9187
Precision:  0.8052
Recall:     0.8694
F1-score:   0.8361

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3805, Test Loss: 0.2970, F1: 0.7786, AUC: 0.9476
Epoch [10/30] Train Loss: 0.2609, Test Loss: 0.2013, F1: 0.8270, AUC: 0.9624
Epoch [20/30] Train Loss: 0.2476, Test Loss: 0.3502, F1: 0.7493, AUC: 0.9654
Mejores resultados en la época:  26
f1-score 0.8351369796236716
AUC según el mejor F1-score 0.9669314872750984
Confusion Matrix:
 [[15651   814]
 [  877  4283]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_442625.png
Accuracy:   0.9218
Precision:  0.8403
Recall:     0.8300
F1-score:   0.8351

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3702, Test Loss: 0.3417, F1: 0.7598, AUC: 0.9494
Epoch [10/30] Train Loss: 0.2612, Test Loss: 0.3228, F1: 0.7697, AUC: 0.9622
Epoch [20/30] Train Loss: 0.2452, Test Loss: 0.2181, F1: 0.8242, AUC: 0.9658
Mejores resultados en la época:  18
f1-score 0.8330897398421514
AUC según el mejor F1-score 0.965283141123878
Confusion Matrix:
 [[15637   828]
 [  885  4275]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_442625.png
Accuracy:   0.9208
Precision:  0.8377
Recall:     0.8285
F1-score:   0.8331
Tiempo total para red 4: 831.80 segundos

Entrenando red 5 con capas [1559, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3747, Test Loss: 0.3143, F1: 0.7758, AUC: 0.9491
Epoch [10/30] Train Loss: 0.2599, Test Loss: 0.2164, F1: 0.8252, AUC: 0.9627
Epoch [20/30] Train Loss: 0.2414, Test Loss: 0.2386, F1: 0.8170, AUC: 0.9662
Mejores resultados en la época:  23
f1-score 0.8353933686482244
AUC según el mejor F1-score 0.9663995273036297
Confusion Matrix:
 [[15689   776]
 [  902  4258]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_971265.png
Accuracy:   0.9224
Precision:  0.8458
Recall:     0.8252
F1-score:   0.8354

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3721, Test Loss: 0.2348, F1: 0.7780, AUC: 0.9504
Epoch [10/30] Train Loss: 0.2629, Test Loss: 0.2994, F1: 0.7748, AUC: 0.9625
Epoch [20/30] Train Loss: 0.2520, Test Loss: 0.2295, F1: 0.8190, AUC: 0.9656
Mejores resultados en la época:  29
f1-score 0.8349239346121138
AUC según el mejor F1-score 0.9681511757380583
Confusion Matrix:
 [[15460  1005]
 [  742  4418]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_971265.png
Accuracy:   0.9192
Precision:  0.8147
Recall:     0.8562
F1-score:   0.8349

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3693, Test Loss: 0.2944, F1: 0.7794, AUC: 0.9503
Epoch [10/30] Train Loss: 0.2636, Test Loss: 0.2621, F1: 0.8004, AUC: 0.9626
Epoch [20/30] Train Loss: 0.2480, Test Loss: 0.3167, F1: 0.7859, AUC: 0.9651
Mejores resultados en la época:  29
f1-score 0.8348597319965294
AUC según el mejor F1-score 0.9674090565611339
Confusion Matrix:
 [[15582   883]
 [  830  4330]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_971265.png
Accuracy:   0.9208
Precision:  0.8306
Recall:     0.8391
F1-score:   0.8349

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3729, Test Loss: 0.2849, F1: 0.7905, AUC: 0.9497
Epoch [10/30] Train Loss: 0.2626, Test Loss: 0.2456, F1: 0.8115, AUC: 0.9625
Epoch [20/30] Train Loss: 0.2425, Test Loss: 0.2328, F1: 0.8182, AUC: 0.9661
Mejores resultados en la época:  24
f1-score 0.8349610338364407
AUC según el mejor F1-score 0.9665947087667754
Confusion Matrix:
 [[15720   745]
Epoch [0/30] Train Loss: 0.3850, Test Loss: 0.3093, F1: 0.7675, AUC: 0.9457
Epoch [10/30] Train Loss: 0.2555, Test Loss: 0.2043, F1: 0.8305, AUC: 0.9614
Epoch [20/30] Train Loss: 0.2444, Test Loss: 0.1963, F1: 0.8299, AUC: 0.9647
Mejores resultados en la época:  23
f1-score 0.8342880112551503
AUC según el mejor F1-score 0.9655065007521239
Confusion Matrix:
 [[15825   640]
 [ 1009  4151]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_101953.png
Accuracy:   0.9237
Precision:  0.8664
Recall:     0.8045
F1-score:   0.8343
Tiempo total para red 2: 807.05 segundos

Entrenando red 3 con capas [1559, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3694, Test Loss: 0.3307, F1: 0.7563, AUC: 0.9490
Epoch [10/30] Train Loss: 0.2648, Test Loss: 0.2057, F1: 0.8287, AUC: 0.9618
Epoch [20/30] Train Loss: 0.2464, Test Loss: 0.2154, F1: 0.8266, AUC: 0.9655
Mejores resultados en la época:  22
f1-score 0.8332683307332294
AUC según el mejor F1-score 0.9660453463654404
Confusion Matrix:
 [[15642   823]
 [  887  4273]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_210049.png
Accuracy:   0.9209
Precision:  0.8385
Recall:     0.8281
F1-score:   0.8333

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3675, Test Loss: 0.3600, F1: 0.7409, AUC: 0.9491
Epoch [10/30] Train Loss: 0.2684, Test Loss: 0.2905, F1: 0.7797, AUC: 0.9616
Epoch [20/30] Train Loss: 0.2507, Test Loss: 0.2495, F1: 0.8014, AUC: 0.9636
Mejores resultados en la época:  26
f1-score 0.833434650455927
AUC según el mejor F1-score 0.9664568134897373
Confusion Matrix:
 [[15868   597]
 [ 1047  4113]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_210049.png
Accuracy:   0.9240
Precision:  0.8732
Recall:     0.7971
F1-score:   0.8334

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3868, Test Loss: 0.2387, F1: 0.7930, AUC: 0.9470
Epoch [10/30] Train Loss: 0.2610, Test Loss: 0.2614, F1: 0.8006, AUC: 0.9614
Epoch [20/30] Train Loss: 0.2444, Test Loss: 0.2632, F1: 0.8048, AUC: 0.9653
Mejores resultados en la época:  27
f1-score 0.8340969835815196
AUC según el mejor F1-score 0.9665746227021376
Confusion Matrix:
 [[15518   947]
 [  791  4369]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_210049.png
Accuracy:   0.9196
Precision:  0.8219
Recall:     0.8467
F1-score:   0.8341

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3660, Test Loss: 0.4208, F1: 0.7058, AUC: 0.9490
Epoch [10/30] Train Loss: 0.2608, Test Loss: 0.3308, F1: 0.7682, AUC: 0.9621
Epoch [20/30] Train Loss: 0.2470, Test Loss: 0.1981, F1: 0.8337, AUC: 0.9654
Mejores resultados en la época:  20
f1-score 0.8337420304070623
AUC según el mejor F1-score 0.9654063352613131
Confusion Matrix:
 [[15680   785]
 [  910  4250]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_210049.png
Accuracy:   0.9216
Precision:  0.8441
Recall:     0.8236
F1-score:   0.8337
Tiempo total para red 3: 817.78 segundos

Entrenando red 4 con capas [1559, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3821, Test Loss: 0.2502, F1: 0.7825, AUC: 0.9478
Epoch [10/30] Train Loss: 0.2581, Test Loss: 0.2194, F1: 0.8284, AUC: 0.9621
Epoch [20/30] Train Loss: 0.2473, Test Loss: 0.2101, F1: 0.8288, AUC: 0.9658
Mejores resultados en la época:  25
f1-score 0.8360413589364845
AUC según el mejor F1-score 0.9667563389101146
Confusion Matrix:
 [[15715   750]
 [  915  4245]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_442625.png
Accuracy:   0.9230
Precision:  0.8498
Recall:     0.8227
F1-score:   0.8360

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3629, Test Loss: 0.8115, F1: 0.5568, AUC: 0.9503
Epoch [10/30] Train Loss: 0.2631, Test Loss: 0.2089, F1: 0.8298, AUC: 0.9622
Epoch [20/30] Train Loss: 0.2450, Test Loss: 0.2115, F1: 0.8293, AUC: 0.9657
Mejores resultados en la época:  26
f1-score 0.8350281370322835
AUC según el mejor F1-score 0.9664539944961946
Confusion Matrix:
 [[15725   740]
 [  931  4229]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_442625.png
Accuracy:   0.9227
Precision:  0.8511
Recall:     0.8196
F1-score:   0.8350

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3731, Test Loss: 0.2798, F1: 0.7937, AUC: 0.9492
Epoch [10/30] Train Loss: 0.2643, Test Loss: 0.2491, F1: 0.8103, AUC: 0.9621
Epoch [20/30] Train Loss: 0.2494, Test Loss: 0.2550, F1: 0.8038, AUC: 0.9656
Mejores resultados en la época:  29
f1-score 0.8372972435068166
AUC según el mejor F1-score 0.9673793364830731
Confusion Matrix:
 [[15783   682]
 [  953  4207]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_442625.png
Accuracy:   0.9244
Precision:  0.8605
Recall:     0.8153
F1-score:   0.8373

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3723, Test Loss: 0.2800, F1: 0.7939, AUC: 0.9499
Epoch [10/30] Train Loss: 0.2635, Test Loss: 0.4412, F1: 0.7032, AUC: 0.9617
Epoch [20/30] Train Loss: 0.2418, Test Loss: 0.1966, F1: 0.8327, AUC: 0.9657
Mejores resultados en la época:  24
f1-score 0.8348542831910509
AUC según el mejor F1-score 0.9667403960009133
Confusion Matrix:
 [[15688   777]
 [  906  4254]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_442625.png
Accuracy:   0.9222
Precision:  0.8456
Recall:     0.8244
F1-score:   0.8349
Tiempo total para red 4: 834.17 segundos

Entrenando red 5 con capas [1559, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3812, Test Loss: 0.2319, F1: 0.8011, AUC: 0.9487
Epoch [10/30] Train Loss: 0.2607, Test Loss: 0.2286, F1: 0.8234, AUC: 0.9627
Epoch [20/30] Train Loss: 0.2440, Test Loss: 0.2010, F1: 0.8349, AUC: 0.9658
Mejores resultados en la época:  27
f1-score 0.8351309707241911
AUC según el mejor F1-score 0.9670751323573376
Confusion Matrix:
 [[15577   888]
 [  824  4336]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_971265.png
Accuracy:   0.9208
Precision:  0.8300
Recall:     0.8403
F1-score:   0.8351

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3824, Test Loss: 0.3990, F1: 0.7224, AUC: 0.9483
Epoch [10/30] Train Loss: 0.2590, Test Loss: 0.5707, F1: 0.6192, AUC: 0.9611
Epoch [20/30] Train Loss: 0.2416, Test Loss: 0.2763, F1: 0.7888, AUC: 0.9659
Mejores resultados en la época:  22
f1-score 0.8328809616130283
AUC según el mejor F1-score 0.9663681770351485
Confusion Matrix:
 [[15605   860]
 [  864  4296]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_971265.png
Accuracy:   0.9203
Precision:  0.8332
Recall:     0.8326
F1-score:   0.8329

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3777, Test Loss: 0.3484, F1: 0.7492, AUC: 0.9488
Epoch [10/30] Train Loss: 0.2606, Test Loss: 0.3528, F1: 0.7420, AUC: 0.9618
Epoch [20/30] Train Loss: 0.2477, Test Loss: 0.2784, F1: 0.7947, AUC: 0.9654
Mejores resultados en la época:  16
f1-score 0.8324903302588516
AUC según el mejor F1-score 0.9643722413293878
Confusion Matrix:
 [[15739   726]
 [  963  4197]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_971265.png
Accuracy:   0.9219
Precision:  0.8525
Recall:     0.8134
F1-score:   0.8325

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3898, Test Loss: 0.4163, F1: 0.7304, AUC: 0.9472
Epoch [10/30] Train Loss: 0.2612, Test Loss: 0.2740, F1: 0.7902, AUC: 0.9621
Epoch [20/30] Train Loss: 0.2425, Test Loss: 0.2137, F1: 0.8252, AUC: 0.9662
Mejores resultados en la época:  28
f1-score 0.8360753221010901
AUC según el mejor F1-score 0.9669227360362715
Confusion Matrix:
 [[15753   712]
 [  942  4218]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [07:46:03] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [07:46:41] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
 [  928  4232]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_971265.png
Accuracy:   0.9226
Precision:  0.8503
Recall:     0.8202
F1-score:   0.8350
Tiempo total para red 5: 872.79 segundos

Entrenando red 6 con capas [1559, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3876, Test Loss: 0.3101, F1: 0.7770, AUC: 0.9484
Epoch [10/30] Train Loss: 0.2563, Test Loss: 0.3438, F1: 0.7058, AUC: 0.9625
Epoch [20/30] Train Loss: 0.2425, Test Loss: 0.2279, F1: 0.8218, AUC: 0.9664
Mejores resultados en la época:  22
f1-score 0.8341410345168628
AUC según el mejor F1-score 0.9661730073423305
Confusion Matrix:
 [[15731   734]
 [  943  4217]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_2296833.png
Accuracy:   0.9225
Precision:  0.8517
Recall:     0.8172
F1-score:   0.8341

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3857, Test Loss: 0.3496, F1: 0.7768, AUC: 0.9490
Epoch [10/30] Train Loss: 0.2598, Test Loss: 0.3175, F1: 0.7648, AUC: 0.9625
Epoch [20/30] Train Loss: 0.2488, Test Loss: 0.2326, F1: 0.8058, AUC: 0.9656
Mejores resultados en la época:  19
f1-score 0.8347437019745161
AUC según el mejor F1-score 0.9657967158430968
Confusion Matrix:
 [[15635   830]
 [  869  4291]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_2296833.png
Accuracy:   0.9214
Precision:  0.8379
Recall:     0.8316
F1-score:   0.8347

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3836, Test Loss: 0.4703, F1: 0.6972, AUC: 0.9487
Epoch [10/30] Train Loss: 0.2652, Test Loss: 0.2977, F1: 0.7890, AUC: 0.9627
Epoch [20/30] Train Loss: 0.2435, Test Loss: 0.2063, F1: 0.8268, AUC: 0.9661
Mejores resultados en la época:  27
f1-score 0.8359206130870505
AUC según el mejor F1-score 0.9674303961656979
Confusion Matrix:
 [[15701   764]
 [  906  4254]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_2296833.png
Accuracy:   0.9228
Precision:  0.8477
Recall:     0.8244
F1-score:   0.8359

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3849, Test Loss: 0.2335, F1: 0.7901, AUC: 0.9481
Epoch [10/30] Train Loss: 0.2623, Test Loss: 0.2794, F1: 0.7932, AUC: 0.9625
Epoch [20/30] Train Loss: 0.2505, Test Loss: 0.1976, F1: 0.8318, AUC: 0.9658
Mejores resultados en la época:  24
f1-score 0.8329629987440827
AUC según el mejor F1-score 0.9665297895229958
Confusion Matrix:
 [[15585   880]
 [  849  4311]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_2296833.png
Accuracy:   0.9200
Precision:  0.8305
Recall:     0.8355
F1-score:   0.8330
Tiempo total para red 6: 911.51 segundos
Saved on: outputs_numerical_categorical_metadata/2/gpt

==============================
Model: Logistic Regression
Accuracy:  0.9070
Precision: 0.7591
Recall:    0.8940
F1-score:  0.8210
              precision    recall  f1-score   support

           0       0.96      0.91      0.94     16465
           1       0.76      0.89      0.82      5160

    accuracy                           0.91     21625
   macro avg       0.86      0.90      0.88     21625
weighted avg       0.92      0.91      0.91     21625

[[15001  1464]
 [  547  4613]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8066
Precision: 0.5654
Recall:    0.8194
F1-score:  0.6691
              precision    recall  f1-score   support

           0       0.93      0.80      0.86     16465
           1       0.57      0.82      0.67      5160

    accuracy                           0.81     21625
   macro avg       0.75      0.81      0.77     21625
weighted avg       0.85      0.81      0.82     21625

[[13215  3250]
 [  932  4228]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_svm.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8179
Precision: 0.5883
Recall:    0.7886
F1-score:  0.6739
              precision    recall  f1-score   support

           0       0.93      0.83      0.87     16465
           1       0.59      0.79      0.67      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.81      0.77     21625
weighted avg       0.85      0.82      0.83     21625

[[13618  2847]
 [ 1091  4069]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8964
Precision: 0.7526
Recall:    0.8426
F1-score:  0.7951
              precision    recall  f1-score   support

           0       0.95      0.91      0.93     16465
           1       0.75      0.84      0.80      5160

    accuracy                           0.90     21625
   macro avg       0.85      0.88      0.86     21625
weighted avg       0.90      0.90      0.90     21625

[[15036  1429]
 [  812  4348]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9181
Precision: 0.7926
Recall:    0.8895
F1-score:  0.8383
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.79      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15264  1201]
 [  570  4590]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8561
Precision: 0.6676
Recall:    0.7907
F1-score:  0.7240
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.79      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

[[14434  2031]
 [ 1080  4080]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.9181, 'precision': 0.7926, 'recall': 0.8895, 'f1_score': 0.8383}
Logistic Regression: {'accuracy': 0.907, 'precision': 0.7591, 'recall': 0.894, 'f1_score': 0.821}
Random Forest: {'accuracy': 0.8964, 'precision': 0.7526, 'recall': 0.8426, 'f1_score': 0.7951}
Naive Bayes: {'accuracy': 0.8561, 'precision': 0.6676, 'recall': 0.7907, 'f1_score': 0.724}
Decision Tree: {'accuracy': 0.8179, 'precision': 0.5883, 'recall': 0.7886, 'f1_score': 0.6739}
SVM: {'accuracy': 0.8066, 'precision': 0.5654, 'recall': 0.8194, 'f1_score': 0.6691}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
MLP_5843969: {'accuracy': 0.9600462427745665, 'precision': 0.9143518518518519, 'recall': 0.9186046511627907, 'f1_score': 0.9164733178654292, 'f1_score_avg': 0.9137997310395869}
MLP_2744833: {'accuracy': 0.9554219653179191, 'precision': 0.8824279985417426, 'recall': 0.9381782945736434, 'f1_score': 0.9110197840007646, 'f1_score_avg': 0.9096670769335431}
MLP_1329409: {'accuracy': 0.9570404624277457, 'precision': 0.9105375509411993, 'recall': 0.9093023255813953, 'f1_score': 0.9099195190536217, 'f1_score_avg': 0.9066608297487367}
MLP_653441: {'accuracy': 0.9475606936416185, 'precision': 0.862180640518172, 'recall': 0.9286821705426357, 'f1_score': 0.9033652942282384, 'f1_score_avg': 0.899685332381764}
MLP_323649: {'accuracy': 0.9445549132947977, 'precision': 0.8497262934840191, 'recall': 0.9325581395348838, 'f1_score': 0.892983293556086, 'f1_score_avg': 0.8902830093205162}
MLP_160801: {'accuracy': 0.944, 'precision': 0.8573755656108597, 'recall': 0.9180232558139535, 'f1_score': 0.8866635470285447, 'f1_score_avg': 0.881443825858503}
Logistic Regression: {'accuracy': 0.9359, 'precision': 0.8261, 'recall': 0.9262, 'f1_score': 0.8733}
XGBoost: {'accuracy': 0.9213, 'precision': 0.8015, 'recall': 0.8911, 'f1_score': 0.8439}
Naive Bayes: {'accuracy': 0.9206, 'precision': 0.8092, 'recall': 0.8729, 'f1_score': 0.8398}
Decision Tree: {'accuracy': 0.8709, 'precision': 0.7116, 'recall': 0.7721, 'f1_score': 0.7406}
Random Forest: {'accuracy': 0.8635, 'precision': 0.6869, 'recall': 0.7866, 'f1_score': 0.7334}
SVM: {'accuracy': 0.7706, 'precision': 0.5108, 'recall': 0.9079, 'f1_score': 0.6538}


EMBEDDINGS TYPE: LYRICS_BERT
XGBoost: {'accuracy': 0.8918, 'precision': 0.7315, 'recall': 0.8636, 'f1_score': 0.792}
MLP_22849: {'accuracy': 0.8812947976878612, 'precision': 0.7221175261264348, 'recall': 0.8168604651162791, 'f1_score': 0.7837510105092966, 'f1_score_avg': 0.7746620759566909}
MLP_51841: {'accuracy': 0.8874913294797688, 'precision': 0.7589743589743589, 'recall': 0.7744186046511627, 'f1_score': 0.7837510105092966, 'f1_score_avg': 0.7697118202880832}
MLP_126209: {'accuracy': 0.8956763005780347, 'precision': 0.7885532591414944, 'recall': 0.7689922480620155, 'f1_score': 0.7837510105092966, 'f1_score_avg': 0.7728391930525607}
MLP_338433: {'accuracy': 0.8962774566473989, 'precision': 0.7881841533293815, 'recall': 0.773062015503876, 'f1_score': 0.7837510105092966, 'f1_score_avg': 0.7769763310000752}
MLP_1031169: {'accuracy': 0.8868901734104047, 'precision': 0.7453887884267631, 'recall': 0.7988372093023256, 'f1_score': 0.7837510105092966, 'f1_score_avg': 0.7744861714791972}
MLP_10401: {'accuracy': 0.8908208092485549, 'precision': 0.7766356987546946, 'recall': 0.7614341085271318, 'f1_score': 0.7689597808004697, 'f1_score_avg': 0.766535329016088}
Random Forest: {'accuracy': 0.8791, 'precision': 0.7167, 'recall': 0.8155, 'f1_score': 0.7629}
Logistic Regression: {'accuracy': 0.8571, 'precision': 0.6574, 'recall': 0.8372, 'f1_score': 0.7365}
Decision Tree: {'accuracy': 0.8268, 'precision': 0.6083, 'recall': 0.7702, 'f1_score': 0.6797}
Naive Bayes: {'accuracy': 0.7637, 'precision': 0.503, 'recall': 0.8141, 'f1_score': 0.6218}
SVM: {'accuracy': 0.5669, 'precision': 0.327, 'recall': 0.7705, 'f1_score': 0.4592}


EMBEDDINGS TYPE: GPT
XGBoost: {'accuracy': 0.9181, 'precision': 0.7926, 'recall': 0.8895, 'f1_score': 0.8383}
MLP_442625: {'accuracy': 0.9207861271676301, 'precision': 0.8377425044091711, 'recall': 0.8284883720930233, 'f1_score': 0.8360823781567421, 'f1_score_avg': 0.8347528148568826}
MLP_971265: {'accuracy': 0.922635838150289, 'precision': 0.8503114325899136, 'recall': 0.8201550387596899, 'f1_score': 0.8360823781567421, 'f1_score_avg': 0.835034517273327}
MLP_2296833: {'accuracy': 0.9200462427745665, 'precision': 0.8304758235407436, 'recall': 0.8354651162790697, 'f1_score': 0.8360823781567421, 'f1_score_avg': 0.8344420870806281}
MLP_101953: {'accuracy': 0.9200924855491329, 'precision': 0.8371316306483301, 'recall': 0.8257751937984497, 'f1_score': 0.8353563038371182, 'f1_score_avg': 0.8325996980140253}
MLP_210049: {'accuracy': 0.923699421965318, 'precision': 0.8620049504950495, 'recall': 0.8098837209302325, 'f1_score': 0.8353563038371182, 'f1_score_avg': 0.8341754915470162}
MLP_49953: {'accuracy': 0.9209248554913295, 'precision': 0.850609756097561, 'recall': 0.811046511627907, 'f1_score': 0.8326473245658067, 'f1_score_avg': 0.8316897081535514}
Logistic Regression: {'accuracy': 0.907, 'precision': 0.7591, 'recall': 0.894, 'f1_score': 0.821}
Random Forest: {'accuracy': 0.8964, 'precision': 0.7526, 'recall': 0.8426, 'f1_score': 0.7951}
Naive Bayes: {'accuracy': 0.8561, 'precision': 0.6676, 'recall': 0.7907, 'f1_score': 0.724}
Decision Tree: {'accuracy': 0.8179, 'precision': 0.5883, 'recall': 0.7886, 'f1_score': 0.6739}
SVM: {'accuracy': 0.8066, 'precision': 0.5654, 'recall': 0.8194, 'f1_score': 0.6691}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
====================================

Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_971265.png
Accuracy:   0.9235
Precision:  0.8556
Recall:     0.8174
F1-score:   0.8361
Tiempo total para red 5: 878.74 segundos

Entrenando red 6 con capas [1559, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3872, Test Loss: 0.3270, F1: 0.7631, AUC: 0.9485
Epoch [10/30] Train Loss: 0.2602, Test Loss: 0.2023, F1: 0.8287, AUC: 0.9627
Epoch [20/30] Train Loss: 0.2437, Test Loss: 0.2471, F1: 0.8039, AUC: 0.9660
Mejores resultados en la época:  19
f1-score 0.8344018147746326
AUC según el mejor F1-score 0.9658331155822663
Confusion Matrix:
 [[15716   749]
 [  930  4230]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_2296833.png
Accuracy:   0.9224
Precision:  0.8496
Recall:     0.8198
F1-score:   0.8344

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3888, Test Loss: 0.3343, F1: 0.7604, AUC: 0.9495
Epoch [10/30] Train Loss: 0.2602, Test Loss: 0.2386, F1: 0.8206, AUC: 0.9628
Epoch [20/30] Train Loss: 0.2437, Test Loss: 0.1984, F1: 0.8300, AUC: 0.9653
Mejores resultados en la época:  27
f1-score 0.8357331766088745
AUC según el mejor F1-score 0.9671001266487288
Confusion Matrix:
 [[15682   783]
 [  894  4266]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_2296833.png
Accuracy:   0.9225
Precision:  0.8449
Recall:     0.8267
F1-score:   0.8357

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3796, Test Loss: 0.6040, F1: 0.6384, AUC: 0.9490
Epoch [10/30] Train Loss: 0.2623, Test Loss: 0.2749, F1: 0.7895, AUC: 0.9625
Epoch [20/30] Train Loss: 0.2427, Test Loss: 0.3584, F1: 0.7628, AUC: 0.9649
Mejores resultados en la época:  14
f1-score 0.8327849237212085
AUC según el mejor F1-score 0.9638659936393147
Confusion Matrix:
 [[15772   693]
 [  984  4176]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_2296833.png
Accuracy:   0.9225
Precision:  0.8577
Recall:     0.8093
F1-score:   0.8328

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3728, Test Loss: 0.3476, F1: 0.7575, AUC: 0.9502
Epoch [10/30] Train Loss: 0.2576, Test Loss: 0.2663, F1: 0.7885, AUC: 0.9627
Epoch [20/30] Train Loss: 0.2424, Test Loss: 0.2974, F1: 0.7848, AUC: 0.9658
Mejores resultados en la época:  29
f1-score 0.8368983957219251
AUC según el mejor F1-score 0.9670508148598036
Confusion Matrix:
 [[15535   930]
 [  778  4382]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_2296833.png
Accuracy:   0.9210
Precision:  0.8249
Recall:     0.8492
F1-score:   0.8369
Tiempo total para red 6: 885.69 segundos
Saved on: outputs_numerical_categorical_metadata/2/gpt

==============================
Model: Logistic Regression
Accuracy:  0.9070
Precision: 0.7591
Recall:    0.8940
F1-score:  0.8210
              precision    recall  f1-score   support

           0       0.96      0.91      0.94     16465
           1       0.76      0.89      0.82      5160

    accuracy                           0.91     21625
   macro avg       0.86      0.90      0.88     21625
weighted avg       0.92      0.91      0.91     21625

[[15001  1464]
 [  547  4613]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8066
Precision: 0.5654
Recall:    0.8194
F1-score:  0.6691
              precision    recall  f1-score   support

           0       0.93      0.80      0.86     16465
           1       0.57      0.82      0.67      5160

    accuracy                           0.81     21625
   macro avg       0.75      0.81      0.77     21625
weighted avg       0.85      0.81      0.82     21625

[[13215  3250]
 [  932  4228]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_svm.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8179
Precision: 0.5883
Recall:    0.7886
F1-score:  0.6739
              precision    recall  f1-score   support

           0       0.93      0.83      0.87     16465
           1       0.59      0.79      0.67      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.81      0.77     21625
weighted avg       0.85      0.82      0.83     21625

[[13618  2847]
 [ 1091  4069]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8964
Precision: 0.7526
Recall:    0.8426
F1-score:  0.7951
              precision    recall  f1-score   support

           0       0.95      0.91      0.93     16465
           1       0.75      0.84      0.80      5160

    accuracy                           0.90     21625
   macro avg       0.85      0.88      0.86     21625
weighted avg       0.90      0.90      0.90     21625

[[15036  1429]
 [  812  4348]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9181
Precision: 0.7926
Recall:    0.8895
F1-score:  0.8383
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.79      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15264  1201]
 [  570  4590]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8561
Precision: 0.6676
Recall:    0.7907
F1-score:  0.7240
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.79      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

[[14434  2031]
 [ 1080  4080]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.9181, 'precision': 0.7926, 'recall': 0.8895, 'f1_score': 0.8383}
Logistic Regression: {'accuracy': 0.907, 'precision': 0.7591, 'recall': 0.894, 'f1_score': 0.821}
Random Forest: {'accuracy': 0.8964, 'precision': 0.7526, 'recall': 0.8426, 'f1_score': 0.7951}
Naive Bayes: {'accuracy': 0.8561, 'precision': 0.6676, 'recall': 0.7907, 'f1_score': 0.724}
Decision Tree: {'accuracy': 0.8179, 'precision': 0.5883, 'recall': 0.7886, 'f1_score': 0.6739}
SVM: {'accuracy': 0.8066, 'precision': 0.5654, 'recall': 0.8194, 'f1_score': 0.6691}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
MLP_5843969: {'accuracy': 0.9549132947976878, 'precision': 0.8769591064673032, 'recall': 0.9434108527131783, 'f1_score': 0.9158067316038191, 'f1_score_avg': 0.9128019372113629}
MLP_2744833: {'accuracy': 0.956485549132948, 'precision': 0.8946679139382601, 'recall': 0.9267441860465117, 'f1_score': 0.911608497723824, 'f1_score_avg': 0.9087320437586952}
MLP_1329409: {'accuracy': 0.9523699421965318, 'precision': 0.8812776957163959, 'recall': 0.925, 'f1_score': 0.9089875935221139, 'f1_score_avg': 0.9054913516681352}
MLP_653441: {'accuracy': 0.9461734104046243, 'precision': 0.8521325343672894, 'recall': 0.937015503875969, 'f1_score': 0.8960414354421014, 'f1_score_avg': 0.8940254344897322}
MLP_323649: {'accuracy': 0.9458959537572255, 'precision': 0.8543516873889876, 'recall': 0.9321705426356589, 'f1_score': 0.891566265060241, 'f1_score_avg': 0.8905506320821275}
MLP_160801: {'accuracy': 0.939699421965318, 'precision': 0.8386020372321742, 'recall': 0.9253875968992248, 'f1_score': 0.8846261813418171, 'f1_score_avg': 0.8823967769390846}
Logistic Regression: {'accuracy': 0.9359, 'precision': 0.8261, 'recall': 0.9262, 'f1_score': 0.8733}
XGBoost: {'accuracy': 0.9213, 'precision': 0.8015, 'recall': 0.8911, 'f1_score': 0.8439}
Naive Bayes: {'accuracy': 0.9206, 'precision': 0.8092, 'recall': 0.8729, 'f1_score': 0.8398}
Decision Tree: {'accuracy': 0.8709, 'precision': 0.7116, 'recall': 0.7721, 'f1_score': 0.7406}
Random Forest: {'accuracy': 0.8635, 'precision': 0.6869, 'recall': 0.7866, 'f1_score': 0.7334}
SVM: {'accuracy': 0.7706, 'precision': 0.5108, 'recall': 0.9079, 'f1_score': 0.6538}


EMBEDDINGS TYPE: LYRICS_BERT
XGBoost: {'accuracy': 0.8918, 'precision': 0.7315, 'recall': 0.8636, 'f1_score': 0.792}
MLP_1031169: {'accuracy': 0.8944277456647399, 'precision': 0.7762627232571538, 'recall': 0.7833333333333333, 'f1_score': 0.7797820005787596, 'f1_score_avg': 0.7746522115814314}
MLP_338433: {'accuracy': 0.8899884393063584, 'precision': 0.7682809183870345, 'recall': 0.7717054263565891, 'f1_score': 0.7794688152790212, 'f1_score_avg': 0.7720489410508544}
MLP_126209: {'accuracy': 0.8944739884393064, 'precision': 0.7779065276168404, 'recall': 0.7806201550387597, 'f1_score': 0.779260978912749, 'f1_score_avg': 0.7748199790120243}
MLP_51841: {'accuracy': 0.8850404624277457, 'precision': 0.7407274036730285, 'recall': 0.7972868217054263, 'f1_score': 0.7789578163771712, 'f1_score_avg': 0.773115816871287}
MLP_10401: {'accuracy': 0.8957225433526012, 'precision': 0.8010362694300518, 'recall': 0.749031007751938, 'f1_score': 0.7741612418627942, 'f1_score_avg': 0.7650553655420933}
MLP_22849: {'accuracy': 0.8882312138728323, 'precision': 0.7527178920213746, 'recall': 0.7916666666666666, 'f1_score': 0.7741612418627942, 'f1_score_avg': 0.7695270627459484}
Random Forest: {'accuracy': 0.8791, 'precision': 0.7167, 'recall': 0.8155, 'f1_score': 0.7629}
Logistic Regression: {'accuracy': 0.8571, 'precision': 0.6574, 'recall': 0.8372, 'f1_score': 0.7365}
Decision Tree: {'accuracy': 0.8268, 'precision': 0.6083, 'recall': 0.7702, 'f1_score': 0.6797}
Naive Bayes: {'accuracy': 0.7637, 'precision': 0.503, 'recall': 0.8141, 'f1_score': 0.6218}
SVM: {'accuracy': 0.5669, 'precision': 0.327, 'recall': 0.7705, 'f1_score': 0.4592}


EMBEDDINGS TYPE: GPT
XGBoost: {'accuracy': 0.9181, 'precision': 0.7926, 'recall': 0.8895, 'f1_score': 0.8383}
MLP_442625: {'accuracy': 0.9221734104046243, 'precision': 0.8455575432319619, 'recall': 0.8244186046511628, 'f1_score': 0.8372972435068166, 'f1_score_avg': 0.8358052556666589}
MLP_971265: {'accuracy': 0.923514450867052, 'precision': 0.855578093306288, 'recall': 0.8174418604651162, 'f1_score': 0.8372972435068166, 'f1_score_avg': 0.8341443961742903}
MLP_2296833: {'accuracy': 0.9210173410404624, 'precision': 0.8249246987951807, 'recall': 0.8492248062015504, 'f1_score': 0.8372972435068166, 'f1_score_avg': 0.8349545777066602}
MLP_101953: {'accuracy': 0.9237456647398844, 'precision': 0.8664161970361094, 'recall': 0.8044573643410853, 'f1_score': 0.8346578794339988, 'f1_score_avg': 0.8343495563562378}
MLP_210049: {'accuracy': 0.9216184971098266, 'precision': 0.8440913604766633, 'recall': 0.8236434108527132, 'f1_score': 0.8346578794339988, 'f1_score_avg': 0.8336354987944345}
MLP_49953: {'accuracy': 0.9220809248554913, 'precision': 0.8583213033615178, 'recall': 0.8065891472868217, 'f1_score': 0.8336134453781513, 'f1_score_avg': 0.831399091592246}
Logistic Regression: {'accuracy': 0.907, 'precision': 0.7591, 'recall': 0.894, 'f1_score': 0.821}
Random Forest: {'accuracy': 0.8964, 'precision': 0.7526, 'recall': 0.8426, 'f1_score': 0.7951}
Naive Bayes: {'accuracy': 0.8561, 'precision': 0.6676, 'recall': 0.7907, 'f1_score': 0.724}
Decision Tree: {'accuracy': 0.8179, 'precision': 0.5883, 'recall': 0.7886, 'f1_score': 0.6739}
SVM: {'accuracy': 0.8066, 'precision': 0.5654, 'recall': 0.8194, 'f1_score': 0.6691}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
====================================

