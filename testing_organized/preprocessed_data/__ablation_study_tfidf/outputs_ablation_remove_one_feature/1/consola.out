/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 15 ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']

CAT_LOW 3 ['emotion', 'Key', 'Time signature']
CAT_HIGH 12 ['Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
Trainning one by one features for TF-IDF embbedings
len_COL_TF_IDF:  15
len_N_cols:  21

==================================================
Loading and preprocessing main DataFrame...
==================================================
Processing 'Loudness (db)'...
Processing 'Length'...
Processing 'Release Date'...
New numeric columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Total de columnas [38]:  ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']

##################################################
Running experiment without EMOTION feature
[emotion] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2931, Test Loss: 0.1893, F1: 0.8637, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0867, Test Loss: 0.1866, F1: 0.8722, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0561, Test Loss: 0.2565, F1: 0.8590, AUC: 0.9811
Mejores resultados en la época:  4
f1-score 0.8811020710059172
AUC según el mejor F1-score 0.9841344041977698
Confusion Matrix:
 [[15574   891]
 [  395  4765]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/emotion/confusion_matrix_param_160801.png
Accuracy:   0.9405
Precision:  0.8425
Recall:     0.9234
F1-score:   0.8811

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2951, Test Loss: 0.1902, F1: 0.8617, AUC: 0.9798
Epoch [10/30] Train Loss: 0.0887, Test Loss: 0.1901, F1: 0.8718, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0629, Test Loss: 0.2492, F1: 0.8589, AUC: 0.9808
Mejores resultados en la época:  2
f1-score 0.8834401411720999
AUC según el mejor F1-score 0.984385477063162
Confusion Matrix:
 [[15614   851]
 [  404  4756]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/emotion/confusion_matrix_param_160801.png
Accuracy:   0.9420
Precision:  0.8482
Recall:     0.9217
F1-score:   0.8834

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2982, Test Loss: 0.1907, F1: 0.8619, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0832, Test Loss: 0.1949, F1: 0.8716, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0487, Test Loss: 0.2481, F1: 0.8666, AUC: 0.9815
Mejores resultados en la época:  6
f1-score 0.8837252532763268
AUC según el mejor F1-score 0.9835097823195551
Confusion Matrix:
 [[15620   845]
 [  406  4754]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/emotion/confusion_matrix_param_160801.png
Accuracy:   0.9422
Precision:  0.8491
Recall:     0.9213
F1-score:   0.8837
Tiempo total para red 1: 292.92 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2278, Test Loss: 0.1643, F1: 0.8702, AUC: 0.9832
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.4888, F1: 0.8887, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0012, Test Loss: 0.4578, F1: 0.9000, AUC: 0.9873
Mejores resultados en la época:  17
f1-score 0.9153388366289115
AUC según el mejor F1-score 0.9877499723397293
Confusion Matrix:
 [[15975   490]
 [  392  4768]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/emotion/confusion_matrix_param_5843969.png
Accuracy:   0.9592
Precision:  0.9068
Recall:     0.9240
F1-score:   0.9153

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2240, Test Loss: 0.1928, F1: 0.8736, AUC: 0.9832
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.3883, F1: 0.9034, AUC: 0.9878
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.9049, F1: 0.8793, AUC: 0.9787
Mejores resultados en la época:  17
f1-score 0.9123238437529556
AUC según el mejor F1-score 0.9876745186524387
Confusion Matrix:
 [[15875   590]
 [  337  4823]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/emotion/confusion_matrix_param_5843969.png
Accuracy:   0.9571
Precision:  0.8910
Recall:     0.9347
F1-score:   0.9123

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2250, Test Loss: 0.1695, F1: 0.8754, AUC: 0.9836
Epoch [10/30] Train Loss: 0.0026, Test Loss: 0.3075, F1: 0.9140, AUC: 0.9879
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5385, F1: 0.8985, AUC: 0.9865
Mejores resultados en la época:  10
f1-score 0.9139847864248098
AUC según el mejor F1-score 0.9879270274978401
Confusion Matrix:
 [[16057   408]
 [  474  4686]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/emotion/confusion_matrix_param_5843969.png
Accuracy:   0.9592
Precision:  0.9199
Recall:     0.9081
F1-score:   0.9140
Tiempo total para red 6: 370.24 segundos
Saved on: outputs_ablation_remove_one_feature/1/emotion

==============================
Model: Logistic Regression
Accuracy:  0.9366
Precision: 0.8275
Recall:    0.9277
F1-score:  0.8747
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15467   998]
 [  373  4787]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/emotion/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/emotion/logistic_regression_model.pkl

For TF-IDF embbedings you are selecteing this columns:
--> ['text']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 15 ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']

CAT_LOW 3 ['emotion', 'Key', 'Time signature']
CAT_HIGH 12 ['Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
Trainning one by one features for TF-IDF embbedings
len_COL_TF_IDF:  15
len_N_cols:  21

==================================================
Loading and preprocessing main DataFrame...
==================================================
Processing 'Loudness (db)'...
Processing 'Length'...
Processing 'Release Date'...
New numeric columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Total de columnas [38]:  ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']

##################################################
Running experiment without EMOTION feature
[emotion] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2974, Test Loss: 0.1820, F1: 0.8661, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0828, Test Loss: 0.1868, F1: 0.8743, AUC: 0.9827
Epoch [20/30] Train Loss: 0.0490, Test Loss: 0.2449, F1: 0.8702, AUC: 0.9813
Mejores resultados en la época:  5
f1-score 0.8812866040717672
AUC según el mejor F1-score 0.9838345374378819
Confusion Matrix:
 [[15608   857]
 [  420  4740]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/emotion/confusion_matrix_param_160801.png
Accuracy:   0.9409
Precision:  0.8469
Recall:     0.9186
F1-score:   0.8813

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2950, Test Loss: 0.1702, F1: 0.8742, AUC: 0.9799
Epoch [10/30] Train Loss: 0.0741, Test Loss: 0.1929, F1: 0.8726, AUC: 0.9833
Epoch [20/30] Train Loss: 0.0336, Test Loss: 0.2606, F1: 0.8690, AUC: 0.9826
Mejores resultados en la época:  9
f1-score 0.8845652578663191
AUC según el mejor F1-score 0.9835378604368674
Confusion Matrix:
 [[15634   831]
 [  409  4751]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/emotion/confusion_matrix_param_160801.png
Accuracy:   0.9427
Precision:  0.8511
Recall:     0.9207
F1-score:   0.8846

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2942, Test Loss: 0.1833, F1: 0.8690, AUC: 0.9802
Epoch [10/30] Train Loss: 0.0888, Test Loss: 0.2150, F1: 0.8610, AUC: 0.9822
Epoch [20/30] Train Loss: 0.0740, Test Loss: 0.2306, F1: 0.8689, AUC: 0.9803
Mejores resultados en la época:  2
f1-score 0.8820798514391829
AUC según el mejor F1-score 0.9841552435633961
Confusion Matrix:
 [[15605   860]
 [  410  4750]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/emotion/confusion_matrix_param_160801.png
Accuracy:   0.9413
Precision:  0.8467
Recall:     0.9205
F1-score:   0.8821
Tiempo total para red 1: 292.93 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2258, Test Loss: 0.1519, F1: 0.8778, AUC: 0.9834
Epoch [10/30] Train Loss: 0.0033, Test Loss: 0.3939, F1: 0.8871, AUC: 0.9874
Epoch [20/30] Train Loss: 0.0015, Test Loss: 0.5481, F1: 0.8970, AUC: 0.9849
Mejores resultados en la época:  23
f1-score 0.9127335773454055
AUC según el mejor F1-score 0.9879347782587917
Confusion Matrix:
 [[15981   484]
 [  422  4738]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/emotion/confusion_matrix_param_5843969.png
Accuracy:   0.9581
Precision:  0.9073
Recall:     0.9182
F1-score:   0.9127

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2217, Test Loss: 0.1948, F1: 0.8694, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.4621, F1: 0.8985, AUC: 0.9869
Epoch [20/30] Train Loss: 0.0003, Test Loss: 0.7861, F1: 0.9091, AUC: 0.9819
Mejores resultados en la época:  13
f1-score 0.9107999232687511
AUC según el mejor F1-score 0.9879366026596234
Confusion Matrix:
 [[15947   518]
 [  412  4748]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/emotion/confusion_matrix_param_5843969.png
Accuracy:   0.9570
Precision:  0.9016
Recall:     0.9202
F1-score:   0.9108

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2271, Test Loss: 0.1422, F1: 0.8828, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0085, Test Loss: 0.2485, F1: 0.9060, AUC: 0.9877
Epoch [20/30] Train Loss: 0.0020, Test Loss: 0.3644, F1: 0.8983, AUC: 0.9887
Mejores resultados en la época:  6
f1-score 0.9085762807812058
AUC según el mejor F1-score 0.9879429468663856
Confusion Matrix:
 [[15841   624]
 [  345  4815]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/emotion/confusion_matrix_param_5843969.png
Accuracy:   0.9552
Precision:  0.8853
Recall:     0.9331
F1-score:   0.9086
Tiempo total para red 6: 370.24 segundos
Saved on: outputs_ablation_remove_one_feature/1/emotion

==============================
Model: Logistic Regression
Accuracy:  0.9366
Precision: 0.8275
Recall:    0.9277
F1-score:  0.8747
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15467   998]
 [  373  4787]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/emotion/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/emotion/logistic_regression_model.pkl

/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [12:25:20] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [12:25:21] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
==============================
Model: SVM
Accuracy:  0.8118
Precision: 0.5673
Recall:    0.8899
F1-score:  0.6929
              precision    recall  f1-score   support

           0       0.96      0.79      0.86     16465
           1       0.57      0.89      0.69      5160

    accuracy                           0.81     21625
   macro avg       0.76      0.84      0.78     21625
weighted avg       0.86      0.81      0.82     21625

[[12963  3502]
 [  568  4592]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/emotion/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/emotion/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8640
Precision: 0.6896
Recall:    0.7822
F1-score:  0.7330
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.78      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14648  1817]
 [ 1124  4036]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/emotion/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/emotion/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8535
Precision: 0.6624
Recall:    0.7870
F1-score:  0.7193
              precision    recall  f1-score   support

           0       0.93      0.87      0.90     16465
           1       0.66      0.79      0.72      5160

    accuracy                           0.85     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.85      0.86     21625

[[14395  2070]
 [ 1099  4061]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/emotion/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/emotion/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9210
Precision: 0.7993
Recall:    0.8932
F1-score:  0.8437
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15308  1157]
 [  551  4609]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/emotion/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/emotion/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7889
Precision: 0.5315
Recall:    0.9698
F1-score:  0.6867
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12055  4410]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/emotion/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/emotion/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9366, 'precision': 0.8275, 'recall': 0.9277, 'f1_score': 0.8747}
XGBoost: {'accuracy': 0.921, 'precision': 0.7993, 'recall': 0.8932, 'f1_score': 0.8437}
Decision Tree: {'accuracy': 0.864, 'precision': 0.6896, 'recall': 0.7822, 'f1_score': 0.733}
Random Forest: {'accuracy': 0.8535, 'precision': 0.6624, 'recall': 0.787, 'f1_score': 0.7193}
SVM: {'accuracy': 0.8118, 'precision': 0.5673, 'recall': 0.8899, 'f1_score': 0.6929}
Naive Bayes: {'accuracy': 0.7889, 'precision': 0.5315, 'recall': 0.9698, 'f1_score': 0.6867}

##################################################
Running experiment without KEY feature
[Key] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2968, Test Loss: 0.1854, F1: 0.8652, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0882, Test Loss: 0.2168, F1: 0.8594, AUC: 0.9821
Epoch [20/30] Train Loss: 0.0643, Test Loss: 0.2467, F1: 0.8611, AUC: 0.9806
Mejores resultados en la época:  3
f1-score 0.8800664758563382
AUC según el mejor F1-score 0.9840116691031245
Confusion Matrix:
 [[15560   905]
 [  394  4766]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Key/confusion_matrix_param_160801.png
Accuracy:   0.9399
Precision:  0.8404
Recall:     0.9236
F1-score:   0.8801

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2925, Test Loss: 0.1878, F1: 0.8637, AUC: 0.9799
Epoch [10/30] Train Loss: 0.0887, Test Loss: 0.1832, F1: 0.8781, AUC: 0.9820
Epoch [20/30] Train Loss: 0.0756, Test Loss: 0.2417, F1: 0.8627, AUC: 0.9800
Mejores resultados en la época:  2
f1-score 0.8817797389225961
AUC según el mejor F1-score 0.9839540180368506
Confusion Matrix:
 [[15543   922]
 [  364  4796]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Key/confusion_matrix_param_160801.png
Accuracy:   0.9405
Precision:  0.8388
Recall:     0.9295
F1-score:   0.8818

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2909, Test Loss: 0.1878, F1: 0.8635, AUC: 0.9798
Epoch [10/30] Train Loss: 0.0887, Test Loss: 0.2074, F1: 0.8620, AUC: 0.9820
Epoch [20/30] Train Loss: 0.0613, Test Loss: 0.2361, F1: 0.8680, AUC: 0.9807
Mejores resultados en la época:  4
f1-score 0.8823917759124776
==============================
Model: SVM
Accuracy:  0.8118
Precision: 0.5673
Recall:    0.8899
F1-score:  0.6929
              precision    recall  f1-score   support

           0       0.96      0.79      0.86     16465
           1       0.57      0.89      0.69      5160

    accuracy                           0.81     21625
   macro avg       0.76      0.84      0.78     21625
weighted avg       0.86      0.81      0.82     21625

[[12963  3502]
 [  568  4592]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/emotion/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/emotion/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8640
Precision: 0.6896
Recall:    0.7822
F1-score:  0.7330
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.78      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14648  1817]
 [ 1124  4036]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/emotion/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/emotion/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8535
Precision: 0.6624
Recall:    0.7870
F1-score:  0.7193
              precision    recall  f1-score   support

           0       0.93      0.87      0.90     16465
           1       0.66      0.79      0.72      5160

    accuracy                           0.85     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.85      0.86     21625

[[14395  2070]
 [ 1099  4061]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/emotion/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/emotion/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9210
Precision: 0.7993
Recall:    0.8932
F1-score:  0.8437
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15308  1157]
 [  551  4609]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/emotion/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/emotion/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7889
Precision: 0.5315
Recall:    0.9698
F1-score:  0.6867
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12055  4410]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/emotion/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/emotion/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9366, 'precision': 0.8275, 'recall': 0.9277, 'f1_score': 0.8747}
XGBoost: {'accuracy': 0.921, 'precision': 0.7993, 'recall': 0.8932, 'f1_score': 0.8437}
Decision Tree: {'accuracy': 0.864, 'precision': 0.6896, 'recall': 0.7822, 'f1_score': 0.733}
Random Forest: {'accuracy': 0.8535, 'precision': 0.6624, 'recall': 0.787, 'f1_score': 0.7193}
SVM: {'accuracy': 0.8118, 'precision': 0.5673, 'recall': 0.8899, 'f1_score': 0.6929}
Naive Bayes: {'accuracy': 0.7889, 'precision': 0.5315, 'recall': 0.9698, 'f1_score': 0.6867}

##################################################
Running experiment without KEY feature
[Key] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2946, Test Loss: 0.1813, F1: 0.8664, AUC: 0.9798
Epoch [10/30] Train Loss: 0.0892, Test Loss: 0.2197, F1: 0.8587, AUC: 0.9822
Epoch [20/30] Train Loss: 0.0680, Test Loss: 0.2420, F1: 0.8607, AUC: 0.9804
Mejores resultados en la época:  1
f1-score 0.8839028023461503
AUC según el mejor F1-score 0.9832728161922047
Confusion Matrix:
 [[15631   834]
 [  413  4747]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Key/confusion_matrix_param_160801.png
Accuracy:   0.9423
Precision:  0.8506
Recall:     0.9200
F1-score:   0.8839

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2975, Test Loss: 0.1873, F1: 0.8645, AUC: 0.9791
Epoch [10/30] Train Loss: 0.0888, Test Loss: 0.1833, F1: 0.8739, AUC: 0.9819
Epoch [20/30] Train Loss: 0.0675, Test Loss: 0.2413, F1: 0.8640, AUC: 0.9807
Mejores resultados en la época:  1
f1-score 0.8832878773629267
AUC según el mejor F1-score 0.9831489864570606
Confusion Matrix:
 [[15688   777]
 [  464  4696]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Key/confusion_matrix_param_160801.png
Accuracy:   0.9426
Precision:  0.8580
Recall:     0.9101
F1-score:   0.8833

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2955, Test Loss: 0.1954, F1: 0.8591, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0882, Test Loss: 0.1781, F1: 0.8760, AUC: 0.9818
Epoch [20/30] Train Loss: 0.0639, Test Loss: 0.2493, F1: 0.8598, AUC: 0.9805
Mejores resultados en la época:  1
f1-score 0.8824846110800224
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [12:52:08] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [12:52:25] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
AUC según el mejor F1-score 0.9837719251783793
Confusion Matrix:
 [[15700   765]
 [  482  4678]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Key/confusion_matrix_param_160801.png
Accuracy:   0.9423
Precision:  0.8595
Recall:     0.9066
F1-score:   0.8824
Tiempo total para red 1: 288.06 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2257, Test Loss: 0.2210, F1: 0.8377, AUC: 0.9823
Epoch [10/30] Train Loss: 0.0039, Test Loss: 0.9031, F1: 0.8704, AUC: 0.9796
Epoch [20/30] Train Loss: 0.0055, Test Loss: 0.3079, F1: 0.9069, AUC: 0.9879
Mejores resultados en la época:  12
f1-score 0.911812067822298
AUC según el mejor F1-score 0.9880201013660642
Confusion Matrix:
 [[15881   584]
 [  347  4813]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Key/confusion_matrix_param_5843969.png
Accuracy:   0.9569
Precision:  0.8918
Recall:     0.9328
F1-score:   0.9118

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2243, Test Loss: 0.1432, F1: 0.8880, AUC: 0.9833
Epoch [10/30] Train Loss: 0.0048, Test Loss: 0.6187, F1: 0.8893, AUC: 0.9843
Epoch [20/30] Train Loss: 0.0016, Test Loss: 0.2954, F1: 0.9038, AUC: 0.9885
Mejores resultados en la época:  15
f1-score 0.9156274356975838
AUC según el mejor F1-score 0.9881289533588985
Confusion Matrix:
 [[16060   405]
 [  461  4699]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Key/confusion_matrix_param_5843969.png
Accuracy:   0.9600
Precision:  0.9207
Recall:     0.9107
F1-score:   0.9156

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2261, Test Loss: 0.1912, F1: 0.8541, AUC: 0.9836
Epoch [10/30] Train Loss: 0.0030, Test Loss: 0.4126, F1: 0.8950, AUC: 0.9874
Epoch [20/30] Train Loss: 0.0021, Test Loss: 0.3446, F1: 0.9097, AUC: 0.9875
Mejores resultados en la época:  16
f1-score 0.9115906053083827
AUC según el mejor F1-score 0.9875067032017646
Confusion Matrix:
 [[15925   540]
 [  386  4774]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Key/confusion_matrix_param_5843969.png
Accuracy:   0.9572
Precision:  0.8984
Recall:     0.9252
F1-score:   0.9116
Tiempo total para red 6: 370.12 segundos
Saved on: outputs_ablation_remove_one_feature/1/Key

==============================
Model: Logistic Regression
Accuracy:  0.9360
Precision: 0.8276
Recall:    0.9246
F1-score:  0.8734
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15471   994]
 [  389  4771]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Key/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Key/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7903
Precision: 0.5365
Recall:    0.8895
F1-score:  0.6693
              precision    recall  f1-score   support

           0       0.96      0.76      0.85     16465
           1       0.54      0.89      0.67      5160

    accuracy                           0.79     21625
   macro avg       0.75      0.82      0.76     21625
weighted avg       0.86      0.79      0.80     21625

[[12500  3965]
 [  570  4590]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Key/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Key/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8706
Precision: 0.7104
Recall:    0.7725
F1-score:  0.7401
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14840  1625]
 [ 1174  3986]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Key/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Key/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8677
Precision: 0.6950
Recall:    0.7940
F1-score:  0.7412
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14667  1798]
 [ 1063  4097]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Key/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Key/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9219
Precision: 0.8036
Recall:    0.8903
F1-score:  0.8447
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15342  1123]
 [  566  4594]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Key/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Key/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7868
Precision: 0.5290
Recall:    0.9696
F1-score:  0.6845
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625
AUC según el mejor F1-score 0.9834772785589352
Confusion Matrix:
 [[15634   831]
 [  429  4731]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Key/confusion_matrix_param_160801.png
Accuracy:   0.9417
Precision:  0.8506
Recall:     0.9169
F1-score:   0.8825
Tiempo total para red 1: 289.91 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2229, Test Loss: 0.1751, F1: 0.8699, AUC: 0.9838
Epoch [10/30] Train Loss: 0.0039, Test Loss: 0.2348, F1: 0.8928, AUC: 0.9883
Epoch [20/30] Train Loss: 0.0029, Test Loss: 0.4179, F1: 0.8767, AUC: 0.9877
Mejores resultados en la época:  22
f1-score 0.9154819863680623
AUC según el mejor F1-score 0.9881608921437769
Confusion Matrix:
 [[16056   409]
 [  459  4701]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Key/confusion_matrix_param_5843969.png
Accuracy:   0.9599
Precision:  0.9200
Recall:     0.9110
F1-score:   0.9155

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2268, Test Loss: 0.1569, F1: 0.8714, AUC: 0.9826
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.4847, F1: 0.8989, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0015, Test Loss: 0.3608, F1: 0.9013, AUC: 0.9876
Mejores resultados en la época:  23
f1-score 0.9116094986807388
AUC según el mejor F1-score 0.9854445005496743
Confusion Matrix:
 [[15850   615]
 [  323  4837]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Key/confusion_matrix_param_5843969.png
Accuracy:   0.9566
Precision:  0.8872
Recall:     0.9374
F1-score:   0.9116

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2243, Test Loss: 0.1660, F1: 0.8638, AUC: 0.9833
Epoch [10/30] Train Loss: 0.0030, Test Loss: 0.5010, F1: 0.8725, AUC: 0.9869
Epoch [20/30] Train Loss: 0.0025, Test Loss: 0.3087, F1: 0.8766, AUC: 0.9888
Mejores resultados en la época:  17
f1-score 0.9140142517814727
AUC según el mejor F1-score 0.9879668818282616
Confusion Matrix:
 [[15910   555]
 [  350  4810]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Key/confusion_matrix_param_5843969.png
Accuracy:   0.9582
Precision:  0.8966
Recall:     0.9322
F1-score:   0.9140
Tiempo total para red 6: 370.76 segundos
Saved on: outputs_ablation_remove_one_feature/1/Key

==============================
Model: Logistic Regression
Accuracy:  0.9360
Precision: 0.8276
Recall:    0.9246
F1-score:  0.8734
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15471   994]
 [  389  4771]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Key/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Key/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7903
Precision: 0.5365
Recall:    0.8895
F1-score:  0.6693
              precision    recall  f1-score   support

           0       0.96      0.76      0.85     16465
           1       0.54      0.89      0.67      5160

    accuracy                           0.79     21625
   macro avg       0.75      0.82      0.76     21625
weighted avg       0.86      0.79      0.80     21625

[[12500  3965]
 [  570  4590]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Key/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Key/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8706
Precision: 0.7104
Recall:    0.7725
F1-score:  0.7401
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14840  1625]
 [ 1174  3986]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Key/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Key/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8677
Precision: 0.6950
Recall:    0.7940
F1-score:  0.7412
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14667  1798]
 [ 1063  4097]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Key/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Key/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9219
Precision: 0.8036
Recall:    0.8903
F1-score:  0.8447
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15342  1123]
 [  566  4594]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Key/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Key/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7868
Precision: 0.5290
Recall:    0.9696
F1-score:  0.6845
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.68      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(

[[12011  4454]
 [  157  5003]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Key/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Key/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.936, 'precision': 0.8276, 'recall': 0.9246, 'f1_score': 0.8734}
XGBoost: {'accuracy': 0.9219, 'precision': 0.8036, 'recall': 0.8903, 'f1_score': 0.8447}
Random Forest: {'accuracy': 0.8677, 'precision': 0.695, 'recall': 0.794, 'f1_score': 0.7412}
Decision Tree: {'accuracy': 0.8706, 'precision': 0.7104, 'recall': 0.7725, 'f1_score': 0.7401}
Naive Bayes: {'accuracy': 0.7868, 'precision': 0.529, 'recall': 0.9696, 'f1_score': 0.6845}
SVM: {'accuracy': 0.7903, 'precision': 0.5365, 'recall': 0.8895, 'f1_score': 0.6693}

##################################################
Running experiment without TIME SIGNATURE feature
[Time signature] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2944, Test Loss: 0.1711, F1: 0.8719, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0884, Test Loss: 0.1750, F1: 0.8799, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0730, Test Loss: 0.2628, F1: 0.8562, AUC: 0.9804
Mejores resultados en la época:  7
f1-score 0.8819412039197387
AUC según el mejor F1-score 0.9828319703293573
Confusion Matrix:
 [[15635   830]
 [  435  4725]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Time signature/confusion_matrix_param_160801.png
Accuracy:   0.9415
Precision:  0.8506
Recall:     0.9157
F1-score:   0.8819

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2969, Test Loss: 0.1857, F1: 0.8634, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0879, Test Loss: 0.2120, F1: 0.8627, AUC: 0.9827
Epoch [20/30] Train Loss: 0.0724, Test Loss: 0.2554, F1: 0.8576, AUC: 0.9807
Mejores resultados en la época:  2
f1-score 0.88182319426576
AUC según el mejor F1-score 0.9843494716299784
Confusion Matrix:
 [[15541   924]
 [  362  4798]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Time signature/confusion_matrix_param_160801.png
Accuracy:   0.9405
Precision:  0.8385
Recall:     0.9298
F1-score:   0.8818

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2901, Test Loss: 0.1976, F1: 0.8565, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0886, Test Loss: 0.1855, F1: 0.8761, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0690, Test Loss: 0.2526, F1: 0.8595, AUC: 0.9808
Mejores resultados en la época:  3
f1-score 0.8812802381838482
AUC según el mejor F1-score 0.9841209860239126
Confusion Matrix:
 [[15613   852]
 [  424  4736]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Time signature/confusion_matrix_param_160801.png
Accuracy:   0.9410
Precision:  0.8475
Recall:     0.9178
F1-score:   0.8813
Tiempo total para red 1: 285.17 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2222, Test Loss: 0.1888, F1: 0.8656, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0041, Test Loss: 0.3229, F1: 0.8921, AUC: 0.9873
Epoch [20/30] Train Loss: 0.0029, Test Loss: 0.3068, F1: 0.9030, AUC: 0.9880
Mejores resultados en la época:  19
f1-score 0.9100600891645668
AUC según el mejor F1-score 0.9875932739637991
Confusion Matrix:
 [[16002   463]
 [  465  4695]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Time signature/confusion_matrix_param_5843969.png
Accuracy:   0.9571
Precision:  0.9102
Recall:     0.9099
F1-score:   0.9101

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2264, Test Loss: 0.1873, F1: 0.8612, AUC: 0.9833
Epoch [10/30] Train Loss: 0.0020, Test Loss: 0.4583, F1: 0.9031, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0015, Test Loss: 0.3768, F1: 0.8890, AUC: 0.9878
Mejores resultados en la época:  27
f1-score 0.9161390863932989
AUC según el mejor F1-score 0.9874285599945384
Confusion Matrix:
 [[16061   404]
 [  457  4703]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Time signature/confusion_matrix_param_5843969.png
Accuracy:   0.9602
Precision:  0.9209
Recall:     0.9114
F1-score:   0.9161

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2234, Test Loss: 0.1597, F1: 0.8739, AUC: 0.9840
Epoch [10/30] Train Loss: 0.0046, Test Loss: 0.3946, F1: 0.8948, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0012, Test Loss: 0.4213, F1: 0.8936, AUC: 0.9882
Mejores resultados en la época:  12
f1-score 0.9117731036451889
AUC según el mejor F1-score 0.9877608834337344
Confusion Matrix:
 [[15908   557]
 [  370  4790]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Time signature/confusion_matrix_param_5843969.png
Accuracy:   0.9571
Precision:  0.8958
Recall:     0.9283
F1-score:   0.9118
Tiempo total para red 6: 366.89 segundos
Saved on: outputs_ablation_remove_one_feature/1/Time signature

==============================
Model: Logistic Regression
Accuracy:  0.9359
Precision: 0.8261
Recall:    0.9262
F1-score:  0.8733
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15459  1006]
 [  381  4779]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Time signature/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Time signature/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7706
Precision: 0.5108
Recall:    0.9079
F1-score:  0.6538
              precision    recall  f1-score   support

           0       0.96      0.73      0.83     16465
           1       0.51      0.91      0.65      5160

    accuracy                           0.77     21625
   macro avg       0.74      0.82      0.74     21625
weighted avg       0.85      0.77      0.79     21625

[[11979  4486]
 [  475  4685]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Time signature/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Time signature/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8709
Precision: 0.7116
Recall:    0.7721
F1-score:  0.7406
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14850  1615]
 [ 1176  3984]]

[[12011  4454]
 [  157  5003]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Key/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Key/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.936, 'precision': 0.8276, 'recall': 0.9246, 'f1_score': 0.8734}
XGBoost: {'accuracy': 0.9219, 'precision': 0.8036, 'recall': 0.8903, 'f1_score': 0.8447}
Random Forest: {'accuracy': 0.8677, 'precision': 0.695, 'recall': 0.794, 'f1_score': 0.7412}
Decision Tree: {'accuracy': 0.8706, 'precision': 0.7104, 'recall': 0.7725, 'f1_score': 0.7401}
Naive Bayes: {'accuracy': 0.7868, 'precision': 0.529, 'recall': 0.9696, 'f1_score': 0.6845}
SVM: {'accuracy': 0.7903, 'precision': 0.5365, 'recall': 0.8895, 'f1_score': 0.6693}

##################################################
Running experiment without TIME SIGNATURE feature
[Time signature] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2935, Test Loss: 0.1765, F1: 0.8702, AUC: 0.9798
Epoch [10/30] Train Loss: 0.0879, Test Loss: 0.2088, F1: 0.8616, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0595, Test Loss: 0.2442, F1: 0.8646, AUC: 0.9810
Mejores resultados en la época:  3
f1-score 0.8838837891549163
AUC según el mejor F1-score 0.983687031688077
Confusion Matrix:
 [[15728   737]
 [  490  4670]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Time signature/confusion_matrix_param_160801.png
Accuracy:   0.9433
Precision:  0.8637
Recall:     0.9050
F1-score:   0.8839

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2985, Test Loss: 0.1828, F1: 0.8658, AUC: 0.9792
Epoch [10/30] Train Loss: 0.0866, Test Loss: 0.1807, F1: 0.8770, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0587, Test Loss: 0.2834, F1: 0.8503, AUC: 0.9811
Mejores resultados en la época:  10
f1-score 0.8770408633889862
AUC según el mejor F1-score 0.9825123411888501
Confusion Matrix:
 [[15538   927]
 [  406  4754]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Time signature/confusion_matrix_param_160801.png
Accuracy:   0.9384
Precision:  0.8368
Recall:     0.9213
F1-score:   0.8770

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2942, Test Loss: 0.1810, F1: 0.8671, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0760, Test Loss: 0.1772, F1: 0.8812, AUC: 0.9832
Epoch [20/30] Train Loss: 0.0461, Test Loss: 0.2151, F1: 0.8822, AUC: 0.9817
Mejores resultados en la época:  1
f1-score 0.8822979041916168
AUC según el mejor F1-score 0.9835665094150854
Confusion Matrix:
 [[15652   813]
 [  445  4715]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Time signature/confusion_matrix_param_160801.png
Accuracy:   0.9418
Precision:  0.8529
Recall:     0.9138
F1-score:   0.8823
Tiempo total para red 1: 291.56 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2255, Test Loss: 0.1404, F1: 0.8895, AUC: 0.9839
Epoch [10/30] Train Loss: 0.0044, Test Loss: 0.3922, F1: 0.9044, AUC: 0.9864
Epoch [20/30] Train Loss: 0.0020, Test Loss: 0.2868, F1: 0.9062, AUC: 0.9883
Mejores resultados en la época:  27
f1-score 0.9165074512800917
AUC según el mejor F1-score 0.9883914669830531
Confusion Matrix:
 [[15954   511]
 [  363  4797]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Time signature/confusion_matrix_param_5843969.png
Accuracy:   0.9596
Precision:  0.9037
Recall:     0.9297
F1-score:   0.9165

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2268, Test Loss: 0.1827, F1: 0.8557, AUC: 0.9837
Epoch [10/30] Train Loss: 0.0031, Test Loss: 0.3621, F1: 0.8885, AUC: 0.9882
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7381, F1: 0.8876, AUC: 0.9852
Mejores resultados en la época:  2
f1-score 0.9064372704426832
AUC según el mejor F1-score 0.9874682436552047
Confusion Matrix:
 [[15968   497]
 [  471  4689]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Time signature/confusion_matrix_param_5843969.png
Accuracy:   0.9552
Precision:  0.9042
Recall:     0.9087
F1-score:   0.9064

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2251, Test Loss: 0.2303, F1: 0.8416, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0035, Test Loss: 0.3755, F1: 0.8884, AUC: 0.9874
Epoch [20/30] Train Loss: 0.0021, Test Loss: 0.5713, F1: 0.8996, AUC: 0.9853
Mejores resultados en la época:  11
f1-score 0.9121459433509361
AUC según el mejor F1-score 0.9871478435582173
Confusion Matrix:
 [[15960   505]
 [  410  4750]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Time signature/confusion_matrix_param_5843969.png
Accuracy:   0.9577
Precision:  0.9039
Recall:     0.9205
F1-score:   0.9121
Tiempo total para red 6: 370.10 segundos
Saved on: outputs_ablation_remove_one_feature/1/Time signature

==============================
Model: Logistic Regression
Accuracy:  0.9359
Precision: 0.8261
Recall:    0.9262
F1-score:  0.8733
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15459  1006]
 [  381  4779]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Time signature/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Time signature/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7706
Precision: 0.5108
Recall:    0.9079
F1-score:  0.6538
              precision    recall  f1-score   support

           0       0.96      0.73      0.83     16465
           1       0.51      0.91      0.65      5160

    accuracy                           0.77     21625
   macro avg       0.74      0.82      0.74     21625
weighted avg       0.85      0.77      0.79     21625

[[11979  4486]
 [  475  4685]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Time signature/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Time signature/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8709
Precision: 0.7116
Recall:    0.7721
F1-score:  0.7406
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14850  1615]
 [ 1176  3984]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [13:19:03] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [13:19:24] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Time signature/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Time signature/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8635
Precision: 0.6869
Recall:    0.7866
F1-score:  0.7334
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14615  1850]
 [ 1101  4059]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Time signature/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Time signature/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9213
Precision: 0.8015
Recall:    0.8911
F1-score:  0.8439
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15326  1139]
 [  562  4598]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Time signature/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Time signature/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Time signature/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Time signature/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9359, 'precision': 0.8261, 'recall': 0.9262, 'f1_score': 0.8733}
XGBoost: {'accuracy': 0.9213, 'precision': 0.8015, 'recall': 0.8911, 'f1_score': 0.8439}
Decision Tree: {'accuracy': 0.8709, 'precision': 0.7116, 'recall': 0.7721, 'f1_score': 0.7406}
Random Forest: {'accuracy': 0.8635, 'precision': 0.6869, 'recall': 0.7866, 'f1_score': 0.7334}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7706, 'precision': 0.5108, 'recall': 0.9079, 'f1_score': 0.6538}

##################################################
Running experiment without ARTIST(S) feature
[Artist(s)] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2955, Test Loss: 0.1794, F1: 0.8672, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0909, Test Loss: 0.1805, F1: 0.8718, AUC: 0.9819
Epoch [20/30] Train Loss: 0.0775, Test Loss: 0.2654, F1: 0.8480, AUC: 0.9802
Mejores resultados en la época:  1
f1-score 0.8760132645541636
AUC según el mejor F1-score 0.9832301546385684
Confusion Matrix:
 [[15524   941]
 [  405  4755]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Artist(s)/confusion_matrix_param_160801.png
Accuracy:   0.9378
Precision:  0.8348
Recall:     0.9215
F1-score:   0.8760

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3016, Test Loss: 0.2003, F1: 0.8565, AUC: 0.9788
Epoch [10/30] Train Loss: 0.0755, Test Loss: 0.1857, F1: 0.8712, AUC: 0.9830
Epoch [20/30] Train Loss: 0.0392, Test Loss: 0.2520, F1: 0.8623, AUC: 0.9814
Mejores resultados en la época:  2
f1-score 0.8812993039443155
AUC según el mejor F1-score 0.9838682594274442
Confusion Matrix:
 [[15598   867]
 [  412  4748]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Artist(s)/confusion_matrix_param_160801.png
Accuracy:   0.9409
Precision:  0.8456
Recall:     0.9202
F1-score:   0.8813

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2954, Test Loss: 0.1689, F1: 0.8696, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0881, Test Loss: 0.1920, F1: 0.8637, AUC: 0.9819
Epoch [20/30] Train Loss: 0.0524, Test Loss: 0.2348, F1: 0.8636, AUC: 0.9810
Mejores resultados en la época:  1
f1-score 0.8819619956940934
AUC según el mejor F1-score 0.9833134179384507
Confusion Matrix:
 [[15653   812]
 [  449  4711]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Artist(s)/confusion_matrix_param_160801.png
Accuracy:   0.9417
Precision:  0.8530
Recall:     0.9130
F1-score:   0.8820
Tiempo total para red 1: 282.51 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2255, Test Loss: 0.1593, F1: 0.8790, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0036, Test Loss: 0.4149, F1: 0.8902, AUC: 0.9870
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8015, F1: 0.8965, AUC: 0.9818
Mejores resultados en la época:  7
f1-score 0.9090909090909091
AUC según el mejor F1-score 0.986688159285494
Confusion Matrix:
 [[15841   624]
 [  340  4820]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Artist(s)/confusion_matrix_param_5843969.png
Accuracy:   0.9554
Precision:  0.8854
Recall:     0.9341
F1-score:   0.9091

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2262, Test Loss: 0.1728, F1: 0.8728, AUC: 0.9834
Epoch [10/30] Train Loss: 0.0048, Test Loss: 0.4259, F1: 0.8855, AUC: 0.9871
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7091, F1: 0.8855, AUC: 0.9849
Mejores resultados en la época:  8
f1-score 0.9101334837375447
AUC según el mejor F1-score 0.9865081321195771
Confusion Matrix:
 [[15828   637]
 [  319  4841]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Artist(s)/confusion_matrix_param_5843969.png
Accuracy:   0.9558
Precision:  0.8837
Recall:     0.9382
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Time signature/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Time signature/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8635
Precision: 0.6869
Recall:    0.7866
F1-score:  0.7334
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14615  1850]
 [ 1101  4059]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Time signature/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Time signature/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9213
Precision: 0.8015
Recall:    0.8911
F1-score:  0.8439
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15326  1139]
 [  562  4598]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Time signature/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Time signature/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Time signature/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Time signature/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9359, 'precision': 0.8261, 'recall': 0.9262, 'f1_score': 0.8733}
XGBoost: {'accuracy': 0.9213, 'precision': 0.8015, 'recall': 0.8911, 'f1_score': 0.8439}
Decision Tree: {'accuracy': 0.8709, 'precision': 0.7116, 'recall': 0.7721, 'f1_score': 0.7406}
Random Forest: {'accuracy': 0.8635, 'precision': 0.6869, 'recall': 0.7866, 'f1_score': 0.7334}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7706, 'precision': 0.5108, 'recall': 0.9079, 'f1_score': 0.6538}

##################################################
Running experiment without ARTIST(S) feature
[Artist(s)] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2957, Test Loss: 0.2014, F1: 0.8548, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0892, Test Loss: 0.2018, F1: 0.8609, AUC: 0.9820
Epoch [20/30] Train Loss: 0.0563, Test Loss: 0.2440, F1: 0.8596, AUC: 0.9806
Mejores resultados en la época:  5
f1-score 0.8785621158502515
AUC según el mejor F1-score 0.9832584858179317
Confusion Matrix:
 [[15604   861]
 [  443  4717]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Artist(s)/confusion_matrix_param_160801.png
Accuracy:   0.9397
Precision:  0.8456
Recall:     0.9141
F1-score:   0.8786

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2979, Test Loss: 0.1806, F1: 0.8643, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0905, Test Loss: 0.2045, F1: 0.8615, AUC: 0.9816
Epoch [20/30] Train Loss: 0.0753, Test Loss: 0.2569, F1: 0.8547, AUC: 0.9800
Mejores resultados en la época:  2
f1-score 0.8797778806108283
AUC según el mejor F1-score 0.9838807771712135
Confusion Matrix:
 [[15573   892]
 [  407  4753]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Artist(s)/confusion_matrix_param_160801.png
Accuracy:   0.9399
Precision:  0.8420
Recall:     0.9211
F1-score:   0.8798

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2939, Test Loss: 0.1948, F1: 0.8568, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0889, Test Loss: 0.1891, F1: 0.8669, AUC: 0.9818
Epoch [20/30] Train Loss: 0.0566, Test Loss: 0.2427, F1: 0.8608, AUC: 0.9806
Mejores resultados en la época:  4
f1-score 0.8817043543262941
AUC según el mejor F1-score 0.9836697881576376
Confusion Matrix:
 [[15641   824]
 [  442  4718]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Artist(s)/confusion_matrix_param_160801.png
Accuracy:   0.9415
Precision:  0.8513
Recall:     0.9143
F1-score:   0.8817
Tiempo total para red 1: 288.93 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2243, Test Loss: 0.2223, F1: 0.8372, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.3388, F1: 0.8991, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6935, F1: 0.8917, AUC: 0.9844
Mejores resultados en la época:  17
f1-score 0.9102612646724726
AUC según el mejor F1-score 0.987507897890051
Confusion Matrix:
 [[15869   596]
 [  352  4808]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Artist(s)/confusion_matrix_param_5843969.png
Accuracy:   0.9562
Precision:  0.8897
Recall:     0.9318
F1-score:   0.9103

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2242, Test Loss: 0.1862, F1: 0.8698, AUC: 0.9834
Epoch [10/30] Train Loss: 0.0027, Test Loss: 0.4646, F1: 0.8811, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0021, Test Loss: 0.4008, F1: 0.8901, AUC: 0.9883
Mejores resultados en la época:  14
f1-score 0.9108967082860386
AUC según el mejor F1-score 0.9882282478454414
Confusion Matrix:
 [[15868   597]
 [  345  4815]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Artist(s)/confusion_matrix_param_5843969.png
Accuracy:   0.9564
Precision:  0.8897
Recall:     0.9331
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [13:45:52] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [13:46:16] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
F1-score:   0.9101

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2268, Test Loss: 0.1736, F1: 0.8417, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0017, Test Loss: 0.7148, F1: 0.8974, AUC: 0.9839
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8499, F1: 0.8986, AUC: 0.9808
Mejores resultados en la época:  6
f1-score 0.9094495585196318
AUC según el mejor F1-score 0.9874569617958695
Confusion Matrix:
 [[15820   645]
 [  319  4841]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Artist(s)/confusion_matrix_param_5843969.png
Accuracy:   0.9554
Precision:  0.8824
Recall:     0.9382
F1-score:   0.9094
Tiempo total para red 6: 365.54 segundos
Saved on: outputs_ablation_remove_one_feature/1/Artist(s)

==============================
Model: Logistic Regression
Accuracy:  0.9353
Precision: 0.8263
Recall:    0.9229
F1-score:  0.8719
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15464  1001]
 [  398  4762]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Artist(s)/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Artist(s)/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7835
Precision: 0.5270
Recall:    0.9054
F1-score:  0.6662
              precision    recall  f1-score   support

           0       0.96      0.75      0.84     16465
           1       0.53      0.91      0.67      5160

    accuracy                           0.78     21625
   macro avg       0.74      0.83      0.75     21625
weighted avg       0.86      0.78      0.80     21625

[[12271  4194]
 [  488  4672]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Artist(s)/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Artist(s)/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8704
Precision: 0.7096
Recall:    0.7734
F1-score:  0.7402
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14832  1633]
 [ 1169  3991]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Artist(s)/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Artist(s)/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8643
Precision: 0.6865
Recall:    0.7940
F1-score:  0.7363
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14594  1871]
 [ 1063  4097]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Artist(s)/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Artist(s)/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9214
Precision: 0.8007
Recall:    0.8928
F1-score:  0.8442
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15318  1147]
 [  553  4607]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Artist(s)/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Artist(s)/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7672
Precision: 0.5063
Recall:    0.9719
F1-score:  0.6658
              precision    recall  f1-score   support

           0       0.99      0.70      0.82     16465
           1       0.51      0.97      0.67      5160

    accuracy                           0.77     21625
   macro avg       0.75      0.84      0.74     21625
weighted avg       0.87      0.77      0.78     21625

[[11575  4890]
 [  145  5015]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Artist(s)/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Artist(s)/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9353, 'precision': 0.8263, 'recall': 0.9229, 'f1_score': 0.8719}
XGBoost: {'accuracy': 0.9214, 'precision': 0.8007, 'recall': 0.8928, 'f1_score': 0.8442}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.7096, 'recall': 0.7734, 'f1_score': 0.7402}
Random Forest: {'accuracy': 0.8643, 'precision': 0.6865, 'recall': 0.794, 'f1_score': 0.7363}
SVM: {'accuracy': 0.7835, 'precision': 0.527, 'recall': 0.9054, 'f1_score': 0.6662}
Naive Bayes: {'accuracy': 0.7672, 'precision': 0.5063, 'recall': 0.9719, 'f1_score': 0.6658}

##################################################
Running experiment without SONG feature
[song] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
F1-score:   0.9109

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2258, Test Loss: 0.1860, F1: 0.8688, AUC: 0.9827
Epoch [10/30] Train Loss: 0.0014, Test Loss: 0.3231, F1: 0.9026, AUC: 0.9878
Epoch [20/30] Train Loss: 0.0010, Test Loss: 0.6759, F1: 0.8947, AUC: 0.9829
Mejores resultados en la época:  19
f1-score 0.9135511726032775
AUC según el mejor F1-score 0.9873440666953863
Confusion Matrix:
 [[16089   376]
 [  505  4655]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Artist(s)/confusion_matrix_param_5843969.png
Accuracy:   0.9593
Precision:  0.9253
Recall:     0.9021
F1-score:   0.9136
Tiempo total para red 6: 365.64 segundos
Saved on: outputs_ablation_remove_one_feature/1/Artist(s)

==============================
Model: Logistic Regression
Accuracy:  0.9353
Precision: 0.8263
Recall:    0.9229
F1-score:  0.8719
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15464  1001]
 [  398  4762]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Artist(s)/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Artist(s)/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7835
Precision: 0.5270
Recall:    0.9054
F1-score:  0.6662
              precision    recall  f1-score   support

           0       0.96      0.75      0.84     16465
           1       0.53      0.91      0.67      5160

    accuracy                           0.78     21625
   macro avg       0.74      0.83      0.75     21625
weighted avg       0.86      0.78      0.80     21625

[[12271  4194]
 [  488  4672]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Artist(s)/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Artist(s)/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8704
Precision: 0.7096
Recall:    0.7734
F1-score:  0.7402
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14832  1633]
 [ 1169  3991]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Artist(s)/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Artist(s)/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8643
Precision: 0.6865
Recall:    0.7940
F1-score:  0.7363
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14594  1871]
 [ 1063  4097]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Artist(s)/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Artist(s)/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9214
Precision: 0.8007
Recall:    0.8928
F1-score:  0.8442
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15318  1147]
 [  553  4607]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Artist(s)/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Artist(s)/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7672
Precision: 0.5063
Recall:    0.9719
F1-score:  0.6658
              precision    recall  f1-score   support

           0       0.99      0.70      0.82     16465
           1       0.51      0.97      0.67      5160

    accuracy                           0.77     21625
   macro avg       0.75      0.84      0.74     21625
weighted avg       0.87      0.77      0.78     21625

[[11575  4890]
 [  145  5015]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Artist(s)/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Artist(s)/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9353, 'precision': 0.8263, 'recall': 0.9229, 'f1_score': 0.8719}
XGBoost: {'accuracy': 0.9214, 'precision': 0.8007, 'recall': 0.8928, 'f1_score': 0.8442}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.7096, 'recall': 0.7734, 'f1_score': 0.7402}
Random Forest: {'accuracy': 0.8643, 'precision': 0.6865, 'recall': 0.794, 'f1_score': 0.7363}
SVM: {'accuracy': 0.7835, 'precision': 0.527, 'recall': 0.9054, 'f1_score': 0.6662}
Naive Bayes: {'accuracy': 0.7672, 'precision': 0.5063, 'recall': 0.9719, 'f1_score': 0.6658}

##################################################
Running experiment without SONG feature
[song] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [14:12:39] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [14:12:52] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2889, Test Loss: 0.1816, F1: 0.8670, AUC: 0.9807
Epoch [10/30] Train Loss: 0.0869, Test Loss: 0.1970, F1: 0.8684, AUC: 0.9838
Epoch [20/30] Train Loss: 0.0667, Test Loss: 0.2405, F1: 0.8650, AUC: 0.9826
Mejores resultados en la época:  4
f1-score 0.8869549115822609
AUC según el mejor F1-score 0.9850069739193074
Confusion Matrix:
 [[15614   851]
 [  370  4790]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song/confusion_matrix_param_160801.png
Accuracy:   0.9435
Precision:  0.8491
Recall:     0.9283
F1-score:   0.8870

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2905, Test Loss: 0.1974, F1: 0.8595, AUC: 0.9809
Epoch [10/30] Train Loss: 0.0867, Test Loss: 0.1846, F1: 0.8787, AUC: 0.9839
Epoch [20/30] Train Loss: 0.0735, Test Loss: 0.2191, F1: 0.8749, AUC: 0.9823
Mejores resultados en la época:  2
f1-score 0.8919531545145448
AUC según el mejor F1-score 0.9850837635388195
Confusion Matrix:
 [[15759   706]
 [  438  4722]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song/confusion_matrix_param_160801.png
Accuracy:   0.9471
Precision:  0.8699
Recall:     0.9151
F1-score:   0.8920

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2864, Test Loss: 0.1656, F1: 0.8766, AUC: 0.9808
Epoch [10/30] Train Loss: 0.0868, Test Loss: 0.1697, F1: 0.8839, AUC: 0.9838
Epoch [20/30] Train Loss: 0.0711, Test Loss: 0.2185, F1: 0.8742, AUC: 0.9823
Mejores resultados en la época:  10
f1-score 0.883851105513574
AUC según el mejor F1-score 0.9838015098976688
Confusion Matrix:
 [[15643   822]
 [  423  4737]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song/confusion_matrix_param_160801.png
Accuracy:   0.9424
Precision:  0.8521
Recall:     0.9180
F1-score:   0.8839
Tiempo total para red 1: 281.21 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2228, Test Loss: 0.1844, F1: 0.8685, AUC: 0.9843
Epoch [10/30] Train Loss: 0.0049, Test Loss: 0.2536, F1: 0.9095, AUC: 0.9887
Epoch [20/30] Train Loss: 0.0019, Test Loss: 0.4942, F1: 0.8680, AUC: 0.9889
Mejores resultados en la época:  29
f1-score 0.9184295788069373
AUC según el mejor F1-score 0.9843037439059128
Confusion Matrix:
 [[15950   515]
 [  341  4819]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song/confusion_matrix_param_5843969.png
Accuracy:   0.9604
Precision:  0.9034
Recall:     0.9339
F1-score:   0.9184

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2213, Test Loss: 0.1544, F1: 0.8829, AUC: 0.9832
Epoch [10/30] Train Loss: 0.0041, Test Loss: 0.2786, F1: 0.9070, AUC: 0.9891
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5707, F1: 0.9060, AUC: 0.9867
Mejores resultados en la época:  5
f1-score 0.9104993000466636
AUC según el mejor F1-score 0.9890200966579331
Confusion Matrix:
 [[15788   677]
 [  282  4878]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song/confusion_matrix_param_5843969.png
Accuracy:   0.9557
Precision:  0.8781
Recall:     0.9453
F1-score:   0.9105

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2178, Test Loss: 0.2157, F1: 0.8444, AUC: 0.9834
Epoch [10/30] Train Loss: 0.0033, Test Loss: 0.3607, F1: 0.8922, AUC: 0.9884
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6204, F1: 0.8963, AUC: 0.9873
Mejores resultados en la época:  12
f1-score 0.9129865269461078
AUC según el mejor F1-score 0.9887189292768076
Confusion Matrix:
 [[15816   649]
 [  281  4879]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song/confusion_matrix_param_5843969.png
Accuracy:   0.9570
Precision:  0.8826
Recall:     0.9455
F1-score:   0.9130
Tiempo total para red 6: 366.97 segundos
Saved on: outputs_ablation_remove_one_feature/1/song

==============================
Model: Logistic Regression
Accuracy:  0.9383
Precision: 0.8312
Recall:    0.9304
F1-score:  0.8780
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.88      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.94      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15490   975]
 [  359  4801]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8203
Precision: 0.5800
Recall:    0.8940
F1-score:  0.7036
              precision    recall  f1-score   support

           0       0.96      0.80      0.87     16465
           1       0.58      0.89      0.70      5160

    accuracy                           0.82     21625
   macro avg       0.77      0.85      0.79     21625
weighted avg       0.87      0.82      0.83     21625

[[13125  3340]
 [  547  4613]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8601
Precision: 0.6770
Recall:    0.7913
F1-score:  0.7297
              precision    recall  f1-score   support

           0       0.93      0.88      0.91     16465
           1       0.68      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.84      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14517  1948]
 [ 1077  4083]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8732
Precision: 0.7102
Recall:    0.7921
F1-score:  0.7489
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.71      0.79      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.87      0.88     21625

[[14797  1668]
 [ 1073  4087]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song/random_forest_model.pkl

==============================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2888, Test Loss: 0.1820, F1: 0.8692, AUC: 0.9805
Epoch [10/30] Train Loss: 0.0754, Test Loss: 0.1836, F1: 0.8765, AUC: 0.9841
Epoch [20/30] Train Loss: 0.0390, Test Loss: 0.2420, F1: 0.8726, AUC: 0.9831
Mejores resultados en la época:  1
f1-score 0.8869824692279
AUC según el mejor F1-score 0.9843616951155493
Confusion Matrix:
 [[15657   808]
 [  404  4756]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song/confusion_matrix_param_160801.png
Accuracy:   0.9440
Precision:  0.8548
Recall:     0.9217
F1-score:   0.8870

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2889, Test Loss: 0.1743, F1: 0.8686, AUC: 0.9805
Epoch [10/30] Train Loss: 0.0847, Test Loss: 0.1831, F1: 0.8740, AUC: 0.9840
Epoch [20/30] Train Loss: 0.0551, Test Loss: 0.2371, F1: 0.8674, AUC: 0.9829
Mejores resultados en la época:  4
f1-score 0.8845060086230621
AUC según el mejor F1-score 0.984817218577344
Confusion Matrix:
 [[15545   920]
 [  339  4821]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song/confusion_matrix_param_160801.png
Accuracy:   0.9418
Precision:  0.8397
Recall:     0.9343
F1-score:   0.8845

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2888, Test Loss: 0.1880, F1: 0.8646, AUC: 0.9809
Epoch [10/30] Train Loss: 0.0865, Test Loss: 0.1818, F1: 0.8745, AUC: 0.9837
Epoch [20/30] Train Loss: 0.0658, Test Loss: 0.2494, F1: 0.8634, AUC: 0.9827
Mejores resultados en la época:  1
f1-score 0.8846189547943933
AUC según el mejor F1-score 0.9845201237296873
Confusion Matrix:
 [[15617   848]
 [  395  4765]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song/confusion_matrix_param_160801.png
Accuracy:   0.9425
Precision:  0.8489
Recall:     0.9234
F1-score:   0.8846
Tiempo total para red 1: 286.76 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2228, Test Loss: 0.1649, F1: 0.8825, AUC: 0.9840
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.4329, F1: 0.8935, AUC: 0.9880
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5202, F1: 0.9054, AUC: 0.9874
Mejores resultados en la época:  6
f1-score 0.9113195282756059
AUC según el mejor F1-score 0.9884973705087371
Confusion Matrix:
 [[15763   702]
 [  253  4907]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song/confusion_matrix_param_5843969.png
Accuracy:   0.9558
Precision:  0.8748
Recall:     0.9510
F1-score:   0.9113

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2203, Test Loss: 0.1433, F1: 0.8874, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0041, Test Loss: 0.4606, F1: 0.8985, AUC: 0.9870
Epoch [20/30] Train Loss: 0.0024, Test Loss: 0.2894, F1: 0.9151, AUC: 0.9892
Mejores resultados en la época:  24
f1-score 0.9160750753671107
AUC según el mejor F1-score 0.9876342876715232
Confusion Matrix:
 [[16052   413]
 [  450  4710]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song/confusion_matrix_param_5843969.png
Accuracy:   0.9601
Precision:  0.9194
Recall:     0.9128
F1-score:   0.9161

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2194, Test Loss: 0.1629, F1: 0.8789, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0024, Test Loss: 0.3201, F1: 0.8880, AUC: 0.9886
Epoch [20/30] Train Loss: 0.0005, Test Loss: 0.2584, F1: 0.9125, AUC: 0.9892
Mejores resultados en la época:  16
f1-score 0.918511358144031
AUC según el mejor F1-score 0.9886977014903591
Confusion Matrix:
 [[16031   434]
 [  409  4751]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song/confusion_matrix_param_5843969.png
Accuracy:   0.9610
Precision:  0.9163
Recall:     0.9207
F1-score:   0.9185
Tiempo total para red 6: 366.95 segundos
Saved on: outputs_ablation_remove_one_feature/1/song

==============================
Model: Logistic Regression
Accuracy:  0.9383
Precision: 0.8312
Recall:    0.9304
F1-score:  0.8780
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.88      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.94      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15490   975]
 [  359  4801]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8203
Precision: 0.5800
Recall:    0.8940
F1-score:  0.7036
              precision    recall  f1-score   support

           0       0.96      0.80      0.87     16465
           1       0.58      0.89      0.70      5160

    accuracy                           0.82     21625
   macro avg       0.77      0.85      0.79     21625
weighted avg       0.87      0.82      0.83     21625

[[13125  3340]
 [  547  4613]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8601
Precision: 0.6770
Recall:    0.7913
F1-score:  0.7297
              precision    recall  f1-score   support

           0       0.93      0.88      0.91     16465
           1       0.68      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.84      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14517  1948]
 [ 1077  4083]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8732
Precision: 0.7102
Recall:    0.7921
F1-score:  0.7489
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.71      0.79      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.87      0.88     21625

[[14797  1668]
 [ 1073  4087]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song/random_forest_model.pkl

==============================
Model: XGBoost
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Model: XGBoost
Accuracy:  0.9219
Precision: 0.8016
Recall:    0.8942
F1-score:  0.8454
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15323  1142]
 [  546  4614]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7985
Precision: 0.5436
Recall:    0.9711
F1-score:  0.6970
              precision    recall  f1-score   support

           0       0.99      0.74      0.85     16465
           1       0.54      0.97      0.70      5160

    accuracy                           0.80     21625
   macro avg       0.77      0.86      0.77     21625
weighted avg       0.88      0.80      0.81     21625

[[12257  4208]
 [  149  5011]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9383, 'precision': 0.8312, 'recall': 0.9304, 'f1_score': 0.878}
XGBoost: {'accuracy': 0.9219, 'precision': 0.8016, 'recall': 0.8942, 'f1_score': 0.8454}
Random Forest: {'accuracy': 0.8732, 'precision': 0.7102, 'recall': 0.7921, 'f1_score': 0.7489}
Decision Tree: {'accuracy': 0.8601, 'precision': 0.677, 'recall': 0.7913, 'f1_score': 0.7297}
SVM: {'accuracy': 0.8203, 'precision': 0.58, 'recall': 0.894, 'f1_score': 0.7036}
Naive Bayes: {'accuracy': 0.7985, 'precision': 0.5436, 'recall': 0.9711, 'f1_score': 0.697}

##################################################
Running experiment without GENRE feature
[Genre] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3158, Test Loss: 0.1718, F1: 0.8699, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0914, Test Loss: 0.2073, F1: 0.8590, AUC: 0.9822
Epoch [20/30] Train Loss: 0.0632, Test Loss: 0.2207, F1: 0.8726, AUC: 0.9808
Mejores resultados en la época:  3
f1-score 0.8790987360322403
AUC según el mejor F1-score 0.9839881519878908
Confusion Matrix:
 [[15506   959]
 [  361  4799]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Genre/confusion_matrix_param_160801.png
Accuracy:   0.9390
Precision:  0.8334
Recall:     0.9300
F1-score:   0.8791

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3083, Test Loss: 0.1862, F1: 0.8663, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0891, Test Loss: 0.1979, F1: 0.8673, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0530, Test Loss: 0.2607, F1: 0.8563, AUC: 0.9814
Mejores resultados en la época:  6
f1-score 0.878905527823997
AUC según el mejor F1-score 0.9831097441836922
Confusion Matrix:
 [[15561   904]
 [  406  4754]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Genre/confusion_matrix_param_160801.png
Accuracy:   0.9394
Precision:  0.8402
Recall:     0.9213
F1-score:   0.8789

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3109, Test Loss: 0.1890, F1: 0.8643, AUC: 0.9791
Epoch [10/30] Train Loss: 0.0923, Test Loss: 0.2044, F1: 0.8634, AUC: 0.9821
Epoch [20/30] Train Loss: 0.0746, Test Loss: 0.2297, F1: 0.8650, AUC: 0.9803
Mejores resultados en la época:  2
f1-score 0.8854471621494003
AUC según el mejor F1-score 0.9838791940621049
Confusion Matrix:
 [[15724   741]
 [  472  4688]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Genre/confusion_matrix_param_160801.png
Accuracy:   0.9439
Precision:  0.8635
Recall:     0.9085
F1-score:   0.8854
Tiempo total para red 1: 285.71 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2295, Test Loss: 0.1656, F1: 0.8790, AUC: 0.9836
Epoch [10/30] Train Loss: 0.0023, Test Loss: 0.3345, F1: 0.9158, AUC: 0.9866
Epoch [20/30] Train Loss: 0.0012, Test Loss: 0.4661, F1: 0.9163, AUC: 0.9880
Mejores resultados en la época:  18
f1-score 0.9203608497429431
AUC según el mejor F1-score 0.9884140307017234
Confusion Matrix:
 [[16060   405]
 [  416  4744]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Genre/confusion_matrix_param_5843969.png
Accuracy:   0.9620
Precision:  0.9213
Recall:     0.9194
F1-score:   0.9204

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2311, Test Loss: 0.1514, F1: 0.8778, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0048, Test Loss: 0.4132, F1: 0.9056, AUC: 0.9878
Epoch [20/30] Train Loss: 0.0022, Test Loss: 0.3341, F1: 0.9125, AUC: 0.9887
Mejores resultados en la época:  22
f1-score 0.919371929486562
AUC según el mejor F1-score 0.9879734849822386
Confusion Matrix:
 [[16016   449]
 [  388  4772]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Genre/confusion_matrix_param_5843969.png
Accuracy:   0.9613
Precision:  0.9140
Recall:     0.9248
F1-score:   0.9194

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2345, Test Loss: 0.1623, F1: 0.8800, AUC: 0.9836
Epoch [10/30] Train Loss: 0.0036, Test Loss: 0.4560, F1: 0.8963, AUC: 0.9878
Epoch [20/30] Train Loss: 0.0001, Test Loss: 0.5543, F1: 0.9004, AUC: 0.9859
Mejores resultados en la época:  14
f1-score 0.9184928873510189
AUC según el mejor F1-score 0.9884545206298537
Confusion Matrix:
 [[15999   466]
 [  382  4778]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Genre/confusion_matrix_param_5843969.png
Accuracy:   0.9608
Precision:  0.9111
Recall:     0.9260
F1-score:   0.9185
Tiempo total para red 6: 365.64 segundos
Saved on: outputs_ablation_remove_one_feature/1/Genre

==============================
Model: Logistic Regression
Accuracy:  0.9354
Precision: 0.8255
Recall:    0.9248
F1-score:  0.8723
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15456  1009]
 [  388  4772]]
Accuracy:  0.9219
Precision: 0.8016
Recall:    0.8942
F1-score:  0.8454
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15323  1142]
 [  546  4614]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7985
Precision: 0.5436
Recall:    0.9711
F1-score:  0.6970
              precision    recall  f1-score   support

           0       0.99      0.74      0.85     16465
           1       0.54      0.97      0.70      5160

    accuracy                           0.80     21625
   macro avg       0.77      0.86      0.77     21625
weighted avg       0.88      0.80      0.81     21625

[[12257  4208]
 [  149  5011]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9383, 'precision': 0.8312, 'recall': 0.9304, 'f1_score': 0.878}
XGBoost: {'accuracy': 0.9219, 'precision': 0.8016, 'recall': 0.8942, 'f1_score': 0.8454}
Random Forest: {'accuracy': 0.8732, 'precision': 0.7102, 'recall': 0.7921, 'f1_score': 0.7489}
Decision Tree: {'accuracy': 0.8601, 'precision': 0.677, 'recall': 0.7913, 'f1_score': 0.7297}
SVM: {'accuracy': 0.8203, 'precision': 0.58, 'recall': 0.894, 'f1_score': 0.7036}
Naive Bayes: {'accuracy': 0.7985, 'precision': 0.5436, 'recall': 0.9711, 'f1_score': 0.697}

##################################################
Running experiment without GENRE feature
[Genre] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3068, Test Loss: 0.1952, F1: 0.8576, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0922, Test Loss: 0.2110, F1: 0.8615, AUC: 0.9821
Epoch [20/30] Train Loss: 0.0788, Test Loss: 0.2281, F1: 0.8656, AUC: 0.9800
Mejores resultados en la época:  2
f1-score 0.8827818283791363
AUC según el mejor F1-score 0.9839324489108916
Confusion Matrix:
 [[15649   816]
 [  438  4722]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Genre/confusion_matrix_param_160801.png
Accuracy:   0.9420
Precision:  0.8527
Recall:     0.9151
F1-score:   0.8828

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3114, Test Loss: 0.1828, F1: 0.8668, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0915, Test Loss: 0.2077, F1: 0.8594, AUC: 0.9820
Epoch [20/30] Train Loss: 0.0682, Test Loss: 0.2246, F1: 0.8675, AUC: 0.9806
Mejores resultados en la época:  1
f1-score 0.8818411451024417
AUC según el mejor F1-score 0.9832159066565911
Confusion Matrix:
 [[15649   816]
 [  447  4713]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Genre/confusion_matrix_param_160801.png
Accuracy:   0.9416
Precision:  0.8524
Recall:     0.9134
F1-score:   0.8818

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3089, Test Loss: 0.1803, F1: 0.8675, AUC: 0.9792
Epoch [10/30] Train Loss: 0.0915, Test Loss: 0.1824, F1: 0.8722, AUC: 0.9819
Epoch [20/30] Train Loss: 0.0616, Test Loss: 0.2315, F1: 0.8676, AUC: 0.9808
Mejores resultados en la época:  3
f1-score 0.8818839236046727
AUC según el mejor F1-score 0.9839101323691081
Confusion Matrix:
 [[15595   870]
 [  404  4756]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Genre/confusion_matrix_param_160801.png
Accuracy:   0.9411
Precision:  0.8454
Recall:     0.9217
F1-score:   0.8819
Tiempo total para red 1: 287.11 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2314, Test Loss: 0.2123, F1: 0.8560, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0020, Test Loss: 0.5658, F1: 0.8864, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6944, F1: 0.9024, AUC: 0.9859
Mejores resultados en la época:  9
f1-score 0.9110608630111174
AUC según el mejor F1-score 0.9881411179928294
Confusion Matrix:
 [[15846   619]
 [  325  4835]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Genre/confusion_matrix_param_5843969.png
Accuracy:   0.9563
Precision:  0.8865
Recall:     0.9370
F1-score:   0.9111

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2315, Test Loss: 0.1705, F1: 0.8715, AUC: 0.9835
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.2380, F1: 0.9126, AUC: 0.9880
Epoch [20/30] Train Loss: 0.0024, Test Loss: 0.3553, F1: 0.9021, AUC: 0.9882
Mejores resultados en la época:  8
f1-score 0.9149543378995434
AUC según el mejor F1-score 0.9880746391805969
Confusion Matrix:
 [[15922   543]
 [  351  4809]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Genre/confusion_matrix_param_5843969.png
Accuracy:   0.9587
Precision:  0.8985
Recall:     0.9320
F1-score:   0.9150

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2259, Test Loss: 0.1843, F1: 0.8757, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0040, Test Loss: 0.2049, F1: 0.9121, AUC: 0.9890
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8404, F1: 0.9031, AUC: 0.9841
Mejores resultados en la época:  11
f1-score 0.9141561998300123
AUC según el mejor F1-score 0.9882403418574047
Confusion Matrix:
 [[15876   589]
 [  320  4840]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Genre/confusion_matrix_param_5843969.png
Accuracy:   0.9580
Precision:  0.8915
Recall:     0.9380
F1-score:   0.9142
Tiempo total para red 6: 366.53 segundos
Saved on: outputs_ablation_remove_one_feature/1/Genre

==============================
Model: Logistic Regression
Accuracy:  0.9354
Precision: 0.8255
Recall:    0.9248
F1-score:  0.8723
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15456  1009]
 [  388  4772]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [14:39:21] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [14:39:23] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Genre/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Genre/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7968
Precision: 0.5460
Recall:    0.8808
F1-score:  0.6741
              precision    recall  f1-score   support

           0       0.95      0.77      0.85     16465
           1       0.55      0.88      0.67      5160

    accuracy                           0.80     21625
   macro avg       0.75      0.83      0.76     21625
weighted avg       0.86      0.80      0.81     21625

[[12686  3779]
 [  615  4545]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Genre/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Genre/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8393
Precision: 0.6313
Recall:    0.7847
F1-score:  0.6997
              precision    recall  f1-score   support

           0       0.93      0.86      0.89     16465
           1       0.63      0.78      0.70      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.82      0.79     21625
weighted avg       0.86      0.84      0.84     21625

[[14100  2365]
 [ 1111  4049]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Genre/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Genre/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8759
Precision: 0.7172
Recall:    0.7926
F1-score:  0.7530
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.72      0.79      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.82      0.85      0.84     21625
weighted avg       0.88      0.88      0.88     21625

[[14852  1613]
 [ 1070  4090]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Genre/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Genre/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9134
Precision: 0.7786
Recall:    0.8903
F1-score:  0.8307
              precision    recall  f1-score   support

           0       0.96      0.92      0.94     16465
           1       0.78      0.89      0.83      5160

    accuracy                           0.91     21625
   macro avg       0.87      0.91      0.89     21625
weighted avg       0.92      0.91      0.92     21625

[[15159  1306]
 [  566  4594]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Genre/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Genre/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7754
Precision: 0.5156
Recall:    0.9717
F1-score:  0.6737
              precision    recall  f1-score   support

           0       0.99      0.71      0.83     16465
           1       0.52      0.97      0.67      5160

    accuracy                           0.78     21625
   macro avg       0.75      0.84      0.75     21625
weighted avg       0.88      0.78      0.79     21625

[[11754  4711]
 [  146  5014]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Genre/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Genre/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.8255, 'recall': 0.9248, 'f1_score': 0.8723}
XGBoost: {'accuracy': 0.9134, 'precision': 0.7786, 'recall': 0.8903, 'f1_score': 0.8307}
Random Forest: {'accuracy': 0.8759, 'precision': 0.7172, 'recall': 0.7926, 'f1_score': 0.753}
Decision Tree: {'accuracy': 0.8393, 'precision': 0.6313, 'recall': 0.7847, 'f1_score': 0.6997}
SVM: {'accuracy': 0.7968, 'precision': 0.546, 'recall': 0.8808, 'f1_score': 0.6741}
Naive Bayes: {'accuracy': 0.7754, 'precision': 0.5156, 'recall': 0.9717, 'f1_score': 0.6737}

##################################################
Running experiment without ALBUM feature
[Album] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2987, Test Loss: 0.1872, F1: 0.8595, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0899, Test Loss: 0.1939, F1: 0.8660, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0704, Test Loss: 0.2282, F1: 0.8617, AUC: 0.9808
Mejores resultados en la época:  2
f1-score 0.8836038052602126
AUC según el mejor F1-score 0.9840115160888613
Confusion Matrix:
 [[15640   825]
 [  423  4737]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Album/confusion_matrix_param_160801.png
Accuracy:   0.9423
Precision:  0.8517
Recall:     0.9180
F1-score:   0.8836

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2939, Test Loss: 0.1941, F1: 0.8575, AUC: 0.9798
Epoch [10/30] Train Loss: 0.0904, Test Loss: 0.1844, F1: 0.8712, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0767, Test Loss: 0.2481, F1: 0.8568, AUC: 0.9808
Mejores resultados en la época:  2
f1-score 0.8764148339008005
AUC según el mejor F1-score 0.9842683976110942
Confusion Matrix:
 [[15520   945]
 [  398  4762]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Album/confusion_matrix_param_160801.png
Accuracy:   0.9379
Precision:  0.8344
Recall:     0.9229
F1-score:   0.8764

--- Iteración 3 de 3 para la red 1 ---
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Genre/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Genre/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7968
Precision: 0.5460
Recall:    0.8808
F1-score:  0.6741
              precision    recall  f1-score   support

           0       0.95      0.77      0.85     16465
           1       0.55      0.88      0.67      5160

    accuracy                           0.80     21625
   macro avg       0.75      0.83      0.76     21625
weighted avg       0.86      0.80      0.81     21625

[[12686  3779]
 [  615  4545]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Genre/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Genre/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8393
Precision: 0.6313
Recall:    0.7847
F1-score:  0.6997
              precision    recall  f1-score   support

           0       0.93      0.86      0.89     16465
           1       0.63      0.78      0.70      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.82      0.79     21625
weighted avg       0.86      0.84      0.84     21625

[[14100  2365]
 [ 1111  4049]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Genre/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Genre/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8759
Precision: 0.7172
Recall:    0.7926
F1-score:  0.7530
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.72      0.79      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.82      0.85      0.84     21625
weighted avg       0.88      0.88      0.88     21625

[[14852  1613]
 [ 1070  4090]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Genre/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Genre/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9134
Precision: 0.7786
Recall:    0.8903
F1-score:  0.8307
              precision    recall  f1-score   support

           0       0.96      0.92      0.94     16465
           1       0.78      0.89      0.83      5160

    accuracy                           0.91     21625
   macro avg       0.87      0.91      0.89     21625
weighted avg       0.92      0.91      0.92     21625

[[15159  1306]
 [  566  4594]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Genre/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Genre/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7754
Precision: 0.5156
Recall:    0.9717
F1-score:  0.6737
              precision    recall  f1-score   support

           0       0.99      0.71      0.83     16465
           1       0.52      0.97      0.67      5160

    accuracy                           0.78     21625
   macro avg       0.75      0.84      0.75     21625
weighted avg       0.88      0.78      0.79     21625

[[11754  4711]
 [  146  5014]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Genre/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Genre/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.8255, 'recall': 0.9248, 'f1_score': 0.8723}
XGBoost: {'accuracy': 0.9134, 'precision': 0.7786, 'recall': 0.8903, 'f1_score': 0.8307}
Random Forest: {'accuracy': 0.8759, 'precision': 0.7172, 'recall': 0.7926, 'f1_score': 0.753}
Decision Tree: {'accuracy': 0.8393, 'precision': 0.6313, 'recall': 0.7847, 'f1_score': 0.6997}
SVM: {'accuracy': 0.7968, 'precision': 0.546, 'recall': 0.8808, 'f1_score': 0.6741}
Naive Bayes: {'accuracy': 0.7754, 'precision': 0.5156, 'recall': 0.9717, 'f1_score': 0.6737}

##################################################
Running experiment without ALBUM feature
[Album] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3003, Test Loss: 0.1883, F1: 0.8607, AUC: 0.9791
Epoch [10/30] Train Loss: 0.0808, Test Loss: 0.1871, F1: 0.8690, AUC: 0.9830
Epoch [20/30] Train Loss: 0.0494, Test Loss: 0.2350, F1: 0.8681, AUC: 0.9818
Mejores resultados en la época:  4
f1-score 0.8833536986558888
AUC según el mejor F1-score 0.9840502286974719
Confusion Matrix:
 [[15685   780]
 [  461  4699]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Album/confusion_matrix_param_160801.png
Accuracy:   0.9426
Precision:  0.8576
Recall:     0.9107
F1-score:   0.8834

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2935, Test Loss: 0.2015, F1: 0.8528, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0902, Test Loss: 0.1897, F1: 0.8673, AUC: 0.9822
Epoch [20/30] Train Loss: 0.0672, Test Loss: 0.2291, F1: 0.8666, AUC: 0.9810
Mejores resultados en la época:  2
f1-score 0.882512892639475
AUC según el mejor F1-score 0.9840821145158746
Confusion Matrix:
 [[15666   799]
 [  454  4706]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Album/confusion_matrix_param_160801.png
Accuracy:   0.9421
Precision:  0.8549
Recall:     0.9120
F1-score:   0.8825

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2945, Test Loss: 0.1923, F1: 0.8598, AUC: 0.9795
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [15:06:03] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [15:06:09] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [0/30] Train Loss: 0.2971, Test Loss: 0.2037, F1: 0.8516, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0902, Test Loss: 0.1987, F1: 0.8654, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0742, Test Loss: 0.2550, F1: 0.8570, AUC: 0.9814
Mejores resultados en la época:  4
f1-score 0.8823198198198198
AUC según el mejor F1-score 0.9839260105415057
Confusion Matrix:
 [[15670   795]
 [  459  4701]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Album/confusion_matrix_param_160801.png
Accuracy:   0.9420
Precision:  0.8553
Recall:     0.9110
F1-score:   0.8823
Tiempo total para red 1: 285.88 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2246, Test Loss: 0.1708, F1: 0.8766, AUC: 0.9827
Epoch [10/30] Train Loss: 0.0035, Test Loss: 0.3711, F1: 0.8989, AUC: 0.9884
Epoch [20/30] Train Loss: 0.0027, Test Loss: 0.4071, F1: 0.9015, AUC: 0.9870
Mejores resultados en la época:  4
f1-score 0.9071322436849926
AUC según el mejor F1-score 0.9880850676911561
Confusion Matrix:
 [[15741   724]
 [  276  4884]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Album/confusion_matrix_param_5843969.png
Accuracy:   0.9538
Precision:  0.8709
Recall:     0.9465
F1-score:   0.9071

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2241, Test Loss: 0.1493, F1: 0.8816, AUC: 0.9822
Epoch [10/30] Train Loss: 0.0037, Test Loss: 0.3125, F1: 0.8959, AUC: 0.9880
Epoch [20/30] Train Loss: 0.0016, Test Loss: 0.4972, F1: 0.9149, AUC: 0.9855
Mejores resultados en la época:  14
f1-score 0.9155010538417321
AUC según el mejor F1-score 0.9887962014797658
Confusion Matrix:
 [[15965   500]
 [  382  4778]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Album/confusion_matrix_param_5843969.png
Accuracy:   0.9592
Precision:  0.9053
Recall:     0.9260
F1-score:   0.9155

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2246, Test Loss: 0.1709, F1: 0.8656, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0023, Test Loss: 0.5411, F1: 0.9031, AUC: 0.9875
Epoch [20/30] Train Loss: 0.0010, Test Loss: 0.5258, F1: 0.9094, AUC: 0.9853
Mejores resultados en la época:  23
f1-score 0.9121077083530862
AUC según el mejor F1-score 0.9883388477319757
Confusion Matrix:
 [[15888   577]
 [  350  4810]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Album/confusion_matrix_param_5843969.png
Accuracy:   0.9571
Precision:  0.8929
Recall:     0.9322
F1-score:   0.9121
Tiempo total para red 6: 363.94 segundos
Saved on: outputs_ablation_remove_one_feature/1/Album

==============================
Model: Logistic Regression
Accuracy:  0.9350
Precision: 0.8254
Recall:    0.9227
F1-score:  0.8713
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.94     21625

[[15458  1007]
 [  399  4761]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Album/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Album/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7669
Precision: 0.5065
Recall:    0.8973
F1-score:  0.6475
              precision    recall  f1-score   support

           0       0.96      0.73      0.83     16465
           1       0.51      0.90      0.65      5160

    accuracy                           0.77     21625
   macro avg       0.73      0.81      0.74     21625
weighted avg       0.85      0.77      0.78     21625

[[11954  4511]
 [  530  4630]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Album/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Album/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8713
Precision: 0.7128
Recall:    0.7715
F1-score:  0.7410
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14861  1604]
 [ 1179  3981]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Album/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Album/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8706
Precision: 0.7042
Recall:    0.7890
F1-score:  0.7442
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.70      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14755  1710]
 [ 1089  4071]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Album/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Album/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9197
Precision: 0.7983
Recall:    0.8880
F1-score:  0.8407
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15307  1158]
 [  578  4582]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Album/conf_matrix_xgboost.png
Epoch [10/30] Train Loss: 0.0904, Test Loss: 0.2017, F1: 0.8621, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0680, Test Loss: 0.2493, F1: 0.8549, AUC: 0.9815
Mejores resultados en la época:  7
f1-score 0.8761362592966669
AUC según el mejor F1-score 0.9834084397959496
Confusion Matrix:
 [[15505   960]
 [  389  4771]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Album/confusion_matrix_param_160801.png
Accuracy:   0.9376
Precision:  0.8325
Recall:     0.9246
F1-score:   0.8761
Tiempo total para red 1: 285.91 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2236, Test Loss: 0.1640, F1: 0.8730, AUC: 0.9834
Epoch [10/30] Train Loss: 0.0050, Test Loss: 0.3403, F1: 0.8946, AUC: 0.9887
Epoch [20/30] Train Loss: 0.0016, Test Loss: 0.4314, F1: 0.9033, AUC: 0.9884
Mejores resultados en la época:  27
f1-score 0.9184599054327898
AUC según el mejor F1-score 0.9867385951407378
Confusion Matrix:
 [[16021   444]
 [  401  4759]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Album/confusion_matrix_param_5843969.png
Accuracy:   0.9609
Precision:  0.9147
Recall:     0.9223
F1-score:   0.9185

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2236, Test Loss: 0.1567, F1: 0.8796, AUC: 0.9833
Epoch [10/30] Train Loss: 0.0050, Test Loss: 0.3237, F1: 0.9052, AUC: 0.9878
Epoch [20/30] Train Loss: 0.0018, Test Loss: 0.4223, F1: 0.9004, AUC: 0.9874
Mejores resultados en la época:  22
f1-score 0.914828897338403
AUC según el mejor F1-score 0.9889796243852945
Confusion Matrix:
 [[15917   548]
 [  348  4812]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Album/confusion_matrix_param_5843969.png
Accuracy:   0.9586
Precision:  0.8978
Recall:     0.9326
F1-score:   0.9148

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2254, Test Loss: 0.2215, F1: 0.8344, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0046, Test Loss: 0.3792, F1: 0.9014, AUC: 0.9877
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5799, F1: 0.9019, AUC: 0.9863
Mejores resultados en la época:  7
f1-score 0.9075014100394811
AUC según el mejor F1-score 0.9877287739791005
Confusion Matrix:
 [[15814   651]
 [  333  4827]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Album/confusion_matrix_param_5843969.png
Accuracy:   0.9545
Precision:  0.8812
Recall:     0.9355
F1-score:   0.9075
Tiempo total para red 6: 363.97 segundos
Saved on: outputs_ablation_remove_one_feature/1/Album

==============================
Model: Logistic Regression
Accuracy:  0.9350
Precision: 0.8254
Recall:    0.9227
F1-score:  0.8713
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.94     21625

[[15458  1007]
 [  399  4761]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Album/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Album/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7669
Precision: 0.5065
Recall:    0.8973
F1-score:  0.6475
              precision    recall  f1-score   support

           0       0.96      0.73      0.83     16465
           1       0.51      0.90      0.65      5160

    accuracy                           0.77     21625
   macro avg       0.73      0.81      0.74     21625
weighted avg       0.85      0.77      0.78     21625

[[11954  4511]
 [  530  4630]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Album/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Album/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8713
Precision: 0.7128
Recall:    0.7715
F1-score:  0.7410
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14861  1604]
 [ 1179  3981]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Album/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Album/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8706
Precision: 0.7042
Recall:    0.7890
F1-score:  0.7442
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.70      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14755  1710]
 [ 1089  4071]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Album/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Album/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9197
Precision: 0.7983
Recall:    0.8880
F1-score:  0.8407
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15307  1158]
 [  578  4582]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Album/conf_matrix_xgboost.png
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Modelo guardado como: outputs_ablation_remove_one_feature/1/Album/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7926
Precision: 0.5362
Recall:    0.9705
F1-score:  0.6908
              precision    recall  f1-score   support

           0       0.99      0.74      0.84     16465
           1       0.54      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.77     21625
weighted avg       0.88      0.79      0.81     21625

[[12133  4332]
 [  152  5008]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Album/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Album/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.935, 'precision': 0.8254, 'recall': 0.9227, 'f1_score': 0.8713}
XGBoost: {'accuracy': 0.9197, 'precision': 0.7983, 'recall': 0.888, 'f1_score': 0.8407}
Random Forest: {'accuracy': 0.8706, 'precision': 0.7042, 'recall': 0.789, 'f1_score': 0.7442}
Decision Tree: {'accuracy': 0.8713, 'precision': 0.7128, 'recall': 0.7715, 'f1_score': 0.741}
Naive Bayes: {'accuracy': 0.7926, 'precision': 0.5362, 'recall': 0.9705, 'f1_score': 0.6908}
SVM: {'accuracy': 0.7669, 'precision': 0.5065, 'recall': 0.8973, 'f1_score': 0.6475}

##################################################
Running experiment without SIMILAR ARTIST 1 feature
[Similar Artist 1] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3104, Test Loss: 0.2125, F1: 0.8442, AUC: 0.9748
Epoch [10/30] Train Loss: 0.1078, Test Loss: 0.2036, F1: 0.8548, AUC: 0.9773
Epoch [20/30] Train Loss: 0.0723, Test Loss: 0.2520, F1: 0.8469, AUC: 0.9766
Mejores resultados en la época:  4
f1-score 0.8645089492720023
AUC según el mejor F1-score 0.9787233784607706
Confusion Matrix:
 [[15503   962]
 [  499  4661]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 1/confusion_matrix_param_160801.png
Accuracy:   0.9324
Precision:  0.8289
Recall:     0.9033
F1-score:   0.8645

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3093, Test Loss: 0.2093, F1: 0.8448, AUC: 0.9744
Epoch [10/30] Train Loss: 0.1079, Test Loss: 0.2263, F1: 0.8421, AUC: 0.9772
Epoch [20/30] Train Loss: 0.0788, Test Loss: 0.2602, F1: 0.8385, AUC: 0.9763
Mejores resultados en la época:  6
f1-score 0.8607897153351699
AUC según el mejor F1-score 0.9779975082215739
Confusion Matrix:
 [[15422  1043]
 [  473  4687]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 1/confusion_matrix_param_160801.png
Accuracy:   0.9299
Precision:  0.8180
Recall:     0.9083
F1-score:   0.8608

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3091, Test Loss: 0.2257, F1: 0.8346, AUC: 0.9746
Epoch [10/30] Train Loss: 0.1084, Test Loss: 0.2199, F1: 0.8462, AUC: 0.9771
Epoch [20/30] Train Loss: 0.0783, Test Loss: 0.2448, F1: 0.8513, AUC: 0.9761
Mejores resultados en la época:  2
f1-score 0.8647952139898758
AUC según el mejor F1-score 0.9789865688787821
Confusion Matrix:
 [[15458  1007]
 [  462  4698]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 1/confusion_matrix_param_160801.png
Accuracy:   0.9321
Precision:  0.8235
Recall:     0.9105
F1-score:   0.8648
Tiempo total para red 1: 284.94 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2470, Test Loss: 0.1847, F1: 0.8530, AUC: 0.9778
Epoch [10/30] Train Loss: 0.0037, Test Loss: 0.4570, F1: 0.8808, AUC: 0.9833
Epoch [20/30] Train Loss: 0.0020, Test Loss: 0.4213, F1: 0.9002, AUC: 0.9839
Mejores resultados en la época:  20
f1-score 0.9002114977888868
AUC según el mejor F1-score 0.9839072898349094
Confusion Matrix:
 [[15905   560]
 [  478  4682]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 1/confusion_matrix_param_5843969.png
Accuracy:   0.9520
Precision:  0.8932
Recall:     0.9074
F1-score:   0.9002

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2452, Test Loss: 0.1608, F1: 0.8699, AUC: 0.9780
Epoch [10/30] Train Loss: 0.0020, Test Loss: 0.6156, F1: 0.8779, AUC: 0.9827
Epoch [20/30] Train Loss: 0.0012, Test Loss: 0.4798, F1: 0.8643, AUC: 0.9831
Mejores resultados en la época:  5
f1-score 0.8936291240045506
AUC según el mejor F1-score 0.9832003109720643
Confusion Matrix:
 [[15790   675]
 [  447  4713]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 1/confusion_matrix_param_5843969.png
Accuracy:   0.9481
Precision:  0.8747
Recall:     0.9134
F1-score:   0.8936

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2448, Test Loss: 0.1967, F1: 0.8521, AUC: 0.9788
Epoch [10/30] Train Loss: 0.0042, Test Loss: 0.3453, F1: 0.8755, AUC: 0.9834
Epoch [20/30] Train Loss: 0.0020, Test Loss: 0.6096, F1: 0.8688, AUC: 0.9824
Mejores resultados en la época:  21
f1-score 0.8962814262498805
AUC según el mejor F1-score 0.9832573794071049
Confusion Matrix:
 [[15852   613]
 [  472  4688]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 1/confusion_matrix_param_5843969.png
Accuracy:   0.9498
Precision:  0.8844
Recall:     0.9085
F1-score:   0.8963
Tiempo total para red 6: 368.26 segundos
Saved on: outputs_ablation_remove_one_feature/1/Similar Artist 1

==============================
Model: Logistic Regression
Accuracy:  0.9277
Precision: 0.8088
Recall:    0.9126
F1-score:  0.8576
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.81      0.91      0.86      5160

    accuracy                           0.93     21625
   macro avg       0.89      0.92      0.90     21625
weighted avg       0.93      0.93      0.93     21625

[[15352  1113]
 [  451  4709]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 1/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 1/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7095
Precision: 0.4480
Recall:    0.9359
F1-score:  0.6059
              precision    recall  f1-score   support

           0       0.97      0.64      0.77     16465
           1       0.45      0.94      0.61      5160

    accuracy                           0.71     21625
   macro avg       0.71      0.79      0.69     21625
weighted avg       0.85      0.71      0.73     21625

[[10514  5951]
 [  331  4829]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 1/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 1/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8745
Precision: 0.7227
Recall:    0.7688
F1-score:  0.7450
              precision    recall  f1-score   support

           0       0.93      0.91      0.92     16465
           1       0.72      0.77      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.88     21625
Modelo guardado como: outputs_ablation_remove_one_feature/1/Album/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7926
Precision: 0.5362
Recall:    0.9705
F1-score:  0.6908
              precision    recall  f1-score   support

           0       0.99      0.74      0.84     16465
           1       0.54      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.77     21625
weighted avg       0.88      0.79      0.81     21625

[[12133  4332]
 [  152  5008]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Album/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Album/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.935, 'precision': 0.8254, 'recall': 0.9227, 'f1_score': 0.8713}
XGBoost: {'accuracy': 0.9197, 'precision': 0.7983, 'recall': 0.888, 'f1_score': 0.8407}
Random Forest: {'accuracy': 0.8706, 'precision': 0.7042, 'recall': 0.789, 'f1_score': 0.7442}
Decision Tree: {'accuracy': 0.8713, 'precision': 0.7128, 'recall': 0.7715, 'f1_score': 0.741}
Naive Bayes: {'accuracy': 0.7926, 'precision': 0.5362, 'recall': 0.9705, 'f1_score': 0.6908}
SVM: {'accuracy': 0.7669, 'precision': 0.5065, 'recall': 0.8973, 'f1_score': 0.6475}

##################################################
Running experiment without SIMILAR ARTIST 1 feature
[Similar Artist 1] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3079, Test Loss: 0.1900, F1: 0.8563, AUC: 0.9744
Epoch [10/30] Train Loss: 0.1071, Test Loss: 0.2316, F1: 0.8398, AUC: 0.9771
Epoch [20/30] Train Loss: 0.0692, Test Loss: 0.2506, F1: 0.8488, AUC: 0.9764
Mejores resultados en la época:  3
f1-score 0.8624222466154409
AUC según el mejor F1-score 0.9790378345421461
Confusion Matrix:
 [[15407  1058]
 [  446  4714]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 1/confusion_matrix_param_160801.png
Accuracy:   0.9305
Precision:  0.8167
Recall:     0.9136
F1-score:   0.8624

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3098, Test Loss: 0.1984, F1: 0.8541, AUC: 0.9744
Epoch [10/30] Train Loss: 0.1093, Test Loss: 0.2136, F1: 0.8512, AUC: 0.9773
Epoch [20/30] Train Loss: 0.0792, Test Loss: 0.2619, F1: 0.8411, AUC: 0.9761
Mejores resultados en la época:  6
f1-score 0.8666159984799544
AUC según el mejor F1-score 0.9776843115652888
Confusion Matrix:
 [[15660   805]
 [  599  4561]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 1/confusion_matrix_param_160801.png
Accuracy:   0.9351
Precision:  0.8500
Recall:     0.8839
F1-score:   0.8666

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3121, Test Loss: 0.2017, F1: 0.8519, AUC: 0.9742
Epoch [10/30] Train Loss: 0.0991, Test Loss: 0.2253, F1: 0.8463, AUC: 0.9776
Epoch [20/30] Train Loss: 0.0578, Test Loss: 0.2507, F1: 0.8600, AUC: 0.9769
Mejores resultados en la época:  4
f1-score 0.863716814159292
AUC según el mejor F1-score 0.9788232614637109
Confusion Matrix:
 [[15526   939]
 [  524  4636]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 1/confusion_matrix_param_160801.png
Accuracy:   0.9323
Precision:  0.8316
Recall:     0.8984
F1-score:   0.8637
Tiempo total para red 1: 286.27 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2453, Test Loss: 0.1882, F1: 0.8554, AUC: 0.9772
Epoch [10/30] Train Loss: 0.0043, Test Loss: 0.3381, F1: 0.8805, AUC: 0.9844
Epoch [20/30] Train Loss: 0.0041, Test Loss: 0.4479, F1: 0.8930, AUC: 0.9837
Mejores resultados en la época:  22
f1-score 0.8935970750914034
AUC según el mejor F1-score 0.9840138701544503
Confusion Matrix:
 [[15724   741]
 [  394  4766]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 1/confusion_matrix_param_5843969.png
Accuracy:   0.9475
Precision:  0.8654
Recall:     0.9236
F1-score:   0.8936

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2446, Test Loss: 0.2230, F1: 0.8400, AUC: 0.9781
Epoch [10/30] Train Loss: 0.0050, Test Loss: 0.4607, F1: 0.8667, AUC: 0.9832
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8568, F1: 0.8724, AUC: 0.9812
Mejores resultados en la época:  9
f1-score 0.8919457356987003
AUC según el mejor F1-score 0.9836898447964558
Confusion Matrix:
 [[15785   680]
 [  459  4701]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 1/confusion_matrix_param_5843969.png
Accuracy:   0.9473
Precision:  0.8736
Recall:     0.9110
F1-score:   0.8919

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2454, Test Loss: 0.2147, F1: 0.8389, AUC: 0.9775
Epoch [10/30] Train Loss: 0.0037, Test Loss: 0.8743, F1: 0.8687, AUC: 0.9795
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7703, F1: 0.8874, AUC: 0.9805
Mejores resultados en la época:  12
f1-score 0.8928705440900563
AUC según el mejor F1-score 0.9844558577391084
Confusion Matrix:
 [[15724   741]
 [  401  4759]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 1/confusion_matrix_param_5843969.png
Accuracy:   0.9472
Precision:  0.8653
Recall:     0.9223
F1-score:   0.8929
Tiempo total para red 6: 368.74 segundos
Saved on: outputs_ablation_remove_one_feature/1/Similar Artist 1

==============================
Model: Logistic Regression
Accuracy:  0.9277
Precision: 0.8088
Recall:    0.9126
F1-score:  0.8576
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.81      0.91      0.86      5160

    accuracy                           0.93     21625
   macro avg       0.89      0.92      0.90     21625
weighted avg       0.93      0.93      0.93     21625

[[15352  1113]
 [  451  4709]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 1/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 1/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7095
Precision: 0.4480
Recall:    0.9359
F1-score:  0.6059
              precision    recall  f1-score   support

           0       0.97      0.64      0.77     16465
           1       0.45      0.94      0.61      5160

    accuracy                           0.71     21625
   macro avg       0.71      0.79      0.69     21625
weighted avg       0.85      0.71      0.73     21625

[[10514  5951]
 [  331  4829]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 1/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 1/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8745
Precision: 0.7227
Recall:    0.7688
F1-score:  0.7450
              precision    recall  f1-score   support

           0       0.93      0.91      0.92     16465
           1       0.72      0.77      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.88     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [15:32:57] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [15:33:06] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[14943  1522]
 [ 1193  3967]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 1/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 1/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8634
Precision: 0.6865
Recall:    0.7872
F1-score:  0.7334
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14610  1855]
 [ 1098  4062]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 1/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 1/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9149
Precision: 0.7869
Recall:    0.8824
F1-score:  0.8319
              precision    recall  f1-score   support

           0       0.96      0.93      0.94     16465
           1       0.79      0.88      0.83      5160

    accuracy                           0.91     21625
   macro avg       0.87      0.90      0.89     21625
weighted avg       0.92      0.91      0.92     21625

[[15232  1233]
 [  607  4553]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 1/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 1/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7514
Precision: 0.4894
Recall:    0.9692
F1-score:  0.6504
              precision    recall  f1-score   support

           0       0.99      0.68      0.81     16465
           1       0.49      0.97      0.65      5160

    accuracy                           0.75     21625
   macro avg       0.74      0.83      0.73     21625
weighted avg       0.87      0.75      0.77     21625

[[11248  5217]
 [  159  5001]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 1/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 1/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9277, 'precision': 0.8088, 'recall': 0.9126, 'f1_score': 0.8576}
XGBoost: {'accuracy': 0.9149, 'precision': 0.7869, 'recall': 0.8824, 'f1_score': 0.8319}
Decision Tree: {'accuracy': 0.8745, 'precision': 0.7227, 'recall': 0.7688, 'f1_score': 0.745}
Random Forest: {'accuracy': 0.8634, 'precision': 0.6865, 'recall': 0.7872, 'f1_score': 0.7334}
Naive Bayes: {'accuracy': 0.7514, 'precision': 0.4894, 'recall': 0.9692, 'f1_score': 0.6504}
SVM: {'accuracy': 0.7095, 'precision': 0.448, 'recall': 0.9359, 'f1_score': 0.6059}

##################################################
Running experiment without SIMILAR SONG 1 feature
[Similar Song 1] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2962, Test Loss: 0.2149, F1: 0.8463, AUC: 0.9790
Epoch [10/30] Train Loss: 0.0955, Test Loss: 0.1895, F1: 0.8692, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0755, Test Loss: 0.2181, F1: 0.8691, AUC: 0.9814
Mejores resultados en la época:  2
f1-score 0.8815911193339501
AUC según el mejor F1-score 0.9837802703408923
Confusion Matrix:
 [[15580   885]
 [  395  4765]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 1/confusion_matrix_param_160801.png
Accuracy:   0.9408
Precision:  0.8434
Recall:     0.9234
F1-score:   0.8816

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2981, Test Loss: 0.1959, F1: 0.8589, AUC: 0.9785
Epoch [10/30] Train Loss: 0.0875, Test Loss: 0.2154, F1: 0.8572, AUC: 0.9828
Epoch [20/30] Train Loss: 0.0524, Test Loss: 0.2541, F1: 0.8608, AUC: 0.9815
Mejores resultados en la época:  4
f1-score 0.8786533481317055
AUC según el mejor F1-score 0.9835961294453586
Confusion Matrix:
 [[15563   902]
 [  410  4750]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 1/confusion_matrix_param_160801.png
Accuracy:   0.9393
Precision:  0.8404
Recall:     0.9205
F1-score:   0.8787

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2985, Test Loss: 0.1847, F1: 0.8630, AUC: 0.9789
Epoch [10/30] Train Loss: 0.0955, Test Loss: 0.1856, F1: 0.8708, AUC: 0.9822
Epoch [20/30] Train Loss: 0.0759, Test Loss: 0.2305, F1: 0.8615, AUC: 0.9812
Mejores resultados en la época:  3
f1-score 0.8852210206791881
AUC según el mejor F1-score 0.9835078990670838
Confusion Matrix:
 [[15749   716]
 [  494  4666]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 1/confusion_matrix_param_160801.png
Accuracy:   0.9440
Precision:  0.8670
Recall:     0.9043
F1-score:   0.8852
Tiempo total para red 1: 284.27 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2272, Test Loss: 0.1667, F1: 0.8785, AUC: 0.9824
Epoch [10/30] Train Loss: 0.0041, Test Loss: 0.8627, F1: 0.8585, AUC: 0.9790
Epoch [20/30] Train Loss: 0.0019, Test Loss: 0.3176, F1: 0.8964, AUC: 0.9881
Mejores resultados en la época:  27
f1-score 0.9077761081893313
AUC según el mejor F1-score 0.984019696466783
Confusion Matrix:
 [[15810   655]
 [  327  4833]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 1/confusion_matrix_param_5843969.png
Accuracy:   0.9546
Precision:  0.8806
Recall:     0.9366
F1-score:   0.9078

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2271, Test Loss: 0.1693, F1: 0.8704, AUC: 0.9827
Epoch [10/30] Train Loss: 0.0048, Test Loss: 0.3375, F1: 0.8893, AUC: 0.9874

[[14943  1522]
 [ 1193  3967]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 1/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 1/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8634
Precision: 0.6865
Recall:    0.7872
F1-score:  0.7334
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14610  1855]
 [ 1098  4062]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 1/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 1/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9149
Precision: 0.7869
Recall:    0.8824
F1-score:  0.8319
              precision    recall  f1-score   support

           0       0.96      0.93      0.94     16465
           1       0.79      0.88      0.83      5160

    accuracy                           0.91     21625
   macro avg       0.87      0.90      0.89     21625
weighted avg       0.92      0.91      0.92     21625

[[15232  1233]
 [  607  4553]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 1/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 1/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7514
Precision: 0.4894
Recall:    0.9692
F1-score:  0.6504
              precision    recall  f1-score   support

           0       0.99      0.68      0.81     16465
           1       0.49      0.97      0.65      5160

    accuracy                           0.75     21625
   macro avg       0.74      0.83      0.73     21625
weighted avg       0.87      0.75      0.77     21625

[[11248  5217]
 [  159  5001]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 1/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 1/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9277, 'precision': 0.8088, 'recall': 0.9126, 'f1_score': 0.8576}
XGBoost: {'accuracy': 0.9149, 'precision': 0.7869, 'recall': 0.8824, 'f1_score': 0.8319}
Decision Tree: {'accuracy': 0.8745, 'precision': 0.7227, 'recall': 0.7688, 'f1_score': 0.745}
Random Forest: {'accuracy': 0.8634, 'precision': 0.6865, 'recall': 0.7872, 'f1_score': 0.7334}
Naive Bayes: {'accuracy': 0.7514, 'precision': 0.4894, 'recall': 0.9692, 'f1_score': 0.6504}
SVM: {'accuracy': 0.7095, 'precision': 0.448, 'recall': 0.9359, 'f1_score': 0.6059}

##################################################
Running experiment without SIMILAR SONG 1 feature
[Similar Song 1] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2974, Test Loss: 0.1951, F1: 0.8579, AUC: 0.9790
Epoch [10/30] Train Loss: 0.0946, Test Loss: 0.1742, F1: 0.8773, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0681, Test Loss: 0.2255, F1: 0.8656, AUC: 0.9814
Mejores resultados en la época:  5
f1-score 0.8783245297006765
AUC según el mejor F1-score 0.9834765605689304
Confusion Matrix:
 [[15573   892]
 [  421  4739]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 1/confusion_matrix_param_160801.png
Accuracy:   0.9393
Precision:  0.8416
Recall:     0.9184
F1-score:   0.8783

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2997, Test Loss: 0.1959, F1: 0.8567, AUC: 0.9786
Epoch [10/30] Train Loss: 0.0920, Test Loss: 0.2297, F1: 0.8489, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0539, Test Loss: 0.2223, F1: 0.8700, AUC: 0.9816
Mejores resultados en la época:  2
f1-score 0.8818105616093881
AUC según el mejor F1-score 0.9837318177859072
Confusion Matrix:
 [[15622   843]
 [  426  4734]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 1/confusion_matrix_param_160801.png
Accuracy:   0.9413
Precision:  0.8488
Recall:     0.9174
F1-score:   0.8818

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3031, Test Loss: 0.2028, F1: 0.8540, AUC: 0.9787
Epoch [10/30] Train Loss: 0.0935, Test Loss: 0.1885, F1: 0.8686, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0629, Test Loss: 0.2175, F1: 0.8710, AUC: 0.9817
Mejores resultados en la época:  5
f1-score 0.8785977859778598
AUC según el mejor F1-score 0.9833995885093351
Confusion Matrix:
 [[15547   918]
 [  398  4762]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 1/confusion_matrix_param_160801.png
Accuracy:   0.9391
Precision:  0.8384
Recall:     0.9229
F1-score:   0.8786
Tiempo total para red 1: 286.72 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2268, Test Loss: 0.1875, F1: 0.8642, AUC: 0.9827
Epoch [10/30] Train Loss: 0.0019, Test Loss: 0.8731, F1: 0.8746, AUC: 0.9800
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5799, F1: 0.9058, AUC: 0.9849
Mejores resultados en la época:  17
f1-score 0.9117873651771957
AUC según el mejor F1-score 0.9877574759237943
Confusion Matrix:
 [[15975   490]
 [  426  4734]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 1/confusion_matrix_param_5843969.png
Accuracy:   0.9576
Precision:  0.9062
Recall:     0.9174
F1-score:   0.9118

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2307, Test Loss: 0.1784, F1: 0.8703, AUC: 0.9822
Epoch [10/30] Train Loss: 0.0053, Test Loss: 0.4129, F1: 0.8951, AUC: 0.9865
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [15:59:41] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [15:59:55] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7597, F1: 0.8902, AUC: 0.9827
Mejores resultados en la época:  11
f1-score 0.9038335976121631
AUC según el mejor F1-score 0.98616459155785
Confusion Matrix:
 [[15749   716]
 [  315  4845]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 1/confusion_matrix_param_5843969.png
Accuracy:   0.9523
Precision:  0.8712
Recall:     0.9390
F1-score:   0.9038

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2336, Test Loss: 0.1624, F1: 0.8750, AUC: 0.9826
Epoch [10/30] Train Loss: 0.0045, Test Loss: 0.4174, F1: 0.8980, AUC: 0.9862
Epoch [20/30] Train Loss: 0.0026, Test Loss: 0.4892, F1: 0.9079, AUC: 0.9866
Mejores resultados en la época:  20
f1-score 0.907867298578199
AUC según el mejor F1-score 0.9866274008526427
Confusion Matrix:
 [[15864   601]
 [  371  4789]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 1/confusion_matrix_param_5843969.png
Accuracy:   0.9551
Precision:  0.8885
Recall:     0.9281
F1-score:   0.9079
Tiempo total para red 6: 363.64 segundos
Saved on: outputs_ablation_remove_one_feature/1/Similar Song 1

==============================
Model: Logistic Regression
Accuracy:  0.9352
Precision: 0.8254
Recall:    0.9236
F1-score:  0.8718
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15457  1008]
 [  394  4766]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 1/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 1/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7384
Precision: 0.4728
Recall:    0.8397
F1-score:  0.6050
              precision    recall  f1-score   support

           0       0.93      0.71      0.80     16465
           1       0.47      0.84      0.60      5160

    accuracy                           0.74     21625
   macro avg       0.70      0.77      0.70     21625
weighted avg       0.82      0.74      0.76     21625

[[11634  4831]
 [  827  4333]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 1/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 1/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8714
Precision: 0.7133
Recall:    0.7713
F1-score:  0.7412
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14865  1600]
 [ 1180  3980]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 1/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 1/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8664
Precision: 0.6935
Recall:    0.7886
F1-score:  0.7380
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.87      0.87     21625

[[14667  1798]
 [ 1091  4069]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 1/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 1/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9200
Precision: 0.7986
Recall:    0.8891
F1-score:  0.8414
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15308  1157]
 [  572  4588]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 1/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 1/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7886
Precision: 0.5312
Recall:    0.9721
F1-score:  0.6870
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12038  4427]
 [  144  5016]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 1/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 1/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9352, 'precision': 0.8254, 'recall': 0.9236, 'f1_score': 0.8718}
XGBoost: {'accuracy': 0.92, 'precision': 0.7986, 'recall': 0.8891, 'f1_score': 0.8414}
Decision Tree: {'accuracy': 0.8714, 'precision': 0.7133, 'recall': 0.7713, 'f1_score': 0.7412}
Random Forest: {'accuracy': 0.8664, 'precision': 0.6935, 'recall': 0.7886, 'f1_score': 0.738}
Naive Bayes: {'accuracy': 0.7886, 'precision': 0.5312, 'recall': 0.9721, 'f1_score': 0.687}
SVM: {'accuracy': 0.7384, 'precision': 0.4728, 'recall': 0.8397, 'f1_score': 0.605}

Epoch [20/30] Train Loss: 0.0017, Test Loss: 0.4137, F1: 0.9040, AUC: 0.9871
Mejores resultados en la época:  20
f1-score 0.9040022390148335
AUC según el mejor F1-score 0.987134448925016
Confusion Matrix:
 [[15751   714]
 [  315  4845]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 1/confusion_matrix_param_5843969.png
Accuracy:   0.9524
Precision:  0.8716
Recall:     0.9390
F1-score:   0.9040

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2283, Test Loss: 0.1523, F1: 0.8816, AUC: 0.9824
Epoch [10/30] Train Loss: 0.0033, Test Loss: 0.4552, F1: 0.8760, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0003, Test Loss: 0.8021, F1: 0.8902, AUC: 0.9814
Mejores resultados en la época:  25
f1-score 0.9027417562060023
AUC según el mejor F1-score 0.9880412585305451
Confusion Matrix:
 [[15702   763]
 [  287  4873]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 1/confusion_matrix_param_5843969.png
Accuracy:   0.9514
Precision:  0.8646
Recall:     0.9444
F1-score:   0.9027
Tiempo total para red 6: 364.55 segundos
Saved on: outputs_ablation_remove_one_feature/1/Similar Song 1

==============================
Model: Logistic Regression
Accuracy:  0.9352
Precision: 0.8254
Recall:    0.9236
F1-score:  0.8718
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15457  1008]
 [  394  4766]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 1/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 1/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7384
Precision: 0.4728
Recall:    0.8397
F1-score:  0.6050
              precision    recall  f1-score   support

           0       0.93      0.71      0.80     16465
           1       0.47      0.84      0.60      5160

    accuracy                           0.74     21625
   macro avg       0.70      0.77      0.70     21625
weighted avg       0.82      0.74      0.76     21625

[[11634  4831]
 [  827  4333]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 1/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 1/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8714
Precision: 0.7133
Recall:    0.7713
F1-score:  0.7412
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14865  1600]
 [ 1180  3980]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 1/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 1/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8664
Precision: 0.6935
Recall:    0.7886
F1-score:  0.7380
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.87      0.87     21625

[[14667  1798]
 [ 1091  4069]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 1/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 1/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9200
Precision: 0.7986
Recall:    0.8891
F1-score:  0.8414
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15308  1157]
 [  572  4588]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 1/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 1/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7886
Precision: 0.5312
Recall:    0.9721
F1-score:  0.6870
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12038  4427]
 [  144  5016]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 1/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 1/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9352, 'precision': 0.8254, 'recall': 0.9236, 'f1_score': 0.8718}
XGBoost: {'accuracy': 0.92, 'precision': 0.7986, 'recall': 0.8891, 'f1_score': 0.8414}
Decision Tree: {'accuracy': 0.8714, 'precision': 0.7133, 'recall': 0.7713, 'f1_score': 0.7412}
Random Forest: {'accuracy': 0.8664, 'precision': 0.6935, 'recall': 0.7886, 'f1_score': 0.738}
Naive Bayes: {'accuracy': 0.7886, 'precision': 0.5312, 'recall': 0.9721, 'f1_score': 0.687}
SVM: {'accuracy': 0.7384, 'precision': 0.4728, 'recall': 0.8397, 'f1_score': 0.605}

/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
##################################################
Running experiment without SIMILAR ARTIST 2 feature
[Similar Artist 2] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3146, Test Loss: 0.1983, F1: 0.8518, AUC: 0.9729
Epoch [10/30] Train Loss: 0.1135, Test Loss: 0.2070, F1: 0.8479, AUC: 0.9761
Epoch [20/30] Train Loss: 0.0828, Test Loss: 0.2397, F1: 0.8477, AUC: 0.9757
Mejores resultados en la época:  4
f1-score 0.8534904805077063
AUC según el mejor F1-score 0.9777373780888283
Confusion Matrix:
 [[15302  1163]
 [  453  4707]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 2/confusion_matrix_param_160801.png
Accuracy:   0.9253
Precision:  0.8019
Recall:     0.9122
F1-score:   0.8535

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3080, Test Loss: 0.2095, F1: 0.8468, AUC: 0.9736
Epoch [10/30] Train Loss: 0.1135, Test Loss: 0.2118, F1: 0.8455, AUC: 0.9761
Epoch [20/30] Train Loss: 0.0774, Test Loss: 0.2539, F1: 0.8409, AUC: 0.9758
Mejores resultados en la época:  2
f1-score 0.8662408416306594
AUC según el mejor F1-score 0.9781371455071483
Confusion Matrix:
 [[15590   875]
 [  549  4611]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 2/confusion_matrix_param_160801.png
Accuracy:   0.9342
Precision:  0.8405
Recall:     0.8936
F1-score:   0.8662

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3157, Test Loss: 0.2191, F1: 0.8379, AUC: 0.9732
Epoch [10/30] Train Loss: 0.1042, Test Loss: 0.2149, F1: 0.8460, AUC: 0.9765
Epoch [20/30] Train Loss: 0.0574, Test Loss: 0.2556, F1: 0.8485, AUC: 0.9760
Mejores resultados en la época:  2
f1-score 0.860950280305119
AUC según el mejor F1-score 0.9780503981901943
Confusion Matrix:
 [[15428  1037]
 [  476  4684]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 2/confusion_matrix_param_160801.png
Accuracy:   0.9300
Precision:  0.8187
Recall:     0.9078
F1-score:   0.8610
Tiempo total para red 1: 284.27 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2474, Test Loss: 0.1799, F1: 0.8558, AUC: 0.9769
Epoch [10/30] Train Loss: 0.0044, Test Loss: 0.6753, F1: 0.8798, AUC: 0.9808
Epoch [20/30] Train Loss: 0.0040, Test Loss: 0.6326, F1: 0.8649, AUC: 0.9811
Mejores resultados en la época:  15
f1-score 0.8938591328232294
AUC según el mejor F1-score 0.98292834577457
Confusion Matrix:
 [[15987   478]
 [  604  4556]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 2/confusion_matrix_param_5843969.png
Accuracy:   0.9500
Precision:  0.9050
Recall:     0.8829
F1-score:   0.8939

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2486, Test Loss: 0.2144, F1: 0.8429, AUC: 0.9775
Epoch [10/30] Train Loss: 0.0051, Test Loss: 0.4771, F1: 0.8826, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7518, F1: 0.8754, AUC: 0.9803
Mejores resultados en la época:  8
f1-score 0.8920214806290756
AUC según el mejor F1-score 0.9830291292075981
Confusion Matrix:
 [[15848   617]
 [  509  4651]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 2/confusion_matrix_param_5843969.png
Accuracy:   0.9479
Precision:  0.8829
Recall:     0.9014
F1-score:   0.8920

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2469, Test Loss: 0.1628, F1: 0.8619, AUC: 0.9775
Epoch [10/30] Train Loss: 0.0035, Test Loss: 0.3806, F1: 0.8840, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6936, F1: 0.8820, AUC: 0.9804
Mejores resultados en la época:  17
f1-score 0.8887840769738704
AUC según el mejor F1-score 0.982691709216402
Confusion Matrix:
 [[15735   730]
 [  449  4711]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 2/confusion_matrix_param_5843969.png
Accuracy:   0.9455
Precision:  0.8658
Recall:     0.9130
F1-score:   0.8888
Tiempo total para red 6: 364.24 segundos
Saved on: outputs_ablation_remove_one_feature/1/Similar Artist 2

==============================
Model: Logistic Regression
Accuracy:  0.9254
Precision: 0.8028
Recall:    0.9112
F1-score:  0.8536
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.91      0.85      5160

    accuracy                           0.93     21625
   macro avg       0.89      0.92      0.90     21625
weighted avg       0.93      0.93      0.93     21625

[[15310  1155]
 [  458  4702]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 2/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 2/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8407
Precision: 0.6591
Recall:    0.6884
F1-score:  0.6734
              precision    recall  f1-score   support

           0       0.90      0.89      0.89     16465
           1       0.66      0.69      0.67      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.79      0.78     21625
weighted avg       0.84      0.84      0.84     21625

[[14628  1837]
 [ 1608  3552]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 2/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 2/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8697
Precision: 0.7090
Recall:    0.7702
F1-score:  0.7383
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14834  1631]
 [ 1186  3974]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 2/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 2/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8641
Precision: 0.6877
Recall:    0.7886
F1-score:  0.7347
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14617  1848]
 [ 1091  4069]]
##################################################
Running experiment without SIMILAR ARTIST 2 feature
[Similar Artist 2] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3201, Test Loss: 0.2242, F1: 0.8343, AUC: 0.9727
Epoch [10/30] Train Loss: 0.0945, Test Loss: 0.2103, F1: 0.8529, AUC: 0.9771
Epoch [20/30] Train Loss: 0.0480, Test Loss: 0.2552, F1: 0.8522, AUC: 0.9762
Mejores resultados en la época:  2
f1-score 0.8664111433112087
AUC según el mejor F1-score 0.9781008281602742
Confusion Matrix:
 [[15562   903]
 [  526  4634]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 2/confusion_matrix_param_160801.png
Accuracy:   0.9339
Precision:  0.8369
Recall:     0.8981
F1-score:   0.8664

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3137, Test Loss: 0.1948, F1: 0.8560, AUC: 0.9731
Epoch [10/30] Train Loss: 0.1135, Test Loss: 0.2084, F1: 0.8473, AUC: 0.9762
Epoch [20/30] Train Loss: 0.0695, Test Loss: 0.2792, F1: 0.8321, AUC: 0.9757
Mejores resultados en la época:  2
f1-score 0.8622222222222222
AUC según el mejor F1-score 0.9779229843901911
Confusion Matrix:
 [[15481   984]
 [  504  4656]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 2/confusion_matrix_param_160801.png
Accuracy:   0.9312
Precision:  0.8255
Recall:     0.9023
F1-score:   0.8622

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3111, Test Loss: 0.2249, F1: 0.8349, AUC: 0.9736
Epoch [10/30] Train Loss: 0.1140, Test Loss: 0.2150, F1: 0.8448, AUC: 0.9760
Epoch [20/30] Train Loss: 0.0784, Test Loss: 0.2536, F1: 0.8425, AUC: 0.9755
Mejores resultados en la época:  1
f1-score 0.8615071283095723
AUC según el mejor F1-score 0.9774367344872963
Confusion Matrix:
 [[15476   989]
 [  507  4653]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 2/confusion_matrix_param_160801.png
Accuracy:   0.9308
Precision:  0.8247
Recall:     0.9017
F1-score:   0.8615
Tiempo total para red 1: 287.41 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2474, Test Loss: 0.2154, F1: 0.8310, AUC: 0.9759
Epoch [10/30] Train Loss: 0.0031, Test Loss: 0.5596, F1: 0.8582, AUC: 0.9808
Epoch [20/30] Train Loss: 0.0029, Test Loss: 0.3974, F1: 0.8876, AUC: 0.9836
Mejores resultados en la época:  12
f1-score 0.8904071127749181
AUC según el mejor F1-score 0.9823045183934915
Confusion Matrix:
 [[15697   768]
 [  403  4757]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 2/confusion_matrix_param_5843969.png
Accuracy:   0.9458
Precision:  0.8610
Recall:     0.9219
F1-score:   0.8904

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2505, Test Loss: 0.2070, F1: 0.8409, AUC: 0.9769
Epoch [10/30] Train Loss: 0.0033, Test Loss: 0.4396, F1: 0.8771, AUC: 0.9827
Epoch [20/30] Train Loss: 0.0025, Test Loss: 0.4860, F1: 0.8634, AUC: 0.9834
Mejores resultados en la época:  21
f1-score 0.8930451127819549
AUC según el mejor F1-score 0.9843502308161309
Confusion Matrix:
 [[15736   729]
 [  409  4751]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 2/confusion_matrix_param_5843969.png
Accuracy:   0.9474
Precision:  0.8670
Recall:     0.9207
F1-score:   0.8930

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2472, Test Loss: 0.1789, F1: 0.8649, AUC: 0.9766
Epoch [10/30] Train Loss: 0.0047, Test Loss: 0.5449, F1: 0.8693, AUC: 0.9818
Epoch [20/30] Train Loss: 0.0018, Test Loss: 0.4936, F1: 0.8904, AUC: 0.9825
Mejores resultados en la época:  9
f1-score 0.8922462941847207
AUC según el mejor F1-score 0.9833024009114942
Confusion Matrix:
 [[15796   669]
 [  465  4695]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 2/confusion_matrix_param_5843969.png
Accuracy:   0.9476
Precision:  0.8753
Recall:     0.9099
F1-score:   0.8922
Tiempo total para red 6: 365.29 segundos
Saved on: outputs_ablation_remove_one_feature/1/Similar Artist 2

==============================
Model: Logistic Regression
Accuracy:  0.9254
Precision: 0.8028
Recall:    0.9112
F1-score:  0.8536
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.91      0.85      5160

    accuracy                           0.93     21625
   macro avg       0.89      0.92      0.90     21625
weighted avg       0.93      0.93      0.93     21625

[[15310  1155]
 [  458  4702]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 2/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 2/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8407
Precision: 0.6591
Recall:    0.6884
F1-score:  0.6734
              precision    recall  f1-score   support

           0       0.90      0.89      0.89     16465
           1       0.66      0.69      0.67      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.79      0.78     21625
weighted avg       0.84      0.84      0.84     21625

[[14628  1837]
 [ 1608  3552]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 2/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 2/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8697
Precision: 0.7090
Recall:    0.7702
F1-score:  0.7383
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14834  1631]
 [ 1186  3974]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 2/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 2/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8641
Precision: 0.6877
Recall:    0.7886
F1-score:  0.7347
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14617  1848]
 [ 1091  4069]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [16:26:25] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [16:26:31] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 2/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 2/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9157
Precision: 0.7889
Recall:    0.8833
F1-score:  0.8334
              precision    recall  f1-score   support

           0       0.96      0.93      0.94     16465
           1       0.79      0.88      0.83      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.90      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15245  1220]
 [  602  4558]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 2/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 2/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7479
Precision: 0.4858
Recall:    0.9667
F1-score:  0.6467
              precision    recall  f1-score   support

           0       0.98      0.68      0.80     16465
           1       0.49      0.97      0.65      5160

    accuracy                           0.75     21625
   macro avg       0.74      0.82      0.73     21625
weighted avg       0.87      0.75      0.77     21625

[[11186  5279]
 [  172  4988]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 2/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 2/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9254, 'precision': 0.8028, 'recall': 0.9112, 'f1_score': 0.8536}
XGBoost: {'accuracy': 0.9157, 'precision': 0.7889, 'recall': 0.8833, 'f1_score': 0.8334}
Decision Tree: {'accuracy': 0.8697, 'precision': 0.709, 'recall': 0.7702, 'f1_score': 0.7383}
Random Forest: {'accuracy': 0.8641, 'precision': 0.6877, 'recall': 0.7886, 'f1_score': 0.7347}
SVM: {'accuracy': 0.8407, 'precision': 0.6591, 'recall': 0.6884, 'f1_score': 0.6734}
Naive Bayes: {'accuracy': 0.7479, 'precision': 0.4858, 'recall': 0.9667, 'f1_score': 0.6467}

##################################################
Running experiment without SIMILAR SONG 2 feature
[Similar Song 2] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3005, Test Loss: 0.2173, F1: 0.8416, AUC: 0.9782
Epoch [10/30] Train Loss: 0.0858, Test Loss: 0.1908, F1: 0.8696, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0577, Test Loss: 0.2241, F1: 0.8699, AUC: 0.9811
Mejores resultados en la época:  2
f1-score 0.8786479497598818
AUC según el mejor F1-score 0.983612037043576
Confusion Matrix:
 [[15554   911]
 [  403  4757]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 2/confusion_matrix_param_160801.png
Accuracy:   0.9392
Precision:  0.8393
Recall:     0.9219
F1-score:   0.8786

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3021, Test Loss: 0.2115, F1: 0.8457, AUC: 0.9783
Epoch [10/30] Train Loss: 0.0944, Test Loss: 0.1990, F1: 0.8629, AUC: 0.9821
Epoch [20/30] Train Loss: 0.0726, Test Loss: 0.2308, F1: 0.8614, AUC: 0.9810
Mejores resultados en la época:  4
f1-score 0.8806445568671538
AUC según el mejor F1-score 0.9833438148103681
Confusion Matrix:
 [[15651   814]
 [  460  4700]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 2/confusion_matrix_param_160801.png
Accuracy:   0.9411
Precision:  0.8524
Recall:     0.9109
F1-score:   0.8806

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2990, Test Loss: 0.1868, F1: 0.8632, AUC: 0.9786
Epoch [10/30] Train Loss: 0.0952, Test Loss: 0.2069, F1: 0.8599, AUC: 0.9822
Epoch [20/30] Train Loss: 0.0829, Test Loss: 0.2360, F1: 0.8578, AUC: 0.9806
Mejores resultados en la época:  2
f1-score 0.8822979041916168
AUC según el mejor F1-score 0.9833617527901561
Confusion Matrix:
 [[15652   813]
 [  445  4715]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 2/confusion_matrix_param_160801.png
Accuracy:   0.9418
Precision:  0.8529
Recall:     0.9138
F1-score:   0.8823
Tiempo total para red 1: 286.50 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2291, Test Loss: 0.1669, F1: 0.8657, AUC: 0.9824
Epoch [10/30] Train Loss: 0.0027, Test Loss: 0.3063, F1: 0.9065, AUC: 0.9864
Epoch [20/30] Train Loss: 0.0039, Test Loss: 0.3103, F1: 0.9094, AUC: 0.9867
Mejores resultados en la época:  20
f1-score 0.9094390197204671
AUC según el mejor F1-score 0.986738971791232
Confusion Matrix:
 [[15929   536]
 [  410  4750]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 2/confusion_matrix_param_5843969.png
Accuracy:   0.9563
Precision:  0.8986
Recall:     0.9205
F1-score:   0.9094

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2257, Test Loss: 0.1937, F1: 0.8593, AUC: 0.9817
Epoch [10/30] Train Loss: 0.0048, Test Loss: 0.3763, F1: 0.9000, AUC: 0.9869
Epoch [20/30] Train Loss: 0.0030, Test Loss: 0.2671, F1: 0.9019, AUC: 0.9874
Mejores resultados en la época:  16
f1-score 0.9095065726805106
AUC según el mejor F1-score 0.986512852021083
Confusion Matrix:
 [[15901   564]
 [  386  4774]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 2/confusion_matrix_param_5843969.png
Accuracy:   0.9561
Precision:  0.8943
Recall:     0.9252
F1-score:   0.9095

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2263, Test Loss: 0.1550, F1: 0.8758, AUC: 0.9821
Epoch [10/30] Train Loss: 0.0039, Test Loss: 0.3888, F1: 0.8842, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0034, Test Loss: 0.4135, F1: 0.8940, AUC: 0.9872
Mejores resultados en la época:  29
f1-score 0.90458767238953
AUC según el mejor F1-score 0.9881769409859298
Confusion Matrix:
 [[15787   678]
 [  339  4821]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 2/confusion_matrix_param_5843969.png
Accuracy:   0.9530
Precision:  0.8767
Recall:     0.9343
F1-score:   0.9046
Tiempo total para red 6: 366.98 segundos
Saved on: outputs_ablation_remove_one_feature/1/Similar Song 2

==============================
Model: Logistic Regression
Accuracy:  0.9349
Precision: 0.8245
Recall:    0.9240
F1-score:  0.8714
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.82      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.94     21625
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 2/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 2/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9157
Precision: 0.7889
Recall:    0.8833
F1-score:  0.8334
              precision    recall  f1-score   support

           0       0.96      0.93      0.94     16465
           1       0.79      0.88      0.83      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.90      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15245  1220]
 [  602  4558]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 2/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 2/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7479
Precision: 0.4858
Recall:    0.9667
F1-score:  0.6467
              precision    recall  f1-score   support

           0       0.98      0.68      0.80     16465
           1       0.49      0.97      0.65      5160

    accuracy                           0.75     21625
   macro avg       0.74      0.82      0.73     21625
weighted avg       0.87      0.75      0.77     21625

[[11186  5279]
 [  172  4988]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 2/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 2/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9254, 'precision': 0.8028, 'recall': 0.9112, 'f1_score': 0.8536}
XGBoost: {'accuracy': 0.9157, 'precision': 0.7889, 'recall': 0.8833, 'f1_score': 0.8334}
Decision Tree: {'accuracy': 0.8697, 'precision': 0.709, 'recall': 0.7702, 'f1_score': 0.7383}
Random Forest: {'accuracy': 0.8641, 'precision': 0.6877, 'recall': 0.7886, 'f1_score': 0.7347}
SVM: {'accuracy': 0.8407, 'precision': 0.6591, 'recall': 0.6884, 'f1_score': 0.6734}
Naive Bayes: {'accuracy': 0.7479, 'precision': 0.4858, 'recall': 0.9667, 'f1_score': 0.6467}

##################################################
Running experiment without SIMILAR SONG 2 feature
[Similar Song 2] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2966, Test Loss: 0.1927, F1: 0.8611, AUC: 0.9787
Epoch [10/30] Train Loss: 0.0949, Test Loss: 0.2005, F1: 0.8625, AUC: 0.9821
Epoch [20/30] Train Loss: 0.0831, Test Loss: 0.2147, F1: 0.8673, AUC: 0.9806
Mejores resultados en la época:  2
f1-score 0.875842333610265
AUC según el mejor F1-score 0.983113263511748
Confusion Matrix:
 [[15536   929]
 [  416  4744]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 2/confusion_matrix_param_160801.png
Accuracy:   0.9378
Precision:  0.8362
Recall:     0.9194
F1-score:   0.8758

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2908, Test Loss: 0.1725, F1: 0.8698, AUC: 0.9791
Epoch [10/30] Train Loss: 0.0928, Test Loss: 0.2034, F1: 0.8589, AUC: 0.9822
Epoch [20/30] Train Loss: 0.0586, Test Loss: 0.2229, F1: 0.8669, AUC: 0.9818
Mejores resultados en la época:  1
f1-score 0.8833775085195001
AUC según el mejor F1-score 0.9827499899952212
Confusion Matrix:
 [[15727   738]
 [  494  4666]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 2/confusion_matrix_param_160801.png
Accuracy:   0.9430
Precision:  0.8634
Recall:     0.9043
F1-score:   0.8834

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2961, Test Loss: 0.1819, F1: 0.8656, AUC: 0.9788
Epoch [10/30] Train Loss: 0.0948, Test Loss: 0.2019, F1: 0.8607, AUC: 0.9821
Epoch [20/30] Train Loss: 0.0714, Test Loss: 0.2451, F1: 0.8571, AUC: 0.9808
Mejores resultados en la época:  2
f1-score 0.8821001494768311
AUC según el mejor F1-score 0.9831196430294941
Confusion Matrix:
 [[15642   823]
 [  439  4721]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 2/confusion_matrix_param_160801.png
Accuracy:   0.9416
Precision:  0.8516
Recall:     0.9149
F1-score:   0.8821
Tiempo total para red 1: 287.81 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2284, Test Loss: 0.1632, F1: 0.8730, AUC: 0.9818
Epoch [10/30] Train Loss: 0.0043, Test Loss: 0.2852, F1: 0.8996, AUC: 0.9867
Epoch [20/30] Train Loss: 0.0020, Test Loss: 0.3191, F1: 0.8872, AUC: 0.9874
Mejores resultados en la época:  11
f1-score 0.9039378916845945
AUC según el mejor F1-score 0.9865806137990616
Confusion Matrix:
 [[15766   699]
 [  328  4832]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 2/confusion_matrix_param_5843969.png
Accuracy:   0.9525
Precision:  0.8736
Recall:     0.9364
F1-score:   0.9039

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2296, Test Loss: 0.1948, F1: 0.8667, AUC: 0.9815
Epoch [10/30] Train Loss: 0.0033, Test Loss: 0.2708, F1: 0.8751, AUC: 0.9871
Epoch [20/30] Train Loss: 0.0017, Test Loss: 0.3443, F1: 0.9092, AUC: 0.9875
Mejores resultados en la época:  9
f1-score 0.9106816652244977
AUC según el mejor F1-score 0.9866124289954967
Confusion Matrix:
 [[15960   505]
 [  424  4736]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 2/confusion_matrix_param_5843969.png
Accuracy:   0.9570
Precision:  0.9036
Recall:     0.9178
F1-score:   0.9107

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2299, Test Loss: 0.1880, F1: 0.8612, AUC: 0.9824
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.5932, F1: 0.8680, AUC: 0.9845
Epoch [20/30] Train Loss: 0.0027, Test Loss: 0.3470, F1: 0.8960, AUC: 0.9871
Mejores resultados en la época:  29
f1-score 0.9083175803402647
AUC según el mejor F1-score 0.9848551778849661
Confusion Matrix:
 [[15850   615]
 [  355  4805]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 2/confusion_matrix_param_5843969.png
Accuracy:   0.9551
Precision:  0.8865
Recall:     0.9312
F1-score:   0.9083
Tiempo total para red 6: 367.84 segundos
Saved on: outputs_ablation_remove_one_feature/1/Similar Song 2

==============================
Model: Logistic Regression
Accuracy:  0.9349
Precision: 0.8245
Recall:    0.9240
F1-score:  0.8714
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.82      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.94     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [16:53:19] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [16:53:29] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[15450  1015]
 [  392  4768]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 2/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 2/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8254
Precision: 0.6041
Recall:    0.7787
F1-score:  0.6804
              precision    recall  f1-score   support

           0       0.92      0.84      0.88     16465
           1       0.60      0.78      0.68      5160

    accuracy                           0.83     21625
   macro avg       0.76      0.81      0.78     21625
weighted avg       0.85      0.83      0.83     21625

[[13832  2633]
 [ 1142  4018]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 2/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 2/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8792
Precision: 0.7427
Recall:    0.7556
F1-score:  0.7491
              precision    recall  f1-score   support

           0       0.92      0.92      0.92     16465
           1       0.74      0.76      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.84      0.83     21625
weighted avg       0.88      0.88      0.88     21625

[[15114  1351]
 [ 1261  3899]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 2/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 2/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8684
Precision: 0.6983
Recall:    0.7897
F1-score:  0.7412
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.70      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14704  1761]
 [ 1085  4075]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 2/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 2/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9206
Precision: 0.8003
Recall:    0.8891
F1-score:  0.8424
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15320  1145]
 [  572  4588]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 2/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 2/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7905
Precision: 0.5335
Recall:    0.9700
F1-score:  0.6884
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.77     21625
weighted avg       0.88      0.79      0.81     21625

[[12089  4376]
 [  155  5005]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 2/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 2/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9349, 'precision': 0.8245, 'recall': 0.924, 'f1_score': 0.8714}
XGBoost: {'accuracy': 0.9206, 'precision': 0.8003, 'recall': 0.8891, 'f1_score': 0.8424}
Decision Tree: {'accuracy': 0.8792, 'precision': 0.7427, 'recall': 0.7556, 'f1_score': 0.7491}
Random Forest: {'accuracy': 0.8684, 'precision': 0.6983, 'recall': 0.7897, 'f1_score': 0.7412}
Naive Bayes: {'accuracy': 0.7905, 'precision': 0.5335, 'recall': 0.97, 'f1_score': 0.6884}
SVM: {'accuracy': 0.8254, 'precision': 0.6041, 'recall': 0.7787, 'f1_score': 0.6804}

##################################################
Running experiment without SIMILAR ARTIST 3 feature
[Similar Artist 3] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3212, Test Loss: 0.1954, F1: 0.8534, AUC: 0.9724
Epoch [10/30] Train Loss: 0.1156, Test Loss: 0.2240, F1: 0.8361, AUC: 0.9762
Epoch [20/30] Train Loss: 0.0686, Test Loss: 0.2578, F1: 0.8432, AUC: 0.9765
Mejores resultados en la época:  28
f1-score 0.8569105691056911
AUC según el mejor F1-score 0.9768114711262086
Confusion Matrix:
 [[15298  1167]
 [  417  4743]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 3/confusion_matrix_param_160801.png
Accuracy:   0.9268
Precision:  0.8025
Recall:     0.9192
F1-score:   0.8569

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3235, Test Loss: 0.2277, F1: 0.8323, AUC: 0.9718
Epoch [10/30] Train Loss: 0.1199, Test Loss: 0.2328, F1: 0.8381, AUC: 0.9756

[[15450  1015]
 [  392  4768]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 2/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 2/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8254
Precision: 0.6041
Recall:    0.7787
F1-score:  0.6804
              precision    recall  f1-score   support

           0       0.92      0.84      0.88     16465
           1       0.60      0.78      0.68      5160

    accuracy                           0.83     21625
   macro avg       0.76      0.81      0.78     21625
weighted avg       0.85      0.83      0.83     21625

[[13832  2633]
 [ 1142  4018]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 2/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 2/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8792
Precision: 0.7427
Recall:    0.7556
F1-score:  0.7491
              precision    recall  f1-score   support

           0       0.92      0.92      0.92     16465
           1       0.74      0.76      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.84      0.83     21625
weighted avg       0.88      0.88      0.88     21625

[[15114  1351]
 [ 1261  3899]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 2/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 2/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8684
Precision: 0.6983
Recall:    0.7897
F1-score:  0.7412
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.70      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14704  1761]
 [ 1085  4075]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 2/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 2/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9206
Precision: 0.8003
Recall:    0.8891
F1-score:  0.8424
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15320  1145]
 [  572  4588]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 2/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 2/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7905
Precision: 0.5335
Recall:    0.9700
F1-score:  0.6884
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.77     21625
weighted avg       0.88      0.79      0.81     21625

[[12089  4376]
 [  155  5005]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 2/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 2/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9349, 'precision': 0.8245, 'recall': 0.924, 'f1_score': 0.8714}
XGBoost: {'accuracy': 0.9206, 'precision': 0.8003, 'recall': 0.8891, 'f1_score': 0.8424}
Decision Tree: {'accuracy': 0.8792, 'precision': 0.7427, 'recall': 0.7556, 'f1_score': 0.7491}
Random Forest: {'accuracy': 0.8684, 'precision': 0.6983, 'recall': 0.7897, 'f1_score': 0.7412}
Naive Bayes: {'accuracy': 0.7905, 'precision': 0.5335, 'recall': 0.97, 'f1_score': 0.6884}
SVM: {'accuracy': 0.8254, 'precision': 0.6041, 'recall': 0.7787, 'f1_score': 0.6804}

##################################################
Running experiment without SIMILAR ARTIST 3 feature
[Similar Artist 3] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3215, Test Loss: 0.2221, F1: 0.8391, AUC: 0.9724
Epoch [10/30] Train Loss: 0.1167, Test Loss: 0.2223, F1: 0.8433, AUC: 0.9759
Epoch [20/30] Train Loss: 0.0851, Test Loss: 0.2724, F1: 0.8342, AUC: 0.9756
Mejores resultados en la época:  2
f1-score 0.8634246320104342
AUC según el mejor F1-score 0.9770346483143715
Confusion Matrix:
 [[15525   940]
 [  526  4634]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 3/confusion_matrix_param_160801.png
Accuracy:   0.9322
Precision:  0.8314
Recall:     0.8981
F1-score:   0.8634

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3205, Test Loss: 0.2207, F1: 0.8389, AUC: 0.9725
Epoch [10/30] Train Loss: 0.1050, Test Loss: 0.2147, F1: 0.8475, AUC: 0.9773
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:19:56] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:20:19] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [20/30] Train Loss: 0.1077, Test Loss: 0.2606, F1: 0.8361, AUC: 0.9747
Mejores resultados en la época:  1
f1-score 0.8636406708992703
AUC según el mejor F1-score 0.9764209551856533
Confusion Matrix:
 [[15629   836]
 [  603  4557]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 3/confusion_matrix_param_160801.png
Accuracy:   0.9335
Precision:  0.8450
Recall:     0.8831
F1-score:   0.8636

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3170, Test Loss: 0.2102, F1: 0.8461, AUC: 0.9726
Epoch [10/30] Train Loss: 0.1193, Test Loss: 0.2380, F1: 0.8343, AUC: 0.9757
Epoch [20/30] Train Loss: 0.0954, Test Loss: 0.2416, F1: 0.8461, AUC: 0.9751
Mejores resultados en la época:  1
f1-score 0.859885545504892
AUC según el mejor F1-score 0.9769027735600769
Confusion Matrix:
 [[15449  1016]
 [  502  4658]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 3/confusion_matrix_param_160801.png
Accuracy:   0.9298
Precision:  0.8209
Recall:     0.9027
F1-score:   0.8599
Tiempo total para red 1: 285.86 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2545, Test Loss: 0.2266, F1: 0.8253, AUC: 0.9767
Epoch [10/30] Train Loss: 0.0061, Test Loss: 0.4723, F1: 0.8897, AUC: 0.9831
Epoch [20/30] Train Loss: 0.0007, Test Loss: 1.2615, F1: 0.8665, AUC: 0.9703
Mejores resultados en la época:  25
f1-score 0.8931801055011304
AUC según el mejor F1-score 0.9843624131055541
Confusion Matrix:
 [[15750   715]
 [  419  4741]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 3/confusion_matrix_param_5843969.png
Accuracy:   0.9476
Precision:  0.8690
Recall:     0.9188
F1-score:   0.8932

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2528, Test Loss: 0.2042, F1: 0.8291, AUC: 0.9762
Epoch [10/30] Train Loss: 0.0042, Test Loss: 0.8000, F1: 0.8529, AUC: 0.9795
Epoch [20/30] Train Loss: 0.0011, Test Loss: 0.5420, F1: 0.8813, AUC: 0.9835
Mejores resultados en la época:  16
f1-score 0.8908833254605574
AUC según el mejor F1-score 0.9836030033168783
Confusion Matrix:
 [[15755   710]
 [  445  4715]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 3/confusion_matrix_param_5843969.png
Accuracy:   0.9466
Precision:  0.8691
Recall:     0.9138
F1-score:   0.8909

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2547, Test Loss: 0.2165, F1: 0.8416, AUC: 0.9758
Epoch [10/30] Train Loss: 0.0030, Test Loss: 0.3991, F1: 0.8556, AUC: 0.9834
Epoch [20/30] Train Loss: 0.0012, Test Loss: 0.6738, F1: 0.8753, AUC: 0.9807
Mejores resultados en la época:  17
f1-score 0.8942031758178688
AUC según el mejor F1-score 0.9828477131429836
Confusion Matrix:
 [[15845   620]
 [  486  4674]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 3/confusion_matrix_param_5843969.png
Accuracy:   0.9489
Precision:  0.8829
Recall:     0.9058
F1-score:   0.8942
Tiempo total para red 6: 368.26 segundos
Saved on: outputs_ablation_remove_one_feature/1/Similar Artist 3

==============================
Model: Logistic Regression
Accuracy:  0.9232
Precision: 0.7986
Recall:    0.9070
F1-score:  0.8494
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.91      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.92      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15285  1180]
 [  480  4680]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 3/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 3/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7638
Precision: 0.5030
Recall:    0.8519
F1-score:  0.6326
              precision    recall  f1-score   support

           0       0.94      0.74      0.83     16465
           1       0.50      0.85      0.63      5160

    accuracy                           0.76     21625
   macro avg       0.72      0.79      0.73     21625
weighted avg       0.84      0.76      0.78     21625

[[12122  4343]
 [  764  4396]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 3/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 3/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8599
Precision: 0.6773
Recall:    0.7890
F1-score:  0.7289
              precision    recall  f1-score   support

           0       0.93      0.88      0.91     16465
           1       0.68      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.84      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14525  1940]
 [ 1089  4071]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 3/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 3/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8641
Precision: 0.6896
Recall:    0.7828
F1-score:  0.7332
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.78      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14647  1818]
 [ 1121  4039]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 3/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 3/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9134
Precision: 0.7856
Recall:    0.8764
F1-score:  0.8285
              precision    recall  f1-score   support

           0       0.96      0.93      0.94     16465
           1       0.79      0.88      0.83      5160

    accuracy                           0.91     21625
   macro avg       0.87      0.90      0.89     21625
weighted avg       0.92      0.91      0.91     21625

[[15231  1234]
 [  638  4522]]
Epoch [20/30] Train Loss: 0.0483, Test Loss: 0.2470, F1: 0.8576, AUC: 0.9774
Mejores resultados en la época:  22
f1-score 0.8595311648191896
AUC según el mejor F1-score 0.9774428374023355
Confusion Matrix:
 [[15349  1116]
 [  430  4730]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 3/confusion_matrix_param_160801.png
Accuracy:   0.9285
Precision:  0.8091
Recall:     0.9167
F1-score:   0.8595

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3164, Test Loss: 0.2248, F1: 0.8349, AUC: 0.9726
Epoch [10/30] Train Loss: 0.1029, Test Loss: 0.1973, F1: 0.8573, AUC: 0.9769
Epoch [20/30] Train Loss: 0.0504, Test Loss: 0.2678, F1: 0.8485, AUC: 0.9770
Mejores resultados en la época:  16
f1-score 0.8588353230657068
AUC según el mejor F1-score 0.9768665798016465
Confusion Matrix:
 [[15364  1101]
 [  448  4712]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 3/confusion_matrix_param_160801.png
Accuracy:   0.9284
Precision:  0.8106
Recall:     0.9132
F1-score:   0.8588
Tiempo total para red 1: 287.63 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2571, Test Loss: 0.1893, F1: 0.8540, AUC: 0.9767
Epoch [10/30] Train Loss: 0.0028, Test Loss: 0.4574, F1: 0.8837, AUC: 0.9834
Epoch [20/30] Train Loss: 0.0020, Test Loss: 0.7648, F1: 0.8350, AUC: 0.9817
Mejores resultados en la época:  26
f1-score 0.8991365404687351
AUC según el mejor F1-score 0.9845429817065563
Confusion Matrix:
 [[15824   641]
 [  422  4738]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 3/confusion_matrix_param_5843969.png
Accuracy:   0.9508
Precision:  0.8808
Recall:     0.9182
F1-score:   0.8991

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2555, Test Loss: 0.2394, F1: 0.8333, AUC: 0.9760
Epoch [10/30] Train Loss: 0.0055, Test Loss: 0.4690, F1: 0.8647, AUC: 0.9816
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.9121, F1: 0.8816, AUC: 0.9780
Mejores resultados en la época:  9
f1-score 0.8888061784684098
AUC según el mejor F1-score 0.9821946659227818
Confusion Matrix:
 [[15654   811]
 [  384  4776]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 3/confusion_matrix_param_5843969.png
Accuracy:   0.9447
Precision:  0.8548
Recall:     0.9256
F1-score:   0.8888

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2561, Test Loss: 0.1857, F1: 0.8571, AUC: 0.9767
Epoch [10/30] Train Loss: 0.0051, Test Loss: 0.7877, F1: 0.8489, AUC: 0.9786
Epoch [20/30] Train Loss: 0.0007, Test Loss: 0.7794, F1: 0.8622, AUC: 0.9824
Mejores resultados en la época:  15
f1-score 0.894937917860554
AUC según el mejor F1-score 0.9831742985473062
Confusion Matrix:
 [[15840   625]
 [  475  4685]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Artist 3/confusion_matrix_param_5843969.png
Accuracy:   0.9491
Precision:  0.8823
Recall:     0.9079
F1-score:   0.8949
Tiempo total para red 6: 368.67 segundos
Saved on: outputs_ablation_remove_one_feature/1/Similar Artist 3

==============================
Model: Logistic Regression
Accuracy:  0.9232
Precision: 0.7986
Recall:    0.9070
F1-score:  0.8494
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.91      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.92      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15285  1180]
 [  480  4680]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 3/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 3/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7638
Precision: 0.5030
Recall:    0.8519
F1-score:  0.6326
              precision    recall  f1-score   support

           0       0.94      0.74      0.83     16465
           1       0.50      0.85      0.63      5160

    accuracy                           0.76     21625
   macro avg       0.72      0.79      0.73     21625
weighted avg       0.84      0.76      0.78     21625

[[12122  4343]
 [  764  4396]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 3/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 3/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8599
Precision: 0.6773
Recall:    0.7890
F1-score:  0.7289
              precision    recall  f1-score   support

           0       0.93      0.88      0.91     16465
           1       0.68      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.84      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14525  1940]
 [ 1089  4071]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 3/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 3/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8641
Precision: 0.6896
Recall:    0.7828
F1-score:  0.7332
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.78      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14647  1818]
 [ 1121  4039]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 3/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 3/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9134
Precision: 0.7856
Recall:    0.8764
F1-score:  0.8285
              precision    recall  f1-score   support

           0       0.96      0.93      0.94     16465
           1       0.79      0.88      0.83      5160

    accuracy                           0.91     21625
   macro avg       0.87      0.90      0.89     21625
weighted avg       0.92      0.91      0.91     21625

[[15231  1234]
 [  638  4522]]
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 3/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 3/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7434
Precision: 0.4812
Recall:    0.9663
F1-score:  0.6425
              precision    recall  f1-score   support

           0       0.98      0.67      0.80     16465
           1       0.48      0.97      0.64      5160

    accuracy                           0.74     21625
   macro avg       0.73      0.82      0.72     21625
weighted avg       0.86      0.74      0.76     21625

[[11090  5375]
 [  174  4986]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 3/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 3/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9232, 'precision': 0.7986, 'recall': 0.907, 'f1_score': 0.8494}
XGBoost: {'accuracy': 0.9134, 'precision': 0.7856, 'recall': 0.8764, 'f1_score': 0.8285}
Random Forest: {'accuracy': 0.8641, 'precision': 0.6896, 'recall': 0.7828, 'f1_score': 0.7332}
Decision Tree: {'accuracy': 0.8599, 'precision': 0.6773, 'recall': 0.789, 'f1_score': 0.7289}
Naive Bayes: {'accuracy': 0.7434, 'precision': 0.4812, 'recall': 0.9663, 'f1_score': 0.6425}
SVM: {'accuracy': 0.7638, 'precision': 0.503, 'recall': 0.8519, 'f1_score': 0.6326}

##################################################
Running experiment without SIMILAR SONG 3 feature
[Similar Song 3] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3049, Test Loss: 0.1823, F1: 0.8666, AUC: 0.9788
Epoch [10/30] Train Loss: 0.0828, Test Loss: 0.2001, F1: 0.8631, AUC: 0.9830
Epoch [20/30] Train Loss: 0.0465, Test Loss: 0.2382, F1: 0.8666, AUC: 0.9820
Mejores resultados en la época:  4
f1-score 0.8797626772967461
AUC según el mejor F1-score 0.9837882329677471
Confusion Matrix:
 [[15583   882]
 [  415  4745]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 3/confusion_matrix_param_160801.png
Accuracy:   0.9400
Precision:  0.8433
Recall:     0.9196
F1-score:   0.8798

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3013, Test Loss: 0.1739, F1: 0.8692, AUC: 0.9781
Epoch [10/30] Train Loss: 0.0944, Test Loss: 0.2035, F1: 0.8601, AUC: 0.9818
Epoch [20/30] Train Loss: 0.0789, Test Loss: 0.2537, F1: 0.8505, AUC: 0.9803
Mejores resultados en la época:  2
f1-score 0.8817507279045741
AUC según el mejor F1-score 0.9833239053006496
Confusion Matrix:
 [[15672   793]
 [  466  4694]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 3/confusion_matrix_param_160801.png
Accuracy:   0.9418
Precision:  0.8555
Recall:     0.9097
F1-score:   0.8818

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2986, Test Loss: 0.1895, F1: 0.8605, AUC: 0.9792
Epoch [10/30] Train Loss: 0.0921, Test Loss: 0.2033, F1: 0.8619, AUC: 0.9818
Epoch [20/30] Train Loss: 0.0622, Test Loss: 0.2251, F1: 0.8683, AUC: 0.9811
Mejores resultados en la época:  3
f1-score 0.8830777913648763
AUC según el mejor F1-score 0.9834644842124592
Confusion Matrix:
 [[15688   777]
 [  466  4694]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 3/confusion_matrix_param_160801.png
Accuracy:   0.9425
Precision:  0.8580
Recall:     0.9097
F1-score:   0.8831
Tiempo total para red 1: 281.42 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2299, Test Loss: 0.1659, F1: 0.8579, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0029, Test Loss: 0.3573, F1: 0.9027, AUC: 0.9870
Epoch [20/30] Train Loss: 0.0010, Test Loss: 0.4214, F1: 0.8944, AUC: 0.9870
Mejores resultados en la época:  8
f1-score 0.9100741037452433
AUC según el mejor F1-score 0.9872171649046486
Confusion Matrix:
 [[16183   282]
 [  616  4544]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 3/confusion_matrix_param_5843969.png
Accuracy:   0.9585
Precision:  0.9416
Recall:     0.8806
F1-score:   0.9101

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2279, Test Loss: 0.1522, F1: 0.8773, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0037, Test Loss: 0.4508, F1: 0.8965, AUC: 0.9863
Epoch [20/30] Train Loss: 0.0016, Test Loss: 0.5345, F1: 0.8982, AUC: 0.9857
Mejores resultados en la época:  22
f1-score 0.9113875598086124
AUC según el mejor F1-score 0.9878919578057282
Confusion Matrix:
 [[15937   528]
 [  398  4762]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 3/confusion_matrix_param_5843969.png
Accuracy:   0.9572
Precision:  0.9002
Recall:     0.9229
F1-score:   0.9114

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2253, Test Loss: 0.1742, F1: 0.8669, AUC: 0.9825
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.3756, F1: 0.9016, AUC: 0.9867
Epoch [20/30] Train Loss: 0.0039, Test Loss: 0.4881, F1: 0.8908, AUC: 0.9863
Mejores resultados en la época:  16
f1-score 0.9053349758362551
AUC según el mejor F1-score 0.9874183551202104
Confusion Matrix:
 [[15849   616]
 [  383  4777]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 3/confusion_matrix_param_5843969.png
Accuracy:   0.9538
Precision:  0.8858
Recall:     0.9258
F1-score:   0.9053
Tiempo total para red 6: 364.89 segundos
Saved on: outputs_ablation_remove_one_feature/1/Similar Song 3

==============================
Model: Logistic Regression
Accuracy:  0.9344
Precision: 0.8241
Recall:    0.9217
F1-score:  0.8702
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.82      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.94     21625

[[15450  1015]
 [  404  4756]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 3/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 3/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7668
Precision: 0.5063
Recall:    0.9178
F1-score:  0.6526
              precision    recall  f1-score   support

           0       0.97      0.72      0.82     16465
           1       0.51      0.92      0.65      5160

    accuracy                           0.77     21625
   macro avg       0.74      0.82      0.74     21625
weighted avg       0.86      0.77      0.78     21625

[[11846  4619]
 [  424  4736]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 3/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 3/svm_model.pkl

Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 3/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 3/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7434
Precision: 0.4812
Recall:    0.9663
F1-score:  0.6425
              precision    recall  f1-score   support

           0       0.98      0.67      0.80     16465
           1       0.48      0.97      0.64      5160

    accuracy                           0.74     21625
   macro avg       0.73      0.82      0.72     21625
weighted avg       0.86      0.74      0.76     21625

[[11090  5375]
 [  174  4986]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Artist 3/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Artist 3/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9232, 'precision': 0.7986, 'recall': 0.907, 'f1_score': 0.8494}
XGBoost: {'accuracy': 0.9134, 'precision': 0.7856, 'recall': 0.8764, 'f1_score': 0.8285}
Random Forest: {'accuracy': 0.8641, 'precision': 0.6896, 'recall': 0.7828, 'f1_score': 0.7332}
Decision Tree: {'accuracy': 0.8599, 'precision': 0.6773, 'recall': 0.789, 'f1_score': 0.7289}
Naive Bayes: {'accuracy': 0.7434, 'precision': 0.4812, 'recall': 0.9663, 'f1_score': 0.6425}
SVM: {'accuracy': 0.7638, 'precision': 0.503, 'recall': 0.8519, 'f1_score': 0.6326}

##################################################
Running experiment without SIMILAR SONG 3 feature
[Similar Song 3] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2980, Test Loss: 0.1883, F1: 0.8623, AUC: 0.9787
Epoch [10/30] Train Loss: 0.0933, Test Loss: 0.1975, F1: 0.8628, AUC: 0.9820
Epoch [20/30] Train Loss: 0.0688, Test Loss: 0.2358, F1: 0.8621, AUC: 0.9811
Mejores resultados en la época:  2
f1-score 0.881508584294962
AUC según el mejor F1-score 0.9834928095066583
Confusion Matrix:
 [[15664   801]
 [  462  4698]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 3/confusion_matrix_param_160801.png
Accuracy:   0.9416
Precision:  0.8543
Recall:     0.9105
F1-score:   0.8815

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3023, Test Loss: 0.1772, F1: 0.8666, AUC: 0.9782
Epoch [10/30] Train Loss: 0.0942, Test Loss: 0.1821, F1: 0.8700, AUC: 0.9819
Epoch [20/30] Train Loss: 0.0716, Test Loss: 0.2462, F1: 0.8562, AUC: 0.9809
Mejores resultados en la época:  2
f1-score 0.8800741427247452
AUC según el mejor F1-score 0.9834797267871477
Confusion Matrix:
 [[15583   882]
 [  412  4748]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 3/confusion_matrix_param_160801.png
Accuracy:   0.9402
Precision:  0.8433
Recall:     0.9202
F1-score:   0.8801

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3015, Test Loss: 0.2016, F1: 0.8550, AUC: 0.9787
Epoch [10/30] Train Loss: 0.0944, Test Loss: 0.2031, F1: 0.8622, AUC: 0.9820
Epoch [20/30] Train Loss: 0.0803, Test Loss: 0.2327, F1: 0.8629, AUC: 0.9804
Mejores resultados en la época:  6
f1-score 0.8770727188513201
AUC según el mejor F1-score 0.9826051737653514
Confusion Matrix:
 [[15564   901]
 [  426  4734]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 3/confusion_matrix_param_160801.png
Accuracy:   0.9386
Precision:  0.8401
Recall:     0.9174
F1-score:   0.8771
Tiempo total para red 1: 286.54 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2295, Test Loss: 0.1852, F1: 0.8731, AUC: 0.9820
Epoch [10/30] Train Loss: 0.0039, Test Loss: 0.2977, F1: 0.8995, AUC: 0.9866
Epoch [20/30] Train Loss: 0.0007, Test Loss: 0.4773, F1: 0.8896, AUC: 0.9849
Mejores resultados en la época:  28
f1-score 0.9063733784545968
AUC según el mejor F1-score 0.9875432324145416
Confusion Matrix:
 [[15808   657]
 [  339  4821]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 3/confusion_matrix_param_5843969.png
Accuracy:   0.9539
Precision:  0.8801
Recall:     0.9343
F1-score:   0.9064

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2286, Test Loss: 0.1574, F1: 0.8777, AUC: 0.9821
Epoch [10/30] Train Loss: 0.0037, Test Loss: 0.4697, F1: 0.8872, AUC: 0.9863
Epoch [20/30] Train Loss: 0.0006, Test Loss: 0.5309, F1: 0.8897, AUC: 0.9867
Mejores resultados en la época:  8
f1-score 0.906603325415677
AUC según el mejor F1-score 0.9875570978608607
Confusion Matrix:
 [[15871   594]
 [  389  4771]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 3/confusion_matrix_param_5843969.png
Accuracy:   0.9545
Precision:  0.8893
Recall:     0.9246
F1-score:   0.9066

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2272, Test Loss: 0.1436, F1: 0.8846, AUC: 0.9825
Epoch [10/30] Train Loss: 0.0037, Test Loss: 0.2915, F1: 0.9041, AUC: 0.9876
Epoch [20/30] Train Loss: 0.0032, Test Loss: 0.2231, F1: 0.9098, AUC: 0.9880
Mejores resultados en la época:  26
f1-score 0.9140579428349213
AUC según el mejor F1-score 0.9879434235646674
Confusion Matrix:
 [[16040   425]
 [  459  4701]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Similar Song 3/confusion_matrix_param_5843969.png
Accuracy:   0.9591
Precision:  0.9171
Recall:     0.9110
F1-score:   0.9141
Tiempo total para red 6: 365.23 segundos
Saved on: outputs_ablation_remove_one_feature/1/Similar Song 3

==============================
Model: Logistic Regression
Accuracy:  0.9344
Precision: 0.8241
Recall:    0.9217
F1-score:  0.8702
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.82      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.94     21625

[[15450  1015]
 [  404  4756]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 3/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 3/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7668
Precision: 0.5063
Recall:    0.9178
F1-score:  0.6526
              precision    recall  f1-score   support

           0       0.97      0.72      0.82     16465
           1       0.51      0.92      0.65      5160

    accuracy                           0.77     21625
   macro avg       0.74      0.82      0.74     21625
weighted avg       0.86      0.77      0.78     21625

[[11846  4619]
 [  424  4736]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 3/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 3/svm_model.pkl

/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:46:41] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:47:05] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
==============================
Model: Decision Tree
Accuracy:  0.8704
Precision: 0.7120
Recall:    0.7671
F1-score:  0.7385
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14864  1601]
 [ 1202  3958]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 3/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 3/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8667
Precision: 0.6945
Recall:    0.7878
F1-score:  0.7382
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.87      0.87     21625

[[14677  1788]
 [ 1095  4065]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 3/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 3/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9204
Precision: 0.7982
Recall:    0.8917
F1-score:  0.8424
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.93      0.92      0.92     21625

[[15302  1163]
 [  559  4601]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 3/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 3/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7891
Precision: 0.5319
Recall:    0.9705
F1-score:  0.6872
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12057  4408]
 [  152  5008]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 3/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 3/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9344, 'precision': 0.8241, 'recall': 0.9217, 'f1_score': 0.8702}
XGBoost: {'accuracy': 0.9204, 'precision': 0.7982, 'recall': 0.8917, 'f1_score': 0.8424}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.712, 'recall': 0.7671, 'f1_score': 0.7385}
Random Forest: {'accuracy': 0.8667, 'precision': 0.6945, 'recall': 0.7878, 'f1_score': 0.7382}
Naive Bayes: {'accuracy': 0.7891, 'precision': 0.5319, 'recall': 0.9705, 'f1_score': 0.6872}
SVM: {'accuracy': 0.7668, 'precision': 0.5063, 'recall': 0.9178, 'f1_score': 0.6526}

##################################################
Running experiment without SONG_NORMALIZED feature
[song_normalized] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2922, Test Loss: 0.1978, F1: 0.8589, AUC: 0.9804
Epoch [10/30] Train Loss: 0.0807, Test Loss: 0.1782, F1: 0.8795, AUC: 0.9838
Epoch [20/30] Train Loss: 0.0506, Test Loss: 0.2224, F1: 0.8769, AUC: 0.9830
Mejores resultados en la época:  14
f1-score 0.8826836236291586
AUC según el mejor F1-score 0.9835585409030667
Confusion Matrix:
 [[15563   902]
 [  371  4789]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song_normalized/confusion_matrix_param_160801.png
Accuracy:   0.9411
Precision:  0.8415
Recall:     0.9281
F1-score:   0.8827

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2911, Test Loss: 0.1683, F1: 0.8744, AUC: 0.9808
Epoch [10/30] Train Loss: 0.0843, Test Loss: 0.1922, F1: 0.8720, AUC: 0.9842
Epoch [20/30] Train Loss: 0.0607, Test Loss: 0.2468, F1: 0.8621, AUC: 0.9827
Mejores resultados en la época:  3
f1-score 0.8886212008895478
AUC según el mejor F1-score 0.9855344435106651
Confusion Matrix:
 [[15628   837]
 [  365  4795]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song_normalized/confusion_matrix_param_160801.png
Accuracy:   0.9444
Precision:  0.8514
Recall:     0.9293
F1-score:   0.8886

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2855, Test Loss: 0.1837, F1: 0.8668, AUC: 0.9810
Epoch [10/30] Train Loss: 0.0854, Test Loss: 0.2207, F1: 0.8593, AUC: 0.9839
Epoch [20/30] Train Loss: 0.0703, Test Loss: 0.2487, F1: 0.8659, AUC: 0.9824
Mejores resultados en la época:  4
f1-score 0.8896551724137931
AUC según el mejor F1-score 0.9849456799365345
Confusion Matrix:
 [[15668   797]
 [  387  4773]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song_normalized/confusion_matrix_param_160801.png
Accuracy:   0.9452
Precision:  0.8569
Recall:     0.9250
F1-score:   0.8897
Tiempo total para red 1: 281.80 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2194, Test Loss: 0.1660, F1: 0.8804, AUC: 0.9842
Epoch [10/30] Train Loss: 0.0033, Test Loss: 0.4153, F1: 0.9063, AUC: 0.9888
Epoch [20/30] Train Loss: 0.0017, Test Loss: 0.4404, F1: 0.9037, AUC: 0.9886
Mejores resultados en la época:  17
f1-score 0.9213634181469035
AUC según el mejor F1-score 0.9892800678912514
Confusion Matrix:
 [[16008   457]
 [  362  4798]]
==============================
Model: Decision Tree
Accuracy:  0.8704
Precision: 0.7120
Recall:    0.7671
F1-score:  0.7385
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14864  1601]
 [ 1202  3958]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 3/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 3/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8667
Precision: 0.6945
Recall:    0.7878
F1-score:  0.7382
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.87      0.87     21625

[[14677  1788]
 [ 1095  4065]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 3/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 3/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9204
Precision: 0.7982
Recall:    0.8917
F1-score:  0.8424
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.93      0.92      0.92     21625

[[15302  1163]
 [  559  4601]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 3/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 3/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7891
Precision: 0.5319
Recall:    0.9705
F1-score:  0.6872
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12057  4408]
 [  152  5008]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Similar Song 3/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Similar Song 3/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9344, 'precision': 0.8241, 'recall': 0.9217, 'f1_score': 0.8702}
XGBoost: {'accuracy': 0.9204, 'precision': 0.7982, 'recall': 0.8917, 'f1_score': 0.8424}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.712, 'recall': 0.7671, 'f1_score': 0.7385}
Random Forest: {'accuracy': 0.8667, 'precision': 0.6945, 'recall': 0.7878, 'f1_score': 0.7382}
Naive Bayes: {'accuracy': 0.7891, 'precision': 0.5319, 'recall': 0.9705, 'f1_score': 0.6872}
SVM: {'accuracy': 0.7668, 'precision': 0.5063, 'recall': 0.9178, 'f1_score': 0.6526}

##################################################
Running experiment without SONG_NORMALIZED feature
[song_normalized] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2940, Test Loss: 0.1823, F1: 0.8649, AUC: 0.9804
Epoch [10/30] Train Loss: 0.0699, Test Loss: 0.1772, F1: 0.8799, AUC: 0.9851
Epoch [20/30] Train Loss: 0.0264, Test Loss: 0.2247, F1: 0.8840, AUC: 0.9843
Mejores resultados en la época:  1
f1-score 0.8893709327548807
AUC según el mejor F1-score 0.9842739002394085
Confusion Matrix:
 [[15737   728]
 [  445  4715]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song_normalized/confusion_matrix_param_160801.png
Accuracy:   0.9458
Precision:  0.8663
Recall:     0.9138
F1-score:   0.8894

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2849, Test Loss: 0.1708, F1: 0.8704, AUC: 0.9807
Epoch [10/30] Train Loss: 0.0828, Test Loss: 0.2091, F1: 0.8616, AUC: 0.9840
Epoch [20/30] Train Loss: 0.0489, Test Loss: 0.2260, F1: 0.8780, AUC: 0.9831
Mejores resultados en la época:  6
f1-score 0.8897912928510718
AUC según el mejor F1-score 0.984458494292568
Confusion Matrix:
 [[15747   718]
 [  449  4711]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song_normalized/confusion_matrix_param_160801.png
Accuracy:   0.9460
Precision:  0.8677
Recall:     0.9130
F1-score:   0.8898

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2878, Test Loss: 0.1961, F1: 0.8600, AUC: 0.9808
Epoch [10/30] Train Loss: 0.0836, Test Loss: 0.1941, F1: 0.8714, AUC: 0.9839
Epoch [20/30] Train Loss: 0.0569, Test Loss: 0.2269, F1: 0.8751, AUC: 0.9828
Mejores resultados en la época:  2
f1-score 0.8838916934373566
AUC según el mejor F1-score 0.9852075226519962
Confusion Matrix:
 [[15545   920]
 [  345  4815]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song_normalized/confusion_matrix_param_160801.png
Accuracy:   0.9415
Precision:  0.8396
Recall:     0.9331
F1-score:   0.8839
Tiempo total para red 1: 288.03 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2210, Test Loss: 0.1515, F1: 0.8830, AUC: 0.9835
Epoch [10/30] Train Loss: 0.0019, Test Loss: 0.3323, F1: 0.9060, AUC: 0.9887
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6206, F1: 0.9124, AUC: 0.9847
Mejores resultados en la época:  16
f1-score 0.9139784946236559
AUC según el mejor F1-score 0.9888595376144369
Confusion Matrix:
 [[15868   597]
 [  315  4845]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [18:13:04] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [18:13:41] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song_normalized/confusion_matrix_param_5843969.png
Accuracy:   0.9621
Precision:  0.9130
Recall:     0.9298
F1-score:   0.9214

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2194, Test Loss: 0.1545, F1: 0.8889, AUC: 0.9837
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.8200, F1: 0.8682, AUC: 0.9815
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5524, F1: 0.9160, AUC: 0.9855
Mejores resultados en la época:  29
f1-score 0.9175073064957103
AUC según el mejor F1-score 0.9835321871387981
Confusion Matrix:
 [[15884   581]
 [  294  4866]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song_normalized/confusion_matrix_param_5843969.png
Accuracy:   0.9595
Precision:  0.8933
Recall:     0.9430
F1-score:   0.9175

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2191, Test Loss: 0.1447, F1: 0.8794, AUC: 0.9838
Epoch [10/30] Train Loss: 0.0033, Test Loss: 0.2466, F1: 0.9166, AUC: 0.9895
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6686, F1: 0.8993, AUC: 0.9861
Mejores resultados en la época:  10
f1-score 0.9166114529105537
AUC según el mejor F1-score 0.9894872727443933
Confusion Matrix:
 [[15902   563]
 [  318  4842]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song_normalized/confusion_matrix_param_5843969.png
Accuracy:   0.9593
Precision:  0.8958
Recall:     0.9384
F1-score:   0.9166
Tiempo total para red 6: 364.78 segundos
Saved on: outputs_ablation_remove_one_feature/1/song_normalized

==============================
Model: Logistic Regression
Accuracy:  0.9381
Precision: 0.8303
Recall:    0.9308
F1-score:  0.8777
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.88      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.94      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15483   982]
 [  357  4803]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song_normalized/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song_normalized/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7983
Precision: 0.5460
Recall:    0.9176
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.97      0.76      0.85     16465
           1       0.55      0.92      0.68      5160

    accuracy                           0.80     21625
   macro avg       0.76      0.84      0.77     21625
weighted avg       0.87      0.80      0.81     21625

[[12528  3937]
 [  425  4735]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song_normalized/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song_normalized/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8707
Precision: 0.7102
Recall:    0.7738
F1-score:  0.7407
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14836  1629]
 [ 1167  3993]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song_normalized/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song_normalized/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8611
Precision: 0.6799
Recall:    0.7899
F1-score:  0.7308
              precision    recall  f1-score   support

           0       0.93      0.88      0.91     16465
           1       0.68      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14546  1919]
 [ 1084  4076]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song_normalized/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song_normalized/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9221
Precision: 0.8026
Recall:    0.8930
F1-score:  0.8454
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15332  1133]
 [  552  4608]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song_normalized/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song_normalized/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7991
Precision: 0.5443
Recall:    0.9711
F1-score:  0.6976
              precision    recall  f1-score   support

           0       0.99      0.75      0.85     16465
           1       0.54      0.97      0.70      5160

    accuracy                           0.80     21625
   macro avg       0.77      0.86      0.77     21625
weighted avg       0.88      0.80      0.81     21625

[[12270  4195]
 [  149  5011]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song_normalized/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song_normalized/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9381, 'precision': 0.8303, 'recall': 0.9308, 'f1_score': 0.8777}
XGBoost: {'accuracy': 0.9221, 'precision': 0.8026, 'recall': 0.893, 'f1_score': 0.8454}
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song_normalized/confusion_matrix_param_5843969.png
Accuracy:   0.9578
Precision:  0.8903
Recall:     0.9390
F1-score:   0.9140

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2211, Test Loss: 0.1492, F1: 0.8821, AUC: 0.9847
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.4922, F1: 0.8838, AUC: 0.9876
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5210, F1: 0.9080, AUC: 0.9873
Mejores resultados en la época:  15
f1-score 0.9164281625643961
AUC según el mejor F1-score 0.9886520973547365
Confusion Matrix:
 [[15946   519]
 [  357  4803]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song_normalized/confusion_matrix_param_5843969.png
Accuracy:   0.9595
Precision:  0.9025
Recall:     0.9308
F1-score:   0.9164

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2200, Test Loss: 0.1448, F1: 0.8877, AUC: 0.9837
Epoch [10/30] Train Loss: 0.0036, Test Loss: 0.4090, F1: 0.9022, AUC: 0.9874
Epoch [20/30] Train Loss: 0.0006, Test Loss: 0.3894, F1: 0.8806, AUC: 0.9894
Mejores resultados en la época:  13
f1-score 0.9174624184551385
AUC según el mejor F1-score 0.9885840530888871
Confusion Matrix:
 [[15900   565]
 [  308  4852]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/song_normalized/confusion_matrix_param_5843969.png
Accuracy:   0.9596
Precision:  0.8957
Recall:     0.9403
F1-score:   0.9175
Tiempo total para red 6: 364.76 segundos
Saved on: outputs_ablation_remove_one_feature/1/song_normalized

==============================
Model: Logistic Regression
Accuracy:  0.9381
Precision: 0.8303
Recall:    0.9308
F1-score:  0.8777
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.88      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.94      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15483   982]
 [  357  4803]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song_normalized/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song_normalized/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7983
Precision: 0.5460
Recall:    0.9176
F1-score:  0.6846
              precision    recall  f1-score   support

           0       0.97      0.76      0.85     16465
           1       0.55      0.92      0.68      5160

    accuracy                           0.80     21625
   macro avg       0.76      0.84      0.77     21625
weighted avg       0.87      0.80      0.81     21625

[[12528  3937]
 [  425  4735]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song_normalized/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song_normalized/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8707
Precision: 0.7102
Recall:    0.7738
F1-score:  0.7407
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14836  1629]
 [ 1167  3993]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song_normalized/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song_normalized/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8611
Precision: 0.6799
Recall:    0.7899
F1-score:  0.7308
              precision    recall  f1-score   support

           0       0.93      0.88      0.91     16465
           1       0.68      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14546  1919]
 [ 1084  4076]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song_normalized/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song_normalized/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9221
Precision: 0.8026
Recall:    0.8930
F1-score:  0.8454
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15332  1133]
 [  552  4608]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song_normalized/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song_normalized/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7991
Precision: 0.5443
Recall:    0.9711
F1-score:  0.6976
              precision    recall  f1-score   support

           0       0.99      0.75      0.85     16465
           1       0.54      0.97      0.70      5160

    accuracy                           0.80     21625
   macro avg       0.77      0.86      0.77     21625
weighted avg       0.88      0.80      0.81     21625

[[12270  4195]
 [  149  5011]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/song_normalized/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/song_normalized/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9381, 'precision': 0.8303, 'recall': 0.9308, 'f1_score': 0.8777}
XGBoost: {'accuracy': 0.9221, 'precision': 0.8026, 'recall': 0.893, 'f1_score': 0.8454}
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Decision Tree: {'accuracy': 0.8707, 'precision': 0.7102, 'recall': 0.7738, 'f1_score': 0.7407}
Random Forest: {'accuracy': 0.8611, 'precision': 0.6799, 'recall': 0.7899, 'f1_score': 0.7308}
Naive Bayes: {'accuracy': 0.7991, 'precision': 0.5443, 'recall': 0.9711, 'f1_score': 0.6976}
SVM: {'accuracy': 0.7983, 'precision': 0.546, 'recall': 0.9176, 'f1_score': 0.6846}

##################################################
Running experiment without ARTIST_NORMALIZED feature
[artist_normalized] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2928, Test Loss: 0.2031, F1: 0.8535, AUC: 0.9800
Epoch [10/30] Train Loss: 0.0886, Test Loss: 0.1992, F1: 0.8652, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0698, Test Loss: 0.2325, F1: 0.8626, AUC: 0.9811
Mejores resultados en la época:  3
f1-score 0.8851262862488307
AUC según el mejor F1-score 0.9841084800504712
Confusion Matrix:
 [[15666   799]
 [  429  4731]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/artist_normalized/confusion_matrix_param_160801.png
Accuracy:   0.9432
Precision:  0.8555
Recall:     0.9169
F1-score:   0.8851

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2904, Test Loss: 0.1792, F1: 0.8654, AUC: 0.9801
Epoch [10/30] Train Loss: 0.0887, Test Loss: 0.1914, F1: 0.8701, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0760, Test Loss: 0.2414, F1: 0.8599, AUC: 0.9807
Mejores resultados en la época:  2
f1-score 0.8818577111666204
AUC según el mejor F1-score 0.9844497783647248
Confusion Matrix:
 [[15582   883]
 [  394  4766]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/artist_normalized/confusion_matrix_param_160801.png
Accuracy:   0.9409
Precision:  0.8437
Recall:     0.9236
F1-score:   0.8819

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2996, Test Loss: 0.1921, F1: 0.8604, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0742, Test Loss: 0.1917, F1: 0.8681, AUC: 0.9836
Epoch [20/30] Train Loss: 0.0363, Test Loss: 0.2588, F1: 0.8626, AUC: 0.9822
Mejores resultados en la época:  2
f1-score 0.87986743993372
AUC según el mejor F1-score 0.9844195815883823
Confusion Matrix:
 [[15541   924]
 [  381  4779]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/artist_normalized/confusion_matrix_param_160801.png
Accuracy:   0.9397
Precision:  0.8380
Recall:     0.9262
F1-score:   0.8799
Tiempo total para red 1: 276.30 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2237, Test Loss: 0.1505, F1: 0.8784, AUC: 0.9839
Epoch [10/30] Train Loss: 0.0036, Test Loss: 0.4918, F1: 0.8797, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0017, Test Loss: 0.3257, F1: 0.9113, AUC: 0.9887
Mejores resultados en la época:  20
f1-score 0.9112850004713868
AUC según el mejor F1-score 0.9886679343309863
Confusion Matrix:
 [[15851   614]
 [  327  4833]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/artist_normalized/confusion_matrix_param_5843969.png
Accuracy:   0.9565
Precision:  0.8873
Recall:     0.9366
F1-score:   0.9113

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2253, Test Loss: 0.1623, F1: 0.8745, AUC: 0.9834
Epoch [10/30] Train Loss: 0.0035, Test Loss: 0.2887, F1: 0.9071, AUC: 0.9882
Epoch [20/30] Train Loss: 0.0023, Test Loss: 0.4153, F1: 0.9064, AUC: 0.9881
Mejores resultados en la época:  17
f1-score 0.9173772419876507
AUC según el mejor F1-score 0.9888747684187977
Confusion Matrix:
 [[16102   363]
 [  480  4680]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/artist_normalized/confusion_matrix_param_5843969.png
Accuracy:   0.9610
Precision:  0.9280
Recall:     0.9070
F1-score:   0.9174

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2230, Test Loss: 0.1489, F1: 0.8875, AUC: 0.9835
Epoch [10/30] Train Loss: 0.0035, Test Loss: 0.3943, F1: 0.8955, AUC: 0.9882
Epoch [20/30] Train Loss: 0.0027, Test Loss: 0.4443, F1: 0.9080, AUC: 0.9868
Mejores resultados en la época:  11
f1-score 0.9110902255639097
AUC según el mejor F1-score 0.9884519605835257
Confusion Matrix:
 [[15832   633]
 [  313  4847]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/artist_normalized/confusion_matrix_param_5843969.png
Accuracy:   0.9563
Precision:  0.8845
Recall:     0.9393
F1-score:   0.9111
Tiempo total para red 6: 365.89 segundos
Saved on: outputs_ablation_remove_one_feature/1/artist_normalized

==============================
Model: Logistic Regression
Accuracy:  0.9365
Precision: 0.8284
Recall:    0.9254
F1-score:  0.8742
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15476   989]
 [  385  4775]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/artist_normalized/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/artist_normalized/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8018
Precision: 0.5527
Recall:    0.8895
F1-score:  0.6818
              precision    recall  f1-score   support

           0       0.96      0.77      0.86     16465
           1       0.55      0.89      0.68      5160

    accuracy                           0.80     21625
   macro avg       0.75      0.83      0.77     21625
weighted avg       0.86      0.80      0.81     21625

[[12750  3715]
 [  570  4590]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/artist_normalized/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/artist_normalized/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8703
Precision: 0.7091
Recall:    0.7738
F1-score:  0.7401
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14827  1638]
 [ 1167  3993]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/artist_normalized/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/artist_normalized/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8662
Precision: 0.6931
Recall:    0.7886
F1-score:  0.7377
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.87      0.87     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [18:39:28] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Decision Tree: {'accuracy': 0.8707, 'precision': 0.7102, 'recall': 0.7738, 'f1_score': 0.7407}
Random Forest: {'accuracy': 0.8611, 'precision': 0.6799, 'recall': 0.7899, 'f1_score': 0.7308}
Naive Bayes: {'accuracy': 0.7991, 'precision': 0.5443, 'recall': 0.9711, 'f1_score': 0.6976}
SVM: {'accuracy': 0.7983, 'precision': 0.546, 'recall': 0.9176, 'f1_score': 0.6846}

##################################################
Running experiment without ARTIST_NORMALIZED feature
[artist_normalized] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 23)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5023)
X_train_Numeric:  (21625, 5023)
==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2941, Test Loss: 0.1950, F1: 0.8573, AUC: 0.9800
Epoch [10/30] Train Loss: 0.0876, Test Loss: 0.1847, F1: 0.8696, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0587, Test Loss: 0.2710, F1: 0.8458, AUC: 0.9813
Mejores resultados en la época:  4
f1-score 0.8841749577623428
AUC según el mejor F1-score 0.984164889347147
Confusion Matrix:
 [[15681   784]
 [  450  4710]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/artist_normalized/confusion_matrix_param_160801.png
Accuracy:   0.9429
Precision:  0.8573
Recall:     0.9128
F1-score:   0.8842

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2953, Test Loss: 0.1854, F1: 0.8612, AUC: 0.9798
Epoch [10/30] Train Loss: 0.0887, Test Loss: 0.2063, F1: 0.8632, AUC: 0.9827
Epoch [20/30] Train Loss: 0.0762, Test Loss: 0.2353, F1: 0.8615, AUC: 0.9806
Mejores resultados en la época:  1
f1-score 0.8785201546107123
AUC según el mejor F1-score 0.9838196420878678
Confusion Matrix:
 [[15532   933]
 [  387  4773]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/artist_normalized/confusion_matrix_param_160801.png
Accuracy:   0.9390
Precision:  0.8365
Recall:     0.9250
F1-score:   0.8785

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2945, Test Loss: 0.1920, F1: 0.8580, AUC: 0.9803
Epoch [10/30] Train Loss: 0.0789, Test Loss: 0.1718, F1: 0.8795, AUC: 0.9833
Epoch [20/30] Train Loss: 0.0369, Test Loss: 0.2504, F1: 0.8667, AUC: 0.9822
Mejores resultados en la época:  2
f1-score 0.8852242744063324
AUC según el mejor F1-score 0.9845406923777711
Confusion Matrix:
 [[15710   755]
 [  463  4697]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/artist_normalized/confusion_matrix_param_160801.png
Accuracy:   0.9437
Precision:  0.8615
Recall:     0.9103
F1-score:   0.8852
Tiempo total para red 1: 287.45 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2196, Test Loss: 0.1698, F1: 0.8744, AUC: 0.9840
Epoch [10/30] Train Loss: 0.0031, Test Loss: 0.5035, F1: 0.8739, AUC: 0.9875
Epoch [20/30] Train Loss: 0.0015, Test Loss: 0.5836, F1: 0.9024, AUC: 0.9860
Mejores resultados en la época:  14
f1-score 0.9138783269961978
AUC según el mejor F1-score 0.9884436271913408
Confusion Matrix:
 [[15912   553]
 [  353  4807]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/artist_normalized/confusion_matrix_param_5843969.png
Accuracy:   0.9581
Precision:  0.8968
Recall:     0.9316
F1-score:   0.9139

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2262, Test Loss: 0.1758, F1: 0.8664, AUC: 0.9836
Epoch [10/30] Train Loss: 0.0023, Test Loss: 0.2392, F1: 0.9033, AUC: 0.9869
Epoch [20/30] Train Loss: 0.0012, Test Loss: 0.4875, F1: 0.8860, AUC: 0.9876
Mejores resultados en la época:  15
f1-score 0.9088669950738916
AUC según el mejor F1-score 0.985870980727265
Confusion Matrix:
 [[15866   599]
 [  363  4797]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/artist_normalized/confusion_matrix_param_5843969.png
Accuracy:   0.9555
Precision:  0.8890
Recall:     0.9297
F1-score:   0.9089

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2251, Test Loss: 0.1783, F1: 0.8701, AUC: 0.9837
Epoch [10/30] Train Loss: 0.0036, Test Loss: 0.2413, F1: 0.9112, AUC: 0.9881
Epoch [20/30] Train Loss: 0.0044, Test Loss: 0.3870, F1: 0.9028, AUC: 0.9883
Mejores resultados en la época:  23
f1-score 0.9129487543809794
AUC según el mejor F1-score 0.9880433183379356
Confusion Matrix:
 [[15887   578]
 [  341  4819]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/artist_normalized/confusion_matrix_param_5843969.png
Accuracy:   0.9575
Precision:  0.8929
Recall:     0.9339
F1-score:   0.9129
Tiempo total para red 6: 366.89 segundos
Saved on: outputs_ablation_remove_one_feature/1/artist_normalized

==============================
Model: Logistic Regression
Accuracy:  0.9365
Precision: 0.8284
Recall:    0.9254
F1-score:  0.8742
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15476   989]
 [  385  4775]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/artist_normalized/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/artist_normalized/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8018
Precision: 0.5527
Recall:    0.8895
F1-score:  0.6818
              precision    recall  f1-score   support

           0       0.96      0.77      0.86     16465
           1       0.55      0.89      0.68      5160

    accuracy                           0.80     21625
   macro avg       0.75      0.83      0.77     21625
weighted avg       0.86      0.80      0.81     21625

[[12750  3715]
 [  570  4590]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/artist_normalized/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/artist_normalized/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8703
Precision: 0.7091
Recall:    0.7738
F1-score:  0.7401
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14827  1638]
 [ 1167  3993]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/artist_normalized/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/artist_normalized/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8662
Precision: 0.6931
Recall:    0.7886
F1-score:  0.7377
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.87      0.87     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [18:40:16] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[14663  1802]
 [ 1091  4069]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/artist_normalized/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/artist_normalized/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9212
Precision: 0.8014
Recall:    0.8901
F1-score:  0.8434
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15327  1138]
 [  567  4593]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/artist_normalized/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/artist_normalized/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7717
Precision: 0.5113
Recall:    0.9717
F1-score:  0.6701
              precision    recall  f1-score   support

           0       0.99      0.71      0.83     16465
           1       0.51      0.97      0.67      5160

    accuracy                           0.77     21625
   macro avg       0.75      0.84      0.75     21625
weighted avg       0.87      0.77      0.79     21625

[[11673  4792]
 [  146  5014]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/artist_normalized/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/artist_normalized/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9365, 'precision': 0.8284, 'recall': 0.9254, 'f1_score': 0.8742}
XGBoost: {'accuracy': 0.9212, 'precision': 0.8014, 'recall': 0.8901, 'f1_score': 0.8434}
Decision Tree: {'accuracy': 0.8703, 'precision': 0.7091, 'recall': 0.7738, 'f1_score': 0.7401}
Random Forest: {'accuracy': 0.8662, 'precision': 0.6931, 'recall': 0.7886, 'f1_score': 0.7377}
SVM: {'accuracy': 0.8018, 'precision': 0.5527, 'recall': 0.8895, 'f1_score': 0.6818}
Naive Bayes: {'accuracy': 0.7717, 'precision': 0.5113, 'recall': 0.9717, 'f1_score': 0.6701}

##################################################
Running experiment without TEMPO feature
[Tempo] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2953, Test Loss: 0.1871, F1: 0.8637, AUC: 0.9799
Epoch [10/30] Train Loss: 0.0886, Test Loss: 0.1883, F1: 0.8722, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0636, Test Loss: 0.2597, F1: 0.8574, AUC: 0.9815
Mejores resultados en la época:  1
f1-score 0.8823037621922898
AUC según el mejor F1-score 0.9835017373004047
Confusion Matrix:
 [[15609   856]
 [  411  4749]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Tempo/confusion_matrix_param_160769.png
Accuracy:   0.9414
Precision:  0.8473
Recall:     0.9203
F1-score:   0.8823

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2989, Test Loss: 0.1728, F1: 0.8697, AUC: 0.9798
Epoch [10/30] Train Loss: 0.0686, Test Loss: 0.1795, F1: 0.8790, AUC: 0.9834
Epoch [20/30] Train Loss: 0.0248, Test Loss: 0.2780, F1: 0.8626, AUC: 0.9827
Mejores resultados en la época:  11
f1-score 0.882590350394423
AUC según el mejor F1-score 0.9835419388555005
Confusion Matrix:
 [[15534   931]
 [  349  4811]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Tempo/confusion_matrix_param_160769.png
Accuracy:   0.9408
Precision:  0.8379
Recall:     0.9324
F1-score:   0.8826

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2937, Test Loss: 0.1815, F1: 0.8659, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0885, Test Loss: 0.1966, F1: 0.8692, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0698, Test Loss: 0.2297, F1: 0.8670, AUC: 0.9810
Mejores resultados en la época:  6
f1-score 0.8783546988840727
AUC según el mejor F1-score 0.9834286259083751
Confusion Matrix:
 [[15544   921]
 [  398  4762]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Tempo/confusion_matrix_param_160769.png
Accuracy:   0.9390
Precision:  0.8379
Recall:     0.9229
F1-score:   0.8784
Tiempo total para red 1: 273.35 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2229, Test Loss: 0.1595, F1: 0.8805, AUC: 0.9827
Epoch [10/30] Train Loss: 0.0027, Test Loss: 0.6136, F1: 0.8910, AUC: 0.9844
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5456, F1: 0.9025, AUC: 0.9865
Mejores resultados en la época:  5
f1-score 0.9146282509910084
AUC según el mejor F1-score 0.9872069070638447
Confusion Matrix:
 [[16012   453]
 [  430  4730]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Tempo/confusion_matrix_param_5842945.png
Accuracy:   0.9592
Precision:  0.9126
Recall:     0.9167
F1-score:   0.9146

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2246, Test Loss: 0.1897, F1: 0.8625, AUC: 0.9832
Epoch [10/30] Train Loss: 0.0021, Test Loss: 0.2966, F1: 0.9092, AUC: 0.9874
Epoch [20/30] Train Loss: 0.0026, Test Loss: 0.3290, F1: 0.8891, AUC: 0.9883
Mejores resultados en la época:  21
f1-score 0.9144917908323053
AUC según el mejor F1-score 0.9873681899825094
Confusion Matrix:
 [[15906   559]
 [  342  4818]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Tempo/confusion_matrix_param_5842945.png
Accuracy:   0.9583
Precision:  0.8960
Recall:     0.9337
F1-score:   0.9145

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2224, Test Loss: 0.1523, F1: 0.8880, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0022, Test Loss: 0.2521, F1: 0.9102, AUC: 0.9885
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7812, F1: 0.9034, AUC: 0.9805
Mejores resultados en la época:  10
f1-score 0.9101580990248982
AUC según el mejor F1-score 0.9884766488463901
Confusion Matrix:
 [[15869   596]
 [  353  4807]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Tempo/confusion_matrix_param_5842945.png
Accuracy:   0.9561
Precision:  0.8897
Recall:     0.9316
F1-score:   0.9102

[[14663  1802]
 [ 1091  4069]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/artist_normalized/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/artist_normalized/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9212
Precision: 0.8014
Recall:    0.8901
F1-score:  0.8434
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15327  1138]
 [  567  4593]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/artist_normalized/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/artist_normalized/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7717
Precision: 0.5113
Recall:    0.9717
F1-score:  0.6701
              precision    recall  f1-score   support

           0       0.99      0.71      0.83     16465
           1       0.51      0.97      0.67      5160

    accuracy                           0.77     21625
   macro avg       0.75      0.84      0.75     21625
weighted avg       0.87      0.77      0.79     21625

[[11673  4792]
 [  146  5014]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/artist_normalized/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/artist_normalized/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9365, 'precision': 0.8284, 'recall': 0.9254, 'f1_score': 0.8742}
XGBoost: {'accuracy': 0.9212, 'precision': 0.8014, 'recall': 0.8901, 'f1_score': 0.8434}
Decision Tree: {'accuracy': 0.8703, 'precision': 0.7091, 'recall': 0.7738, 'f1_score': 0.7401}
Random Forest: {'accuracy': 0.8662, 'precision': 0.6931, 'recall': 0.7886, 'f1_score': 0.7377}
SVM: {'accuracy': 0.8018, 'precision': 0.5527, 'recall': 0.8895, 'f1_score': 0.6818}
Naive Bayes: {'accuracy': 0.7717, 'precision': 0.5113, 'recall': 0.9717, 'f1_score': 0.6701}

##################################################
Running experiment without TEMPO feature
[Tempo] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2949, Test Loss: 0.1789, F1: 0.8676, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0851, Test Loss: 0.1790, F1: 0.8747, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0464, Test Loss: 0.2315, F1: 0.8730, AUC: 0.9812
Mejores resultados en la época:  1
f1-score 0.8803615902592011
AUC según el mejor F1-score 0.9835708408957691
Confusion Matrix:
 [[15556   909]
 [  388  4772]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Tempo/confusion_matrix_param_160769.png
Accuracy:   0.9400
Precision:  0.8400
Recall:     0.9248
F1-score:   0.8804

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2938, Test Loss: 0.1929, F1: 0.8627, AUC: 0.9800
Epoch [10/30] Train Loss: 0.0879, Test Loss: 0.1912, F1: 0.8721, AUC: 0.9821
Epoch [20/30] Train Loss: 0.0737, Test Loss: 0.2561, F1: 0.8571, AUC: 0.9809
Mejores resultados en la época:  3
f1-score 0.8871166809076236
AUC según el mejor F1-score 0.9839035821816067
Confusion Matrix:
 [[15764   701]
 [  488  4672]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Tempo/confusion_matrix_param_160769.png
Accuracy:   0.9450
Precision:  0.8695
Recall:     0.9054
F1-score:   0.8871

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2919, Test Loss: 0.1916, F1: 0.8569, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0869, Test Loss: 0.1904, F1: 0.8691, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0533, Test Loss: 0.2376, F1: 0.8676, AUC: 0.9812
Mejores resultados en la época:  6
f1-score 0.8804891606448026
AUC según el mejor F1-score 0.9836476305152815
Confusion Matrix:
 [[15583   882]
 [  408  4752]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Tempo/confusion_matrix_param_160769.png
Accuracy:   0.9403
Precision:  0.8435
Recall:     0.9209
F1-score:   0.8805
Tiempo total para red 1: 286.31 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2226, Test Loss: 0.1876, F1: 0.8642, AUC: 0.9833
Epoch [10/30] Train Loss: 0.0030, Test Loss: 0.4995, F1: 0.8825, AUC: 0.9869
Epoch [20/30] Train Loss: 0.0018, Test Loss: 0.3070, F1: 0.9102, AUC: 0.9884
Mejores resultados en la época:  22
f1-score 0.9193705070915096
AUC según el mejor F1-score 0.9885773145761387
Confusion Matrix:
 [[16063   402]
 [  428  4732]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Tempo/confusion_matrix_param_5842945.png
Accuracy:   0.9616
Precision:  0.9217
Recall:     0.9171
F1-score:   0.9194

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2241, Test Loss: 0.1506, F1: 0.8871, AUC: 0.9832
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.4565, F1: 0.8816, AUC: 0.9877
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5472, F1: 0.9021, AUC: 0.9869
Mejores resultados en la época:  15
f1-score 0.9111111111111111
AUC según el mejor F1-score 0.9883830570837365
Confusion Matrix:
 [[15892   573]
 [  363  4797]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Tempo/confusion_matrix_param_5842945.png
Accuracy:   0.9567
Precision:  0.8933
Recall:     0.9297
F1-score:   0.9111

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2260, Test Loss: 0.1871, F1: 0.8712, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0022, Test Loss: 0.4723, F1: 0.8850, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.9374, F1: 0.8960, AUC: 0.9828
Mejores resultados en la época:  13
f1-score 0.9066641601654291
AUC según el mejor F1-score 0.9874466804144096
Confusion Matrix:
 [[15809   656]
 [  337  4823]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Tempo/confusion_matrix_param_5842945.png
Accuracy:   0.9541
Precision:  0.8803
Recall:     0.9347
F1-score:   0.9067
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [19:05:55] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [19:06:51] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Tiempo total para red 6: 364.29 segundos
Saved on: outputs_ablation_remove_one_feature/1/Tempo

==============================
Model: Logistic Regression
Accuracy:  0.9365
Precision: 0.8275
Recall:    0.9269
F1-score:  0.8744
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15468   997]
 [  377  4783]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Tempo/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Tempo/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7918
Precision: 0.5392
Recall:    0.8754
F1-score:  0.6674
              precision    recall  f1-score   support

           0       0.95      0.77      0.85     16465
           1       0.54      0.88      0.67      5160

    accuracy                           0.79     21625
   macro avg       0.75      0.82      0.76     21625
weighted avg       0.85      0.79      0.81     21625

[[12605  3860]
 [  643  4517]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Tempo/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Tempo/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8697
Precision: 0.7083
Recall:    0.7717
F1-score:  0.7386
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14825  1640]
 [ 1178  3982]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Tempo/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Tempo/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8674
Precision: 0.6952
Recall:    0.7909
F1-score:  0.7400
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.70      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14676  1789]
 [ 1079  4081]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Tempo/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Tempo/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9214
Precision: 0.8025
Recall:    0.8897
F1-score:  0.8439
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15335  1130]
 [  569  4591]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Tempo/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Tempo/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Tempo/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Tempo/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9365, 'precision': 0.8275, 'recall': 0.9269, 'f1_score': 0.8744}
XGBoost: {'accuracy': 0.9214, 'precision': 0.8025, 'recall': 0.8897, 'f1_score': 0.8439}
Random Forest: {'accuracy': 0.8674, 'precision': 0.6952, 'recall': 0.7909, 'f1_score': 0.74}
Decision Tree: {'accuracy': 0.8697, 'precision': 0.7083, 'recall': 0.7717, 'f1_score': 0.7386}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7918, 'precision': 0.5392, 'recall': 0.8754, 'f1_score': 0.6674}

##################################################
Running experiment without LENGTH feature
[Length] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2972, Test Loss: 0.1887, F1: 0.8608, AUC: 0.9792
Epoch [10/30] Train Loss: 0.0789, Test Loss: 0.2002, F1: 0.8657, AUC: 0.9830
Epoch [20/30] Train Loss: 0.0449, Test Loss: 0.2578, F1: 0.8609, AUC: 0.9816
Mejores resultados en la época:  3
f1-score 0.8859060402684564
AUC según el mejor F1-score 0.9840894945114962
Confusion Matrix:
 [[15732   733]
 [  474  4686]]
Tiempo total para red 6: 366.33 segundos
Saved on: outputs_ablation_remove_one_feature/1/Tempo

==============================
Model: Logistic Regression
Accuracy:  0.9365
Precision: 0.8275
Recall:    0.9269
F1-score:  0.8744
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15468   997]
 [  377  4783]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Tempo/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Tempo/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7918
Precision: 0.5392
Recall:    0.8754
F1-score:  0.6674
              precision    recall  f1-score   support

           0       0.95      0.77      0.85     16465
           1       0.54      0.88      0.67      5160

    accuracy                           0.79     21625
   macro avg       0.75      0.82      0.76     21625
weighted avg       0.85      0.79      0.81     21625

[[12605  3860]
 [  643  4517]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Tempo/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Tempo/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8697
Precision: 0.7083
Recall:    0.7717
F1-score:  0.7386
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14825  1640]
 [ 1178  3982]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Tempo/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Tempo/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8674
Precision: 0.6952
Recall:    0.7909
F1-score:  0.7400
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.70      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14676  1789]
 [ 1079  4081]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Tempo/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Tempo/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9214
Precision: 0.8025
Recall:    0.8897
F1-score:  0.8439
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15335  1130]
 [  569  4591]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Tempo/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Tempo/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Tempo/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Tempo/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9365, 'precision': 0.8275, 'recall': 0.9269, 'f1_score': 0.8744}
XGBoost: {'accuracy': 0.9214, 'precision': 0.8025, 'recall': 0.8897, 'f1_score': 0.8439}
Random Forest: {'accuracy': 0.8674, 'precision': 0.6952, 'recall': 0.7909, 'f1_score': 0.74}
Decision Tree: {'accuracy': 0.8697, 'precision': 0.7083, 'recall': 0.7717, 'f1_score': 0.7386}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7918, 'precision': 0.5392, 'recall': 0.8754, 'f1_score': 0.6674}

##################################################
Running experiment without LENGTH feature
[Length] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2939, Test Loss: 0.1978, F1: 0.8577, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0884, Test Loss: 0.1778, F1: 0.8792, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0686, Test Loss: 0.2272, F1: 0.8709, AUC: 0.9807
Mejores resultados en la época:  7
f1-score 0.8824294205052006
AUC según el mejor F1-score 0.9830588316301669
Confusion Matrix:
 [[15608   857]
 [  409  4751]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [19:32:30] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [19:33:11] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Length/confusion_matrix_param_160769.png
Accuracy:   0.9442
Precision:  0.8647
Recall:     0.9081
F1-score:   0.8859

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2949, Test Loss: 0.1740, F1: 0.8717, AUC: 0.9799
Epoch [10/30] Train Loss: 0.0783, Test Loss: 0.1860, F1: 0.8722, AUC: 0.9829
Epoch [20/30] Train Loss: 0.0399, Test Loss: 0.2428, F1: 0.8760, AUC: 0.9818
Mejores resultados en la época:  8
f1-score 0.8769739258171135
AUC según el mejor F1-score 0.9830064301301562
Confusion Matrix:
 [[15509   956]
 [  384  4776]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Length/confusion_matrix_param_160769.png
Accuracy:   0.9380
Precision:  0.8332
Recall:     0.9256
F1-score:   0.8770

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2954, Test Loss: 0.1736, F1: 0.8699, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0861, Test Loss: 0.1987, F1: 0.8639, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0507, Test Loss: 0.2337, F1: 0.8692, AUC: 0.9813
Mejores resultados en la época:  3
f1-score 0.8859411489957963
AUC según el mejor F1-score 0.9843435099588743
Confusion Matrix:
 [[15662   803]
 [  418  4742]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Length/confusion_matrix_param_160769.png
Accuracy:   0.9435
Precision:  0.8552
Recall:     0.9190
F1-score:   0.8859
Tiempo total para red 1: 271.28 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2227, Test Loss: 0.1670, F1: 0.8614, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0028, Test Loss: 0.2213, F1: 0.9147, AUC: 0.9884
Epoch [20/30] Train Loss: 0.0007, Test Loss: 0.6352, F1: 0.8691, AUC: 0.9861
Mejores resultados en la época:  10
f1-score 0.9147212958551691
AUC según el mejor F1-score 0.9883753416337685
Confusion Matrix:
 [[15930   535]
 [  360  4800]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Length/confusion_matrix_param_5842945.png
Accuracy:   0.9586
Precision:  0.8997
Recall:     0.9302
F1-score:   0.9147

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2260, Test Loss: 0.1734, F1: 0.8710, AUC: 0.9835
Epoch [10/30] Train Loss: 0.0041, Test Loss: 0.3440, F1: 0.9123, AUC: 0.9873
Epoch [20/30] Train Loss: 0.0010, Test Loss: 0.4141, F1: 0.9000, AUC: 0.9882
Mejores resultados en la época:  29
f1-score 0.9139937553221686
AUC según el mejor F1-score 0.9882552666332389
Confusion Matrix:
 [[15886   579]
 [  330  4830]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Length/confusion_matrix_param_5842945.png
Accuracy:   0.9580
Precision:  0.8930
Recall:     0.9360
F1-score:   0.9140

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2247, Test Loss: 0.1960, F1: 0.8674, AUC: 0.9835
Epoch [10/30] Train Loss: 0.0077, Test Loss: 0.4333, F1: 0.8945, AUC: 0.9870
Epoch [20/30] Train Loss: 0.0012, Test Loss: 0.5444, F1: 0.9061, AUC: 0.9855
Mejores resultados en la época:  7
f1-score 0.9123379740758522
AUC según el mejor F1-score 0.9881970976725354
Confusion Matrix:
 [[15961   504]
 [  409  4751]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Length/confusion_matrix_param_5842945.png
Accuracy:   0.9578
Precision:  0.9041
Recall:     0.9207
F1-score:   0.9123
Tiempo total para red 6: 365.22 segundos
Saved on: outputs_ablation_remove_one_feature/1/Length

==============================
Model: Logistic Regression
Accuracy:  0.9361
Precision: 0.8266
Recall:    0.9266
F1-score:  0.8737
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15462  1003]
 [  379  4781]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Length/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Length/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8702
Precision: 0.7261
Recall:    0.7326
F1-score:  0.7293
              precision    recall  f1-score   support

           0       0.92      0.91      0.91     16465
           1       0.73      0.73      0.73      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.82      0.82     21625
weighted avg       0.87      0.87      0.87     21625

[[15039  1426]
 [ 1380  3780]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Length/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Length/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8697
Precision: 0.7082
Recall:    0.7723
F1-score:  0.7389
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14823  1642]
 [ 1175  3985]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Length/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Length/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8690
Precision: 0.7008
Recall:    0.7870
F1-score:  0.7414
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.70      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14731  1734]
 [ 1099  4061]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Length/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Length/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9230
Precision: 0.8060
Recall:    0.8921
F1-score:  0.8468
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.81      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.89      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15357  1108]
 [  557  4603]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Length/confusion_matrix_param_160769.png
Accuracy:   0.9415
Precision:  0.8472
Recall:     0.9207
F1-score:   0.8824

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2936, Test Loss: 0.2041, F1: 0.8549, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0886, Test Loss: 0.2142, F1: 0.8603, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0714, Test Loss: 0.2469, F1: 0.8597, AUC: 0.9805
Mejores resultados en la época:  7
f1-score 0.8802068901819525
AUC según el mejor F1-score 0.9833718988128448
Confusion Matrix:
 [[15563   902]
 [  395  4765]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Length/confusion_matrix_param_160769.png
Accuracy:   0.9400
Precision:  0.8408
Recall:     0.9234
F1-score:   0.8802

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2966, Test Loss: 0.1750, F1: 0.8680, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0875, Test Loss: 0.1938, F1: 0.8671, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0649, Test Loss: 0.2415, F1: 0.8631, AUC: 0.9809
Mejores resultados en la época:  3
f1-score 0.8805776708017033
AUC según el mejor F1-score 0.9842186797458552
Confusion Matrix:
 [[15579   886]
 [  404  4756]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Length/confusion_matrix_param_160769.png
Accuracy:   0.9403
Precision:  0.8430
Recall:     0.9217
F1-score:   0.8806
Tiempo total para red 1: 287.26 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2239, Test Loss: 0.1569, F1: 0.8795, AUC: 0.9835
Epoch [10/30] Train Loss: 0.0037, Test Loss: 0.3700, F1: 0.9069, AUC: 0.9882
Epoch [20/30] Train Loss: 0.0023, Test Loss: 0.4129, F1: 0.9019, AUC: 0.9874
Mejores resultados en la época:  8
f1-score 0.9158752997601919
AUC según el mejor F1-score 0.9887016504353844
Confusion Matrix:
 [[15974   491]
 [  386  4774]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Length/confusion_matrix_param_5842945.png
Accuracy:   0.9594
Precision:  0.9067
Recall:     0.9252
F1-score:   0.9159

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2276, Test Loss: 0.1935, F1: 0.8422, AUC: 0.9836
Epoch [10/30] Train Loss: 0.0027, Test Loss: 0.5779, F1: 0.8835, AUC: 0.9857
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5968, F1: 0.9019, AUC: 0.9872
Mejores resultados en la época:  11
f1-score 0.9137440758293839
AUC según el mejor F1-score 0.9888515514469264
Confusion Matrix:
 [[15895   570]
 [  340  4820]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Length/confusion_matrix_param_5842945.png
Accuracy:   0.9579
Precision:  0.8942
Recall:     0.9341
F1-score:   0.9137

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2243, Test Loss: 0.1638, F1: 0.8805, AUC: 0.9834
Epoch [10/30] Train Loss: 0.0052, Test Loss: 0.2623, F1: 0.8795, AUC: 0.9888
Epoch [20/30] Train Loss: 0.0011, Test Loss: 0.5839, F1: 0.8795, AUC: 0.9860
Mejores resultados en la época:  29
f1-score 0.904540379605508
AUC según el mejor F1-score 0.9814161528918519
Confusion Matrix:
 [[15738   727]
 [  299  4861]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Length/confusion_matrix_param_5842945.png
Accuracy:   0.9526
Precision:  0.8699
Recall:     0.9421
F1-score:   0.9045
Tiempo total para red 6: 367.69 segundos
Saved on: outputs_ablation_remove_one_feature/1/Length

==============================
Model: Logistic Regression
Accuracy:  0.9361
Precision: 0.8266
Recall:    0.9266
F1-score:  0.8737
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15462  1003]
 [  379  4781]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Length/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Length/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8702
Precision: 0.7261
Recall:    0.7326
F1-score:  0.7293
              precision    recall  f1-score   support

           0       0.92      0.91      0.91     16465
           1       0.73      0.73      0.73      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.82      0.82     21625
weighted avg       0.87      0.87      0.87     21625

[[15039  1426]
 [ 1380  3780]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Length/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Length/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8697
Precision: 0.7082
Recall:    0.7723
F1-score:  0.7389
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14823  1642]
 [ 1175  3985]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Length/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Length/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8690
Precision: 0.7008
Recall:    0.7870
F1-score:  0.7414
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.70      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14731  1734]
 [ 1099  4061]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Length/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Length/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9230
Precision: 0.8060
Recall:    0.8921
F1-score:  0.8468
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.81      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.89      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15357  1108]
 [  557  4603]]
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Length/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Length/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Length/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Length/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8266, 'recall': 0.9266, 'f1_score': 0.8737}
XGBoost: {'accuracy': 0.923, 'precision': 0.806, 'recall': 0.8921, 'f1_score': 0.8468}
Random Forest: {'accuracy': 0.869, 'precision': 0.7008, 'recall': 0.787, 'f1_score': 0.7414}
Decision Tree: {'accuracy': 0.8697, 'precision': 0.7082, 'recall': 0.7723, 'f1_score': 0.7389}
SVM: {'accuracy': 0.8702, 'precision': 0.7261, 'recall': 0.7326, 'f1_score': 0.7293}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}

##################################################
Running experiment without LOUDNESS (DB) feature
[Loudness (db)] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2937, Test Loss: 0.1901, F1: 0.8634, AUC: 0.9800
Epoch [10/30] Train Loss: 0.0876, Test Loss: 0.1974, F1: 0.8663, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0606, Test Loss: 0.2390, F1: 0.8646, AUC: 0.9809
Mejores resultados en la época:  3
f1-score 0.8765049252097774
AUC según el mejor F1-score 0.9842548558487937
Confusion Matrix:
 [[15466   999]
 [  355  4805]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Loudness (db)/confusion_matrix_param_160769.png
Accuracy:   0.9374
Precision:  0.8279
Recall:     0.9312
F1-score:   0.8765

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2943, Test Loss: 0.1955, F1: 0.8596, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0884, Test Loss: 0.2068, F1: 0.8637, AUC: 0.9822
Epoch [20/30] Train Loss: 0.0737, Test Loss: 0.2606, F1: 0.8564, AUC: 0.9805
Mejores resultados en la época:  6
f1-score 0.882264434831672
AUC según el mejor F1-score 0.9833877769852424
Confusion Matrix:
 [[15682   783]
 [  469  4691]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Loudness (db)/confusion_matrix_param_160769.png
Accuracy:   0.9421
Precision:  0.8570
Recall:     0.9091
F1-score:   0.8823

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2974, Test Loss: 0.1745, F1: 0.8689, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0830, Test Loss: 0.1940, F1: 0.8714, AUC: 0.9827
Epoch [20/30] Train Loss: 0.0558, Test Loss: 0.2489, F1: 0.8640, AUC: 0.9810
Mejores resultados en la época:  6
f1-score 0.8794510385756676
AUC según el mejor F1-score 0.9837203417161607
Confusion Matrix:
 [[15583   882]
 [  418  4742]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Loudness (db)/confusion_matrix_param_160769.png
Accuracy:   0.9399
Precision:  0.8432
Recall:     0.9190
F1-score:   0.8795
Tiempo total para red 1: 277.24 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2228, Test Loss: 0.1614, F1: 0.8764, AUC: 0.9832
Epoch [10/30] Train Loss: 0.0052, Test Loss: 0.2885, F1: 0.8968, AUC: 0.9882
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8499, F1: 0.8928, AUC: 0.9824
Mejores resultados en la época:  8
f1-score 0.9043155452436195
AUC según el mejor F1-score 0.9880952784506482
Confusion Matrix:
 [[15722   743]
 [  288  4872]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Loudness (db)/confusion_matrix_param_5842945.png
Accuracy:   0.9523
Precision:  0.8677
Recall:     0.9442
F1-score:   0.9043

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2213, Test Loss: 0.1790, F1: 0.8787, AUC: 0.9832
Epoch [10/30] Train Loss: 0.0027, Test Loss: 0.5088, F1: 0.8913, AUC: 0.9866
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5651, F1: 0.9038, AUC: 0.9860
Mejores resultados en la época:  4
f1-score 0.9089345437171524
AUC según el mejor F1-score 0.9869517381243276
Confusion Matrix:
 [[15916   549]
 [  404  4756]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Loudness (db)/confusion_matrix_param_5842945.png
Accuracy:   0.9559
Precision:  0.8965
Recall:     0.9217
F1-score:   0.9089

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2264, Test Loss: 0.1674, F1: 0.8741, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0053, Test Loss: 0.3820, F1: 0.8760, AUC: 0.9875
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6035, F1: 0.8932, AUC: 0.9851
Mejores resultados en la época:  12
f1-score 0.9076102218477956
AUC según el mejor F1-score 0.9864350854643513
Confusion Matrix:
 [[15790   675]
 [  312  4848]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Loudness (db)/confusion_matrix_param_5842945.png
Accuracy:   0.9544
Precision:  0.8778
Recall:     0.9395
F1-score:   0.9076
Tiempo total para red 6: 367.08 segundos
Saved on: outputs_ablation_remove_one_feature/1/Loudness (db)

==============================
Model: Logistic Regression
Accuracy:  0.9364
Precision: 0.8271
Recall:    0.9271
F1-score:  0.8743
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15465  1000]
 [  376  4784]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Loudness (db)/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Loudness (db)/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7542
Precision: 0.4918
Recall:    0.9112
F1-score:  0.6389
              precision    recall  f1-score   support

           0       0.96      0.70      0.81     16465
           1       0.49      0.91      0.64      5160

    accuracy                           0.75     21625
   macro avg       0.73      0.81      0.73     21625
weighted avg       0.85      0.75      0.77     21625

[[11607  4858]
 [  458  4702]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Loudness (db)/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Loudness (db)/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8718
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Length/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Length/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Length/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Length/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8266, 'recall': 0.9266, 'f1_score': 0.8737}
XGBoost: {'accuracy': 0.923, 'precision': 0.806, 'recall': 0.8921, 'f1_score': 0.8468}
Random Forest: {'accuracy': 0.869, 'precision': 0.7008, 'recall': 0.787, 'f1_score': 0.7414}
Decision Tree: {'accuracy': 0.8697, 'precision': 0.7082, 'recall': 0.7723, 'f1_score': 0.7389}
SVM: {'accuracy': 0.8702, 'precision': 0.7261, 'recall': 0.7326, 'f1_score': 0.7293}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}

##################################################
Running experiment without LOUDNESS (DB) feature
[Loudness (db)] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2970, Test Loss: 0.1809, F1: 0.8653, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0807, Test Loss: 0.2084, F1: 0.8631, AUC: 0.9830
Epoch [20/30] Train Loss: 0.0503, Test Loss: 0.2490, F1: 0.8676, AUC: 0.9819
Mejores resultados en la época:  5
f1-score 0.8820370370370371
AUC según el mejor F1-score 0.9839460554099959
Confusion Matrix:
 [[15588   877]
 [  397  4763]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Loudness (db)/confusion_matrix_param_160769.png
Accuracy:   0.9411
Precision:  0.8445
Recall:     0.9231
F1-score:   0.8820

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2965, Test Loss: 0.1717, F1: 0.8693, AUC: 0.9792
Epoch [10/30] Train Loss: 0.0874, Test Loss: 0.2085, F1: 0.8594, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0634, Test Loss: 0.2487, F1: 0.8587, AUC: 0.9811
Mejores resultados en la época:  4
f1-score 0.8805763369354391
AUC según el mejor F1-score 0.9841120994263144
Confusion Matrix:
 [[15565   900]
 [  393  4767]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Loudness (db)/confusion_matrix_param_160769.png
Accuracy:   0.9402
Precision:  0.8412
Recall:     0.9238
F1-score:   0.8806

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2939, Test Loss: 0.1867, F1: 0.8629, AUC: 0.9799
Epoch [10/30] Train Loss: 0.0858, Test Loss: 0.1808, F1: 0.8754, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0515, Test Loss: 0.2273, F1: 0.8705, AUC: 0.9812
Mejores resultados en la época:  1
f1-score 0.8805486053192475
AUC según el mejor F1-score 0.983306496985619
Confusion Matrix:
 [[15585   880]
 [  409  4751]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Loudness (db)/confusion_matrix_param_160769.png
Accuracy:   0.9404
Precision:  0.8437
Recall:     0.9207
F1-score:   0.8805
Tiempo total para red 1: 287.64 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2261, Test Loss: 0.1743, F1: 0.8777, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0024, Test Loss: 0.3860, F1: 0.8966, AUC: 0.9874
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8053, F1: 0.9073, AUC: 0.9820
Mejores resultados en la época:  17
f1-score 0.9091418956814358
AUC según el mejor F1-score 0.986122536176103
Confusion Matrix:
 [[15790   675]
 [  297  4863]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Loudness (db)/confusion_matrix_param_5842945.png
Accuracy:   0.9551
Precision:  0.8781
Recall:     0.9424
F1-score:   0.9091

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2196, Test Loss: 0.1746, F1: 0.8703, AUC: 0.9833
Epoch [10/30] Train Loss: 0.0047, Test Loss: 0.3743, F1: 0.9008, AUC: 0.9878
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.9106, F1: 0.8985, AUC: 0.9803
Mejores resultados en la época:  8
f1-score 0.9081535230093565
AUC según el mejor F1-score 0.9870312113786113
Confusion Matrix:
 [[15907   558]
 [  404  4756]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Loudness (db)/confusion_matrix_param_5842945.png
Accuracy:   0.9555
Precision:  0.8950
Recall:     0.9217
F1-score:   0.9082

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2239, Test Loss: 0.1556, F1: 0.8804, AUC: 0.9825
Epoch [10/30] Train Loss: 0.0041, Test Loss: 0.5964, F1: 0.8961, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0027, Test Loss: 0.3852, F1: 0.8767, AUC: 0.9876
Mejores resultados en la época:  22
f1-score 0.9119955156950673
AUC según el mejor F1-score 0.9882782423133872
Confusion Matrix:
 [[15802   663]
 [  279  4881]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Loudness (db)/confusion_matrix_param_5842945.png
Accuracy:   0.9564
Precision:  0.8804
Recall:     0.9459
F1-score:   0.9120
Tiempo total para red 6: 368.57 segundos
Saved on: outputs_ablation_remove_one_feature/1/Loudness (db)

==============================
Model: Logistic Regression
Accuracy:  0.9364
Precision: 0.8271
Recall:    0.9271
F1-score:  0.8743
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15465  1000]
 [  376  4784]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Loudness (db)/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Loudness (db)/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7542
Precision: 0.4918
Recall:    0.9112
F1-score:  0.6389
              precision    recall  f1-score   support

           0       0.96      0.70      0.81     16465
           1       0.49      0.91      0.64      5160

    accuracy                           0.75     21625
   macro avg       0.73      0.81      0.73     21625
weighted avg       0.85      0.75      0.77     21625

[[11607  4858]
 [  458  4702]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Loudness (db)/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Loudness (db)/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8718
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [19:59:14] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [19:59:52] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Precision: 0.7151
Recall:    0.7692
F1-score:  0.7412
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.72      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14884  1581]
 [ 1191  3969]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Loudness (db)/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Loudness (db)/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8723
Precision: 0.7094
Recall:    0.7876
F1-score:  0.7464
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.79      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14800  1665]
 [ 1096  4064]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Loudness (db)/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Loudness (db)/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9222
Precision: 0.8031
Recall:    0.8926
F1-score:  0.8455
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15336  1129]
 [  554  4606]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Loudness (db)/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Loudness (db)/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7881
Precision: 0.5306
Recall:    0.9698
F1-score:  0.6859
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12039  4426]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Loudness (db)/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Loudness (db)/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9364, 'precision': 0.8271, 'recall': 0.9271, 'f1_score': 0.8743}
XGBoost: {'accuracy': 0.9222, 'precision': 0.8031, 'recall': 0.8926, 'f1_score': 0.8455}
Random Forest: {'accuracy': 0.8723, 'precision': 0.7094, 'recall': 0.7876, 'f1_score': 0.7464}
Decision Tree: {'accuracy': 0.8718, 'precision': 0.7151, 'recall': 0.7692, 'f1_score': 0.7412}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.7542, 'precision': 0.4918, 'recall': 0.9112, 'f1_score': 0.6389}

##################################################
Running experiment without POPULARITY feature
[Popularity] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2938, Test Loss: 0.1940, F1: 0.8593, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0894, Test Loss: 0.1776, F1: 0.8795, AUC: 0.9820
Epoch [20/30] Train Loss: 0.0639, Test Loss: 0.2340, F1: 0.8672, AUC: 0.9806
Mejores resultados en la época:  3
f1-score 0.8800811134666789
AUC según el mejor F1-score 0.9838883866882299
Confusion Matrix:
 [[15550   915]
 [  386  4774]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Popularity/confusion_matrix_param_160769.png
Accuracy:   0.9398
Precision:  0.8392
Recall:     0.9252
F1-score:   0.8801

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2911, Test Loss: 0.1948, F1: 0.8593, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0898, Test Loss: 0.2078, F1: 0.8603, AUC: 0.9822
Epoch [20/30] Train Loss: 0.0663, Test Loss: 0.2505, F1: 0.8603, AUC: 0.9807
Mejores resultados en la época:  3
f1-score 0.8816485658590921
AUC según el mejor F1-score 0.9842276722764051
Confusion Matrix:
 [[15601   864]
 [  411  4749]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Popularity/confusion_matrix_param_160769.png
Accuracy:   0.9410
Precision:  0.8461
Recall:     0.9203
F1-score:   0.8816

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2959, Test Loss: 0.1992, F1: 0.8589, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0901, Test Loss: 0.1771, F1: 0.8785, AUC: 0.9819
Epoch [20/30] Train Loss: 0.0764, Test Loss: 0.2394, F1: 0.8651, AUC: 0.9806
Mejores resultados en la época:  10
f1-score 0.8784680056048575
AUC según el mejor F1-score 0.9819249429727611
Confusion Matrix:
 [[15622   843]
 [  458  4702]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Popularity/confusion_matrix_param_160769.png
Accuracy:   0.9398
Precision:  0.8480
Recall:     0.9112
F1-score:   0.8785
Tiempo total para red 1: 277.12 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2256, Test Loss: 0.1539, F1: 0.8775, AUC: 0.9820
Epoch [10/30] Train Loss: 0.0027, Test Loss: 0.5256, F1: 0.9008, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6161, F1: 0.8978, AUC: 0.9849
Mejores resultados en la época:  17
f1-score 0.911672820221528
AUC según el mejor F1-score 0.9865655360089642
Confusion Matrix:
 [[15877   588]
 [  345  4815]]
Precision: 0.7151
Recall:    0.7692
F1-score:  0.7412
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.72      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14884  1581]
 [ 1191  3969]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Loudness (db)/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Loudness (db)/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8723
Precision: 0.7094
Recall:    0.7876
F1-score:  0.7464
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.79      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14800  1665]
 [ 1096  4064]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Loudness (db)/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Loudness (db)/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9222
Precision: 0.8031
Recall:    0.8926
F1-score:  0.8455
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15336  1129]
 [  554  4606]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Loudness (db)/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Loudness (db)/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7881
Precision: 0.5306
Recall:    0.9698
F1-score:  0.6859
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12039  4426]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Loudness (db)/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Loudness (db)/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9364, 'precision': 0.8271, 'recall': 0.9271, 'f1_score': 0.8743}
XGBoost: {'accuracy': 0.9222, 'precision': 0.8031, 'recall': 0.8926, 'f1_score': 0.8455}
Random Forest: {'accuracy': 0.8723, 'precision': 0.7094, 'recall': 0.7876, 'f1_score': 0.7464}
Decision Tree: {'accuracy': 0.8718, 'precision': 0.7151, 'recall': 0.7692, 'f1_score': 0.7412}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.7542, 'precision': 0.4918, 'recall': 0.9112, 'f1_score': 0.6389}

##################################################
Running experiment without POPULARITY feature
[Popularity] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2931, Test Loss: 0.1720, F1: 0.8713, AUC: 0.9804
Epoch [10/30] Train Loss: 0.0884, Test Loss: 0.1975, F1: 0.8665, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0630, Test Loss: 0.2246, F1: 0.8715, AUC: 0.9809
Mejores resultados en la época:  3
f1-score 0.8830447263635521
AUC según el mejor F1-score 0.9838688597141694
Confusion Matrix:
 [[15594   871]
 [  392  4768]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Popularity/confusion_matrix_param_160769.png
Accuracy:   0.9416
Precision:  0.8455
Recall:     0.9240
F1-score:   0.8830

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2945, Test Loss: 0.1863, F1: 0.8629, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0871, Test Loss: 0.1978, F1: 0.8685, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0544, Test Loss: 0.2391, F1: 0.8695, AUC: 0.9812
Mejores resultados en la época:  3
f1-score 0.8759456749612615
AUC según el mejor F1-score 0.9839888582075675
Confusion Matrix:
 [[15459  1006]
 [  355  4805]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Popularity/confusion_matrix_param_160769.png
Accuracy:   0.9371
Precision:  0.8269
Recall:     0.9312
F1-score:   0.8759

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2934, Test Loss: 0.1735, F1: 0.8695, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0789, Test Loss: 0.1807, F1: 0.8780, AUC: 0.9830
Epoch [20/30] Train Loss: 0.0406, Test Loss: 0.2521, F1: 0.8638, AUC: 0.9822
Mejores resultados en la época:  16
f1-score 0.8807867149086186
AUC según el mejor F1-score 0.9823378637325594
Confusion Matrix:
 [[15593   872]
 [  413  4747]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Popularity/confusion_matrix_param_160769.png
Accuracy:   0.9406
Precision:  0.8448
Recall:     0.9200
F1-score:   0.8808
Tiempo total para red 1: 287.02 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2212, Test Loss: 0.1499, F1: 0.8858, AUC: 0.9833
Epoch [10/30] Train Loss: 0.0028, Test Loss: 0.4046, F1: 0.8905, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0012, Test Loss: 0.6382, F1: 0.9009, AUC: 0.9849
Mejores resultados en la época:  19
f1-score 0.9140580535002846
AUC según el mejor F1-score 0.9868035320400097
Confusion Matrix:
 [[15901   564]
 [  342  4818]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [20:25:37] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [20:26:25] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Popularity/confusion_matrix_param_5842945.png
Accuracy:   0.9569
Precision:  0.8912
Recall:     0.9331
F1-score:   0.9117

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2244, Test Loss: 0.1870, F1: 0.8572, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0031, Test Loss: 0.2196, F1: 0.9075, AUC: 0.9880
Epoch [20/30] Train Loss: 0.0031, Test Loss: 0.3888, F1: 0.8904, AUC: 0.9879
Mejores resultados en la época:  10
f1-score 0.907459338194055
AUC según el mejor F1-score 0.9879858850227285
Confusion Matrix:
 [[15781   684]
 [  306  4854]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Popularity/confusion_matrix_param_5842945.png
Accuracy:   0.9542
Precision:  0.8765
Recall:     0.9407
F1-score:   0.9075

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2282, Test Loss: 0.1469, F1: 0.8830, AUC: 0.9825
Epoch [10/30] Train Loss: 0.0029, Test Loss: 0.6515, F1: 0.8730, AUC: 0.9848
Epoch [20/30] Train Loss: 0.0033, Test Loss: 0.7476, F1: 0.8880, AUC: 0.9811
Mejores resultados en la época:  17
f1-score 0.9138946968256986
AUC según el mejor F1-score 0.9872782764473383
Confusion Matrix:
 [[15911   554]
 [  352  4808]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Popularity/confusion_matrix_param_5842945.png
Accuracy:   0.9581
Precision:  0.8967
Recall:     0.9318
F1-score:   0.9139
Tiempo total para red 6: 366.57 segundos
Saved on: outputs_ablation_remove_one_feature/1/Popularity

==============================
Model: Logistic Regression
Accuracy:  0.9359
Precision: 0.8268
Recall:    0.9250
F1-score:  0.8731
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15465  1000]
 [  387  4773]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Popularity/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Popularity/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7464
Precision: 0.4838
Recall:    0.9353
F1-score:  0.6377
              precision    recall  f1-score   support

           0       0.97      0.69      0.80     16465
           1       0.48      0.94      0.64      5160

    accuracy                           0.75     21625
   macro avg       0.73      0.81      0.72     21625
weighted avg       0.85      0.75      0.77     21625

[[11315  5150]
 [  334  4826]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Popularity/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Popularity/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8706
Precision: 0.7105
Recall:    0.7725
F1-score:  0.7402
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14841  1624]
 [ 1174  3986]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Popularity/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Popularity/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8680
Precision: 0.6966
Recall:    0.7919
F1-score:  0.7412
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.70      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14685  1780]
 [ 1074  4086]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Popularity/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Popularity/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9214
Precision: 0.8017
Recall:    0.8911
F1-score:  0.8441
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15328  1137]
 [  562  4598]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Popularity/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Popularity/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Popularity/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Popularity/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9359, 'precision': 0.8268, 'recall': 0.925, 'f1_score': 0.8731}
XGBoost: {'accuracy': 0.9214, 'precision': 0.8017, 'recall': 0.8911, 'f1_score': 0.8441}
Random Forest: {'accuracy': 0.868, 'precision': 0.6966, 'recall': 0.7919, 'f1_score': 0.7412}
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Popularity/confusion_matrix_param_5842945.png
Accuracy:   0.9581
Precision:  0.8952
Recall:     0.9337
F1-score:   0.9141

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2245, Test Loss: 0.1748, F1: 0.8631, AUC: 0.9823
Epoch [10/30] Train Loss: 0.0039, Test Loss: 0.3479, F1: 0.8828, AUC: 0.9878
Epoch [20/30] Train Loss: 0.0026, Test Loss: 0.4547, F1: 0.8872, AUC: 0.9879
Mejores resultados en la época:  7
f1-score 0.9086753642510237
AUC según el mejor F1-score 0.9882180665117691
Confusion Matrix:
 [[15895   570]
 [  389  4771]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Popularity/confusion_matrix_param_5842945.png
Accuracy:   0.9557
Precision:  0.8933
Recall:     0.9246
F1-score:   0.9087

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2275, Test Loss: 0.1595, F1: 0.8754, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0048, Test Loss: 0.2392, F1: 0.8948, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0027, Test Loss: 0.2569, F1: 0.9049, AUC: 0.9884
Mejores resultados en la época:  26
f1-score 0.9130804880355623
AUC según el mejor F1-score 0.988639803247198
Confusion Matrix:
 [[15879   586]
 [  333  4827]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Popularity/confusion_matrix_param_5842945.png
Accuracy:   0.9575
Precision:  0.8917
Recall:     0.9355
F1-score:   0.9131
Tiempo total para red 6: 367.41 segundos
Saved on: outputs_ablation_remove_one_feature/1/Popularity

==============================
Model: Logistic Regression
Accuracy:  0.9359
Precision: 0.8268
Recall:    0.9250
F1-score:  0.8731
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15465  1000]
 [  387  4773]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Popularity/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Popularity/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7464
Precision: 0.4838
Recall:    0.9353
F1-score:  0.6377
              precision    recall  f1-score   support

           0       0.97      0.69      0.80     16465
           1       0.48      0.94      0.64      5160

    accuracy                           0.75     21625
   macro avg       0.73      0.81      0.72     21625
weighted avg       0.85      0.75      0.77     21625

[[11315  5150]
 [  334  4826]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Popularity/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Popularity/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8706
Precision: 0.7105
Recall:    0.7725
F1-score:  0.7402
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14841  1624]
 [ 1174  3986]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Popularity/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Popularity/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8680
Precision: 0.6966
Recall:    0.7919
F1-score:  0.7412
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.70      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14685  1780]
 [ 1074  4086]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Popularity/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Popularity/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9214
Precision: 0.8017
Recall:    0.8911
F1-score:  0.8441
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15328  1137]
 [  562  4598]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Popularity/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Popularity/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Popularity/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Popularity/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9359, 'precision': 0.8268, 'recall': 0.925, 'f1_score': 0.8731}
XGBoost: {'accuracy': 0.9214, 'precision': 0.8017, 'recall': 0.8911, 'f1_score': 0.8441}
Random Forest: {'accuracy': 0.868, 'precision': 0.6966, 'recall': 0.7919, 'f1_score': 0.7412}
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Decision Tree: {'accuracy': 0.8706, 'precision': 0.7105, 'recall': 0.7725, 'f1_score': 0.7402}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7464, 'precision': 0.4838, 'recall': 0.9353, 'f1_score': 0.6377}

##################################################
Running experiment without ENERGY feature
[Energy] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2993, Test Loss: 0.1928, F1: 0.8594, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0876, Test Loss: 0.1835, F1: 0.8760, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0673, Test Loss: 0.2465, F1: 0.8644, AUC: 0.9809
Mejores resultados en la época:  2
f1-score 0.8862841478400605
AUC según el mejor F1-score 0.9840474920962248
Confusion Matrix:
 [[15734   731]
 [  472  4688]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Energy/confusion_matrix_param_160769.png
Accuracy:   0.9444
Precision:  0.8651
Recall:     0.9085
F1-score:   0.8863

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2972, Test Loss: 0.1828, F1: 0.8660, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0864, Test Loss: 0.1878, F1: 0.8737, AUC: 0.9821
Epoch [20/30] Train Loss: 0.0654, Test Loss: 0.2619, F1: 0.8563, AUC: 0.9810
Mejores resultados en la época:  1
f1-score 0.8816288755205923
AUC según el mejor F1-score 0.9837671640807255
Confusion Matrix:
 [[15583   882]
 [  397  4763]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Energy/confusion_matrix_param_160769.png
Accuracy:   0.9409
Precision:  0.8438
Recall:     0.9231
F1-score:   0.8816

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2973, Test Loss: 0.1896, F1: 0.8617, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0851, Test Loss: 0.1815, F1: 0.8738, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0513, Test Loss: 0.2380, F1: 0.8654, AUC: 0.9812
Mejores resultados en la época:  1
f1-score 0.8823419027960218
AUC según el mejor F1-score 0.9833567974820915
Confusion Matrix:
 [[15669   796]
 [  458  4702]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Energy/confusion_matrix_param_160769.png
Accuracy:   0.9420
Precision:  0.8552
Recall:     0.9112
F1-score:   0.8823
Tiempo total para red 1: 274.43 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2256, Test Loss: 0.1792, F1: 0.8618, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0029, Test Loss: 0.3414, F1: 0.8979, AUC: 0.9873
Epoch [20/30] Train Loss: 0.0018, Test Loss: 0.4741, F1: 0.9097, AUC: 0.9872
Mejores resultados en la época:  29
f1-score 0.9133383571966842
AUC según el mejor F1-score 0.9857252522969796
Confusion Matrix:
 [[15857   608]
 [  312  4848]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Energy/confusion_matrix_param_5842945.png
Accuracy:   0.9575
Precision:  0.8886
Recall:     0.9395
F1-score:   0.9133

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2246, Test Loss: 0.1882, F1: 0.8781, AUC: 0.9835
Epoch [10/30] Train Loss: 0.0016, Test Loss: 0.4287, F1: 0.8908, AUC: 0.9867
Epoch [20/30] Train Loss: 0.0016, Test Loss: 0.4523, F1: 0.8994, AUC: 0.9881
Mejores resultados en la época:  25
f1-score 0.9096532333645736
AUC según el mejor F1-score 0.9859880484090049
Confusion Matrix:
 [[15808   657]
 [  307  4853]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Energy/confusion_matrix_param_5842945.png
Accuracy:   0.9554
Precision:  0.8808
Recall:     0.9405
F1-score:   0.9097

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2227, Test Loss: 0.1586, F1: 0.8721, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.3788, F1: 0.9023, AUC: 0.9879
Epoch [20/30] Train Loss: 0.0036, Test Loss: 0.3307, F1: 0.9027, AUC: 0.9881
Mejores resultados en la época:  13
f1-score 0.9129357972947229
AUC según el mejor F1-score 0.9876380070951537
Confusion Matrix:
 [[15919   546]
 [  368  4792]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Energy/confusion_matrix_param_5842945.png
Accuracy:   0.9577
Precision:  0.8977
Recall:     0.9287
F1-score:   0.9129
Tiempo total para red 6: 365.77 segundos
Saved on: outputs_ablation_remove_one_feature/1/Energy

==============================
Model: Logistic Regression
Accuracy:  0.9362
Precision: 0.8274
Recall:    0.9260
F1-score:  0.8739
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15468   997]
 [  382  4778]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Energy/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Energy/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8418
Precision: 0.6288
Recall:    0.8221
F1-score:  0.7126
              precision    recall  f1-score   support

           0       0.94      0.85      0.89     16465
           1       0.63      0.82      0.71      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.84      0.80     21625
weighted avg       0.86      0.84      0.85     21625

[[13961  2504]
 [  918  4242]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Energy/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Energy/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8711
Precision: 0.7122
Recall:    0.7715
F1-score:  0.7407
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14856  1609]
 [ 1179  3981]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Energy/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Energy/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8741
Precision: 0.7129
Recall:    0.7907
F1-score:  0.7498
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.71      0.79      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.87      0.88     21625

[[14822  1643]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [20:52:14] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Decision Tree: {'accuracy': 0.8706, 'precision': 0.7105, 'recall': 0.7725, 'f1_score': 0.7402}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7464, 'precision': 0.4838, 'recall': 0.9353, 'f1_score': 0.6377}

##################################################
Running experiment without ENERGY feature
[Energy] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2949, Test Loss: 0.1945, F1: 0.8582, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0858, Test Loss: 0.1949, F1: 0.8668, AUC: 0.9827
Epoch [20/30] Train Loss: 0.0491, Test Loss: 0.2417, F1: 0.8665, AUC: 0.9814
Mejores resultados en la época:  7
f1-score 0.8794642857142857
AUC según el mejor F1-score 0.9830648639232387
Confusion Matrix:
 [[15601   864]
 [  432  4728]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Energy/confusion_matrix_param_160769.png
Accuracy:   0.9401
Precision:  0.8455
Recall:     0.9163
F1-score:   0.8795

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2937, Test Loss: 0.2076, F1: 0.8537, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0882, Test Loss: 0.1879, F1: 0.8733, AUC: 0.9822
Epoch [20/30] Train Loss: 0.0733, Test Loss: 0.2280, F1: 0.8703, AUC: 0.9806
Mejores resultados en la época:  2
f1-score 0.8790904048807543
AUC según el mejor F1-score 0.9839088905995099
Confusion Matrix:
 [[15562   903]
 [  405  4755]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Energy/confusion_matrix_param_160769.png
Accuracy:   0.9395
Precision:  0.8404
Recall:     0.9215
F1-score:   0.8791

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2950, Test Loss: 0.1929, F1: 0.8577, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0785, Test Loss: 0.1966, F1: 0.8697, AUC: 0.9828
Epoch [20/30] Train Loss: 0.0443, Test Loss: 0.2523, F1: 0.8662, AUC: 0.9813
Mejores resultados en la época:  2
f1-score 0.8825490922563912
AUC según el mejor F1-score 0.9841596574363755
Confusion Matrix:
 [[15593   872]
 [  396  4764]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Energy/confusion_matrix_param_160769.png
Accuracy:   0.9414
Precision:  0.8453
Recall:     0.9233
F1-score:   0.8825
Tiempo total para red 1: 286.95 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2262, Test Loss: 0.1721, F1: 0.8767, AUC: 0.9835
Epoch [10/30] Train Loss: 0.0031, Test Loss: 0.2453, F1: 0.8896, AUC: 0.9882
Epoch [20/30] Train Loss: 0.0021, Test Loss: 0.5045, F1: 0.9124, AUC: 0.9865
Mejores resultados en la época:  20
f1-score 0.9124493450193196
AUC según el mejor F1-score 0.9864605623391879
Confusion Matrix:
 [[15855   610]
 [  319  4841]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Energy/confusion_matrix_param_5842945.png
Accuracy:   0.9570
Precision:  0.8881
Recall:     0.9382
F1-score:   0.9124

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2226, Test Loss: 0.1608, F1: 0.8771, AUC: 0.9833
Epoch [10/30] Train Loss: 0.0018, Test Loss: 0.5772, F1: 0.8846, AUC: 0.9855
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6479, F1: 0.8976, AUC: 0.9852
Mejores resultados en la época:  9
f1-score 0.9087656529516994
AUC según el mejor F1-score 0.9881294065165244
Confusion Matrix:
 [[15830   635]
 [  334  4826]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Energy/confusion_matrix_param_5842945.png
Accuracy:   0.9552
Precision:  0.8837
Recall:     0.9353
F1-score:   0.9088

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2274, Test Loss: 0.1427, F1: 0.8870, AUC: 0.9841
Epoch [10/30] Train Loss: 0.0028, Test Loss: 0.3283, F1: 0.8922, AUC: 0.9871
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.4949, F1: 0.9021, AUC: 0.9865
Mejores resultados en la época:  5
f1-score 0.9079071522928854
AUC según el mejor F1-score 0.9878188169878789
Confusion Matrix:
 [[15838   627]
 [  349  4811]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Energy/confusion_matrix_param_5842945.png
Accuracy:   0.9549
Precision:  0.8847
Recall:     0.9324
F1-score:   0.9079
Tiempo total para red 6: 367.05 segundos
Saved on: outputs_ablation_remove_one_feature/1/Energy

==============================
Model: Logistic Regression
Accuracy:  0.9362
Precision: 0.8274
Recall:    0.9260
F1-score:  0.8739
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15468   997]
 [  382  4778]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Energy/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Energy/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8418
Precision: 0.6288
Recall:    0.8221
F1-score:  0.7126
              precision    recall  f1-score   support

           0       0.94      0.85      0.89     16465
           1       0.63      0.82      0.71      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.84      0.80     21625
weighted avg       0.86      0.84      0.85     21625

[[13961  2504]
 [  918  4242]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Energy/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Energy/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8711
Precision: 0.7122
Recall:    0.7715
F1-score:  0.7407
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14856  1609]
 [ 1179  3981]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Energy/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Energy/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8741
Precision: 0.7129
Recall:    0.7907
F1-score:  0.7498
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.71      0.79      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.87      0.88     21625

[[14822  1643]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [20:52:58] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
 [ 1080  4080]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Energy/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Energy/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9217
Precision: 0.8030
Recall:    0.8903
F1-score:  0.8444
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15338  1127]
 [  566  4594]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Energy/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Energy/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7881
Precision: 0.5306
Recall:    0.9698
F1-score:  0.6859
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12039  4426]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Energy/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Energy/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9362, 'precision': 0.8274, 'recall': 0.926, 'f1_score': 0.8739}
XGBoost: {'accuracy': 0.9217, 'precision': 0.803, 'recall': 0.8903, 'f1_score': 0.8444}
Random Forest: {'accuracy': 0.8741, 'precision': 0.7129, 'recall': 0.7907, 'f1_score': 0.7498}
Decision Tree: {'accuracy': 0.8711, 'precision': 0.7122, 'recall': 0.7715, 'f1_score': 0.7407}
SVM: {'accuracy': 0.8418, 'precision': 0.6288, 'recall': 0.8221, 'f1_score': 0.7126}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}

##################################################
Running experiment without DANCEABILITY feature
[Danceability] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2956, Test Loss: 0.1813, F1: 0.8676, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0798, Test Loss: 0.1794, F1: 0.8756, AUC: 0.9831
Epoch [20/30] Train Loss: 0.0456, Test Loss: 0.2388, F1: 0.8684, AUC: 0.9815
Mejores resultados en la época:  2
f1-score 0.8861142640938328
AUC según el mejor F1-score 0.9839153642798797
Confusion Matrix:
 [[15737   728]
 [  476  4684]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Danceability/confusion_matrix_param_160769.png
Accuracy:   0.9443
Precision:  0.8655
Recall:     0.9078
F1-score:   0.8861

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3002, Test Loss: 0.1827, F1: 0.8682, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0837, Test Loss: 0.1991, F1: 0.8654, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0480, Test Loss: 0.2544, F1: 0.8593, AUC: 0.9813
Mejores resultados en la época:  8
f1-score 0.8790494755407036
AUC según el mejor F1-score 0.9826493007248168
Confusion Matrix:
 [[15587   878]
 [  425  4735]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Danceability/confusion_matrix_param_160769.png
Accuracy:   0.9397
Precision:  0.8436
Recall:     0.9176
F1-score:   0.8790

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3030, Test Loss: 0.1847, F1: 0.8652, AUC: 0.9791
Epoch [10/30] Train Loss: 0.0894, Test Loss: 0.1995, F1: 0.8687, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0765, Test Loss: 0.2509, F1: 0.8584, AUC: 0.9805
Mejores resultados en la época:  4
f1-score 0.8792786161207213
AUC según el mejor F1-score 0.9841066556496395
Confusion Matrix:
 [[15535   930]
 [  382  4778]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Danceability/confusion_matrix_param_160769.png
Accuracy:   0.9393
Precision:  0.8371
Recall:     0.9260
F1-score:   0.8793
Tiempo total para red 1: 275.34 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2246, Test Loss: 0.1704, F1: 0.8667, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0031, Test Loss: 0.3139, F1: 0.9101, AUC: 0.9877
Epoch [20/30] Train Loss: 0.0024, Test Loss: 0.3460, F1: 0.9054, AUC: 0.9879
Mejores resultados en la época:  8
f1-score 0.9110731892301858
AUC según el mejor F1-score 0.9874528127552691
Confusion Matrix:
 [[15882   583]
 [  355  4805]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Danceability/confusion_matrix_param_5842945.png
Accuracy:   0.9566
Precision:  0.8918
Recall:     0.9312
F1-score:   0.9111

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2209, Test Loss: 0.1902, F1: 0.8666, AUC: 0.9837
Epoch [10/30] Train Loss: 0.0049, Test Loss: 0.2886, F1: 0.8943, AUC: 0.9880
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6374, F1: 0.9006, AUC: 0.9847
Mejores resultados en la época:  7
f1-score 0.9031721076482012
AUC según el mejor F1-score 0.9852203228836363
Confusion Matrix:
 [[15695   770]
 [  277  4883]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Danceability/confusion_matrix_param_5842945.png
Accuracy:   0.9516
Precision:  0.8638
Recall:     0.9463
F1-score:   0.9032

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2235, Test Loss: 0.1642, F1: 0.8611, AUC: 0.9833
Epoch [10/30] Train Loss: 0.0058, Test Loss: 0.4167, F1: 0.9020, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0005, Test Loss: 0.7062, F1: 0.9002, AUC: 0.9832
Mejores resultados en la época:  4
f1-score 0.9094382881161636
AUC según el mejor F1-score 0.9873279590015936
Confusion Matrix:
 [[15917   548]
 [  400  4760]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Danceability/confusion_matrix_param_5842945.png
Accuracy:   0.9562
Precision:  0.8968
Recall:     0.9225
F1-score:   0.9094
Tiempo total para red 6: 367.05 segundos
 [ 1080  4080]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Energy/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Energy/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9217
Precision: 0.8030
Recall:    0.8903
F1-score:  0.8444
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15338  1127]
 [  566  4594]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Energy/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Energy/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7881
Precision: 0.5306
Recall:    0.9698
F1-score:  0.6859
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12039  4426]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Energy/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Energy/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9362, 'precision': 0.8274, 'recall': 0.926, 'f1_score': 0.8739}
XGBoost: {'accuracy': 0.9217, 'precision': 0.803, 'recall': 0.8903, 'f1_score': 0.8444}
Random Forest: {'accuracy': 0.8741, 'precision': 0.7129, 'recall': 0.7907, 'f1_score': 0.7498}
Decision Tree: {'accuracy': 0.8711, 'precision': 0.7122, 'recall': 0.7715, 'f1_score': 0.7407}
SVM: {'accuracy': 0.8418, 'precision': 0.6288, 'recall': 0.8221, 'f1_score': 0.7126}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}

##################################################
Running experiment without DANCEABILITY feature
[Danceability] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2926, Test Loss: 0.1907, F1: 0.8595, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0880, Test Loss: 0.2024, F1: 0.8654, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0674, Test Loss: 0.2455, F1: 0.8613, AUC: 0.9807
Mejores resultados en la época:  2
f1-score 0.8857036485480269
AUC según el mejor F1-score 0.9843042029487026
Confusion Matrix:
 [[15639   826]
 [  402  4758]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Danceability/confusion_matrix_param_160769.png
Accuracy:   0.9432
Precision:  0.8521
Recall:     0.9221
F1-score:   0.8857

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2925, Test Loss: 0.1858, F1: 0.8639, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0881, Test Loss: 0.1922, F1: 0.8704, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0644, Test Loss: 0.2362, F1: 0.8647, AUC: 0.9808
Mejores resultados en la época:  11
f1-score 0.8770469053566472
AUC según el mejor F1-score 0.9817430207840451
Confusion Matrix:
 [[15556   909]
 [  420  4740]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Danceability/confusion_matrix_param_160769.png
Accuracy:   0.9385
Precision:  0.8391
Recall:     0.9186
F1-score:   0.8770

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2954, Test Loss: 0.1696, F1: 0.8722, AUC: 0.9792
Epoch [10/30] Train Loss: 0.0887, Test Loss: 0.2257, F1: 0.8548, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0749, Test Loss: 0.2401, F1: 0.8674, AUC: 0.9806
Mejores resultados en la época:  4
f1-score 0.8848089468779125
AUC según el mejor F1-score 0.9836786335590884
Confusion Matrix:
 [[15642   823]
 [  413  4747]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Danceability/confusion_matrix_param_160769.png
Accuracy:   0.9428
Precision:  0.8522
Recall:     0.9200
F1-score:   0.8848
Tiempo total para red 1: 287.07 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2255, Test Loss: 0.1504, F1: 0.8828, AUC: 0.9833
Epoch [10/30] Train Loss: 0.0047, Test Loss: 0.2926, F1: 0.9133, AUC: 0.9882
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.4868, F1: 0.9086, AUC: 0.9868
Mejores resultados en la época:  10
f1-score 0.9132585124595777
AUC según el mejor F1-score 0.9882053898685726
Confusion Matrix:
 [[15912   553]
 [  359  4801]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Danceability/confusion_matrix_param_5842945.png
Accuracy:   0.9578
Precision:  0.8967
Recall:     0.9304
F1-score:   0.9133

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2252, Test Loss: 0.1726, F1: 0.8710, AUC: 0.9833
Epoch [10/30] Train Loss: 0.0033, Test Loss: 0.6715, F1: 0.8762, AUC: 0.9840
Epoch [20/30] Train Loss: 0.0008, Test Loss: 0.2433, F1: 0.9044, AUC: 0.9884
Mejores resultados en la época:  13
f1-score 0.91003591003591
AUC según el mejor F1-score 0.9873683724225927
Confusion Matrix:
 [[15858   607]
 [  345  4815]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Danceability/confusion_matrix_param_5842945.png
Accuracy:   0.9560
Precision:  0.8880
Recall:     0.9331
F1-score:   0.9100

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2247, Test Loss: 0.1486, F1: 0.8837, AUC: 0.9838
Epoch [10/30] Train Loss: 0.0028, Test Loss: 0.3901, F1: 0.8800, AUC: 0.9870
Epoch [20/30] Train Loss: 0.0019, Test Loss: 0.3250, F1: 0.8990, AUC: 0.9881
Mejores resultados en la época:  18
f1-score 0.9084579922852574
AUC según el mejor F1-score 0.9881334731648295
Confusion Matrix:
 [[15824   641]
 [  332  4828]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Danceability/confusion_matrix_param_5842945.png
Accuracy:   0.9550
Precision:  0.8828
Recall:     0.9357
F1-score:   0.9085
Tiempo total para red 6: 368.50 segundos
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [21:18:51] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [21:19:21] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Saved on: outputs_ablation_remove_one_feature/1/Danceability

==============================
Model: Logistic Regression
Accuracy:  0.9369
Precision: 0.8296
Recall:    0.9258
F1-score:  0.8751
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.88      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15484   981]
 [  383  4777]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Danceability/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Danceability/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7461
Precision: 0.4835
Recall:    0.9399
F1-score:  0.6385
              precision    recall  f1-score   support

           0       0.97      0.69      0.80     16465
           1       0.48      0.94      0.64      5160

    accuracy                           0.75     21625
   macro avg       0.73      0.81      0.72     21625
weighted avg       0.86      0.75      0.76     21625

[[11284  5181]
 [  310  4850]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Danceability/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Danceability/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8598
Precision: 0.6761
Recall:    0.7917
F1-score:  0.7293
              precision    recall  f1-score   support

           0       0.93      0.88      0.91     16465
           1       0.68      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.84      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14508  1957]
 [ 1075  4085]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Danceability/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Danceability/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8655
Precision: 0.6897
Recall:    0.7930
F1-score:  0.7378
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.87      0.87     21625

[[14624  1841]
 [ 1068  4092]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Danceability/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Danceability/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9197
Precision: 0.7968
Recall:    0.8907
F1-score:  0.8411
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15293  1172]
 [  564  4596]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Danceability/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Danceability/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7881
Precision: 0.5306
Recall:    0.9698
F1-score:  0.6859
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12039  4426]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Danceability/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Danceability/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9369, 'precision': 0.8296, 'recall': 0.9258, 'f1_score': 0.8751}
XGBoost: {'accuracy': 0.9197, 'precision': 0.7968, 'recall': 0.8907, 'f1_score': 0.8411}
Random Forest: {'accuracy': 0.8655, 'precision': 0.6897, 'recall': 0.793, 'f1_score': 0.7378}
Decision Tree: {'accuracy': 0.8598, 'precision': 0.6761, 'recall': 0.7917, 'f1_score': 0.7293}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.7461, 'precision': 0.4835, 'recall': 0.9399, 'f1_score': 0.6385}

##################################################
Running experiment without POSITIVENESS feature
[Positiveness] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3009, Test Loss: 0.1888, F1: 0.8626, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0893, Test Loss: 0.2164, F1: 0.8602, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0754, Test Loss: 0.2455, F1: 0.8611, AUC: 0.9806
Mejores resultados en la época:  4
f1-score 0.8783808647654093
AUC según el mejor F1-score 0.9839946668644081
Confusion Matrix:
 [[15529   936]
 [  386  4774]]
Saved on: outputs_ablation_remove_one_feature/1/Danceability

==============================
Model: Logistic Regression
Accuracy:  0.9369
Precision: 0.8296
Recall:    0.9258
F1-score:  0.8751
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.88      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15484   981]
 [  383  4777]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Danceability/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Danceability/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7461
Precision: 0.4835
Recall:    0.9399
F1-score:  0.6385
              precision    recall  f1-score   support

           0       0.97      0.69      0.80     16465
           1       0.48      0.94      0.64      5160

    accuracy                           0.75     21625
   macro avg       0.73      0.81      0.72     21625
weighted avg       0.86      0.75      0.76     21625

[[11284  5181]
 [  310  4850]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Danceability/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Danceability/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8598
Precision: 0.6761
Recall:    0.7917
F1-score:  0.7293
              precision    recall  f1-score   support

           0       0.93      0.88      0.91     16465
           1       0.68      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.84      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14508  1957]
 [ 1075  4085]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Danceability/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Danceability/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8655
Precision: 0.6897
Recall:    0.7930
F1-score:  0.7378
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.87      0.87     21625

[[14624  1841]
 [ 1068  4092]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Danceability/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Danceability/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9197
Precision: 0.7968
Recall:    0.8907
F1-score:  0.8411
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15293  1172]
 [  564  4596]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Danceability/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Danceability/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7881
Precision: 0.5306
Recall:    0.9698
F1-score:  0.6859
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12039  4426]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Danceability/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Danceability/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9369, 'precision': 0.8296, 'recall': 0.9258, 'f1_score': 0.8751}
XGBoost: {'accuracy': 0.9197, 'precision': 0.7968, 'recall': 0.8907, 'f1_score': 0.8411}
Random Forest: {'accuracy': 0.8655, 'precision': 0.6897, 'recall': 0.793, 'f1_score': 0.7378}
Decision Tree: {'accuracy': 0.8598, 'precision': 0.6761, 'recall': 0.7917, 'f1_score': 0.7293}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.7461, 'precision': 0.4835, 'recall': 0.9399, 'f1_score': 0.6385}

##################################################
Running experiment without POSITIVENESS feature
[Positiveness] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2955, Test Loss: 0.1894, F1: 0.8626, AUC: 0.9799
Epoch [10/30] Train Loss: 0.0880, Test Loss: 0.1932, F1: 0.8673, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0503, Test Loss: 0.2303, F1: 0.8709, AUC: 0.9817
Mejores resultados en la época:  2
f1-score 0.8821025356283546
AUC según el mejor F1-score 0.9842968406085729
Confusion Matrix:
 [[15585   880]
 [  394  4766]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [21:45:31] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [21:45:49] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Positiveness/confusion_matrix_param_160769.png
Accuracy:   0.9389
Precision:  0.8361
Recall:     0.9252
F1-score:   0.8784

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2922, Test Loss: 0.1749, F1: 0.8724, AUC: 0.9799
Epoch [10/30] Train Loss: 0.0879, Test Loss: 0.2132, F1: 0.8579, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0608, Test Loss: 0.2402, F1: 0.8637, AUC: 0.9809
Mejores resultados en la época:  1
f1-score 0.8809611829944547
AUC según el mejor F1-score 0.9836960359889546
Confusion Matrix:
 [[15571   894]
 [  394  4766]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Positiveness/confusion_matrix_param_160769.png
Accuracy:   0.9404
Precision:  0.8420
Recall:     0.9236
F1-score:   0.8810

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2957, Test Loss: 0.1731, F1: 0.8695, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0894, Test Loss: 0.1939, F1: 0.8695, AUC: 0.9822
Epoch [20/30] Train Loss: 0.0750, Test Loss: 0.2617, F1: 0.8549, AUC: 0.9805
Mejores resultados en la época:  2
f1-score 0.8823094004441154
AUC según el mejor F1-score 0.9843352354183293
Confusion Matrix:
 [[15585   880]
 [  392  4768]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Positiveness/confusion_matrix_param_160769.png
Accuracy:   0.9412
Precision:  0.8442
Recall:     0.9240
F1-score:   0.8823
Tiempo total para red 1: 279.54 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2242, Test Loss: 0.1785, F1: 0.8662, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.2596, F1: 0.8980, AUC: 0.9877
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5129, F1: 0.9084, AUC: 0.9860
Mejores resultados en la época:  29
f1-score 0.9140030441400304
AUC según el mejor F1-score 0.982900355934717
Confusion Matrix:
 [[15917   548]
 [  356  4804]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Positiveness/confusion_matrix_param_5842945.png
Accuracy:   0.9582
Precision:  0.8976
Recall:     0.9310
F1-score:   0.9140

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2227, Test Loss: 0.1567, F1: 0.8744, AUC: 0.9837
Epoch [10/30] Train Loss: 0.0042, Test Loss: 0.4355, F1: 0.8871, AUC: 0.9871
Epoch [20/30] Train Loss: 0.0010, Test Loss: 0.6163, F1: 0.8866, AUC: 0.9854
Mejores resultados en la época:  9
f1-score 0.9073759667987172
AUC según el mejor F1-score 0.9878524448148174
Confusion Matrix:
 [[15833   632]
 [  350  4810]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Positiveness/confusion_matrix_param_5842945.png
Accuracy:   0.9546
Precision:  0.8839
Recall:     0.9322
F1-score:   0.9074

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2249, Test Loss: 0.1582, F1: 0.8766, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.6451, F1: 0.8905, AUC: 0.9849
Epoch [20/30] Train Loss: 0.0033, Test Loss: 0.7462, F1: 0.8893, AUC: 0.9820
Mejores resultados en la época:  26
f1-score 0.9132110954395862
AUC según el mejor F1-score 0.9864205785351592
Confusion Matrix:
 [[15846   619]
 [  304  4856]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Positiveness/confusion_matrix_param_5842945.png
Accuracy:   0.9573
Precision:  0.8869
Recall:     0.9411
F1-score:   0.9132
Tiempo total para red 6: 366.49 segundos
Saved on: outputs_ablation_remove_one_feature/1/Positiveness

==============================
Model: Logistic Regression
Accuracy:  0.9362
Precision: 0.8265
Recall:    0.9271
F1-score:  0.8739
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15461  1004]
 [  376  4784]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Positiveness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Positiveness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8278
Precision: 0.5931
Recall:    0.8868
F1-score:  0.7108
              precision    recall  f1-score   support

           0       0.96      0.81      0.88     16465
           1       0.59      0.89      0.71      5160

    accuracy                           0.83     21625
   macro avg       0.78      0.85      0.79     21625
weighted avg       0.87      0.83      0.84     21625

[[13325  3140]
 [  584  4576]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Positiveness/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Positiveness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8704
Precision: 0.7098
Recall:    0.7727
F1-score:  0.7399
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14835  1630]
 [ 1173  3987]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Positiveness/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Positiveness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8688
Precision: 0.6988
Recall:    0.7909
F1-score:  0.7420
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.70      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14706  1759]
 [ 1079  4081]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Positiveness/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Positiveness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9209
Precision: 0.8005
Recall:    0.8903
F1-score:  0.8430
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15320  1145]
 [  566  4594]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Positiveness/confusion_matrix_param_160769.png
Accuracy:   0.9411
Precision:  0.8441
Recall:     0.9236
F1-score:   0.8821

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2937, Test Loss: 0.1920, F1: 0.8618, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0867, Test Loss: 0.1954, F1: 0.8673, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0566, Test Loss: 0.2324, F1: 0.8695, AUC: 0.9812
Mejores resultados en la época:  1
f1-score 0.8839902858210349
AUC según el mejor F1-score 0.983489084197864
Confusion Matrix:
 [[15651   814]
 [  428  4732]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Positiveness/confusion_matrix_param_160769.png
Accuracy:   0.9426
Precision:  0.8532
Recall:     0.9171
F1-score:   0.8840

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2974, Test Loss: 0.1925, F1: 0.8633, AUC: 0.9799
Epoch [10/30] Train Loss: 0.0780, Test Loss: 0.1837, F1: 0.8744, AUC: 0.9830
Epoch [20/30] Train Loss: 0.0418, Test Loss: 0.2905, F1: 0.8479, AUC: 0.9816
Mejores resultados en la época:  3
f1-score 0.8828979062442097
AUC según el mejor F1-score 0.9841979992796559
Confusion Matrix:
 [[15596   869]
 [  395  4765]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Positiveness/confusion_matrix_param_160769.png
Accuracy:   0.9415
Precision:  0.8458
Recall:     0.9234
F1-score:   0.8829
Tiempo total para red 1: 286.99 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2240, Test Loss: 0.1439, F1: 0.8839, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0047, Test Loss: 0.5350, F1: 0.8825, AUC: 0.9864
Epoch [20/30] Train Loss: 0.0007, Test Loss: 0.8908, F1: 0.8703, AUC: 0.9803
Mejores resultados en la época:  18
f1-score 0.9105406456631661
AUC según el mejor F1-score 0.9875699863699604
Confusion Matrix:
 [[16023   442]
 [  478  4682]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Positiveness/confusion_matrix_param_5842945.png
Accuracy:   0.9575
Precision:  0.9137
Recall:     0.9074
F1-score:   0.9105

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2227, Test Loss: 0.1797, F1: 0.8585, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.3121, F1: 0.9041, AUC: 0.9873
Epoch [20/30] Train Loss: 0.0013, Test Loss: 0.3526, F1: 0.9075, AUC: 0.9877
Mejores resultados en la época:  15
f1-score 0.9142365426905058
AUC según el mejor F1-score 0.9879489556188015
Confusion Matrix:
 [[15947   518]
 [  379  4781]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Positiveness/confusion_matrix_param_5842945.png
Accuracy:   0.9585
Precision:  0.9022
Recall:     0.9266
F1-score:   0.9142

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2228, Test Loss: 0.2358, F1: 0.8273, AUC: 0.9823
Epoch [10/30] Train Loss: 0.0021, Test Loss: 0.5325, F1: 0.9008, AUC: 0.9852
Epoch [20/30] Train Loss: 0.0017, Test Loss: 0.4103, F1: 0.9055, AUC: 0.9872
Mejores resultados en la época:  12
f1-score 0.9148956447155246
AUC según el mejor F1-score 0.9869263848379344
Confusion Matrix:
 [[15932   533]
 [  360  4800]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Positiveness/confusion_matrix_param_5842945.png
Accuracy:   0.9587
Precision:  0.9001
Recall:     0.9302
F1-score:   0.9149
Tiempo total para red 6: 367.31 segundos
Saved on: outputs_ablation_remove_one_feature/1/Positiveness

==============================
Model: Logistic Regression
Accuracy:  0.9362
Precision: 0.8265
Recall:    0.9271
F1-score:  0.8739
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15461  1004]
 [  376  4784]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Positiveness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Positiveness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8278
Precision: 0.5931
Recall:    0.8868
F1-score:  0.7108
              precision    recall  f1-score   support

           0       0.96      0.81      0.88     16465
           1       0.59      0.89      0.71      5160

    accuracy                           0.83     21625
   macro avg       0.78      0.85      0.79     21625
weighted avg       0.87      0.83      0.84     21625

[[13325  3140]
 [  584  4576]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Positiveness/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Positiveness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8704
Precision: 0.7098
Recall:    0.7727
F1-score:  0.7399
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14835  1630]
 [ 1173  3987]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Positiveness/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Positiveness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8688
Precision: 0.6988
Recall:    0.7909
F1-score:  0.7420
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.70      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14706  1759]
 [ 1079  4081]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Positiveness/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Positiveness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9209
Precision: 0.8005
Recall:    0.8903
F1-score:  0.8430
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15320  1145]
 [  566  4594]]
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Positiveness/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Positiveness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Positiveness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Positiveness/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9362, 'precision': 0.8265, 'recall': 0.9271, 'f1_score': 0.8739}
XGBoost: {'accuracy': 0.9209, 'precision': 0.8005, 'recall': 0.8903, 'f1_score': 0.843}
Random Forest: {'accuracy': 0.8688, 'precision': 0.6988, 'recall': 0.7909, 'f1_score': 0.742}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.7098, 'recall': 0.7727, 'f1_score': 0.7399}
SVM: {'accuracy': 0.8278, 'precision': 0.5931, 'recall': 0.8868, 'f1_score': 0.7108}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}

##################################################
Running experiment without SPEECHINESS feature
[Speechiness] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3004, Test Loss: 0.1966, F1: 0.8598, AUC: 0.9791
Epoch [10/30] Train Loss: 0.0902, Test Loss: 0.1965, F1: 0.8664, AUC: 0.9816
Epoch [20/30] Train Loss: 0.0752, Test Loss: 0.2496, F1: 0.8608, AUC: 0.9801
Mejores resultados en la época:  4
f1-score 0.8809257185516984
AUC según el mejor F1-score 0.9833951746363557
Confusion Matrix:
 [[15629   836]
 [  440  4720]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Speechiness/confusion_matrix_param_160769.png
Accuracy:   0.9410
Precision:  0.8495
Recall:     0.9147
F1-score:   0.8809

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3066, Test Loss: 0.1900, F1: 0.8597, AUC: 0.9780
Epoch [10/30] Train Loss: 0.0685, Test Loss: 0.2010, F1: 0.8674, AUC: 0.9832
Epoch [20/30] Train Loss: 0.0209, Test Loss: 0.2693, F1: 0.8703, AUC: 0.9820
Mejores resultados en la época:  2
f1-score 0.8819794584500467
AUC según el mejor F1-score 0.983628103541221
Confusion Matrix:
 [[15638   827]
 [  437  4723]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Speechiness/confusion_matrix_param_160769.png
Accuracy:   0.9415
Precision:  0.8510
Recall:     0.9153
F1-score:   0.8820

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3010, Test Loss: 0.1849, F1: 0.8637, AUC: 0.9788
Epoch [10/30] Train Loss: 0.0903, Test Loss: 0.1940, F1: 0.8661, AUC: 0.9819
Epoch [20/30] Train Loss: 0.0699, Test Loss: 0.2342, F1: 0.8623, AUC: 0.9804
Mejores resultados en la época:  1
f1-score 0.872934263679765
AUC según el mejor F1-score 0.9827706469207644
Confusion Matrix:
 [[15487   978]
 [  406  4754]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Speechiness/confusion_matrix_param_160769.png
Accuracy:   0.9360
Precision:  0.8294
Recall:     0.9213
F1-score:   0.8729
Tiempo total para red 1: 283.19 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2250, Test Loss: 0.1697, F1: 0.8746, AUC: 0.9825
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.5296, F1: 0.9015, AUC: 0.9852
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6744, F1: 0.8893, AUC: 0.9848
Mejores resultados en la época:  9
f1-score 0.9128007699711261
AUC según el mejor F1-score 0.9876858122821016
Confusion Matrix:
 [[15977   488]
 [  418  4742]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Speechiness/confusion_matrix_param_5842945.png
Accuracy:   0.9581
Precision:  0.9067
Recall:     0.9190
F1-score:   0.9128

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2252, Test Loss: 0.1953, F1: 0.8578, AUC: 0.9821
Epoch [10/30] Train Loss: 0.0031, Test Loss: 0.4082, F1: 0.8751, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0020, Test Loss: 0.4637, F1: 0.9040, AUC: 0.9862
Mejores resultados en la época:  25
f1-score 0.9139927852667553
AUC según el mejor F1-score 0.9876467524488168
Confusion Matrix:
 [[15905   560]
 [  346  4814]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Speechiness/confusion_matrix_param_5842945.png
Accuracy:   0.9581
Precision:  0.8958
Recall:     0.9329
F1-score:   0.9140

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2302, Test Loss: 0.1665, F1: 0.8689, AUC: 0.9823
Epoch [10/30] Train Loss: 0.0033, Test Loss: 0.4246, F1: 0.9027, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0021, Test Loss: 0.2795, F1: 0.9026, AUC: 0.9881
Mejores resultados en la época:  5
f1-score 0.9100047415836889
AUC según el mejor F1-score 0.98694310458878
Confusion Matrix:
 [[15878   587]
 [  362  4798]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Speechiness/confusion_matrix_param_5842945.png
Accuracy:   0.9561
Precision:  0.8910
Recall:     0.9298
F1-score:   0.9100
Tiempo total para red 6: 367.37 segundos
Saved on: outputs_ablation_remove_one_feature/1/Speechiness

==============================
Model: Logistic Regression
Accuracy:  0.9339
Precision: 0.8220
Recall:    0.9227
F1-score:  0.8694
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.82      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.94     21625

[[15434  1031]
 [  399  4761]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Speechiness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Speechiness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7509
Precision: 0.4884
Recall:    0.9252
F1-score:  0.6393
              precision    recall  f1-score   support

           0       0.97      0.70      0.81     16465
           1       0.49      0.93      0.64      5160

    accuracy                           0.75     21625
   macro avg       0.73      0.81      0.72     21625
weighted avg       0.85      0.75      0.77     21625

[[11464  5001]
 [  386  4774]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Speechiness/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Speechiness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8627
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Positiveness/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Positiveness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Positiveness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Positiveness/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9362, 'precision': 0.8265, 'recall': 0.9271, 'f1_score': 0.8739}
XGBoost: {'accuracy': 0.9209, 'precision': 0.8005, 'recall': 0.8903, 'f1_score': 0.843}
Random Forest: {'accuracy': 0.8688, 'precision': 0.6988, 'recall': 0.7909, 'f1_score': 0.742}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.7098, 'recall': 0.7727, 'f1_score': 0.7399}
SVM: {'accuracy': 0.8278, 'precision': 0.5931, 'recall': 0.8868, 'f1_score': 0.7108}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}

##################################################
Running experiment without SPEECHINESS feature
[Speechiness] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3071, Test Loss: 0.1953, F1: 0.8605, AUC: 0.9785
Epoch [10/30] Train Loss: 0.0898, Test Loss: 0.1817, F1: 0.8731, AUC: 0.9818
Epoch [20/30] Train Loss: 0.0674, Test Loss: 0.2344, F1: 0.8658, AUC: 0.9805
Mejores resultados en la época:  3
f1-score 0.881371640407785
AUC según el mejor F1-score 0.9838051528141679
Confusion Matrix:
 [[15590   875]
 [  405  4755]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Speechiness/confusion_matrix_param_160769.png
Accuracy:   0.9408
Precision:  0.8446
Recall:     0.9215
F1-score:   0.8814

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2999, Test Loss: 0.1728, F1: 0.8689, AUC: 0.9787
Epoch [10/30] Train Loss: 0.0818, Test Loss: 0.1961, F1: 0.8649, AUC: 0.9821
Epoch [20/30] Train Loss: 0.0400, Test Loss: 0.2377, F1: 0.8721, AUC: 0.9820
Mejores resultados en la época:  4
f1-score 0.8763341921236658
AUC según el mejor F1-score 0.9834803329590368
Confusion Matrix:
 [[15519   946]
 [  398  4762]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Speechiness/confusion_matrix_param_160769.png
Accuracy:   0.9378
Precision:  0.8343
Recall:     0.9229
F1-score:   0.8763

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3055, Test Loss: 0.1820, F1: 0.8652, AUC: 0.9786
Epoch [10/30] Train Loss: 0.0879, Test Loss: 0.1918, F1: 0.8677, AUC: 0.9822
Epoch [20/30] Train Loss: 0.0595, Test Loss: 0.2414, F1: 0.8627, AUC: 0.9808
Mejores resultados en la época:  2
f1-score 0.8780487804878049
AUC según el mejor F1-score 0.9834905496036929
Confusion Matrix:
 [[15553   912]
 [  408  4752]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Speechiness/confusion_matrix_param_160769.png
Accuracy:   0.9390
Precision:  0.8390
Recall:     0.9209
F1-score:   0.8780
Tiempo total para red 1: 287.04 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2293, Test Loss: 0.2175, F1: 0.8343, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0079, Test Loss: 0.4134, F1: 0.8912, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5222, F1: 0.8994, AUC: 0.9867
Mejores resultados en la época:  11
f1-score 0.9130978731594649
AUC según el mejor F1-score 0.9879012445944769
Confusion Matrix:
 [[15978   487]
 [  416  4744]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Speechiness/confusion_matrix_param_5842945.png
Accuracy:   0.9582
Precision:  0.9069
Recall:     0.9194
F1-score:   0.9131

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2311, Test Loss: 0.2045, F1: 0.8498, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0035, Test Loss: 0.5095, F1: 0.8888, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5322, F1: 0.8955, AUC: 0.9864
Mejores resultados en la época:  8
f1-score 0.9085708837830108
AUC según el mejor F1-score 0.9870872616802849
Confusion Matrix:
 [[15901   564]
 [  395  4765]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Speechiness/confusion_matrix_param_5842945.png
Accuracy:   0.9557
Precision:  0.8942
Recall:     0.9234
F1-score:   0.9086

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2277, Test Loss: 0.1903, F1: 0.8661, AUC: 0.9832
Epoch [10/30] Train Loss: 0.0047, Test Loss: 0.4632, F1: 0.8924, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8910, F1: 0.9043, AUC: 0.9814
Mejores resultados en la época:  17
f1-score 0.9088191330343797
AUC según el mejor F1-score 0.9844938170467306
Confusion Matrix:
 [[15785   680]
 [  296  4864]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Speechiness/confusion_matrix_param_5842945.png
Accuracy:   0.9549
Precision:  0.8773
Recall:     0.9426
F1-score:   0.9088
Tiempo total para red 6: 367.33 segundos
Saved on: outputs_ablation_remove_one_feature/1/Speechiness

==============================
Model: Logistic Regression
Accuracy:  0.9339
Precision: 0.8220
Recall:    0.9227
F1-score:  0.8694
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.82      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.94     21625

[[15434  1031]
 [  399  4761]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Speechiness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Speechiness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7509
Precision: 0.4884
Recall:    0.9252
F1-score:  0.6393
              precision    recall  f1-score   support

           0       0.97      0.70      0.81     16465
           1       0.49      0.93      0.64      5160

    accuracy                           0.75     21625
   macro avg       0.73      0.81      0.72     21625
weighted avg       0.85      0.75      0.77     21625

[[11464  5001]
 [  386  4774]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Speechiness/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Speechiness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8627
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [22:12:15] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [22:12:36] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Precision: 0.7083
Recall:    0.7215
F1-score:  0.7149
              precision    recall  f1-score   support

           0       0.91      0.91      0.91     16465
           1       0.71      0.72      0.71      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.81      0.81     21625
weighted avg       0.86      0.86      0.86     21625

[[14932  1533]
 [ 1437  3723]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Speechiness/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Speechiness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8507
Precision: 0.6569
Recall:    0.7837
F1-score:  0.7147
              precision    recall  f1-score   support

           0       0.93      0.87      0.90     16465
           1       0.66      0.78      0.71      5160

    accuracy                           0.85     21625
   macro avg       0.79      0.83      0.81     21625
weighted avg       0.86      0.85      0.85     21625

[[14353  2112]
 [ 1116  4044]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Speechiness/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Speechiness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9200
Precision: 0.8014
Recall:    0.8837
F1-score:  0.8406
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15335  1130]
 [  600  4560]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Speechiness/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Speechiness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7881
Precision: 0.5306
Recall:    0.9698
F1-score:  0.6859
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12039  4426]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Speechiness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Speechiness/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9339, 'precision': 0.822, 'recall': 0.9227, 'f1_score': 0.8694}
XGBoost: {'accuracy': 0.92, 'precision': 0.8014, 'recall': 0.8837, 'f1_score': 0.8406}
Decision Tree: {'accuracy': 0.8627, 'precision': 0.7083, 'recall': 0.7215, 'f1_score': 0.7149}
Random Forest: {'accuracy': 0.8507, 'precision': 0.6569, 'recall': 0.7837, 'f1_score': 0.7147}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.7509, 'precision': 0.4884, 'recall': 0.9252, 'f1_score': 0.6393}

##################################################
Running experiment without LIVENESS feature
[Liveness] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2948, Test Loss: 0.1958, F1: 0.8594, AUC: 0.9792
Epoch [10/30] Train Loss: 0.0885, Test Loss: 0.2139, F1: 0.8602, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0696, Test Loss: 0.2461, F1: 0.8620, AUC: 0.9809
Mejores resultados en la época:  2
f1-score 0.8806038847463868
AUC según el mejor F1-score 0.9840083322151522
Confusion Matrix:
 [[15545   920]
 [  377  4783]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Liveness/confusion_matrix_param_160769.png
Accuracy:   0.9400
Precision:  0.8387
Recall:     0.9269
F1-score:   0.8806

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2958, Test Loss: 0.1797, F1: 0.8686, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0896, Test Loss: 0.2117, F1: 0.8620, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0766, Test Loss: 0.2407, F1: 0.8631, AUC: 0.9807
Mejores resultados en la época:  4
f1-score 0.8819968333799013
AUC según el mejor F1-score 0.9840345035393375
Confusion Matrix:
 [[15623   842]
 [  425  4735]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Liveness/confusion_matrix_param_160769.png
Accuracy:   0.9414
Precision:  0.8490
Recall:     0.9176
F1-score:   0.8820

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3005, Test Loss: 0.1768, F1: 0.8683, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0868, Test Loss: 0.1929, F1: 0.8669, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0565, Test Loss: 0.2492, F1: 0.8588, AUC: 0.9814
Mejores resultados en la época:  5
f1-score 0.8789420142421159
AUC según el mejor F1-score 0.983642375063854
Confusion Matrix:
 [[15564   901]
 [  408  4752]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Liveness/confusion_matrix_param_160769.png
Accuracy:   0.9395
Precision:  0.8406
Recall:     0.9209
F1-score:   0.8789
Tiempo total para red 1: 282.21 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2260, Test Loss: 0.1885, F1: 0.8632, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0033, Test Loss: 0.3387, F1: 0.9090, AUC: 0.9885
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5900, F1: 0.9056, AUC: 0.9858
Mejores resultados en la época:  10
f1-score 0.908988974023547
AUC según el mejor F1-score 0.9884637897631103
Confusion Matrix:
 [[15787   678]
 [  296  4864]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Liveness/confusion_matrix_param_5842945.png
Precision: 0.7083
Recall:    0.7215
F1-score:  0.7149
              precision    recall  f1-score   support

           0       0.91      0.91      0.91     16465
           1       0.71      0.72      0.71      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.81      0.81     21625
weighted avg       0.86      0.86      0.86     21625

[[14932  1533]
 [ 1437  3723]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Speechiness/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Speechiness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8507
Precision: 0.6569
Recall:    0.7837
F1-score:  0.7147
              precision    recall  f1-score   support

           0       0.93      0.87      0.90     16465
           1       0.66      0.78      0.71      5160

    accuracy                           0.85     21625
   macro avg       0.79      0.83      0.81     21625
weighted avg       0.86      0.85      0.85     21625

[[14353  2112]
 [ 1116  4044]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Speechiness/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Speechiness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9200
Precision: 0.8014
Recall:    0.8837
F1-score:  0.8406
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15335  1130]
 [  600  4560]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Speechiness/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Speechiness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7881
Precision: 0.5306
Recall:    0.9698
F1-score:  0.6859
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12039  4426]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Speechiness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Speechiness/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9339, 'precision': 0.822, 'recall': 0.9227, 'f1_score': 0.8694}
XGBoost: {'accuracy': 0.92, 'precision': 0.8014, 'recall': 0.8837, 'f1_score': 0.8406}
Decision Tree: {'accuracy': 0.8627, 'precision': 0.7083, 'recall': 0.7215, 'f1_score': 0.7149}
Random Forest: {'accuracy': 0.8507, 'precision': 0.6569, 'recall': 0.7837, 'f1_score': 0.7147}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.7509, 'precision': 0.4884, 'recall': 0.9252, 'f1_score': 0.6393}

##################################################
Running experiment without LIVENESS feature
[Liveness] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2977, Test Loss: 0.1922, F1: 0.8611, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0789, Test Loss: 0.1901, F1: 0.8730, AUC: 0.9829
Epoch [20/30] Train Loss: 0.0482, Test Loss: 0.2602, F1: 0.8660, AUC: 0.9815
Mejores resultados en la época:  3
f1-score 0.8882175226586103
AUC según el mejor F1-score 0.9841859700044963
Confusion Matrix:
 [[15737   728]
 [  456  4704]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Liveness/confusion_matrix_param_160769.png
Accuracy:   0.9452
Precision:  0.8660
Recall:     0.9116
F1-score:   0.8882

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2932, Test Loss: 0.1733, F1: 0.8677, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0881, Test Loss: 0.2022, F1: 0.8667, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0693, Test Loss: 0.2381, F1: 0.8656, AUC: 0.9806
Mejores resultados en la época:  4
f1-score 0.8821729860016687
AUC según el mejor F1-score 0.9840915778595423
Confusion Matrix:
 [[15596   869]
 [  402  4758]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Liveness/confusion_matrix_param_160769.png
Accuracy:   0.9412
Precision:  0.8456
Recall:     0.9221
F1-score:   0.8822

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2932, Test Loss: 0.1884, F1: 0.8606, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0875, Test Loss: 0.1866, F1: 0.8713, AUC: 0.9822
Epoch [20/30] Train Loss: 0.0613, Test Loss: 0.2279, F1: 0.8704, AUC: 0.9813
Mejores resultados en la época:  4
f1-score 0.8835469086147226
AUC según el mejor F1-score 0.9840148412065057
Confusion Matrix:
 [[15657   808]
 [  437  4723]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Liveness/confusion_matrix_param_160769.png
Accuracy:   0.9424
Precision:  0.8539
Recall:     0.9153
F1-score:   0.8835
Tiempo total para red 1: 287.15 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2240, Test Loss: 0.1915, F1: 0.8677, AUC: 0.9835
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.2932, F1: 0.9047, AUC: 0.9885
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6010, F1: 0.8935, AUC: 0.9853
Mejores resultados en la época:  13
f1-score 0.9102064666926373
AUC según el mejor F1-score 0.9866700741766067
Confusion Matrix:
 [[16030   435]
 [  487  4673]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Liveness/confusion_matrix_param_5842945.png
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [22:38:49] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [22:39:13] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Accuracy:   0.9550
Precision:  0.8777
Recall:     0.9426
F1-score:   0.9090

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2302, Test Loss: 0.1886, F1: 0.8191, AUC: 0.9822
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.4289, F1: 0.8895, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0015, Test Loss: 0.3163, F1: 0.9033, AUC: 0.9878
Mejores resultados en la época:  12
f1-score 0.91260084517864
AUC según el mejor F1-score 0.9871182411834358
Confusion Matrix:
 [[15964   501]
 [  409  4751]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Liveness/confusion_matrix_param_5842945.png
Accuracy:   0.9579
Precision:  0.9046
Recall:     0.9207
F1-score:   0.9126

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2247, Test Loss: 0.1585, F1: 0.8816, AUC: 0.9823
Epoch [10/30] Train Loss: 0.0023, Test Loss: 0.3959, F1: 0.9163, AUC: 0.9883
Epoch [20/30] Train Loss: 0.0018, Test Loss: 0.3258, F1: 0.9104, AUC: 0.9883
Mejores resultados en la época:  10
f1-score 0.9162970106075217
AUC según el mejor F1-score 0.9882722806422832
Confusion Matrix:
 [[16006   459]
 [  409  4751]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Liveness/confusion_matrix_param_5842945.png
Accuracy:   0.9599
Precision:  0.9119
Recall:     0.9207
F1-score:   0.9163
Tiempo total para red 6: 365.17 segundos
Saved on: outputs_ablation_remove_one_feature/1/Liveness

==============================
Model: Logistic Regression
Accuracy:  0.9364
Precision: 0.8277
Recall:    0.9264
F1-score:  0.8743
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15470   995]
 [  380  4780]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Liveness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Liveness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8630
Precision: 0.6744
Recall:    0.8236
F1-score:  0.7416
              precision    recall  f1-score   support

           0       0.94      0.88      0.91     16465
           1       0.67      0.82      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.85      0.82     21625
weighted avg       0.88      0.86      0.87     21625

[[14413  2052]
 [  910  4250]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Liveness/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Liveness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8703
Precision: 0.7094
Recall:    0.7731
F1-score:  0.7399
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14831  1634]
 [ 1171  3989]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Liveness/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Liveness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8728
Precision: 0.7079
Recall:    0.7948
F1-score:  0.7488
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.79      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.87      0.88     21625

[[14773  1692]
 [ 1059  4101]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Liveness/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Liveness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9215
Precision: 0.8004
Recall:    0.8938
F1-score:  0.8445
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15315  1150]
 [  548  4612]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Liveness/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Liveness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Liveness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Liveness/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9364, 'precision': 0.8277, 'recall': 0.9264, 'f1_score': 0.8743}
XGBoost: {'accuracy': 0.9215, 'precision': 0.8004, 'recall': 0.8938, 'f1_score': 0.8445}
Random Forest: {'accuracy': 0.8728, 'precision': 0.7079, 'recall': 0.7948, 'f1_score': 0.7488}
Accuracy:   0.9574
Precision:  0.9148
Recall:     0.9056
F1-score:   0.9102

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2224, Test Loss: 0.1579, F1: 0.8853, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0015, Test Loss: 0.9148, F1: 0.8965, AUC: 0.9801
Epoch [20/30] Train Loss: 0.0044, Test Loss: 0.3826, F1: 0.9026, AUC: 0.9873
Mejores resultados en la época:  16
f1-score 0.9077167117663525
AUC según el mejor F1-score 0.9886157505820429
Confusion Matrix:
 [[15772   693]
 [  296  4864]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Liveness/confusion_matrix_param_5842945.png
Accuracy:   0.9543
Precision:  0.8753
Recall:     0.9426
F1-score:   0.9077

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2242, Test Loss: 0.1579, F1: 0.8733, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0033, Test Loss: 0.3793, F1: 0.8875, AUC: 0.9874
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6440, F1: 0.9092, AUC: 0.9837
Mejores resultados en la época:  15
f1-score 0.9112872782181956
AUC según el mejor F1-score 0.9855186006492513
Confusion Matrix:
 [[15857   608]
 [  332  4828]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Liveness/confusion_matrix_param_5842945.png
Accuracy:   0.9565
Precision:  0.8882
Recall:     0.9357
F1-score:   0.9113
Tiempo total para red 6: 366.29 segundos
Saved on: outputs_ablation_remove_one_feature/1/Liveness

==============================
Model: Logistic Regression
Accuracy:  0.9364
Precision: 0.8277
Recall:    0.9264
F1-score:  0.8743
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15470   995]
 [  380  4780]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Liveness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Liveness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8630
Precision: 0.6744
Recall:    0.8236
F1-score:  0.7416
              precision    recall  f1-score   support

           0       0.94      0.88      0.91     16465
           1       0.67      0.82      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.85      0.82     21625
weighted avg       0.88      0.86      0.87     21625

[[14413  2052]
 [  910  4250]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Liveness/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Liveness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8703
Precision: 0.7094
Recall:    0.7731
F1-score:  0.7399
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14831  1634]
 [ 1171  3989]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Liveness/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Liveness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8728
Precision: 0.7079
Recall:    0.7948
F1-score:  0.7488
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.79      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.87      0.88     21625

[[14773  1692]
 [ 1059  4101]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Liveness/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Liveness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9215
Precision: 0.8004
Recall:    0.8938
F1-score:  0.8445
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15315  1150]
 [  548  4612]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Liveness/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Liveness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Liveness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Liveness/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9364, 'precision': 0.8277, 'recall': 0.9264, 'f1_score': 0.8743}
XGBoost: {'accuracy': 0.9215, 'precision': 0.8004, 'recall': 0.8938, 'f1_score': 0.8445}
Random Forest: {'accuracy': 0.8728, 'precision': 0.7079, 'recall': 0.7948, 'f1_score': 0.7488}
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
SVM: {'accuracy': 0.863, 'precision': 0.6744, 'recall': 0.8236, 'f1_score': 0.7416}
Decision Tree: {'accuracy': 0.8703, 'precision': 0.7094, 'recall': 0.7731, 'f1_score': 0.7399}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}

##################################################
Running experiment without ACOUSTICNESS feature
[Acousticness] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2935, Test Loss: 0.1819, F1: 0.8668, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0862, Test Loss: 0.1832, F1: 0.8744, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0518, Test Loss: 0.2603, F1: 0.8575, AUC: 0.9814
Mejores resultados en la época:  5
f1-score 0.8794969018773698
AUC según el mejor F1-score 0.9836934465168068
Confusion Matrix:
 [[15567   898]
 [  405  4755]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Acousticness/confusion_matrix_param_160769.png
Accuracy:   0.9397
Precision:  0.8411
Recall:     0.9215
F1-score:   0.8795

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2929, Test Loss: 0.1808, F1: 0.8676, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0871, Test Loss: 0.1806, F1: 0.8759, AUC: 0.9827
Epoch [20/30] Train Loss: 0.0607, Test Loss: 0.2301, F1: 0.8684, AUC: 0.9809
Mejores resultados en la época:  4
f1-score 0.8781739847832065
AUC según el mejor F1-score 0.9840665423720035
Confusion Matrix:
 [[15506   959]
 [  370  4790]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Acousticness/confusion_matrix_param_160769.png
Accuracy:   0.9385
Precision:  0.8332
Recall:     0.9283
F1-score:   0.8782

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2963, Test Loss: 0.1788, F1: 0.8671, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0876, Test Loss: 0.1948, F1: 0.8716, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0721, Test Loss: 0.2362, F1: 0.8677, AUC: 0.9807
Mejores resultados en la época:  4
f1-score 0.8792324015130547
AUC según el mejor F1-score 0.9841986937290047
Confusion Matrix:
 [[15551   914]
 [  395  4765]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Acousticness/confusion_matrix_param_160769.png
Accuracy:   0.9395
Precision:  0.8391
Recall:     0.9234
F1-score:   0.8792
Tiempo total para red 1: 281.57 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2219, Test Loss: 0.1776, F1: 0.8745, AUC: 0.9837
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.3942, F1: 0.8574, AUC: 0.9883
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5133, F1: 0.8975, AUC: 0.9877
Mejores resultados en la época:  12
f1-score 0.9112605360356094
AUC según el mejor F1-score 0.9885737128557875
Confusion Matrix:
 [[15877   588]
 [  349  4811]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Acousticness/confusion_matrix_param_5842945.png
Accuracy:   0.9567
Precision:  0.8911
Recall:     0.9324
F1-score:   0.9113

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2264, Test Loss: 0.1774, F1: 0.8601, AUC: 0.9827
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.4155, F1: 0.8960, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.9336, F1: 0.8896, AUC: 0.9799
Mejores resultados en la época:  12
f1-score 0.9086452342792496
AUC según el mejor F1-score 0.9876097053416103
Confusion Matrix:
 [[15837   628]
 [  341  4819]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Acousticness/confusion_matrix_param_5842945.png
Accuracy:   0.9552
Precision:  0.8847
Recall:     0.9339
F1-score:   0.9086

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2226, Test Loss: 0.1439, F1: 0.8849, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0027, Test Loss: 0.3706, F1: 0.9041, AUC: 0.9878
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5412, F1: 0.8948, AUC: 0.9874
Mejores resultados en la época:  12
f1-score 0.9126831148804935
AUC según el mejor F1-score 0.988658188499448
Confusion Matrix:
 [[15984   481]
 [  425  4735]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Acousticness/confusion_matrix_param_5842945.png
Accuracy:   0.9581
Precision:  0.9078
Recall:     0.9176
F1-score:   0.9127
Tiempo total para red 6: 368.26 segundos
Saved on: outputs_ablation_remove_one_feature/1/Acousticness

==============================
Model: Logistic Regression
Accuracy:  0.9361
Precision: 0.8267
Recall:    0.9264
F1-score:  0.8737
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15463  1002]
 [  380  4780]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Acousticness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Acousticness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8043
Precision: 0.5568
Recall:    0.8828
F1-score:  0.6829
              precision    recall  f1-score   support

           0       0.95      0.78      0.86     16465
           1       0.56      0.88      0.68      5160

    accuracy                           0.80     21625
   macro avg       0.76      0.83      0.77     21625
weighted avg       0.86      0.80      0.82     21625

[[12839  3626]
 [  605  4555]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Acousticness/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Acousticness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8691
Precision: 0.7048
Recall:    0.7767
F1-score:  0.7390
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.70      0.78      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14786  1679]
 [ 1152  4008]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Acousticness/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Acousticness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8745
Precision: 0.7141
Recall:    0.7905
F1-score:  0.7504
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.71      0.79      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.87      0.88     21625
SVM: {'accuracy': 0.863, 'precision': 0.6744, 'recall': 0.8236, 'f1_score': 0.7416}
Decision Tree: {'accuracy': 0.8703, 'precision': 0.7094, 'recall': 0.7731, 'f1_score': 0.7399}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}

##################################################
Running experiment without ACOUSTICNESS feature
[Acousticness] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2925, Test Loss: 0.1782, F1: 0.8670, AUC: 0.9803
Epoch [10/30] Train Loss: 0.0853, Test Loss: 0.2254, F1: 0.8498, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0536, Test Loss: 0.2435, F1: 0.8666, AUC: 0.9815
Mejores resultados en la época:  6
f1-score 0.8834870075440067
AUC según el mejor F1-score 0.9834702281324961
Confusion Matrix:
 [[15631   834]
 [  417  4743]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Acousticness/confusion_matrix_param_160769.png
Accuracy:   0.9422
Precision:  0.8505
Recall:     0.9192
F1-score:   0.8835

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2949, Test Loss: 0.1869, F1: 0.8633, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0880, Test Loss: 0.1835, F1: 0.8752, AUC: 0.9820
Epoch [20/30] Train Loss: 0.0701, Test Loss: 0.2351, F1: 0.8685, AUC: 0.9806
Mejores resultados en la época:  3
f1-score 0.8839427662957074
AUC según el mejor F1-score 0.9843957466742939
Confusion Matrix:
 [[15658   807]
 [  434  4726]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Acousticness/confusion_matrix_param_160769.png
Accuracy:   0.9426
Precision:  0.8541
Recall:     0.9159
F1-score:   0.8839

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2935, Test Loss: 0.1710, F1: 0.8715, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0824, Test Loss: 0.1891, F1: 0.8738, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0546, Test Loss: 0.2314, F1: 0.8734, AUC: 0.9811
Mejores resultados en la época:  2
f1-score 0.8800734618916437
AUC según el mejor F1-score 0.9845274507588331
Confusion Matrix:
 [[15527   938]
 [  368  4792]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Acousticness/confusion_matrix_param_160769.png
Accuracy:   0.9396
Precision:  0.8363
Recall:     0.9287
F1-score:   0.8801
Tiempo total para red 1: 287.01 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2254, Test Loss: 0.1972, F1: 0.8591, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0031, Test Loss: 0.3440, F1: 0.8966, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0012, Test Loss: 0.3206, F1: 0.9016, AUC: 0.9875
Mejores resultados en la época:  23
f1-score 0.9117144484526296
AUC según el mejor F1-score 0.9881690960623546
Confusion Matrix:
 [[15893   572]
 [  358  4802]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Acousticness/confusion_matrix_param_5842945.png
Accuracy:   0.9570
Precision:  0.8936
Recall:     0.9306
F1-score:   0.9117

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2216, Test Loss: 0.1753, F1: 0.8736, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0046, Test Loss: 0.4083, F1: 0.8980, AUC: 0.9871
Epoch [20/30] Train Loss: 0.0015, Test Loss: 0.3438, F1: 0.9122, AUC: 0.9882
Mejores resultados en la época:  18
f1-score 0.9127581547963713
AUC según el mejor F1-score 0.9876263662408162
Confusion Matrix:
 [[15992   473]
 [  431  4729]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Acousticness/confusion_matrix_param_5842945.png
Accuracy:   0.9582
Precision:  0.9091
Recall:     0.9165
F1-score:   0.9128

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2246, Test Loss: 0.1831, F1: 0.8617, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0022, Test Loss: 0.4039, F1: 0.9018, AUC: 0.9873
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8505, F1: 0.9044, AUC: 0.9799
Mejores resultados en la época:  26
f1-score 0.9045819120494146
AUC según el mejor F1-score 0.9788316066262237
Confusion Matrix:
 [[15684   781]
 [  254  4906]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Acousticness/confusion_matrix_param_5842945.png
Accuracy:   0.9521
Precision:  0.8627
Recall:     0.9508
F1-score:   0.9046
Tiempo total para red 6: 369.22 segundos
Saved on: outputs_ablation_remove_one_feature/1/Acousticness

==============================
Model: Logistic Regression
Accuracy:  0.9361
Precision: 0.8267
Recall:    0.9264
F1-score:  0.8737
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15463  1002]
 [  380  4780]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Acousticness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Acousticness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8043
Precision: 0.5568
Recall:    0.8828
F1-score:  0.6829
              precision    recall  f1-score   support

           0       0.95      0.78      0.86     16465
           1       0.56      0.88      0.68      5160

    accuracy                           0.80     21625
   macro avg       0.76      0.83      0.77     21625
weighted avg       0.86      0.80      0.82     21625

[[12839  3626]
 [  605  4555]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Acousticness/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Acousticness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8691
Precision: 0.7048
Recall:    0.7767
F1-score:  0.7390
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.70      0.78      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14786  1679]
 [ 1152  4008]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Acousticness/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Acousticness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8745
Precision: 0.7141
Recall:    0.7905
F1-score:  0.7504
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.71      0.79      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.87      0.88     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [23:05:21] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [23:05:34] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[14832  1633]
 [ 1081  4079]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Acousticness/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Acousticness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9200
Precision: 0.7981
Recall:    0.8899
F1-score:  0.8415
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15303  1162]
 [  568  4592]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Acousticness/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Acousticness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7881
Precision: 0.5306
Recall:    0.9698
F1-score:  0.6859
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12038  4427]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Acousticness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Acousticness/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8267, 'recall': 0.9264, 'f1_score': 0.8737}
XGBoost: {'accuracy': 0.92, 'precision': 0.7981, 'recall': 0.8899, 'f1_score': 0.8415}
Random Forest: {'accuracy': 0.8745, 'precision': 0.7141, 'recall': 0.7905, 'f1_score': 0.7504}
Decision Tree: {'accuracy': 0.8691, 'precision': 0.7048, 'recall': 0.7767, 'f1_score': 0.739}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.8043, 'precision': 0.5568, 'recall': 0.8828, 'f1_score': 0.6829}

##################################################
Running experiment without INSTRUMENTALNESS feature
[Instrumentalness] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2986, Test Loss: 0.1936, F1: 0.8588, AUC: 0.9791
Epoch [10/30] Train Loss: 0.0872, Test Loss: 0.1892, F1: 0.8691, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0547, Test Loss: 0.2351, F1: 0.8682, AUC: 0.9811
Mejores resultados en la época:  1
f1-score 0.8774157923799006
AUC según el mejor F1-score 0.9829627622134808
Confusion Matrix:
 [[15526   939]
 [  393  4767]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Instrumentalness/confusion_matrix_param_160769.png
Accuracy:   0.9384
Precision:  0.8354
Recall:     0.9238
F1-score:   0.8774

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2902, Test Loss: 0.1657, F1: 0.8719, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0898, Test Loss: 0.2037, F1: 0.8659, AUC: 0.9821
Epoch [20/30] Train Loss: 0.0734, Test Loss: 0.2493, F1: 0.8579, AUC: 0.9802
Mejores resultados en la época:  2
f1-score 0.8823694291320191
AUC según el mejor F1-score 0.9840080203014614
Confusion Matrix:
 [[15644   821]
 [  438  4722]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Instrumentalness/confusion_matrix_param_160769.png
Accuracy:   0.9418
Precision:  0.8519
Recall:     0.9151
F1-score:   0.8824

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2929, Test Loss: 0.1831, F1: 0.8649, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0893, Test Loss: 0.1994, F1: 0.8642, AUC: 0.9821
Epoch [20/30] Train Loss: 0.0656, Test Loss: 0.2519, F1: 0.8556, AUC: 0.9805
Mejores resultados en la época:  16
f1-score 0.8755109624674842
AUC según el mejor F1-score 0.9806669656329964
Confusion Matrix:
 [[15573   892]
 [  448  4712]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Instrumentalness/confusion_matrix_param_160769.png
Accuracy:   0.9380
Precision:  0.8408
Recall:     0.9132
F1-score:   0.8755
Tiempo total para red 1: 287.20 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2268, Test Loss: 0.1657, F1: 0.8617, AUC: 0.9823
Epoch [10/30] Train Loss: 0.0020, Test Loss: 0.4461, F1: 0.9019, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0027, Test Loss: 0.3977, F1: 0.9022, AUC: 0.9865
Mejores resultados en la época:  13
f1-score 0.9095709570957096
AUC según el mejor F1-score 0.9865013406403528
Confusion Matrix:
 [[15843   622]
 [  337  4823]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Instrumentalness/confusion_matrix_param_5842945.png
Accuracy:   0.9557
Precision:  0.8858
Recall:     0.9347
F1-score:   0.9096

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2245, Test Loss: 0.1819, F1: 0.8688, AUC: 0.9822
Epoch [10/30] Train Loss: 0.0027, Test Loss: 0.2236, F1: 0.9074, AUC: 0.9878
Epoch [20/30] Train Loss: 0.0005, Test Loss: 0.5018, F1: 0.9019, AUC: 0.9857
Mejores resultados en la época:  19
f1-score 0.9128175519630485
AUC según el mejor F1-score 0.9887512447121802
Confusion Matrix:
 [[15976   489]
 [  417  4743]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Instrumentalness/confusion_matrix_param_5842945.png
Accuracy:   0.9581
Precision:  0.9065
Recall:     0.9192
F1-score:   0.9128

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2237, Test Loss: 0.1584, F1: 0.8718, AUC: 0.9825
Epoch [10/30] Train Loss: 0.0031, Test Loss: 0.3277, F1: 0.9123, AUC: 0.9881
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8723, F1: 0.8941, AUC: 0.9805
Mejores resultados en la época:  10
f1-score 0.9122603137710633
AUC según el mejor F1-score 0.9881498633464925
Confusion Matrix:
 [[16009   456]
 [  450  4710]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Instrumentalness/confusion_matrix_param_5842945.png
Accuracy:   0.9581
Precision:  0.9117

[[14832  1633]
 [ 1081  4079]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Acousticness/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Acousticness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9200
Precision: 0.7981
Recall:    0.8899
F1-score:  0.8415
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15303  1162]
 [  568  4592]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Acousticness/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Acousticness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7881
Precision: 0.5306
Recall:    0.9698
F1-score:  0.6859
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12038  4427]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Acousticness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Acousticness/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8267, 'recall': 0.9264, 'f1_score': 0.8737}
XGBoost: {'accuracy': 0.92, 'precision': 0.7981, 'recall': 0.8899, 'f1_score': 0.8415}
Random Forest: {'accuracy': 0.8745, 'precision': 0.7141, 'recall': 0.7905, 'f1_score': 0.7504}
Decision Tree: {'accuracy': 0.8691, 'precision': 0.7048, 'recall': 0.7767, 'f1_score': 0.739}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.8043, 'precision': 0.5568, 'recall': 0.8828, 'f1_score': 0.6829}

##################################################
Running experiment without INSTRUMENTALNESS feature
[Instrumentalness] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2993, Test Loss: 0.2051, F1: 0.8493, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0897, Test Loss: 0.2098, F1: 0.8580, AUC: 0.9818
Epoch [20/30] Train Loss: 0.0654, Test Loss: 0.2452, F1: 0.8582, AUC: 0.9806
Mejores resultados en la época:  7
f1-score 0.8783167302858207
AUC según el mejor F1-score 0.9828123256520173
Confusion Matrix:
 [[15601   864]
 [  443  4717]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Instrumentalness/confusion_matrix_param_160769.png
Accuracy:   0.9396
Precision:  0.8452
Recall:     0.9141
F1-score:   0.8783

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2931, Test Loss: 0.1741, F1: 0.8675, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0892, Test Loss: 0.1882, F1: 0.8716, AUC: 0.9821
Epoch [20/30] Train Loss: 0.0607, Test Loss: 0.2451, F1: 0.8612, AUC: 0.9806
Mejores resultados en la época:  2
f1-score 0.8794221152065197
AUC según el mejor F1-score 0.9838103435287913
Confusion Matrix:
 [[15575   890]
 [  412  4748]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Instrumentalness/confusion_matrix_param_160769.png
Accuracy:   0.9398
Precision:  0.8421
Recall:     0.9202
F1-score:   0.8794

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3003, Test Loss: 0.1972, F1: 0.8532, AUC: 0.9791
Epoch [10/30] Train Loss: 0.0836, Test Loss: 0.2055, F1: 0.8612, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0477, Test Loss: 0.2217, F1: 0.8734, AUC: 0.9816
Mejores resultados en la época:  3
f1-score 0.8806424813071172
AUC según el mejor F1-score 0.983928488195538
Confusion Matrix:
 [[15562   903]
 [  390  4770]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Instrumentalness/confusion_matrix_param_160769.png
Accuracy:   0.9402
Precision:  0.8408
Recall:     0.9244
F1-score:   0.8806
Tiempo total para red 1: 288.94 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2298, Test Loss: 0.1586, F1: 0.8782, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0028, Test Loss: 0.2568, F1: 0.9102, AUC: 0.9878
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6015, F1: 0.9017, AUC: 0.9852
Mejores resultados en la época:  11
f1-score 0.9122672297945863
AUC según el mejor F1-score 0.9880218728004201
Confusion Matrix:
 [[15959   506]
 [  408  4752]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Instrumentalness/confusion_matrix_param_5842945.png
Accuracy:   0.9577
Precision:  0.9038
Recall:     0.9209
F1-score:   0.9123

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2285, Test Loss: 0.1690, F1: 0.8711, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0039, Test Loss: 0.3802, F1: 0.9001, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0025, Test Loss: 0.3830, F1: 0.8933, AUC: 0.9874
Mejores resultados en la época:  8
f1-score 0.9109176155391828
AUC según el mejor F1-score 0.986785982481044
Confusion Matrix:
 [[15934   531]
 [  400  4760]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Instrumentalness/confusion_matrix_param_5842945.png
Accuracy:   0.9569
Precision:  0.8996
Recall:     0.9225
F1-score:   0.9109

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2300, Test Loss: 0.2199, F1: 0.8392, AUC: 0.9822
Epoch [10/30] Train Loss: 0.0017, Test Loss: 0.3507, F1: 0.9051, AUC: 0.9871
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5896, F1: 0.9025, AUC: 0.9850
Mejores resultados en la época:  17
f1-score 0.9129568106312292
AUC según el mejor F1-score 0.9870183817211515
Confusion Matrix:
 [[15899   566]
 [  351  4809]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Instrumentalness/confusion_matrix_param_5842945.png
Accuracy:   0.9576
Precision:  0.8947
Recall:     0.9320
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [23:31:48] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [23:32:15] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Recall:     0.9128
F1-score:   0.9123
Tiempo total para red 6: 365.20 segundos
Saved on: outputs_ablation_remove_one_feature/1/Instrumentalness

==============================
Model: Logistic Regression
Accuracy:  0.9347
Precision: 0.8239
Recall:    0.9236
F1-score:  0.8709
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.82      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.94     21625

[[15446  1019]
 [  394  4766]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Instrumentalness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Instrumentalness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7972
Precision: 0.5454
Recall:    0.9014
F1-score:  0.6796
              precision    recall  f1-score   support

           0       0.96      0.76      0.85     16465
           1       0.55      0.90      0.68      5160

    accuracy                           0.80     21625
   macro avg       0.75      0.83      0.77     21625
weighted avg       0.86      0.80      0.81     21625

[[12588  3877]
 [  509  4651]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Instrumentalness/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Instrumentalness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8742
Precision: 0.7221
Recall:    0.7684
F1-score:  0.7445
              precision    recall  f1-score   support

           0       0.93      0.91      0.92     16465
           1       0.72      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.88     21625

[[14939  1526]
 [ 1195  3965]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Instrumentalness/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Instrumentalness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8733
Precision: 0.7103
Recall:    0.7922
F1-score:  0.7491
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.71      0.79      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.87      0.88     21625

[[14798  1667]
 [ 1072  4088]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Instrumentalness/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Instrumentalness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9201
Precision: 0.7993
Recall:    0.8884
F1-score:  0.8415
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15314  1151]
 [  576  4584]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Instrumentalness/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Instrumentalness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7880
Precision: 0.5305
Recall:    0.9698
F1-score:  0.6858
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12036  4429]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Instrumentalness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Instrumentalness/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9347, 'precision': 0.8239, 'recall': 0.9236, 'f1_score': 0.8709}
XGBoost: {'accuracy': 0.9201, 'precision': 0.7993, 'recall': 0.8884, 'f1_score': 0.8415}
Random Forest: {'accuracy': 0.8733, 'precision': 0.7103, 'recall': 0.7922, 'f1_score': 0.7491}
Decision Tree: {'accuracy': 0.8742, 'precision': 0.7221, 'recall': 0.7684, 'f1_score': 0.7445}
Naive Bayes: {'accuracy': 0.788, 'precision': 0.5305, 'recall': 0.9698, 'f1_score': 0.6858}
SVM: {'accuracy': 0.7972, 'precision': 0.5454, 'recall': 0.9014, 'f1_score': 0.6796}

##################################################
Running experiment without GOOD FOR PARTY feature
[Good for Party] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2966, Test Loss: 0.1843, F1: 0.8616, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0885, Test Loss: 0.1908, F1: 0.8709, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0699, Test Loss: 0.2388, F1: 0.8642, AUC: 0.9810
F1-score:   0.9130
Tiempo total para red 6: 365.73 segundos
Saved on: outputs_ablation_remove_one_feature/1/Instrumentalness

==============================
Model: Logistic Regression
Accuracy:  0.9347
Precision: 0.8239
Recall:    0.9236
F1-score:  0.8709
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.82      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.94     21625

[[15446  1019]
 [  394  4766]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Instrumentalness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Instrumentalness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7972
Precision: 0.5454
Recall:    0.9014
F1-score:  0.6796
              precision    recall  f1-score   support

           0       0.96      0.76      0.85     16465
           1       0.55      0.90      0.68      5160

    accuracy                           0.80     21625
   macro avg       0.75      0.83      0.77     21625
weighted avg       0.86      0.80      0.81     21625

[[12588  3877]
 [  509  4651]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Instrumentalness/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Instrumentalness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8742
Precision: 0.7221
Recall:    0.7684
F1-score:  0.7445
              precision    recall  f1-score   support

           0       0.93      0.91      0.92     16465
           1       0.72      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.88     21625

[[14939  1526]
 [ 1195  3965]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Instrumentalness/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Instrumentalness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8733
Precision: 0.7103
Recall:    0.7922
F1-score:  0.7491
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.71      0.79      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.87      0.88     21625

[[14798  1667]
 [ 1072  4088]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Instrumentalness/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Instrumentalness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9201
Precision: 0.7993
Recall:    0.8884
F1-score:  0.8415
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15314  1151]
 [  576  4584]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Instrumentalness/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Instrumentalness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7880
Precision: 0.5305
Recall:    0.9698
F1-score:  0.6858
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12036  4429]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Instrumentalness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Instrumentalness/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9347, 'precision': 0.8239, 'recall': 0.9236, 'f1_score': 0.8709}
XGBoost: {'accuracy': 0.9201, 'precision': 0.7993, 'recall': 0.8884, 'f1_score': 0.8415}
Random Forest: {'accuracy': 0.8733, 'precision': 0.7103, 'recall': 0.7922, 'f1_score': 0.7491}
Decision Tree: {'accuracy': 0.8742, 'precision': 0.7221, 'recall': 0.7684, 'f1_score': 0.7445}
Naive Bayes: {'accuracy': 0.788, 'precision': 0.5305, 'recall': 0.9698, 'f1_score': 0.6858}
SVM: {'accuracy': 0.7972, 'precision': 0.5454, 'recall': 0.9014, 'f1_score': 0.6796}

##################################################
Running experiment without GOOD FOR PARTY feature
[Good for Party] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2974, Test Loss: 0.1807, F1: 0.8668, AUC: 0.9792
Epoch [10/30] Train Loss: 0.0804, Test Loss: 0.1863, F1: 0.8740, AUC: 0.9829
Epoch [20/30] Train Loss: 0.0511, Test Loss: 0.2557, F1: 0.8630, AUC: 0.9818
Mejores resultados en la época:  4
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [23:58:27] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [23:58:54] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Mejores resultados en la época:  6
f1-score 0.8817103911866305
AUC según el mejor F1-score 0.9835091938031578
Confusion Matrix:
 [[15636   829]
 [  438  4722]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Party/confusion_matrix_param_160769.png
Accuracy:   0.9414
Precision:  0.8507
Recall:     0.9151
F1-score:   0.8817

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2935, Test Loss: 0.1763, F1: 0.8676, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0889, Test Loss: 0.2043, F1: 0.8661, AUC: 0.9828
Epoch [20/30] Train Loss: 0.0763, Test Loss: 0.2160, F1: 0.8730, AUC: 0.9806
Mejores resultados en la época:  2
f1-score 0.8838769804287046
AUC según el mejor F1-score 0.9841255646814832
Confusion Matrix:
 [[15637   828]
 [  418  4742]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Party/confusion_matrix_param_160769.png
Accuracy:   0.9424
Precision:  0.8513
Recall:     0.9190
F1-score:   0.8839

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2970, Test Loss: 0.1835, F1: 0.8655, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0784, Test Loss: 0.1879, F1: 0.8755, AUC: 0.9829
Epoch [20/30] Train Loss: 0.0493, Test Loss: 0.2705, F1: 0.8582, AUC: 0.9813
Mejores resultados en la época:  5
f1-score 0.885520617586142
AUC según el mejor F1-score 0.9840584385012136
Confusion Matrix:
 [[15706   759]
 [  457  4703]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Party/confusion_matrix_param_160769.png
Accuracy:   0.9438
Precision:  0.8610
Recall:     0.9114
F1-score:   0.8855
Tiempo total para red 1: 281.40 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2260, Test Loss: 0.1548, F1: 0.8794, AUC: 0.9833
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.2796, F1: 0.8912, AUC: 0.9882
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5244, F1: 0.9061, AUC: 0.9865
Mejores resultados en la época:  16
f1-score 0.9128743679502139
AUC según el mejor F1-score 0.9883295138619153
Confusion Matrix:
 [[16035   430]
 [  466  4694]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Party/confusion_matrix_param_5842945.png
Accuracy:   0.9586
Precision:  0.9161
Recall:     0.9097
F1-score:   0.9129

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2240, Test Loss: 0.1719, F1: 0.8705, AUC: 0.9823
Epoch [10/30] Train Loss: 0.0044, Test Loss: 0.2886, F1: 0.9092, AUC: 0.9879
Epoch [20/30] Train Loss: 0.0008, Test Loss: 0.4754, F1: 0.8786, AUC: 0.9876
Mejores resultados en la época:  6
f1-score 0.9116492773044893
AUC según el mejor F1-score 0.9869941760417329
Confusion Matrix:
 [[15940   525]
 [  398  4762]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Party/confusion_matrix_param_5842945.png
Accuracy:   0.9573
Precision:  0.9007
Recall:     0.9229
F1-score:   0.9116

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2257, Test Loss: 0.1566, F1: 0.8871, AUC: 0.9826
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.2543, F1: 0.9092, AUC: 0.9884
Epoch [20/30] Train Loss: 0.0014, Test Loss: 0.3615, F1: 0.9051, AUC: 0.9885
Mejores resultados en la época:  18
f1-score 0.9125719136093559
AUC según el mejor F1-score 0.9881462498558136
Confusion Matrix:
 [[15860   605]
 [  322  4838]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Party/confusion_matrix_param_5842945.png
Accuracy:   0.9571
Precision:  0.8888
Recall:     0.9376
F1-score:   0.9126
Tiempo total para red 6: 365.92 segundos
Saved on: outputs_ablation_remove_one_feature/1/Good for Party

==============================
Model: Logistic Regression
Accuracy:  0.9365
Precision: 0.8273
Recall:    0.9273
F1-score:  0.8745
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15466   999]
 [  375  4785]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Party/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Party/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7640
Precision: 0.5032
Recall:    0.8593
F1-score:  0.6347
              precision    recall  f1-score   support

           0       0.94      0.73      0.83     16465
           1       0.50      0.86      0.63      5160

    accuracy                           0.76     21625
   macro avg       0.72      0.80      0.73     21625
weighted avg       0.84      0.76      0.78     21625

[[12087  4378]
 [  726  4434]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Party/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Party/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8703
Precision: 0.7091
Recall:    0.7738
F1-score:  0.7401
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14827  1638]
 [ 1167  3993]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Party/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Party/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8777
Precision: 0.7218
Recall:    0.7930
F1-score:  0.7557
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.72      0.79      0.76      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.85      0.84     21625
weighted avg       0.88      0.88      0.88     21625

[[14888  1577]
 [ 1068  4092]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Party/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Party/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9222
Precision: 0.8032
Recall:    0.8924
F1-score:  0.8455
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625
f1-score 0.8873810196965414
AUC según el mejor F1-score 0.9841016356047714
Confusion Matrix:
 [[15722   743]
 [  452  4708]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Party/confusion_matrix_param_160769.png
Accuracy:   0.9447
Precision:  0.8637
Recall:     0.9124
F1-score:   0.8874

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3010, Test Loss: 0.1727, F1: 0.8713, AUC: 0.9787
Epoch [10/30] Train Loss: 0.0733, Test Loss: 0.1998, F1: 0.8688, AUC: 0.9835
Epoch [20/30] Train Loss: 0.0380, Test Loss: 0.2450, F1: 0.8704, AUC: 0.9818
Mejores resultados en la época:  5
f1-score 0.8843767625493514
AUC según el mejor F1-score 0.9839687956835853
Confusion Matrix:
 [[15691   774]
 [  456  4704]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Party/confusion_matrix_param_160769.png
Accuracy:   0.9431
Precision:  0.8587
Recall:     0.9116
F1-score:   0.8844

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2944, Test Loss: 0.1834, F1: 0.8675, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0856, Test Loss: 0.1893, F1: 0.8691, AUC: 0.9829
Epoch [20/30] Train Loss: 0.0513, Test Loss: 0.2530, F1: 0.8578, AUC: 0.9813
Mejores resultados en la época:  5
f1-score 0.8820465116279069
AUC según el mejor F1-score 0.9838972556303363
Confusion Matrix:
 [[15616   849]
 [  419  4741]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Party/confusion_matrix_param_160769.png
Accuracy:   0.9414
Precision:  0.8481
Recall:     0.9188
F1-score:   0.8820
Tiempo total para red 1: 287.58 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2240, Test Loss: 0.1611, F1: 0.8717, AUC: 0.9827
Epoch [10/30] Train Loss: 0.0026, Test Loss: 0.3359, F1: 0.8631, AUC: 0.9885
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7816, F1: 0.9055, AUC: 0.9831
Mejores resultados en la época:  9
f1-score 0.9103854186443896
AUC según el mejor F1-score 0.9881311073289124
Confusion Matrix:
 [[15886   579]
 [  365  4795]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Party/confusion_matrix_param_5842945.png
Accuracy:   0.9563
Precision:  0.8923
Recall:     0.9293
F1-score:   0.9104

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2240, Test Loss: 0.1565, F1: 0.8767, AUC: 0.9824
Epoch [10/30] Train Loss: 0.0031, Test Loss: 0.4175, F1: 0.8993, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5517, F1: 0.8976, AUC: 0.9862
Mejores resultados en la época:  18
f1-score 0.9129487543809794
AUC según el mejor F1-score 0.9881699847221143
Confusion Matrix:
 [[15887   578]
 [  341  4819]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Party/confusion_matrix_param_5842945.png
Accuracy:   0.9575
Precision:  0.8929
Recall:     0.9339
F1-score:   0.9129

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2236, Test Loss: 0.1843, F1: 0.8602, AUC: 0.9834
Epoch [10/30] Train Loss: 0.0031, Test Loss: 0.2107, F1: 0.9075, AUC: 0.9880
Epoch [20/30] Train Loss: 0.0022, Test Loss: 0.3362, F1: 0.9019, AUC: 0.9878
Mejores resultados en la época:  11
f1-score 0.912477430390573
AUC según el mejor F1-score 0.9877708882124873
Confusion Matrix:
 [[15903   562]
 [  359  4801]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Party/confusion_matrix_param_5842945.png
Accuracy:   0.9574
Precision:  0.8952
Recall:     0.9304
F1-score:   0.9125
Tiempo total para red 6: 367.74 segundos
Saved on: outputs_ablation_remove_one_feature/1/Good for Party

==============================
Model: Logistic Regression
Accuracy:  0.9365
Precision: 0.8273
Recall:    0.9273
F1-score:  0.8745
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15466   999]
 [  375  4785]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Party/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Party/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7640
Precision: 0.5032
Recall:    0.8593
F1-score:  0.6347
              precision    recall  f1-score   support

           0       0.94      0.73      0.83     16465
           1       0.50      0.86      0.63      5160

    accuracy                           0.76     21625
   macro avg       0.72      0.80      0.73     21625
weighted avg       0.84      0.76      0.78     21625

[[12087  4378]
 [  726  4434]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Party/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Party/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8703
Precision: 0.7091
Recall:    0.7738
F1-score:  0.7401
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14827  1638]
 [ 1167  3993]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Party/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Party/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8777
Precision: 0.7218
Recall:    0.7930
F1-score:  0.7557
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.72      0.79      0.76      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.85      0.84     21625
weighted avg       0.88      0.88      0.88     21625

[[14888  1577]
 [ 1068  4092]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Party/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Party/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9222
Precision: 0.8032
Recall:    0.8924
F1-score:  0.8455
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[15337  1128]
 [  555  4605]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Party/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Party/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Party/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Party/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9365, 'precision': 0.8273, 'recall': 0.9273, 'f1_score': 0.8745}
XGBoost: {'accuracy': 0.9222, 'precision': 0.8032, 'recall': 0.8924, 'f1_score': 0.8455}
Random Forest: {'accuracy': 0.8777, 'precision': 0.7218, 'recall': 0.793, 'f1_score': 0.7557}
Decision Tree: {'accuracy': 0.8703, 'precision': 0.7091, 'recall': 0.7738, 'f1_score': 0.7401}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.764, 'precision': 0.5032, 'recall': 0.8593, 'f1_score': 0.6347}

##################################################
Running experiment without GOOD FOR WORK/STUDY feature
[Good for Work/Study] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2953, Test Loss: 0.1920, F1: 0.8620, AUC: 0.9798
Epoch [10/30] Train Loss: 0.0799, Test Loss: 0.1872, F1: 0.8741, AUC: 0.9829
Epoch [20/30] Train Loss: 0.0426, Test Loss: 0.2501, F1: 0.8676, AUC: 0.9820
Mejores resultados en la época:  1
f1-score 0.8850226928895613
AUC según el mejor F1-score 0.983753669399737
Confusion Matrix:
 [[15729   736]
 [  480  4680]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Work/Study/confusion_matrix_param_160769.png
Accuracy:   0.9438
Precision:  0.8641
Recall:     0.9070
F1-score:   0.8850

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2962, Test Loss: 0.1998, F1: 0.8560, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0863, Test Loss: 0.1843, F1: 0.8736, AUC: 0.9829
Epoch [20/30] Train Loss: 0.0507, Test Loss: 0.2457, F1: 0.8633, AUC: 0.9815
Mejores resultados en la época:  5
f1-score 0.8829149494192582
AUC según el mejor F1-score 0.983731664771644
Confusion Matrix:
 [[15662   803]
 [  447  4713]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Work/Study/confusion_matrix_param_160769.png
Accuracy:   0.9422
Precision:  0.8544
Recall:     0.9134
F1-score:   0.8829

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2935, Test Loss: 0.2001, F1: 0.8567, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0776, Test Loss: 0.1967, F1: 0.8680, AUC: 0.9829
Epoch [20/30] Train Loss: 0.0349, Test Loss: 0.2581, F1: 0.8664, AUC: 0.9819
Mejores resultados en la época:  2
f1-score 0.8838247604873966
AUC según el mejor F1-score 0.9842595522096437
Confusion Matrix:
 [[15625   840]
 [  409  4751]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Work/Study/confusion_matrix_param_160769.png
Accuracy:   0.9422
Precision:  0.8498
Recall:     0.9207
F1-score:   0.8838
Tiempo total para red 1: 281.72 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2231, Test Loss: 0.1411, F1: 0.8866, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0033, Test Loss: 0.4544, F1: 0.8878, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7018, F1: 0.9040, AUC: 0.9831
Mejores resultados en la época:  9
f1-score 0.9120847922778461
AUC según el mejor F1-score 0.9881968681511403
Confusion Matrix:
 [[15877   588]
 [  341  4819]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Work/Study/confusion_matrix_param_5842945.png
Accuracy:   0.9570
Precision:  0.8913
Recall:     0.9339
F1-score:   0.9121

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2252, Test Loss: 0.1765, F1: 0.8700, AUC: 0.9825
Epoch [10/30] Train Loss: 0.0055, Test Loss: 0.2660, F1: 0.8979, AUC: 0.9880
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5851, F1: 0.9040, AUC: 0.9852
Mejores resultados en la época:  6
f1-score 0.9083010671451506
AUC según el mejor F1-score 0.9873421304764394
Confusion Matrix:
 [[15845   620]
 [  351  4809]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Work/Study/confusion_matrix_param_5842945.png
Accuracy:   0.9551
Precision:  0.8858
Recall:     0.9320
F1-score:   0.9083

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2251, Test Loss: 0.1639, F1: 0.8787, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0041, Test Loss: 0.5661, F1: 0.8792, AUC: 0.9852
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5731, F1: 0.9078, AUC: 0.9857
Mejores resultados en la época:  29
f1-score 0.9105645612073784
AUC según el mejor F1-score 0.982942423086792
Confusion Matrix:
 [[15778   687]
 [  273  4887]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Work/Study/confusion_matrix_param_5842945.png
Accuracy:   0.9556
Precision:  0.8767
Recall:     0.9471
F1-score:   0.9106
Tiempo total para red 6: 365.96 segundos
Saved on: outputs_ablation_remove_one_feature/1/Good for Work/Study

==============================
Model: Logistic Regression
Accuracy:  0.9359
Precision: 0.8260
Recall:    0.9266
F1-score:  0.8734
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15458  1007]
 [  379  4781]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Work/Study/conf_matrix_logistic_regression.png

[[15337  1128]
 [  555  4605]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Party/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Party/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Party/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Party/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9365, 'precision': 0.8273, 'recall': 0.9273, 'f1_score': 0.8745}
XGBoost: {'accuracy': 0.9222, 'precision': 0.8032, 'recall': 0.8924, 'f1_score': 0.8455}
Random Forest: {'accuracy': 0.8777, 'precision': 0.7218, 'recall': 0.793, 'f1_score': 0.7557}
Decision Tree: {'accuracy': 0.8703, 'precision': 0.7091, 'recall': 0.7738, 'f1_score': 0.7401}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.764, 'precision': 0.5032, 'recall': 0.8593, 'f1_score': 0.6347}

##################################################
Running experiment without GOOD FOR WORK/STUDY feature
[Good for Work/Study] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2966, Test Loss: 0.2005, F1: 0.8559, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0881, Test Loss: 0.2016, F1: 0.8638, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0676, Test Loss: 0.2495, F1: 0.8606, AUC: 0.9808
Mejores resultados en la época:  3
f1-score 0.8837512818122495
AUC según el mejor F1-score 0.9843116005998158
Confusion Matrix:
 [[15638   827]
 [  420  4740]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Work/Study/confusion_matrix_param_160769.png
Accuracy:   0.9423
Precision:  0.8514
Recall:     0.9186
F1-score:   0.8838

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2911, Test Loss: 0.1911, F1: 0.8598, AUC: 0.9798
Epoch [10/30] Train Loss: 0.0881, Test Loss: 0.2023, F1: 0.8620, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0618, Test Loss: 0.2743, F1: 0.8511, AUC: 0.9810
Mejores resultados en la época:  5
f1-score 0.8822253653936822
AUC según el mejor F1-score 0.9833336393618598
Confusion Matrix:
 [[15698   767]
 [  482  4678]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Work/Study/confusion_matrix_param_160769.png
Accuracy:   0.9422
Precision:  0.8591
Recall:     0.9066
F1-score:   0.8822

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3014, Test Loss: 0.1818, F1: 0.8667, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0802, Test Loss: 0.1860, F1: 0.8766, AUC: 0.9829
Epoch [20/30] Train Loss: 0.0516, Test Loss: 0.2336, F1: 0.8715, AUC: 0.9813
Mejores resultados en la época:  4
f1-score 0.8825326298250485
AUC según el mejor F1-score 0.9843244891089156
Confusion Matrix:
 [[15589   876]
 [  393  4767]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Work/Study/confusion_matrix_param_160769.png
Accuracy:   0.9413
Precision:  0.8448
Recall:     0.9238
F1-score:   0.8825
Tiempo total para red 1: 287.49 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2256, Test Loss: 0.1537, F1: 0.8832, AUC: 0.9825
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.3849, F1: 0.8913, AUC: 0.9879
Epoch [20/30] Train Loss: 0.0005, Test Loss: 0.6402, F1: 0.8817, AUC: 0.9857
Mejores resultados en la época:  22
f1-score 0.911977186311787
AUC según el mejor F1-score 0.988559323629875
Confusion Matrix:
 [[15902   563]
 [  363  4797]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Work/Study/confusion_matrix_param_5842945.png
Accuracy:   0.9572
Precision:  0.8950
Recall:     0.9297
F1-score:   0.9120

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2249, Test Loss: 0.1749, F1: 0.8772, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.4504, F1: 0.8808, AUC: 0.9876
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6826, F1: 0.8904, AUC: 0.9863
Mejores resultados en la época:  7
f1-score 0.9138214527517168
AUC según el mejor F1-score 0.9874134939747692
Confusion Matrix:
 [[16010   455]
 [  436  4724]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Work/Study/confusion_matrix_param_5842945.png
Accuracy:   0.9588
Precision:  0.9121
Recall:     0.9155
F1-score:   0.9138

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2246, Test Loss: 0.1441, F1: 0.8878, AUC: 0.9827
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.3366, F1: 0.9005, AUC: 0.9882
Epoch [20/30] Train Loss: 0.0012, Test Loss: 0.6097, F1: 0.9013, AUC: 0.9839
Mejores resultados en la época:  11
f1-score 0.9076561766087365
AUC según el mejor F1-score 0.9873969154678587
Confusion Matrix:
 [[15811   654]
 [  329  4831]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Work/Study/confusion_matrix_param_5842945.png
Accuracy:   0.9545
Precision:  0.8808
Recall:     0.9362
F1-score:   0.9077
Tiempo total para red 6: 366.86 segundos
Saved on: outputs_ablation_remove_one_feature/1/Good for Work/Study

==============================
Model: Logistic Regression
Accuracy:  0.9359
Precision: 0.8260
Recall:    0.9266
F1-score:  0.8734
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15458  1007]
 [  379  4781]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Work/Study/conf_matrix_logistic_regression.png
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [00:25:08] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [00:25:33] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Work/Study/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7694
Precision: 0.5093
Recall:    0.9178
F1-score:  0.6551
              precision    recall  f1-score   support

           0       0.97      0.72      0.83     16465
           1       0.51      0.92      0.66      5160

    accuracy                           0.77     21625
   macro avg       0.74      0.82      0.74     21625
weighted avg       0.86      0.77      0.79     21625

[[11902  4563]
 [  424  4736]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Work/Study/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Work/Study/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8703
Precision: 0.7095
Recall:    0.7729
F1-score:  0.7398
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14832  1633]
 [ 1172  3988]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Work/Study/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Work/Study/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8768
Precision: 0.7214
Recall:    0.7882
F1-score:  0.7533
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.72      0.79      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.85      0.84     21625
weighted avg       0.88      0.88      0.88     21625

[[14894  1571]
 [ 1093  4067]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Work/Study/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Work/Study/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9218
Precision: 0.8022
Recall:    0.8921
F1-score:  0.8447
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15330  1135]
 [  557  4603]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Work/Study/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Work/Study/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7881
Precision: 0.5306
Recall:    0.9698
F1-score:  0.6859
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12038  4427]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Work/Study/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Work/Study/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9359, 'precision': 0.826, 'recall': 0.9266, 'f1_score': 0.8734}
XGBoost: {'accuracy': 0.9218, 'precision': 0.8022, 'recall': 0.8921, 'f1_score': 0.8447}
Random Forest: {'accuracy': 0.8768, 'precision': 0.7214, 'recall': 0.7882, 'f1_score': 0.7533}
Decision Tree: {'accuracy': 0.8703, 'precision': 0.7095, 'recall': 0.7729, 'f1_score': 0.7398}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.7694, 'precision': 0.5093, 'recall': 0.9178, 'f1_score': 0.6551}

##################################################
Running experiment without GOOD FOR RELAXATION/MEDITATION feature
[Good for Relaxation/Meditation] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2944, Test Loss: 0.1985, F1: 0.8568, AUC: 0.9798
Epoch [10/30] Train Loss: 0.0876, Test Loss: 0.1817, F1: 0.8780, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0652, Test Loss: 0.2502, F1: 0.8604, AUC: 0.9809
Mejores resultados en la época:  1
f1-score 0.8806011689396048
AUC según el mejor F1-score 0.9835066043310099
Confusion Matrix:
 [[15592   873]
 [  414  4746]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/confusion_matrix_param_160769.png
Accuracy:   0.9405
Precision:  0.8446
Recall:     0.9198
F1-score:   0.8806

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3046, Test Loss: 0.1841, F1: 0.8637, AUC: 0.9791
Epoch [10/30] Train Loss: 0.0863, Test Loss: 0.1843, F1: 0.8715, AUC: 0.9827
Epoch [20/30] Train Loss: 0.0565, Test Loss: 0.2315, F1: 0.8668, AUC: 0.9811
Mejores resultados en la época:  7
f1-score 0.87893864013267
AUC según el mejor F1-score 0.983459393545623
Confusion Matrix:
 [[15541   924]
 [  390  4770]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/confusion_matrix_param_160769.png
Accuracy:   0.9392
Precision:  0.8377
Recall:     0.9244
F1-score:   0.8789

--- Iteración 3 de 3 para la red 1 ---
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Work/Study/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7694
Precision: 0.5093
Recall:    0.9178
F1-score:  0.6551
              precision    recall  f1-score   support

           0       0.97      0.72      0.83     16465
           1       0.51      0.92      0.66      5160

    accuracy                           0.77     21625
   macro avg       0.74      0.82      0.74     21625
weighted avg       0.86      0.77      0.79     21625

[[11902  4563]
 [  424  4736]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Work/Study/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Work/Study/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8703
Precision: 0.7095
Recall:    0.7729
F1-score:  0.7398
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14832  1633]
 [ 1172  3988]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Work/Study/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Work/Study/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8768
Precision: 0.7214
Recall:    0.7882
F1-score:  0.7533
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.72      0.79      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.85      0.84     21625
weighted avg       0.88      0.88      0.88     21625

[[14894  1571]
 [ 1093  4067]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Work/Study/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Work/Study/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9218
Precision: 0.8022
Recall:    0.8921
F1-score:  0.8447
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15330  1135]
 [  557  4603]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Work/Study/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Work/Study/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7881
Precision: 0.5306
Recall:    0.9698
F1-score:  0.6859
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12038  4427]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Work/Study/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Work/Study/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9359, 'precision': 0.826, 'recall': 0.9266, 'f1_score': 0.8734}
XGBoost: {'accuracy': 0.9218, 'precision': 0.8022, 'recall': 0.8921, 'f1_score': 0.8447}
Random Forest: {'accuracy': 0.8768, 'precision': 0.7214, 'recall': 0.7882, 'f1_score': 0.7533}
Decision Tree: {'accuracy': 0.8703, 'precision': 0.7095, 'recall': 0.7729, 'f1_score': 0.7398}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.7694, 'precision': 0.5093, 'recall': 0.9178, 'f1_score': 0.6551}

##################################################
Running experiment without GOOD FOR RELAXATION/MEDITATION feature
[Good for Relaxation/Meditation] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2960, Test Loss: 0.1793, F1: 0.8677, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0869, Test Loss: 0.1910, F1: 0.8691, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0582, Test Loss: 0.2396, F1: 0.8637, AUC: 0.9810
Mejores resultados en la época:  4
f1-score 0.8835944095300629
AUC según el mejor F1-score 0.9838394338943073
Confusion Matrix:
 [[15674   791]
 [  450  4710]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/confusion_matrix_param_160769.png
Accuracy:   0.9426
Precision:  0.8562
Recall:     0.9128
F1-score:   0.8836

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2918, Test Loss: 0.1897, F1: 0.8602, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0880, Test Loss: 0.1879, F1: 0.8721, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0640, Test Loss: 0.2503, F1: 0.8591, AUC: 0.9810
Mejores resultados en la época:  2
f1-score 0.8819205359635247
AUC según el mejor F1-score 0.9840957798666186
Confusion Matrix:
 [[15617   848]
 [  421  4739]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/confusion_matrix_param_160769.png
Accuracy:   0.9413
Precision:  0.8482
Recall:     0.9184
F1-score:   0.8819

--- Iteración 3 de 3 para la red 1 ---
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [00:51:51] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [00:52:18] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [0/30] Train Loss: 0.2980, Test Loss: 0.1808, F1: 0.8663, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0873, Test Loss: 0.1897, F1: 0.8709, AUC: 0.9827
Epoch [20/30] Train Loss: 0.0566, Test Loss: 0.2532, F1: 0.8583, AUC: 0.9813
Mejores resultados en la época:  5
f1-score 0.8843409688321442
AUC según el mejor F1-score 0.9837893217230819
Confusion Matrix:
 [[15683   782]
 [  450  4710]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/confusion_matrix_param_160769.png
Accuracy:   0.9430
Precision:  0.8576
Recall:     0.9128
F1-score:   0.8843
Tiempo total para red 1: 281.41 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2250, Test Loss: 0.1535, F1: 0.8785, AUC: 0.9835
Epoch [10/30] Train Loss: 0.0055, Test Loss: 0.5220, F1: 0.8702, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0007, Test Loss: 0.4716, F1: 0.8981, AUC: 0.9871
Mejores resultados en la época:  13
f1-score 0.9108574662767663
AUC según el mejor F1-score 0.9874672078663456
Confusion Matrix:
 [[15852   613]
 [  332  4828]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/confusion_matrix_param_5842945.png
Accuracy:   0.9563
Precision:  0.8873
Recall:     0.9357
F1-score:   0.9109

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2220, Test Loss: 0.1555, F1: 0.8797, AUC: 0.9834
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.5008, F1: 0.9016, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5264, F1: 0.9010, AUC: 0.9867
Mejores resultados en la época:  5
f1-score 0.9147452044177485
AUC según el mejor F1-score 0.9875906550658314
Confusion Matrix:
 [[16024   441]
 [  439  4721]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/confusion_matrix_param_5842945.png
Accuracy:   0.9593
Precision:  0.9146
Recall:     0.9149
F1-score:   0.9147

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2227, Test Loss: 0.1828, F1: 0.8630, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0033, Test Loss: 0.3719, F1: 0.8980, AUC: 0.9879
Epoch [20/30] Train Loss: 0.0021, Test Loss: 0.2258, F1: 0.9099, AUC: 0.9887
Mejores resultados en la época:  24
f1-score 0.9141983565832219
AUC según el mejor F1-score 0.9881088731794245
Confusion Matrix:
 [[15943   522]
 [  376  4784]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/confusion_matrix_param_5842945.png
Accuracy:   0.9585
Precision:  0.9016
Recall:     0.9271
F1-score:   0.9142
Tiempo total para red 6: 366.13 segundos
Saved on: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation

==============================
Model: Logistic Regression
Accuracy:  0.9364
Precision: 0.8278
Recall:    0.9260
F1-score:  0.8741
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15471   994]
 [  382  4778]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8134
Precision: 0.5698
Recall:    0.8895
F1-score:  0.6946
              precision    recall  f1-score   support

           0       0.96      0.79      0.87     16465
           1       0.57      0.89      0.69      5160

    accuracy                           0.81     21625
   macro avg       0.76      0.84      0.78     21625
weighted avg       0.87      0.81      0.82     21625

[[12999  3466]
 [  570  4590]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8709
Precision: 0.7110
Recall:    0.7736
F1-score:  0.7410
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14842  1623]
 [ 1168  3992]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8766
Precision: 0.7199
Recall:    0.7903
F1-score:  0.7534
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.72      0.79      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.85      0.84     21625
weighted avg       0.88      0.88      0.88     21625

[[14878  1587]
 [ 1082  4078]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9218
Precision: 0.8023
Recall:    0.8924
F1-score:  0.8450
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15330  1135]
 [  555  4605]]
Epoch [0/30] Train Loss: 0.2943, Test Loss: 0.1766, F1: 0.8688, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0760, Test Loss: 0.1980, F1: 0.8687, AUC: 0.9832
Epoch [20/30] Train Loss: 0.0388, Test Loss: 0.2392, F1: 0.8730, AUC: 0.9819
Mejores resultados en la época:  4
f1-score 0.8833240586162122
AUC según el mejor F1-score 0.9843226823635761
Confusion Matrix:
 [[15605   860]
 [  398  4762]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/confusion_matrix_param_160769.png
Accuracy:   0.9418
Precision:  0.8470
Recall:     0.9229
F1-score:   0.8833
Tiempo total para red 1: 287.45 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2239, Test Loss: 0.1533, F1: 0.8833, AUC: 0.9835
Epoch [10/30] Train Loss: 0.0025, Test Loss: 0.4448, F1: 0.8927, AUC: 0.9870
Epoch [20/30] Train Loss: 0.0027, Test Loss: 0.3832, F1: 0.9105, AUC: 0.9876
Mejores resultados en la época:  20
f1-score 0.910467537384062
AUC según el mejor F1-score 0.9876452105358561
Confusion Matrix:
 [[15869   596]
 [  350  4810]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/confusion_matrix_param_5842945.png
Accuracy:   0.9563
Precision:  0.8898
Recall:     0.9322
F1-score:   0.9105

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2231, Test Loss: 0.1575, F1: 0.8790, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0047, Test Loss: 0.5048, F1: 0.8868, AUC: 0.9864
Epoch [20/30] Train Loss: 0.0016, Test Loss: 0.3908, F1: 0.8955, AUC: 0.9875
Mejores resultados en la época:  17
f1-score 0.9133767727098505
AUC según el mejor F1-score 0.987983631004927
Confusion Matrix:
 [[15955   510]
 [  394  4766]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/confusion_matrix_param_5842945.png
Accuracy:   0.9582
Precision:  0.9033
Recall:     0.9236
F1-score:   0.9134

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2266, Test Loss: 0.1496, F1: 0.8827, AUC: 0.9832
Epoch [10/30] Train Loss: 0.0060, Test Loss: 0.3368, F1: 0.8937, AUC: 0.9879
Epoch [20/30] Train Loss: 0.0028, Test Loss: 0.3865, F1: 0.9004, AUC: 0.9870
Mejores resultados en la época:  11
f1-score 0.9084373817631479
AUC según el mejor F1-score 0.9872343083873003
Confusion Matrix:
 [[15855   610]
 [  358  4802]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/confusion_matrix_param_5842945.png
Accuracy:   0.9552
Precision:  0.8873
Recall:     0.9306
F1-score:   0.9084
Tiempo total para red 6: 367.32 segundos
Saved on: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation

==============================
Model: Logistic Regression
Accuracy:  0.9364
Precision: 0.8278
Recall:    0.9260
F1-score:  0.8741
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15471   994]
 [  382  4778]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8134
Precision: 0.5698
Recall:    0.8895
F1-score:  0.6946
              precision    recall  f1-score   support

           0       0.96      0.79      0.87     16465
           1       0.57      0.89      0.69      5160

    accuracy                           0.81     21625
   macro avg       0.76      0.84      0.78     21625
weighted avg       0.87      0.81      0.82     21625

[[12999  3466]
 [  570  4590]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8709
Precision: 0.7110
Recall:    0.7736
F1-score:  0.7410
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14842  1623]
 [ 1168  3992]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8766
Precision: 0.7199
Recall:    0.7903
F1-score:  0.7534
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.72      0.79      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.85      0.84     21625
weighted avg       0.88      0.88      0.88     21625

[[14878  1587]
 [ 1082  4078]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9218
Precision: 0.8023
Recall:    0.8924
F1-score:  0.8450
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15330  1135]
 [  555  4605]]
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7881
Precision: 0.5306
Recall:    0.9698
F1-score:  0.6859
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12038  4427]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9364, 'precision': 0.8278, 'recall': 0.926, 'f1_score': 0.8741}
XGBoost: {'accuracy': 0.9218, 'precision': 0.8023, 'recall': 0.8924, 'f1_score': 0.845}
Random Forest: {'accuracy': 0.8766, 'precision': 0.7199, 'recall': 0.7903, 'f1_score': 0.7534}
Decision Tree: {'accuracy': 0.8709, 'precision': 0.711, 'recall': 0.7736, 'f1_score': 0.741}
SVM: {'accuracy': 0.8134, 'precision': 0.5698, 'recall': 0.8895, 'f1_score': 0.6946}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}

##################################################
Running experiment without GOOD FOR EXERCISE feature
[Good for Exercise] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2929, Test Loss: 0.1950, F1: 0.8589, AUC: 0.9800
Epoch [10/30] Train Loss: 0.0867, Test Loss: 0.2022, F1: 0.8627, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0551, Test Loss: 0.2220, F1: 0.8725, AUC: 0.9812
Mejores resultados en la época:  1
f1-score 0.8822041045825133
AUC según el mejor F1-score 0.9835281146053292
Confusion Matrix:
 [[15661   804]
 [  453  4707]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Exercise/confusion_matrix_param_160769.png
Accuracy:   0.9419
Precision:  0.8541
Recall:     0.9122
F1-score:   0.8822

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2935, Test Loss: 0.1804, F1: 0.8668, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0880, Test Loss: 0.1835, F1: 0.8718, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0584, Test Loss: 0.2372, F1: 0.8658, AUC: 0.9809
Mejores resultados en la época:  2
f1-score 0.8796833287305532
AUC según el mejor F1-score 0.9843535441634476
Confusion Matrix:
 [[15540   925]
 [  382  4778]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Exercise/confusion_matrix_param_160769.png
Accuracy:   0.9396
Precision:  0.8378
Recall:     0.9260
F1-score:   0.8797

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2968, Test Loss: 0.2033, F1: 0.8547, AUC: 0.9801
Epoch [10/30] Train Loss: 0.0869, Test Loss: 0.1852, F1: 0.8718, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0560, Test Loss: 0.2444, F1: 0.8606, AUC: 0.9813
Mejores resultados en la época:  5
f1-score 0.8803765922097102
AUC según el mejor F1-score 0.9838525813506215
Confusion Matrix:
 [[15560   905]
 [  391  4769]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Exercise/confusion_matrix_param_160769.png
Accuracy:   0.9401
Precision:  0.8405
Recall:     0.9242
F1-score:   0.8804
Tiempo total para red 1: 280.78 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2275, Test Loss: 0.1532, F1: 0.8784, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0033, Test Loss: 0.2876, F1: 0.9106, AUC: 0.9882
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8216, F1: 0.8960, AUC: 0.9818
Mejores resultados en la época:  10
f1-score 0.9105922551252847
AUC según el mejor F1-score 0.9882028298222445
Confusion Matrix:
 [[15886   579]
 [  363  4797]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Exercise/confusion_matrix_param_5842945.png
Accuracy:   0.9564
Precision:  0.8923
Recall:     0.9297
F1-score:   0.9106

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2230, Test Loss: 0.1897, F1: 0.8668, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.5070, F1: 0.8933, AUC: 0.9852
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8136, F1: 0.8922, AUC: 0.9823
Mejores resultados en la época:  5
f1-score 0.9059622360710632
AUC según el mejor F1-score 0.9868641609992538
Confusion Matrix:
 [[15744   721]
 [  290  4870]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Exercise/confusion_matrix_param_5842945.png
Accuracy:   0.9532
Precision:  0.8710
Recall:     0.9438
F1-score:   0.9060

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2238, Test Loss: 0.1686, F1: 0.8758, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0044, Test Loss: 0.2675, F1: 0.9107, AUC: 0.9879
Epoch [20/30] Train Loss: 0.0020, Test Loss: 0.4745, F1: 0.9073, AUC: 0.9879
Mejores resultados en la época:  26
f1-score 0.9139387038618465
AUC según el mejor F1-score 0.9882904069473184
Confusion Matrix:
 [[15902   563]
 [  344  4816]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Exercise/confusion_matrix_param_5842945.png
Accuracy:   0.9581
Precision:  0.8953
Recall:     0.9333
F1-score:   0.9139
Tiempo total para red 6: 368.05 segundos
Saved on: outputs_ablation_remove_one_feature/1/Good for Exercise

==============================
Model: Logistic Regression
Accuracy:  0.9364
Precision: 0.8277
Recall:    0.9262
F1-score:  0.8742
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15470   995]
 [  381  4779]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Exercise/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Exercise/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8729
Precision: 0.7014
Recall:    0.8140
F1-score:  0.7535
              precision    recall  f1-score   support

           0       0.94      0.89      0.91     16465
           1       0.70      0.81      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.87      0.88     21625

[[14677  1788]
 [  960  4200]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Exercise/conf_matrix_svm.png
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7881
Precision: 0.5306
Recall:    0.9698
F1-score:  0.6859
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12038  4427]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Relaxation/Meditation/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9364, 'precision': 0.8278, 'recall': 0.926, 'f1_score': 0.8741}
XGBoost: {'accuracy': 0.9218, 'precision': 0.8023, 'recall': 0.8924, 'f1_score': 0.845}
Random Forest: {'accuracy': 0.8766, 'precision': 0.7199, 'recall': 0.7903, 'f1_score': 0.7534}
Decision Tree: {'accuracy': 0.8709, 'precision': 0.711, 'recall': 0.7736, 'f1_score': 0.741}
SVM: {'accuracy': 0.8134, 'precision': 0.5698, 'recall': 0.8895, 'f1_score': 0.6946}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}

##################################################
Running experiment without GOOD FOR EXERCISE feature
[Good for Exercise] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2930, Test Loss: 0.1830, F1: 0.8669, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0884, Test Loss: 0.2047, F1: 0.8637, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0708, Test Loss: 0.2394, F1: 0.8657, AUC: 0.9805
Mejores resultados en la época:  1
f1-score 0.879445727482679
AUC según el mejor F1-score 0.9834697749748702
Confusion Matrix:
 [[15560   905]
 [  400  4760]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Exercise/confusion_matrix_param_160769.png
Accuracy:   0.9397
Precision:  0.8402
Recall:     0.9225
F1-score:   0.8794

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2951, Test Loss: 0.1912, F1: 0.8612, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0839, Test Loss: 0.2053, F1: 0.8646, AUC: 0.9828
Epoch [20/30] Train Loss: 0.0414, Test Loss: 0.2526, F1: 0.8694, AUC: 0.9819
Mejores resultados en la época:  4
f1-score 0.8840458228555462
AUC según el mejor F1-score 0.9838539055125154
Confusion Matrix:
 [[15634   831]
 [  414  4746]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Exercise/confusion_matrix_param_160769.png
Accuracy:   0.9424
Precision:  0.8510
Recall:     0.9198
F1-score:   0.8840

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2945, Test Loss: 0.1963, F1: 0.8574, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0877, Test Loss: 0.2140, F1: 0.8598, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0631, Test Loss: 0.2447, F1: 0.8617, AUC: 0.9807
Mejores resultados en la época:  6
f1-score 0.8827431561244511
AUC según el mejor F1-score 0.9833963987504619
Confusion Matrix:
 [[15646   819]
 [  436  4724]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Exercise/confusion_matrix_param_160769.png
Accuracy:   0.9420
Precision:  0.8522
Recall:     0.9155
F1-score:   0.8827
Tiempo total para red 1: 287.13 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2290, Test Loss: 0.1512, F1: 0.8838, AUC: 0.9825
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.2191, F1: 0.9026, AUC: 0.9880
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7636, F1: 0.8955, AUC: 0.9834
Mejores resultados en la época:  16
f1-score 0.9111006140765234
AUC según el mejor F1-score 0.9874635178685349
Confusion Matrix:
 [[15862   603]
 [  338  4822]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Exercise/confusion_matrix_param_5842945.png
Accuracy:   0.9565
Precision:  0.8888
Recall:     0.9345
F1-score:   0.9111

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2269, Test Loss: 0.1613, F1: 0.8744, AUC: 0.9834
Epoch [10/30] Train Loss: 0.0053, Test Loss: 0.2800, F1: 0.8946, AUC: 0.9881
Epoch [20/30] Train Loss: 0.0029, Test Loss: 0.4108, F1: 0.9124, AUC: 0.9879
Mejores resultados en la época:  20
f1-score 0.9124363087374976
AUC según el mejor F1-score 0.9879043637313824
Confusion Matrix:
 [[15862   603]
 [  325  4835]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Exercise/confusion_matrix_param_5842945.png
Accuracy:   0.9571
Precision:  0.8891
Recall:     0.9370
F1-score:   0.9124

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2221, Test Loss: 0.2053, F1: 0.8489, AUC: 0.9832
Epoch [10/30] Train Loss: 0.0037, Test Loss: 0.3604, F1: 0.8960, AUC: 0.9878
Epoch [20/30] Train Loss: 0.0011, Test Loss: 0.3800, F1: 0.9048, AUC: 0.9884
Mejores resultados en la época:  11
f1-score 0.9105307815593476
AUC según el mejor F1-score 0.9884020073117276
Confusion Matrix:
 [[15847   618]
 [  331  4829]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Exercise/confusion_matrix_param_5842945.png
Accuracy:   0.9561
Precision:  0.8865
Recall:     0.9359
F1-score:   0.9105
Tiempo total para red 6: 369.46 segundos
Saved on: outputs_ablation_remove_one_feature/1/Good for Exercise

==============================
Model: Logistic Regression
Accuracy:  0.9364
Precision: 0.8277
Recall:    0.9262
F1-score:  0.8742
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15470   995]
 [  381  4779]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Exercise/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Exercise/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8729
Precision: 0.7014
Recall:    0.8140
F1-score:  0.7535
              precision    recall  f1-score   support

           0       0.94      0.89      0.91     16465
           1       0.70      0.81      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.87      0.88     21625

[[14677  1788]
 [  960  4200]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Exercise/conf_matrix_svm.png
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [01:18:36] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [01:19:03] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Exercise/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8707
Precision: 0.7103
Recall:    0.7733
F1-score:  0.7405
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14838  1627]
 [ 1170  3990]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Exercise/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Exercise/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8750
Precision: 0.7148
Recall:    0.7922
F1-score:  0.7515
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.71      0.79      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.88      0.88     21625

[[14834  1631]
 [ 1072  4088]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Exercise/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Exercise/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9213
Precision: 0.8020
Recall:    0.8901
F1-score:  0.8438
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15331  1134]
 [  567  4593]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Exercise/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Exercise/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7877
Precision: 0.5302
Recall:    0.9698
F1-score:  0.6856
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12031  4434]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Exercise/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Exercise/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9364, 'precision': 0.8277, 'recall': 0.9262, 'f1_score': 0.8742}
XGBoost: {'accuracy': 0.9213, 'precision': 0.802, 'recall': 0.8901, 'f1_score': 0.8438}
SVM: {'accuracy': 0.8729, 'precision': 0.7014, 'recall': 0.814, 'f1_score': 0.7535}
Random Forest: {'accuracy': 0.875, 'precision': 0.7148, 'recall': 0.7922, 'f1_score': 0.7515}
Decision Tree: {'accuracy': 0.8707, 'precision': 0.7103, 'recall': 0.7733, 'f1_score': 0.7405}
Naive Bayes: {'accuracy': 0.7877, 'precision': 0.5302, 'recall': 0.9698, 'f1_score': 0.6856}

##################################################
Running experiment without GOOD FOR RUNNING feature
[Good for Running] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2947, Test Loss: 0.1700, F1: 0.8697, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0891, Test Loss: 0.1963, F1: 0.8713, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0767, Test Loss: 0.2200, F1: 0.8703, AUC: 0.9806
Mejores resultados en la época:  11
f1-score 0.8790270169900659
AUC según el mejor F1-score 0.9823427543038205
Confusion Matrix:
 [[15588   877]
 [  426  4734]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Running/confusion_matrix_param_160769.png
Accuracy:   0.9397
Precision:  0.8437
Recall:     0.9174
F1-score:   0.8790

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2921, Test Loss: 0.1825, F1: 0.8644, AUC: 0.9799
Epoch [10/30] Train Loss: 0.0848, Test Loss: 0.1962, F1: 0.8681, AUC: 0.9827
Epoch [20/30] Train Loss: 0.0461, Test Loss: 0.2759, F1: 0.8523, AUC: 0.9814
Mejores resultados en la época:  1
f1-score 0.8817184129068568
AUC según el mejor F1-score 0.9835667683623001
Confusion Matrix:
 [[15664   801]
 [  460  4700]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Running/confusion_matrix_param_160769.png
Accuracy:   0.9417
Precision:  0.8544
Recall:     0.9109
F1-score:   0.8817

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2947, Test Loss: 0.1957, F1: 0.8586, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0880, Test Loss: 0.1950, F1: 0.8674, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0611, Test Loss: 0.2310, F1: 0.8711, AUC: 0.9812
Mejores resultados en la época:  2
f1-score 0.8832543884090276
AUC según el mejor F1-score 0.9842729233021891
Confusion Matrix:
 [[15613   852]
 [  405  4755]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Running/confusion_matrix_param_160769.png
Accuracy:   0.9419
Precision:  0.8480
Recall:     0.9215
F1-score:   0.8833
Tiempo total para red 1: 281.91 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2207, Test Loss: 0.1471, F1: 0.8806, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0014, Test Loss: 0.3687, F1: 0.9037, AUC: 0.9876
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6959, F1: 0.9057, AUC: 0.9831
Mejores resultados en la época:  29
f1-score 0.9114593165942987
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Exercise/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8707
Precision: 0.7103
Recall:    0.7733
F1-score:  0.7405
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14838  1627]
 [ 1170  3990]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Exercise/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Exercise/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8750
Precision: 0.7148
Recall:    0.7922
F1-score:  0.7515
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.71      0.79      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.88      0.88     21625

[[14834  1631]
 [ 1072  4088]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Exercise/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Exercise/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9213
Precision: 0.8020
Recall:    0.8901
F1-score:  0.8438
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15331  1134]
 [  567  4593]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Exercise/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Exercise/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7877
Precision: 0.5302
Recall:    0.9698
F1-score:  0.6856
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12031  4434]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Exercise/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Exercise/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9364, 'precision': 0.8277, 'recall': 0.9262, 'f1_score': 0.8742}
XGBoost: {'accuracy': 0.9213, 'precision': 0.802, 'recall': 0.8901, 'f1_score': 0.8438}
SVM: {'accuracy': 0.8729, 'precision': 0.7014, 'recall': 0.814, 'f1_score': 0.7535}
Random Forest: {'accuracy': 0.875, 'precision': 0.7148, 'recall': 0.7922, 'f1_score': 0.7515}
Decision Tree: {'accuracy': 0.8707, 'precision': 0.7103, 'recall': 0.7733, 'f1_score': 0.7405}
Naive Bayes: {'accuracy': 0.7877, 'precision': 0.5302, 'recall': 0.9698, 'f1_score': 0.6856}

##################################################
Running experiment without GOOD FOR RUNNING feature
[Good for Running] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2942, Test Loss: 0.1880, F1: 0.8637, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0859, Test Loss: 0.2147, F1: 0.8581, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0536, Test Loss: 0.2460, F1: 0.8642, AUC: 0.9812
Mejores resultados en la época:  2
f1-score 0.8756953944368445
AUC según el mejor F1-score 0.9842628773272882
Confusion Matrix:
 [[15461  1004]
 [  359  4801]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Running/confusion_matrix_param_160769.png
Accuracy:   0.9370
Precision:  0.8270
Recall:     0.9304
F1-score:   0.8757

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3003, Test Loss: 0.1835, F1: 0.8646, AUC: 0.9789
Epoch [10/30] Train Loss: 0.0884, Test Loss: 0.2063, F1: 0.8641, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0751, Test Loss: 0.2312, F1: 0.8683, AUC: 0.9808
Mejores resultados en la época:  2
f1-score 0.8829915560916767
AUC según el mejor F1-score 0.984162964898528
Confusion Matrix:
 [[15606   859]
 [  402  4758]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Running/confusion_matrix_param_160769.png
Accuracy:   0.9417
Precision:  0.8471
Recall:     0.9221
F1-score:   0.8830

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2985, Test Loss: 0.1854, F1: 0.8660, AUC: 0.9798
Epoch [10/30] Train Loss: 0.0880, Test Loss: 0.1998, F1: 0.8686, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0739, Test Loss: 0.2591, F1: 0.8577, AUC: 0.9808
Mejores resultados en la época:  6
f1-score 0.8831731675146034
AUC según el mejor F1-score 0.9834202748606982
Confusion Matrix:
 [[15698   767]
 [  473  4687]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Running/confusion_matrix_param_160769.png
Accuracy:   0.9427
Precision:  0.8594
Recall:     0.9083
F1-score:   0.8832
Tiempo total para red 1: 288.60 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2220, Test Loss: 0.1558, F1: 0.8821, AUC: 0.9823
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.4569, F1: 0.8930, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5546, F1: 0.9019, AUC: 0.9863
Mejores resultados en la época:  15
f1-score 0.9075692765482611
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [01:45:19] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [01:45:46] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
AUC según el mejor F1-score 0.9830404934592287
Confusion Matrix:
 [[15859   606]
 [  332  4828]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Running/confusion_matrix_param_5842945.png
Accuracy:   0.9566
Precision:  0.8885
Recall:     0.9357
F1-score:   0.9115

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2256, Test Loss: 0.1624, F1: 0.8792, AUC: 0.9832
Epoch [10/30] Train Loss: 0.0046, Test Loss: 0.3836, F1: 0.9055, AUC: 0.9870
Epoch [20/30] Train Loss: 0.0009, Test Loss: 0.5829, F1: 0.8976, AUC: 0.9844
Mejores resultados en la época:  23
f1-score 0.9134994707976523
AUC según el mejor F1-score 0.987861072465201
Confusion Matrix:
 [[15979   486]
 [  413  4747]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Running/confusion_matrix_param_5842945.png
Accuracy:   0.9584
Precision:  0.9071
Recall:     0.9200
F1-score:   0.9135

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2218, Test Loss: 0.1674, F1: 0.8568, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0044, Test Loss: 0.3749, F1: 0.9130, AUC: 0.9880
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6242, F1: 0.8973, AUC: 0.9867
Mejores resultados en la época:  17
f1-score 0.9156214367160775
AUC según el mejor F1-score 0.9879361730426532
Confusion Matrix:
 [[15919   546]
 [  342  4818]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Running/confusion_matrix_param_5842945.png
Accuracy:   0.9589
Precision:  0.8982
Recall:     0.9337
F1-score:   0.9156
Tiempo total para red 6: 366.85 segundos
Saved on: outputs_ablation_remove_one_feature/1/Good for Running

==============================
Model: Logistic Regression
Accuracy:  0.9360
Precision: 0.8266
Recall:    0.9260
F1-score:  0.8735
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15463  1002]
 [  382  4778]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Running/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Running/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8250
Precision: 0.5912
Recall:    0.8640
F1-score:  0.7020
              precision    recall  f1-score   support

           0       0.95      0.81      0.88     16465
           1       0.59      0.86      0.70      5160

    accuracy                           0.83     21625
   macro avg       0.77      0.84      0.79     21625
weighted avg       0.86      0.83      0.83     21625

[[13383  3082]
 [  702  4458]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Running/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Running/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8709
Precision: 0.7108
Recall:    0.7736
F1-score:  0.7409
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14841  1624]
 [ 1168  3992]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Running/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Running/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8747
Precision: 0.7144
Recall:    0.7915
F1-score:  0.7509
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.71      0.79      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.87      0.88     21625

[[14832  1633]
 [ 1076  4084]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Running/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Running/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9218
Precision: 0.8016
Recall:    0.8932
F1-score:  0.8449
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15324  1141]
 [  551  4609]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Running/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Running/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Running/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Running/naive_bayes_model.pkl


Resumen de métricas:
AUC según el mejor F1-score 0.9882015645119904
Confusion Matrix:
 [[15751   714]
 [  280  4880]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Running/confusion_matrix_param_5842945.png
Accuracy:   0.9540
Precision:  0.8724
Recall:     0.9457
F1-score:   0.9076

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2263, Test Loss: 0.1948, F1: 0.8666, AUC: 0.9833
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.4699, F1: 0.8985, AUC: 0.9867
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5770, F1: 0.9072, AUC: 0.9859
Mejores resultados en la época:  7
f1-score 0.9135259230916396
AUC según el mejor F1-score 0.9875145952066517
Confusion Matrix:
 [[15946   519]
 [  385  4775]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Running/confusion_matrix_param_5842945.png
Accuracy:   0.9582
Precision:  0.9020
Recall:     0.9254
F1-score:   0.9135

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2247, Test Loss: 0.1792, F1: 0.8673, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0045, Test Loss: 0.4461, F1: 0.8895, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5958, F1: 0.9026, AUC: 0.9865
Mejores resultados en la época:  13
f1-score 0.9166422359034496
AUC según el mejor F1-score 0.9874790311607662
Confusion Matrix:
 [[16082   383]
 [  470  4690]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Running/confusion_matrix_param_5842945.png
Accuracy:   0.9606
Precision:  0.9245
Recall:     0.9089
F1-score:   0.9166
Tiempo total para red 6: 367.25 segundos
Saved on: outputs_ablation_remove_one_feature/1/Good for Running

==============================
Model: Logistic Regression
Accuracy:  0.9360
Precision: 0.8266
Recall:    0.9260
F1-score:  0.8735
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15463  1002]
 [  382  4778]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Running/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Running/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8250
Precision: 0.5912
Recall:    0.8640
F1-score:  0.7020
              precision    recall  f1-score   support

           0       0.95      0.81      0.88     16465
           1       0.59      0.86      0.70      5160

    accuracy                           0.83     21625
   macro avg       0.77      0.84      0.79     21625
weighted avg       0.86      0.83      0.83     21625

[[13383  3082]
 [  702  4458]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Running/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Running/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8709
Precision: 0.7108
Recall:    0.7736
F1-score:  0.7409
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14841  1624]
 [ 1168  3992]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Running/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Running/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8747
Precision: 0.7144
Recall:    0.7915
F1-score:  0.7509
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.71      0.79      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.87      0.88     21625

[[14832  1633]
 [ 1076  4084]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Running/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Running/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9218
Precision: 0.8016
Recall:    0.8932
F1-score:  0.8449
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15324  1141]
 [  551  4609]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Running/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Running/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Running/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Running/naive_bayes_model.pkl


Resumen de métricas:
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Logistic Regression: {'accuracy': 0.936, 'precision': 0.8266, 'recall': 0.926, 'f1_score': 0.8735}
XGBoost: {'accuracy': 0.9218, 'precision': 0.8016, 'recall': 0.8932, 'f1_score': 0.8449}
Random Forest: {'accuracy': 0.8747, 'precision': 0.7144, 'recall': 0.7915, 'f1_score': 0.7509}
Decision Tree: {'accuracy': 0.8709, 'precision': 0.7108, 'recall': 0.7736, 'f1_score': 0.7409}
SVM: {'accuracy': 0.825, 'precision': 0.5912, 'recall': 0.864, 'f1_score': 0.702}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}

##################################################
Running experiment without GOOD FOR YOGA/STRETCHING feature
[Good for Yoga/Stretching] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2924, Test Loss: 0.1839, F1: 0.8666, AUC: 0.9799
Epoch [10/30] Train Loss: 0.0766, Test Loss: 0.1966, F1: 0.8685, AUC: 0.9832
Epoch [20/30] Train Loss: 0.0505, Test Loss: 0.2550, F1: 0.8624, AUC: 0.9817
Mejores resultados en la época:  6
f1-score 0.8792324015130547
AUC según el mejor F1-score 0.9838472493920626
Confusion Matrix:
 [[15551   914]
 [  395  4765]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/confusion_matrix_param_160769.png
Accuracy:   0.9395
Precision:  0.8391
Recall:     0.9234
F1-score:   0.8792

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2967, Test Loss: 0.1781, F1: 0.8678, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0751, Test Loss: 0.2001, F1: 0.8661, AUC: 0.9832
Epoch [20/30] Train Loss: 0.0315, Test Loss: 0.2672, F1: 0.8667, AUC: 0.9822
Mejores resultados en la época:  2
f1-score 0.8850928778120041
AUC según el mejor F1-score 0.9842799089918244
Confusion Matrix:
 [[15653   812]
 [  419  4741]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/confusion_matrix_param_160769.png
Accuracy:   0.9431
Precision:  0.8538
Recall:     0.9188
F1-score:   0.8851

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2963, Test Loss: 0.1770, F1: 0.8689, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0761, Test Loss: 0.2141, F1: 0.8622, AUC: 0.9831
Epoch [20/30] Train Loss: 0.0481, Test Loss: 0.2594, F1: 0.8641, AUC: 0.9816
Mejores resultados en la época:  4
f1-score 0.8833333333333333
AUC según el mejor F1-score 0.9843030906527117
Confusion Matrix:
 [[15595   870]
 [  390  4770]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/confusion_matrix_param_160769.png
Accuracy:   0.9417
Precision:  0.8457
Recall:     0.9244
F1-score:   0.8833
Tiempo total para red 1: 281.64 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2256, Test Loss: 0.1624, F1: 0.8710, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0026, Test Loss: 0.2774, F1: 0.8881, AUC: 0.9884
Epoch [20/30] Train Loss: 0.0003, Test Loss: 0.2218, F1: 0.9158, AUC: 0.9885
Mejores resultados en la época:  20
f1-score 0.9157915208090237
AUC según el mejor F1-score 0.9885285442222992
Confusion Matrix:
 [[16050   415]
 [  451  4709]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/confusion_matrix_param_5842945.png
Accuracy:   0.9600
Precision:  0.9190
Recall:     0.9126
F1-score:   0.9158

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2248, Test Loss: 0.1927, F1: 0.8508, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0025, Test Loss: 0.3240, F1: 0.9080, AUC: 0.9879
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6137, F1: 0.9020, AUC: 0.9856
Mejores resultados en la época:  11
f1-score 0.9099437148217636
AUC según el mejor F1-score 0.9849104866559794
Confusion Matrix:
 [[15815   650]
 [  310  4850]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/confusion_matrix_param_5842945.png
Accuracy:   0.9556
Precision:  0.8818
Recall:     0.9399
F1-score:   0.9099

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2239, Test Loss: 0.1637, F1: 0.8690, AUC: 0.9834
Epoch [10/30] Train Loss: 0.0020, Test Loss: 0.4029, F1: 0.9035, AUC: 0.9873
Epoch [20/30] Train Loss: 0.0008, Test Loss: 0.4534, F1: 0.9021, AUC: 0.9872
Mejores resultados en la época:  14
f1-score 0.9149898912101666
AUC según el mejor F1-score 0.9884818631016697
Confusion Matrix:
 [[15990   475]
 [  408  4752]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/confusion_matrix_param_5842945.png
Accuracy:   0.9592
Precision:  0.9091
Recall:     0.9209
F1-score:   0.9150
Tiempo total para red 6: 365.68 segundos
Saved on: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching

==============================
Model: Logistic Regression
Accuracy:  0.9363
Precision: 0.8275
Recall:    0.9262
F1-score:  0.8741
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15469   996]
 [  381  4779]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8434
Precision: 0.6322
Recall:    0.8215
F1-score:  0.7145
              precision    recall  f1-score   support

           0       0.94      0.85      0.89     16465
           1       0.63      0.82      0.71      5160

    accuracy                           0.84     21625
   macro avg       0.79      0.84      0.80     21625
weighted avg       0.87      0.84      0.85     21625

[[13999  2466]
 [  921  4239]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8705
Precision: 0.7104
Recall:    0.7721
F1-score:  0.7400
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14841  1624]
 [ 1176  3984]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/conf_matrix_decision_tree.png
Logistic Regression: {'accuracy': 0.936, 'precision': 0.8266, 'recall': 0.926, 'f1_score': 0.8735}
XGBoost: {'accuracy': 0.9218, 'precision': 0.8016, 'recall': 0.8932, 'f1_score': 0.8449}
Random Forest: {'accuracy': 0.8747, 'precision': 0.7144, 'recall': 0.7915, 'f1_score': 0.7509}
Decision Tree: {'accuracy': 0.8709, 'precision': 0.7108, 'recall': 0.7736, 'f1_score': 0.7409}
SVM: {'accuracy': 0.825, 'precision': 0.5912, 'recall': 0.864, 'f1_score': 0.702}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}

##################################################
Running experiment without GOOD FOR YOGA/STRETCHING feature
[Good for Yoga/Stretching] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2960, Test Loss: 0.2042, F1: 0.8535, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0883, Test Loss: 0.2024, F1: 0.8657, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0742, Test Loss: 0.2610, F1: 0.8546, AUC: 0.9808
Mejores resultados en la época:  4
f1-score 0.8823965869040994
AUC según el mejor F1-score 0.9841582391118582
Confusion Matrix:
 [[15600   865]
 [  403  4757]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/confusion_matrix_param_160769.png
Accuracy:   0.9414
Precision:  0.8461
Recall:     0.9219
F1-score:   0.8824

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2937, Test Loss: 0.1718, F1: 0.8742, AUC: 0.9800
Epoch [10/30] Train Loss: 0.0880, Test Loss: 0.1771, F1: 0.8761, AUC: 0.9821
Epoch [20/30] Train Loss: 0.0639, Test Loss: 0.2519, F1: 0.8589, AUC: 0.9809
Mejores resultados en la época:  3
f1-score 0.879492600422833
AUC según el mejor F1-score 0.9841604342780197
Confusion Matrix:
 [[15530   935]
 [  376  4784]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/confusion_matrix_param_160769.png
Accuracy:   0.9394
Precision:  0.8365
Recall:     0.9271
F1-score:   0.8795

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2943, Test Loss: 0.1729, F1: 0.8693, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0894, Test Loss: 0.1856, F1: 0.8731, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0652, Test Loss: 0.2474, F1: 0.8587, AUC: 0.9809
Mejores resultados en la época:  3
f1-score 0.8801325722703002
AUC según el mejor F1-score 0.9843580875100342
Confusion Matrix:
 [[15543   922]
 [  380  4780]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/confusion_matrix_param_160769.png
Accuracy:   0.9398
Precision:  0.8383
Recall:     0.9264
F1-score:   0.8801
Tiempo total para red 1: 288.39 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2222, Test Loss: 0.1510, F1: 0.8789, AUC: 0.9827
Epoch [10/30] Train Loss: 0.0021, Test Loss: 0.2809, F1: 0.9086, AUC: 0.9885
Epoch [20/30] Train Loss: 0.0016, Test Loss: 0.5179, F1: 0.8925, AUC: 0.9866
Mejores resultados en la época:  18
f1-score 0.916023166023166
AUC según el mejor F1-score 0.987915992815392
Confusion Matrix:
 [[16010   455]
 [  415  4745]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/confusion_matrix_param_5842945.png
Accuracy:   0.9598
Precision:  0.9125
Recall:     0.9196
F1-score:   0.9160

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2200, Test Loss: 0.1544, F1: 0.8740, AUC: 0.9823
Epoch [10/30] Train Loss: 0.0031, Test Loss: 0.4759, F1: 0.9000, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0020, Test Loss: 0.3105, F1: 0.9137, AUC: 0.9887
Mejores resultados en la época:  22
f1-score 0.914324880054832
AUC según el mejor F1-score 0.9878740727924161
Confusion Matrix:
 [[16081   384]
 [  491  4669]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/confusion_matrix_param_5842945.png
Accuracy:   0.9595
Precision:  0.9240
Recall:     0.9048
F1-score:   0.9143

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2222, Test Loss: 0.1578, F1: 0.8732, AUC: 0.9832
Epoch [10/30] Train Loss: 0.0030, Test Loss: 0.4422, F1: 0.8970, AUC: 0.9874
Epoch [20/30] Train Loss: 0.0017, Test Loss: 0.3806, F1: 0.9084, AUC: 0.9883
Mejores resultados en la época:  11
f1-score 0.9125353440150801
AUC según el mejor F1-score 0.9862875502887262
Confusion Matrix:
 [[15856   609]
 [  319  4841]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/confusion_matrix_param_5842945.png
Accuracy:   0.9571
Precision:  0.8883
Recall:     0.9382
F1-score:   0.9125
Tiempo total para red 6: 366.73 segundos
Saved on: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching

==============================
Model: Logistic Regression
Accuracy:  0.9363
Precision: 0.8275
Recall:    0.9262
F1-score:  0.8741
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15469   996]
 [  381  4779]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8434
Precision: 0.6322
Recall:    0.8215
F1-score:  0.7145
              precision    recall  f1-score   support

           0       0.94      0.85      0.89     16465
           1       0.63      0.82      0.71      5160

    accuracy                           0.84     21625
   macro avg       0.79      0.84      0.80     21625
weighted avg       0.87      0.84      0.85     21625

[[13999  2466]
 [  921  4239]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8705
Precision: 0.7104
Recall:    0.7721
F1-score:  0.7400
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14841  1624]
 [ 1176  3984]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/conf_matrix_decision_tree.png
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [02:11:47] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [02:12:26] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8750
Precision: 0.7157
Recall:    0.7899
F1-score:  0.7510
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.72      0.79      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.88      0.88     21625

[[14846  1619]
 [ 1084  4076]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9222
Precision: 0.8031
Recall:    0.8930
F1-score:  0.8457
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15335  1130]
 [  552  4608]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7881
Precision: 0.5306
Recall:    0.9698
F1-score:  0.6859
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12038  4427]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9363, 'precision': 0.8275, 'recall': 0.9262, 'f1_score': 0.8741}
XGBoost: {'accuracy': 0.9222, 'precision': 0.8031, 'recall': 0.893, 'f1_score': 0.8457}
Random Forest: {'accuracy': 0.875, 'precision': 0.7157, 'recall': 0.7899, 'f1_score': 0.751}
Decision Tree: {'accuracy': 0.8705, 'precision': 0.7104, 'recall': 0.7721, 'f1_score': 0.74}
SVM: {'accuracy': 0.8434, 'precision': 0.6322, 'recall': 0.8215, 'f1_score': 0.7145}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}

##################################################
Running experiment without GOOD FOR DRIVING feature
[Good for Driving] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2921, Test Loss: 0.1775, F1: 0.8674, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0876, Test Loss: 0.1830, F1: 0.8752, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0567, Test Loss: 0.2271, F1: 0.8721, AUC: 0.9810
Mejores resultados en la época:  4
f1-score 0.8857008907641819
AUC según el mejor F1-score 0.984058114817195
Confusion Matrix:
 [[15683   782]
 [  437  4723]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Driving/confusion_matrix_param_160769.png
Accuracy:   0.9436
Precision:  0.8579
Recall:     0.9153
F1-score:   0.8857

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2984, Test Loss: 0.1970, F1: 0.8580, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0712, Test Loss: 0.1819, F1: 0.8780, AUC: 0.9835
Epoch [20/30] Train Loss: 0.0352, Test Loss: 0.2448, F1: 0.8731, AUC: 0.9825
Mejores resultados en la época:  5
f1-score 0.8833797585886722
AUC según el mejor F1-score 0.9843016899836862
Confusion Matrix:
 [[15612   853]
 [  403  4757]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Driving/confusion_matrix_param_160769.png
Accuracy:   0.9419
Precision:  0.8480
Recall:     0.9219
F1-score:   0.8834

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2952, Test Loss: 0.1896, F1: 0.8617, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0859, Test Loss: 0.2035, F1: 0.8614, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0493, Test Loss: 0.2316, F1: 0.8721, AUC: 0.9815
Mejores resultados en la época:  5
f1-score 0.8809655426570849
AUC según el mejor F1-score 0.9838591256529589
Confusion Matrix:
 [[15552   913]
 [  379  4781]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Driving/confusion_matrix_param_160769.png
Accuracy:   0.9403
Precision:  0.8397
Recall:     0.9266
F1-score:   0.8810
Tiempo total para red 1: 277.19 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2270, Test Loss: 0.1928, F1: 0.8497, AUC: 0.9833
Epoch [10/30] Train Loss: 0.0033, Test Loss: 0.2623, F1: 0.9079, AUC: 0.9882
Epoch [20/30] Train Loss: 0.0009, Test Loss: 0.3646, F1: 0.8995, AUC: 0.9879
Mejores resultados en la época:  13
f1-score 0.9128712871287129
AUC según el mejor F1-score 0.985711298573201
Confusion Matrix:
 [[16135   330]
 [  550  4610]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Driving/confusion_matrix_param_5842945.png
Accuracy:   0.9593
Precision:  0.9332
Recall:     0.8934
F1-score:   0.9129

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2230, Test Loss: 0.1919, F1: 0.8784, AUC: 0.9833
Epoch [10/30] Train Loss: 0.0020, Test Loss: 0.4576, F1: 0.9089, AUC: 0.9867
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6330, F1: 0.9057, AUC: 0.9845
Mejores resultados en la época:  12
f1-score 0.9143984220907297
AUC según el mejor F1-score 0.9879946421467195
Confusion Matrix:
 [[16121   344]
 [  524  4636]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Driving/confusion_matrix_param_5842945.png
Accuracy:   0.9599
Precision:  0.9309
Recall:     0.8984
F1-score:   0.9144

--- Iteración 3 de 3 para la red 6 ---
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8750
Precision: 0.7157
Recall:    0.7899
F1-score:  0.7510
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.72      0.79      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.88      0.88     21625

[[14846  1619]
 [ 1084  4076]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9222
Precision: 0.8031
Recall:    0.8930
F1-score:  0.8457
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15335  1130]
 [  552  4608]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7881
Precision: 0.5306
Recall:    0.9698
F1-score:  0.6859
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12038  4427]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Yoga/Stretching/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9363, 'precision': 0.8275, 'recall': 0.9262, 'f1_score': 0.8741}
XGBoost: {'accuracy': 0.9222, 'precision': 0.8031, 'recall': 0.893, 'f1_score': 0.8457}
Random Forest: {'accuracy': 0.875, 'precision': 0.7157, 'recall': 0.7899, 'f1_score': 0.751}
Decision Tree: {'accuracy': 0.8705, 'precision': 0.7104, 'recall': 0.7721, 'f1_score': 0.74}
SVM: {'accuracy': 0.8434, 'precision': 0.6322, 'recall': 0.8215, 'f1_score': 0.7145}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}

##################################################
Running experiment without GOOD FOR DRIVING feature
[Good for Driving] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2962, Test Loss: 0.1889, F1: 0.8613, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0862, Test Loss: 0.2015, F1: 0.8650, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0558, Test Loss: 0.2583, F1: 0.8583, AUC: 0.9811
Mejores resultados en la época:  5
f1-score 0.8825055596738325
AUC según el mejor F1-score 0.9837955835375485
Confusion Matrix:
 [[15595   870]
 [  398  4762]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Driving/confusion_matrix_param_160769.png
Accuracy:   0.9414
Precision:  0.8455
Recall:     0.9229
F1-score:   0.8825

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2926, Test Loss: 0.1993, F1: 0.8574, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0881, Test Loss: 0.2082, F1: 0.8594, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0591, Test Loss: 0.2530, F1: 0.8576, AUC: 0.9811
Mejores resultados en la época:  2
f1-score 0.8845864661654136
AUC según el mejor F1-score 0.9842098225740766
Confusion Matrix:
 [[15691   774]
 [  454  4706]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Driving/confusion_matrix_param_160769.png
Accuracy:   0.9432
Precision:  0.8588
Recall:     0.9120
F1-score:   0.8846

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2957, Test Loss: 0.1815, F1: 0.8657, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0886, Test Loss: 0.2031, F1: 0.8636, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0669, Test Loss: 0.2405, F1: 0.8626, AUC: 0.9808
Mejores resultados en la época:  5
f1-score 0.8823420418751158
AUC según el mejor F1-score 0.9837891922494744
Confusion Matrix:
 [[15593   872]
 [  398  4762]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Driving/confusion_matrix_param_160769.png
Accuracy:   0.9413
Precision:  0.8452
Recall:     0.9229
F1-score:   0.8823
Tiempo total para red 1: 287.17 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2225, Test Loss: 0.1507, F1: 0.8827, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0030, Test Loss: 0.2170, F1: 0.8963, AUC: 0.9882
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7339, F1: 0.8923, AUC: 0.9840
Mejores resultados en la época:  4
f1-score 0.9121226641111644
AUC según el mejor F1-score 0.9873527708529014
Confusion Matrix:
 [[15949   516]
 [  401  4759]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Driving/confusion_matrix_param_5842945.png
Accuracy:   0.9576
Precision:  0.9022
Recall:     0.9223
F1-score:   0.9121

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2245, Test Loss: 0.1724, F1: 0.8581, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.3901, F1: 0.8932, AUC: 0.9870
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5418, F1: 0.9040, AUC: 0.9861
Mejores resultados en la época:  17
f1-score 0.9151862464183381
AUC según el mejor F1-score 0.9885518023903183
Confusion Matrix:
 [[15946   519]
 [  369  4791]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Driving/confusion_matrix_param_5842945.png
Accuracy:   0.9589
Precision:  0.9023
Recall:     0.9285
F1-score:   0.9152

--- Iteración 3 de 3 para la red 6 ---
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [02:38:22] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [02:38:58] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [0/30] Train Loss: 0.2276, Test Loss: 0.1404, F1: 0.8829, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0045, Test Loss: 0.2440, F1: 0.8856, AUC: 0.9882
Epoch [20/30] Train Loss: 0.0024, Test Loss: 0.6755, F1: 0.8935, AUC: 0.9840
Mejores resultados en la época:  23
f1-score 0.9129724011924224
AUC según el mejor F1-score 0.9876485297683363
Confusion Matrix:
 [[15973   492]
 [  413  4747]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Driving/confusion_matrix_param_5842945.png
Accuracy:   0.9582
Precision:  0.9061
Recall:     0.9200
F1-score:   0.9130
Tiempo total para red 6: 367.75 segundos
Saved on: outputs_ablation_remove_one_feature/1/Good for Driving

==============================
Model: Logistic Regression
Accuracy:  0.9362
Precision: 0.8274
Recall:    0.9260
F1-score:  0.8739
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15468   997]
 [  382  4778]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Driving/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Driving/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7729
Precision: 0.5133
Recall:    0.9304
F1-score:  0.6616
              precision    recall  f1-score   support

           0       0.97      0.72      0.83     16465
           1       0.51      0.93      0.66      5160

    accuracy                           0.77     21625
   macro avg       0.74      0.83      0.75     21625
weighted avg       0.86      0.77      0.79     21625

[[11912  4553]
 [  359  4801]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Driving/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Driving/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8707
Precision: 0.7102
Recall:    0.7736
F1-score:  0.7406
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14836  1629]
 [ 1168  3992]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Driving/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Driving/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8769
Precision: 0.7212
Recall:    0.7895
F1-score:  0.7538
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.72      0.79      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.85      0.84     21625
weighted avg       0.88      0.88      0.88     21625

[[14890  1575]
 [ 1086  4074]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Driving/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Driving/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9215
Precision: 0.8016
Recall:    0.8919
F1-score:  0.8443
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15326  1139]
 [  558  4602]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Driving/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Driving/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Driving/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Driving/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9362, 'precision': 0.8274, 'recall': 0.926, 'f1_score': 0.8739}
XGBoost: {'accuracy': 0.9215, 'precision': 0.8016, 'recall': 0.8919, 'f1_score': 0.8443}
Random Forest: {'accuracy': 0.8769, 'precision': 0.7212, 'recall': 0.7895, 'f1_score': 0.7538}
Decision Tree: {'accuracy': 0.8707, 'precision': 0.7102, 'recall': 0.7736, 'f1_score': 0.7406}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7729, 'precision': 0.5133, 'recall': 0.9304, 'f1_score': 0.6616}

##################################################
Running experiment without GOOD FOR SOCIAL GATHERINGS feature
[Good for Social Gatherings] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Epoch [0/30] Train Loss: 0.2286, Test Loss: 0.1970, F1: 0.8600, AUC: 0.9837
Epoch [10/30] Train Loss: 0.0026, Test Loss: 0.5595, F1: 0.8793, AUC: 0.9854
Epoch [20/30] Train Loss: 0.0016, Test Loss: 0.5804, F1: 0.9005, AUC: 0.9848
Mejores resultados en la época:  22
f1-score 0.9157792836398838
AUC según el mejor F1-score 0.9855462962309056
Confusion Matrix:
 [[16025   440]
 [  430  4730]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Driving/confusion_matrix_param_5842945.png
Accuracy:   0.9598
Precision:  0.9149
Recall:     0.9167
F1-score:   0.9158
Tiempo total para red 6: 368.87 segundos
Saved on: outputs_ablation_remove_one_feature/1/Good for Driving

==============================
Model: Logistic Regression
Accuracy:  0.9362
Precision: 0.8274
Recall:    0.9260
F1-score:  0.8739
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15468   997]
 [  382  4778]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Driving/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Driving/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7729
Precision: 0.5133
Recall:    0.9304
F1-score:  0.6616
              precision    recall  f1-score   support

           0       0.97      0.72      0.83     16465
           1       0.51      0.93      0.66      5160

    accuracy                           0.77     21625
   macro avg       0.74      0.83      0.75     21625
weighted avg       0.86      0.77      0.79     21625

[[11912  4553]
 [  359  4801]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Driving/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Driving/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8707
Precision: 0.7102
Recall:    0.7736
F1-score:  0.7406
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14836  1629]
 [ 1168  3992]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Driving/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Driving/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8769
Precision: 0.7212
Recall:    0.7895
F1-score:  0.7538
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.72      0.79      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.85      0.84     21625
weighted avg       0.88      0.88      0.88     21625

[[14890  1575]
 [ 1086  4074]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Driving/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Driving/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9215
Precision: 0.8016
Recall:    0.8919
F1-score:  0.8443
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15326  1139]
 [  558  4602]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Driving/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Driving/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Driving/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Driving/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9362, 'precision': 0.8274, 'recall': 0.926, 'f1_score': 0.8739}
XGBoost: {'accuracy': 0.9215, 'precision': 0.8016, 'recall': 0.8919, 'f1_score': 0.8443}
Random Forest: {'accuracy': 0.8769, 'precision': 0.7212, 'recall': 0.7895, 'f1_score': 0.7538}
Decision Tree: {'accuracy': 0.8707, 'precision': 0.7102, 'recall': 0.7736, 'f1_score': 0.7406}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7729, 'precision': 0.5133, 'recall': 0.9304, 'f1_score': 0.6616}

##################################################
Running experiment without GOOD FOR SOCIAL GATHERINGS feature
[Good for Social Gatherings] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2980, Test Loss: 0.1881, F1: 0.8628, AUC: 0.9791
Epoch [10/30] Train Loss: 0.0875, Test Loss: 0.2334, F1: 0.8472, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0622, Test Loss: 0.2327, F1: 0.8650, AUC: 0.9806
Mejores resultados en la época:  5
f1-score 0.8766221897276549
AUC según el mejor F1-score 0.9837746029279868
Confusion Matrix:
 [[15479   986]
 [  364  4796]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/confusion_matrix_param_160769.png
Accuracy:   0.9376
Precision:  0.8295
Recall:     0.9295
F1-score:   0.8766

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2950, Test Loss: 0.2020, F1: 0.8547, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0895, Test Loss: 0.1923, F1: 0.8704, AUC: 0.9820
Epoch [20/30] Train Loss: 0.0764, Test Loss: 0.2251, F1: 0.8705, AUC: 0.9807
Mejores resultados en la época:  2
f1-score 0.8799037749814952
AUC según el mejor F1-score 0.984148652179747
Confusion Matrix:
 [[15572   893]
 [  405  4755]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/confusion_matrix_param_160769.png
Accuracy:   0.9400
Precision:  0.8419
Recall:     0.9215
F1-score:   0.8799

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2934, Test Loss: 0.1920, F1: 0.8630, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0882, Test Loss: 0.2097, F1: 0.8627, AUC: 0.9821
Epoch [20/30] Train Loss: 0.0651, Test Loss: 0.2697, F1: 0.8515, AUC: 0.9807
Mejores resultados en la época:  2
f1-score 0.8789855738307452
AUC según el mejor F1-score 0.9839980920298401
Confusion Matrix:
 [[15525   940]
 [  377  4783]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/confusion_matrix_param_160769.png
Accuracy:   0.9391
Precision:  0.8358
Recall:     0.9269
F1-score:   0.8790
Tiempo total para red 1: 278.66 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2224, Test Loss: 0.1549, F1: 0.8826, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0041, Test Loss: 0.1996, F1: 0.9068, AUC: 0.9879
Epoch [20/30] Train Loss: 0.0040, Test Loss: 0.3935, F1: 0.8885, AUC: 0.9871
Mejores resultados en la época:  21
f1-score 0.9070180356975983
AUC según el mejor F1-score 0.9861761971012035
Confusion Matrix:
 [[15777   688]
 [  307  4853]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/confusion_matrix_param_5842945.png
Accuracy:   0.9540
Precision:  0.8758
Recall:     0.9405
F1-score:   0.9070

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2247, Test Loss: 0.2135, F1: 0.8449, AUC: 0.9823
Epoch [10/30] Train Loss: 0.0021, Test Loss: 0.8072, F1: 0.8774, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0020, Test Loss: 0.5720, F1: 0.8870, AUC: 0.9854
Mejores resultados en la época:  14
f1-score 0.9161819395441976
AUC según el mejor F1-score 0.9885156557131995
Confusion Matrix:
 [[15942   523]
 [  356  4804]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/confusion_matrix_param_5842945.png
Accuracy:   0.9594
Precision:  0.9018
Recall:     0.9310
F1-score:   0.9162

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2248, Test Loss: 0.1700, F1: 0.8655, AUC: 0.9826
Epoch [10/30] Train Loss: 0.0037, Test Loss: 0.3595, F1: 0.8953, AUC: 0.9877
Epoch [20/30] Train Loss: 0.0014, Test Loss: 0.5010, F1: 0.8879, AUC: 0.9872
Mejores resultados en la época:  19
f1-score 0.911940011536243
AUC según el mejor F1-score 0.9873260345529747
Confusion Matrix:
 [[15966   499]
 [  417  4743]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/confusion_matrix_param_5842945.png
Accuracy:   0.9576
Precision:  0.9048
Recall:     0.9192
F1-score:   0.9119
Tiempo total para red 6: 366.56 segundos
Saved on: outputs_ablation_remove_one_feature/1/Good for Social Gatherings

==============================
Model: Logistic Regression
Accuracy:  0.9363
Precision: 0.8270
Recall:    0.9267
F1-score:  0.8741
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15465  1000]
 [  378  4782]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7200
Precision: 0.4574
Recall:    0.9318
F1-score:  0.6136
              precision    recall  f1-score   support

           0       0.97      0.65      0.78     16465
           1       0.46      0.93      0.61      5160

    accuracy                           0.72     21625
   macro avg       0.71      0.79      0.70     21625
weighted avg       0.85      0.72      0.74     21625

[[10762  5703]
 [  352  4808]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8704
Precision: 0.7097
Recall:    0.7729
F1-score:  0.7400
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14834  1631]
 [ 1172  3988]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8756
Precision: 0.7172
Recall:    0.7903
F1-score:  0.7520
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.72      0.79      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.88      0.88     21625

[[14857  1608]
 [ 1082  4078]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:05:04] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2964, Test Loss: 0.1981, F1: 0.8542, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0888, Test Loss: 0.1896, F1: 0.8737, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0675, Test Loss: 0.2409, F1: 0.8647, AUC: 0.9810
Mejores resultados en la época:  1
f1-score 0.8825903331769123
AUC según el mejor F1-score 0.9836554518982008
Confusion Matrix:
 [[15672   793]
 [  458  4702]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/confusion_matrix_param_160769.png
Accuracy:   0.9422
Precision:  0.8557
Recall:     0.9112
F1-score:   0.8826

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2934, Test Loss: 0.1701, F1: 0.8720, AUC: 0.9798
Epoch [10/30] Train Loss: 0.0870, Test Loss: 0.1996, F1: 0.8662, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0562, Test Loss: 0.2654, F1: 0.8519, AUC: 0.9814
Mejores resultados en la época:  4
f1-score 0.8831265049083163
AUC según el mejor F1-score 0.9840904773338793
Confusion Matrix:
 [[15595   870]
 [  392  4768]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/confusion_matrix_param_160769.png
Accuracy:   0.9416
Precision:  0.8457
Recall:     0.9240
F1-score:   0.8831

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2940, Test Loss: 0.1760, F1: 0.8715, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0890, Test Loss: 0.2088, F1: 0.8636, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0638, Test Loss: 0.2386, F1: 0.8652, AUC: 0.9814
Mejores resultados en la época:  3
f1-score 0.8812869822485208
AUC según el mejor F1-score 0.9842505184829461
Confusion Matrix:
 [[15575   890]
 [  394  4766]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/confusion_matrix_param_160769.png
Accuracy:   0.9406
Precision:  0.8426
Recall:     0.9236
F1-score:   0.8813
Tiempo total para red 1: 288.69 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2265, Test Loss: 0.1579, F1: 0.8787, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0017, Test Loss: 0.4316, F1: 0.8973, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0044, Test Loss: 0.3096, F1: 0.9060, AUC: 0.9879
Mejores resultados en la época:  12
f1-score 0.9120588790337799
AUC según el mejor F1-score 0.9879709072804187
Confusion Matrix:
 [[15860   605]
 [  327  4833]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/confusion_matrix_param_5842945.png
Accuracy:   0.9569
Precision:  0.8887
Recall:     0.9366
F1-score:   0.9121

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2214, Test Loss: 0.1615, F1: 0.8703, AUC: 0.9834
Epoch [10/30] Train Loss: 0.0043, Test Loss: 0.4531, F1: 0.8814, AUC: 0.9864
Epoch [20/30] Train Loss: 0.0012, Test Loss: 0.3812, F1: 0.9097, AUC: 0.9883
Mejores resultados en la época:  8
f1-score 0.9102782509937536
AUC según el mejor F1-score 0.988061156269936
Confusion Matrix:
 [[15868   597]
 [  351  4809]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/confusion_matrix_param_5842945.png
Accuracy:   0.9562
Precision:  0.8896
Recall:     0.9320
F1-score:   0.9103

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2248, Test Loss: 0.1613, F1: 0.8694, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0036, Test Loss: 0.4979, F1: 0.8910, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5962, F1: 0.9024, AUC: 0.9854
Mejores resultados en la época:  12
f1-score 0.9132024634334103
AUC según el mejor F1-score 0.9878782453736726
Confusion Matrix:
 [[15978   487]
 [  415  4745]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/confusion_matrix_param_5842945.png
Accuracy:   0.9583
Precision:  0.9069
Recall:     0.9196
F1-score:   0.9132
Tiempo total para red 6: 368.24 segundos
Saved on: outputs_ablation_remove_one_feature/1/Good for Social Gatherings

==============================
Model: Logistic Regression
Accuracy:  0.9363
Precision: 0.8270
Recall:    0.9267
F1-score:  0.8741
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15465  1000]
 [  378  4782]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7200
Precision: 0.4574
Recall:    0.9318
F1-score:  0.6136
              precision    recall  f1-score   support

           0       0.97      0.65      0.78     16465
           1       0.46      0.93      0.61      5160

    accuracy                           0.72     21625
   macro avg       0.71      0.79      0.70     21625
weighted avg       0.85      0.72      0.74     21625

[[10762  5703]
 [  352  4808]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8704
Precision: 0.7097
Recall:    0.7729
F1-score:  0.7400
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.87      0.87      0.87     21625

[[14834  1631]
 [ 1172  3988]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8756
Precision: 0.7172
Recall:    0.7903
F1-score:  0.7520
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.72      0.79      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.88      0.88     21625

[[14857  1608]
 [ 1082  4078]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:05:39] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9219
Precision: 0.8026
Recall:    0.8922
F1-score:  0.8451
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15333  1132]
 [  556  4604]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9363, 'precision': 0.827, 'recall': 0.9267, 'f1_score': 0.8741}
XGBoost: {'accuracy': 0.9219, 'precision': 0.8026, 'recall': 0.8922, 'f1_score': 0.8451}
Random Forest: {'accuracy': 0.8756, 'precision': 0.7172, 'recall': 0.7903, 'f1_score': 0.752}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.7097, 'recall': 0.7729, 'f1_score': 0.74}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.72, 'precision': 0.4574, 'recall': 0.9318, 'f1_score': 0.6136}

##################################################
Running experiment without GOOD FOR MORNING ROUTINE feature
[Good for Morning Routine] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2934, Test Loss: 0.1992, F1: 0.8557, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0858, Test Loss: 0.1908, F1: 0.8671, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0453, Test Loss: 0.2429, F1: 0.8680, AUC: 0.9817
Mejores resultados en la época:  3
f1-score 0.8806478328885617
AUC según el mejor F1-score 0.984215725393541
Confusion Matrix:
 [[15543   922]
 [  375  4785]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Morning Routine/confusion_matrix_param_160769.png
Accuracy:   0.9400
Precision:  0.8384
Recall:     0.9273
F1-score:   0.8806

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2953, Test Loss: 0.1933, F1: 0.8603, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0862, Test Loss: 0.1861, F1: 0.8742, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0574, Test Loss: 0.2306, F1: 0.8742, AUC: 0.9813
Mejores resultados en la época:  2
f1-score 0.878690333854502
AUC según el mejor F1-score 0.9842162315176426
Confusion Matrix:
 [[15529   936]
 [  383  4777]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Morning Routine/confusion_matrix_param_160769.png
Accuracy:   0.9390
Precision:  0.8362
Recall:     0.9258
F1-score:   0.8787

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2972, Test Loss: 0.1803, F1: 0.8675, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0885, Test Loss: 0.1870, F1: 0.8750, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0704, Test Loss: 0.2474, F1: 0.8604, AUC: 0.9807
Mejores resultados en la época:  5
f1-score 0.8838516298239041
AUC según el mejor F1-score 0.9835636727660507
Confusion Matrix:
 [[15667   798]
 [  442  4718]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Morning Routine/confusion_matrix_param_160769.png
Accuracy:   0.9427
Precision:  0.8553
Recall:     0.9143
F1-score:   0.8839
Tiempo total para red 1: 278.13 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2213, Test Loss: 0.1474, F1: 0.8871, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0046, Test Loss: 0.3162, F1: 0.8886, AUC: 0.9882
Epoch [20/30] Train Loss: 0.0010, Test Loss: 0.5469, F1: 0.9020, AUC: 0.9853
Mejores resultados en la época:  24
f1-score 0.9140841041289215
AUC según el mejor F1-score 0.9879169344416275
Confusion Matrix:
 [[15931   534]
 [  367  4793]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Morning Routine/confusion_matrix_param_5842945.png
Accuracy:   0.9583
Precision:  0.8998
Recall:     0.9289
F1-score:   0.9141

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2248, Test Loss: 0.1564, F1: 0.8759, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0040, Test Loss: 0.3671, F1: 0.8999, AUC: 0.9875
Epoch [20/30] Train Loss: 0.0031, Test Loss: 0.4442, F1: 0.8858, AUC: 0.9867
Mejores resultados en la época:  24
f1-score 0.9151452085895374
AUC según el mejor F1-score 0.9878458004646925
Confusion Matrix:
 [[15891   574]
 [  323  4837]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Morning Routine/confusion_matrix_param_5842945.png
Accuracy:   0.9585
Precision:  0.8939
Recall:     0.9374
F1-score:   0.9151

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2257, Test Loss: 0.1897, F1: 0.8604, AUC: 0.9824
Epoch [10/30] Train Loss: 0.0036, Test Loss: 0.4471, F1: 0.8874, AUC: 0.9850
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7322, F1: 0.8929, AUC: 0.9845
Mejores resultados en la época:  11
f1-score 0.9152247462542291
AUC según el mejor F1-score 0.9881090203085239
Confusion Matrix:
 [[16014   451]
 [  426  4734]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Morning Routine/confusion_matrix_param_5842945.png
Accuracy:   0.9594
Precision:  0.9130
Recall:     0.9174
F1-score:   0.9152
Tiempo total para red 6: 368.03 segundos
Saved on: outputs_ablation_remove_one_feature/1/Good for Morning Routine

==============================
Model: Logistic Regression
Accuracy:  0.9361
Precision: 0.8268
Recall:    0.9264
F1-score:  0.8738
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9219
Precision: 0.8026
Recall:    0.8922
F1-score:  0.8451
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15333  1132]
 [  556  4604]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Social Gatherings/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9363, 'precision': 0.827, 'recall': 0.9267, 'f1_score': 0.8741}
XGBoost: {'accuracy': 0.9219, 'precision': 0.8026, 'recall': 0.8922, 'f1_score': 0.8451}
Random Forest: {'accuracy': 0.8756, 'precision': 0.7172, 'recall': 0.7903, 'f1_score': 0.752}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.7097, 'recall': 0.7729, 'f1_score': 0.74}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.72, 'precision': 0.4574, 'recall': 0.9318, 'f1_score': 0.6136}

##################################################
Running experiment without GOOD FOR MORNING ROUTINE feature
[Good for Morning Routine] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2944, Test Loss: 0.1776, F1: 0.8683, AUC: 0.9792
Epoch [10/30] Train Loss: 0.0890, Test Loss: 0.1869, F1: 0.8748, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0706, Test Loss: 0.2439, F1: 0.8584, AUC: 0.9804
Mejores resultados en la época:  3
f1-score 0.8769906644700713
AUC según el mejor F1-score 0.9842706222030758
Confusion Matrix:
 [[15490   975]
 [  369  4791]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Morning Routine/confusion_matrix_param_160769.png
Accuracy:   0.9378
Precision:  0.8309
Recall:     0.9285
F1-score:   0.8770

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2957, Test Loss: 0.1868, F1: 0.8625, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0873, Test Loss: 0.1884, F1: 0.8704, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0556, Test Loss: 0.2670, F1: 0.8524, AUC: 0.9815
Mejores resultados en la época:  2
f1-score 0.8818122736978925
AUC según el mejor F1-score 0.984035933634183
Confusion Matrix:
 [[15603   862]
 [  411  4749]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Morning Routine/confusion_matrix_param_160769.png
Accuracy:   0.9411
Precision:  0.8464
Recall:     0.9203
F1-score:   0.8818

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2944, Test Loss: 0.1962, F1: 0.8572, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0874, Test Loss: 0.1830, F1: 0.8739, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0585, Test Loss: 0.2300, F1: 0.8695, AUC: 0.9811
Mejores resultados en la época:  3
f1-score 0.8810028574062125
AUC según el mejor F1-score 0.9842790732985404
Confusion Matrix:
 [[15555   910]
 [  381  4779]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Morning Routine/confusion_matrix_param_160769.png
Accuracy:   0.9403
Precision:  0.8400
Recall:     0.9262
F1-score:   0.8810
Tiempo total para red 1: 286.87 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2240, Test Loss: 0.1830, F1: 0.8578, AUC: 0.9838
Epoch [10/30] Train Loss: 0.0030, Test Loss: 0.4589, F1: 0.8893, AUC: 0.9873
Epoch [20/30] Train Loss: 0.0027, Test Loss: 0.4720, F1: 0.9047, AUC: 0.9866
Mejores resultados en la época:  15
f1-score 0.9093137254901961
AUC según el mejor F1-score 0.9885255369035093
Confusion Matrix:
 [[15840   625]
 [  337  4823]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Morning Routine/confusion_matrix_param_5842945.png
Accuracy:   0.9555
Precision:  0.8853
Recall:     0.9347
F1-score:   0.9093

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2215, Test Loss: 0.1436, F1: 0.8902, AUC: 0.9832
Epoch [10/30] Train Loss: 0.0043, Test Loss: 0.4321, F1: 0.8945, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0002, Test Loss: 0.8225, F1: 0.8367, AUC: 0.9850
Mejores resultados en la época:  9
f1-score 0.9090224147673761
AUC según el mejor F1-score 0.9870712069529681
Confusion Matrix:
 [[15833   632]
 [  334  4826]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Morning Routine/confusion_matrix_param_5842945.png
Accuracy:   0.9553
Precision:  0.8842
Recall:     0.9353
F1-score:   0.9090

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2212, Test Loss: 0.1592, F1: 0.8836, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0033, Test Loss: 0.3180, F1: 0.9047, AUC: 0.9874
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6221, F1: 0.9020, AUC: 0.9850
Mejores resultados en la época:  8
f1-score 0.9056963231091637
AUC según el mejor F1-score 0.9871947306595855
Confusion Matrix:
 [[15881   584]
 [  406  4754]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Good for Morning Routine/confusion_matrix_param_5842945.png
Accuracy:   0.9542
Precision:  0.8906
Recall:     0.9213
F1-score:   0.9057
Tiempo total para red 6: 369.30 segundos
Saved on: outputs_ablation_remove_one_feature/1/Good for Morning Routine

==============================
Model: Logistic Regression
Accuracy:  0.9361
Precision: 0.8268
Recall:    0.9264
F1-score:  0.8738
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:31:32] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:32:18] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[15464  1001]
 [  380  4780]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Morning Routine/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Morning Routine/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7869
Precision: 0.5319
Recall:    0.8909
F1-score:  0.6661
              precision    recall  f1-score   support

           0       0.96      0.75      0.84     16465
           1       0.53      0.89      0.67      5160

    accuracy                           0.79     21625
   macro avg       0.74      0.82      0.75     21625
weighted avg       0.86      0.79      0.80     21625

[[12420  4045]
 [  563  4597]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Morning Routine/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Morning Routine/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8708
Precision: 0.7105
Recall:    0.7740
F1-score:  0.7409
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14838  1627]
 [ 1166  3994]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Morning Routine/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Morning Routine/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8759
Precision: 0.7177
Recall:    0.7909
F1-score:  0.7525
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.72      0.79      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.85      0.83     21625
weighted avg       0.88      0.88      0.88     21625

[[14860  1605]
 [ 1079  4081]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Morning Routine/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Morning Routine/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9214
Precision: 0.8009
Recall:    0.8924
F1-score:  0.8442
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15320  1145]
 [  555  4605]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Morning Routine/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Morning Routine/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Morning Routine/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Morning Routine/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8268, 'recall': 0.9264, 'f1_score': 0.8738}
XGBoost: {'accuracy': 0.9214, 'precision': 0.8009, 'recall': 0.8924, 'f1_score': 0.8442}
Random Forest: {'accuracy': 0.8759, 'precision': 0.7177, 'recall': 0.7909, 'f1_score': 0.7525}
Decision Tree: {'accuracy': 0.8708, 'precision': 0.7105, 'recall': 0.774, 'f1_score': 0.7409}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7869, 'precision': 0.5319, 'recall': 0.8909, 'f1_score': 0.6661}

##################################################
Running experiment without RELEASE_YEAR feature
[Release_Year] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2937, Test Loss: 0.1968, F1: 0.8587, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0898, Test Loss: 0.2051, F1: 0.8673, AUC: 0.9822
Epoch [20/30] Train Loss: 0.0753, Test Loss: 0.2591, F1: 0.8577, AUC: 0.9805
Mejores resultados en la época:  3
f1-score 0.8734819648359615
AUC según el mejor F1-score 0.9842365765294953
Confusion Matrix:
 [[15410  1055]
 [  341  4819]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Year/confusion_matrix_param_160769.png
Accuracy:   0.9354
Precision:  0.8204
Recall:     0.9339
F1-score:   0.8735

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2912, Test Loss: 0.1822, F1: 0.8661, AUC: 0.9796

[[15464  1001]
 [  380  4780]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Morning Routine/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Morning Routine/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7869
Precision: 0.5319
Recall:    0.8909
F1-score:  0.6661
              precision    recall  f1-score   support

           0       0.96      0.75      0.84     16465
           1       0.53      0.89      0.67      5160

    accuracy                           0.79     21625
   macro avg       0.74      0.82      0.75     21625
weighted avg       0.86      0.79      0.80     21625

[[12420  4045]
 [  563  4597]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Morning Routine/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Morning Routine/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8708
Precision: 0.7105
Recall:    0.7740
F1-score:  0.7409
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14838  1627]
 [ 1166  3994]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Morning Routine/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Morning Routine/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8759
Precision: 0.7177
Recall:    0.7909
F1-score:  0.7525
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.72      0.79      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.85      0.83     21625
weighted avg       0.88      0.88      0.88     21625

[[14860  1605]
 [ 1079  4081]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Morning Routine/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Morning Routine/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9214
Precision: 0.8009
Recall:    0.8924
F1-score:  0.8442
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15320  1145]
 [  555  4605]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Morning Routine/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Morning Routine/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Good for Morning Routine/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Good for Morning Routine/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8268, 'recall': 0.9264, 'f1_score': 0.8738}
XGBoost: {'accuracy': 0.9214, 'precision': 0.8009, 'recall': 0.8924, 'f1_score': 0.8442}
Random Forest: {'accuracy': 0.8759, 'precision': 0.7177, 'recall': 0.7909, 'f1_score': 0.7525}
Decision Tree: {'accuracy': 0.8708, 'precision': 0.7105, 'recall': 0.774, 'f1_score': 0.7409}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7869, 'precision': 0.5319, 'recall': 0.8909, 'f1_score': 0.6661}

##################################################
Running experiment without RELEASE_YEAR feature
[Release_Year] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2931, Test Loss: 0.1834, F1: 0.8654, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0864, Test Loss: 0.1790, F1: 0.8767, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0518, Test Loss: 0.2377, F1: 0.8694, AUC: 0.9812
Mejores resultados en la época:  3
f1-score 0.8852151042645124
AUC según el mejor F1-score 0.9840077554690829
Confusion Matrix:
 [[15691   774]
 [  448  4712]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Year/confusion_matrix_param_160769.png
Accuracy:   0.9435
Precision:  0.8589
Recall:     0.9132
F1-score:   0.8852

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2941, Test Loss: 0.1719, F1: 0.8711, AUC: 0.9795
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:58:05] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:58:36] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [10/30] Train Loss: 0.0886, Test Loss: 0.2125, F1: 0.8619, AUC: 0.9821
Epoch [20/30] Train Loss: 0.0740, Test Loss: 0.2586, F1: 0.8577, AUC: 0.9806
Mejores resultados en la época:  2
f1-score 0.8842975206611571
AUC según el mejor F1-score 0.9842084572160349
Confusion Matrix:
 [[15685   780]
 [  452  4708]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Year/confusion_matrix_param_160769.png
Accuracy:   0.9430
Precision:  0.8579
Recall:     0.9124
F1-score:   0.8843

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2920, Test Loss: 0.1906, F1: 0.8597, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0876, Test Loss: 0.1814, F1: 0.8740, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0555, Test Loss: 0.2305, F1: 0.8701, AUC: 0.9812
Mejores resultados en la época:  4
f1-score 0.8762044599431036
AUC según el mejor F1-score 0.9838889928601189
Confusion Matrix:
 [[15502   963]
 [  386  4774]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Year/confusion_matrix_param_160769.png
Accuracy:   0.9376
Precision:  0.8321
Recall:     0.9252
F1-score:   0.8762
Tiempo total para red 1: 276.22 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2233, Test Loss: 0.1861, F1: 0.8626, AUC: 0.9825
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.2297, F1: 0.9007, AUC: 0.9883
Epoch [20/30] Train Loss: 0.0028, Test Loss: 0.4273, F1: 0.9025, AUC: 0.9876
Mejores resultados en la época:  23
f1-score 0.9121640869812934
AUC según el mejor F1-score 0.9856247278111663
Confusion Matrix:
 [[15897   568]
 [  357  4803]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Year/confusion_matrix_param_5842945.png
Accuracy:   0.9572
Precision:  0.8942
Recall:     0.9308
F1-score:   0.9122

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2239, Test Loss: 0.1961, F1: 0.8544, AUC: 0.9836
Epoch [10/30] Train Loss: 0.0024, Test Loss: 0.4384, F1: 0.8592, AUC: 0.9876
Epoch [20/30] Train Loss: 0.0021, Test Loss: 0.4390, F1: 0.8866, AUC: 0.9883
Mejores resultados en la época:  21
f1-score 0.9181528969439892
AUC según el mejor F1-score 0.9889309481940787
Confusion Matrix:
 [[16014   451]
 [  398  4762]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Year/confusion_matrix_param_5842945.png
Accuracy:   0.9607
Precision:  0.9135
Recall:     0.9229
F1-score:   0.9182

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2255, Test Loss: 0.1674, F1: 0.8758, AUC: 0.9834
Epoch [10/30] Train Loss: 0.0012, Test Loss: 0.2961, F1: 0.9057, AUC: 0.9881
Epoch [20/30] Train Loss: 0.0021, Test Loss: 0.2783, F1: 0.9049, AUC: 0.9879
Mejores resultados en la época:  6
f1-score 0.9135779122541604
AUC según el mejor F1-score 0.988470728371434
Confusion Matrix:
 [[15880   585]
 [  329  4831]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Year/confusion_matrix_param_5842945.png
Accuracy:   0.9577
Precision:  0.8920
Recall:     0.9362
F1-score:   0.9136
Tiempo total para red 6: 366.41 segundos
Saved on: outputs_ablation_remove_one_feature/1/Release_Year

==============================
Model: Logistic Regression
Accuracy:  0.9361
Precision: 0.8304
Recall:    0.9203
F1-score:  0.8731
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15495   970]
 [  411  4749]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Year/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Year/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8635
Precision: 0.6794
Recall:    0.8103
F1-score:  0.7391
              precision    recall  f1-score   support

           0       0.94      0.88      0.91     16465
           1       0.68      0.81      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.85      0.82     21625
weighted avg       0.88      0.86      0.87     21625

[[14492  1973]
 [  979  4181]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Year/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Year/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8745
Precision: 0.7238
Recall:    0.7663
F1-score:  0.7444
              precision    recall  f1-score   support

           0       0.93      0.91      0.92     16465
           1       0.72      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.88     21625

[[14956  1509]
 [ 1206  3954]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Year/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Year/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8790
Precision: 0.7287
Recall:    0.7855
F1-score:  0.7560
              precision    recall  f1-score   support

           0       0.93      0.91      0.92     16465
           1       0.73      0.79      0.76      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.85      0.84     21625
weighted avg       0.88      0.88      0.88     21625

[[14956  1509]
 [ 1107  4053]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Year/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Year/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9216
Precision: 0.8046
Recall:    0.8868
F1-score:  0.8437
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15354  1111]
 [  584  4576]]
Epoch [10/30] Train Loss: 0.0890, Test Loss: 0.1908, F1: 0.8727, AUC: 0.9821
Epoch [20/30] Train Loss: 0.0759, Test Loss: 0.2372, F1: 0.8645, AUC: 0.9804
Mejores resultados en la época:  3
f1-score 0.8840985442329228
AUC según el mejor F1-score 0.9841884947398404
Confusion Matrix:
 [[15646   819]
 [  423  4737]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Year/confusion_matrix_param_160769.png
Accuracy:   0.9426
Precision:  0.8526
Recall:     0.9180
F1-score:   0.8841

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2894, Test Loss: 0.1915, F1: 0.8615, AUC: 0.9798
Epoch [10/30] Train Loss: 0.0895, Test Loss: 0.2279, F1: 0.8524, AUC: 0.9820
Epoch [20/30] Train Loss: 0.0718, Test Loss: 0.2476, F1: 0.8584, AUC: 0.9805
Mejores resultados en la época:  1
f1-score 0.8827315980968374
AUC según el mejor F1-score 0.983536671633745
Confusion Matrix:
 [[15637   828]
 [  429  4731]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Year/confusion_matrix_param_160769.png
Accuracy:   0.9419
Precision:  0.8511
Recall:     0.9169
F1-score:   0.8827
Tiempo total para red 1: 288.53 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2231, Test Loss: 0.2166, F1: 0.8528, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0028, Test Loss: 0.4253, F1: 0.8855, AUC: 0.9869
Epoch [20/30] Train Loss: 0.0024, Test Loss: 0.3484, F1: 0.9042, AUC: 0.9879
Mejores resultados en la época:  19
f1-score 0.9132125580954188
AUC según el mejor F1-score 0.9879183292254888
Confusion Matrix:
 [[15896   569]
 [  346  4814]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Year/confusion_matrix_param_5842945.png
Accuracy:   0.9577
Precision:  0.8943
Recall:     0.9329
F1-score:   0.9132

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2240, Test Loss: 0.1744, F1: 0.8699, AUC: 0.9832
Epoch [10/30] Train Loss: 0.0046, Test Loss: 0.3310, F1: 0.9083, AUC: 0.9875
Epoch [20/30] Train Loss: 0.0024, Test Loss: 0.3851, F1: 0.9151, AUC: 0.9871
Mejores resultados en la época:  20
f1-score 0.915069018404908
AUC según el mejor F1-score 0.9871072300416435
Confusion Matrix:
 [[15966   499]
 [  387  4773]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Year/confusion_matrix_param_5842945.png
Accuracy:   0.9590
Precision:  0.9053
Recall:     0.9250
F1-score:   0.9151

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2250, Test Loss: 0.1728, F1: 0.8711, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0005, Test Loss: 0.4349, F1: 0.9104, AUC: 0.9877
Epoch [20/30] Train Loss: 0.0011, Test Loss: 0.6404, F1: 0.8803, AUC: 0.9855
Mejores resultados en la época:  16
f1-score 0.9112933994658527
AUC según el mejor F1-score 0.9874388531463265
Confusion Matrix:
 [[15918   547]
 [  383  4777]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Year/confusion_matrix_param_5842945.png
Accuracy:   0.9570
Precision:  0.8973
Recall:     0.9258
F1-score:   0.9113
Tiempo total para red 6: 368.10 segundos
Saved on: outputs_ablation_remove_one_feature/1/Release_Year

==============================
Model: Logistic Regression
Accuracy:  0.9361
Precision: 0.8304
Recall:    0.9203
F1-score:  0.8731
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15495   970]
 [  411  4749]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Year/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Year/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8635
Precision: 0.6794
Recall:    0.8103
F1-score:  0.7391
              precision    recall  f1-score   support

           0       0.94      0.88      0.91     16465
           1       0.68      0.81      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.85      0.82     21625
weighted avg       0.88      0.86      0.87     21625

[[14492  1973]
 [  979  4181]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Year/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Year/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8745
Precision: 0.7238
Recall:    0.7663
F1-score:  0.7444
              precision    recall  f1-score   support

           0       0.93      0.91      0.92     16465
           1       0.72      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.88     21625

[[14956  1509]
 [ 1206  3954]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Year/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Year/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8790
Precision: 0.7287
Recall:    0.7855
F1-score:  0.7560
              precision    recall  f1-score   support

           0       0.93      0.91      0.92     16465
           1       0.73      0.79      0.76      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.85      0.84     21625
weighted avg       0.88      0.88      0.88     21625

[[14956  1509]
 [ 1107  4053]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Year/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Year/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9216
Precision: 0.8046
Recall:    0.8868
F1-score:  0.8437
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15354  1111]
 [  584  4576]]
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Year/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Year/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7881
Precision: 0.5306
Recall:    0.9698
F1-score:  0.6859
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12039  4426]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Year/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Year/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8304, 'recall': 0.9203, 'f1_score': 0.8731}
XGBoost: {'accuracy': 0.9216, 'precision': 0.8046, 'recall': 0.8868, 'f1_score': 0.8437}
Random Forest: {'accuracy': 0.879, 'precision': 0.7287, 'recall': 0.7855, 'f1_score': 0.756}
Decision Tree: {'accuracy': 0.8745, 'precision': 0.7238, 'recall': 0.7663, 'f1_score': 0.7444}
SVM: {'accuracy': 0.8635, 'precision': 0.6794, 'recall': 0.8103, 'f1_score': 0.7391}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}

##################################################
Running experiment without RELEASE_MONTH feature
[Release_Month] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2933, Test Loss: 0.1730, F1: 0.8693, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0871, Test Loss: 0.1908, F1: 0.8678, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0573, Test Loss: 0.2398, F1: 0.8653, AUC: 0.9809
Mejores resultados en la época:  4
f1-score 0.8857492700386174
AUC según el mejor F1-score 0.9838701485650792
Confusion Matrix:
 [[15710   755]
 [  458  4702]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Month/confusion_matrix_param_160769.png
Accuracy:   0.9439
Precision:  0.8616
Recall:     0.9112
F1-score:   0.8857

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2952, Test Loss: 0.1809, F1: 0.8656, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0888, Test Loss: 0.2144, F1: 0.8595, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0761, Test Loss: 0.2691, F1: 0.8533, AUC: 0.9808
Mejores resultados en la época:  2
f1-score 0.8823859386686612
AUC según el mejor F1-score 0.984228319644442
Confusion Matrix:
 [[15648   817]
 [  441  4719]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Month/confusion_matrix_param_160769.png
Accuracy:   0.9418
Precision:  0.8524
Recall:     0.9145
F1-score:   0.8824

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2906, Test Loss: 0.1778, F1: 0.8680, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0872, Test Loss: 0.2123, F1: 0.8591, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0564, Test Loss: 0.2435, F1: 0.8668, AUC: 0.9813
Mejores resultados en la época:  3
f1-score 0.8838388581087426
AUC según el mejor F1-score 0.9840767825573156
Confusion Matrix:
 [[15682   783]
 [  454  4706]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Month/confusion_matrix_param_160769.png
Accuracy:   0.9428
Precision:  0.8574
Recall:     0.9120
F1-score:   0.8838
Tiempo total para red 1: 280.37 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2242, Test Loss: 0.1416, F1: 0.8853, AUC: 0.9824
Epoch [10/30] Train Loss: 0.0031, Test Loss: 0.8092, F1: 0.8475, AUC: 0.9829
Epoch [20/30] Train Loss: 0.0021, Test Loss: 0.4501, F1: 0.9062, AUC: 0.9873
Mejores resultados en la época:  25
f1-score 0.9166347259486394
AUC según el mejor F1-score 0.9877036796399221
Confusion Matrix:
 [[15972   493]
 [  377  4783]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Month/confusion_matrix_param_5842945.png
Accuracy:   0.9598
Precision:  0.9066
Recall:     0.9269
F1-score:   0.9166

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2246, Test Loss: 0.1653, F1: 0.8774, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.3605, F1: 0.8836, AUC: 0.9875
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6228, F1: 0.9009, AUC: 0.9845
Mejores resultados en la época:  7
f1-score 0.9121474024123848
AUC según el mejor F1-score 0.9881090497343437
Confusion Matrix:
 [[15898   567]
 [  358  4802]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Month/confusion_matrix_param_5842945.png
Accuracy:   0.9572
Precision:  0.8944
Recall:     0.9306
F1-score:   0.9121

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2258, Test Loss: 0.1652, F1: 0.8677, AUC: 0.9826
Epoch [10/30] Train Loss: 0.0008, Test Loss: 0.4834, F1: 0.8884, AUC: 0.9852
Epoch [20/30] Train Loss: 0.0013, Test Loss: 0.6073, F1: 0.8890, AUC: 0.9857
Mejores resultados en la época:  6
f1-score 0.9073950699533644
AUC según el mejor F1-score 0.9872700489881049
Confusion Matrix:
 [[15885   580]
 [  393  4767]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Month/confusion_matrix_param_5842945.png
Accuracy:   0.9550
Precision:  0.8915
Recall:     0.9238
F1-score:   0.9074
Tiempo total para red 6: 367.76 segundos
Saved on: outputs_ablation_remove_one_feature/1/Release_Month

==============================
Model: Logistic Regression
Accuracy:  0.9361
Precision: 0.8268
Recall:    0.9264
F1-score:  0.8738
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15464  1001]
 [  380  4780]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Month/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Month/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7647
Precision: 0.5038
Recall:    0.9192
F1-score:  0.6508
              precision    recall  f1-score   support

           0       0.97      0.72      0.82     16465
           1       0.50      0.92      0.65      5160

    accuracy                           0.76     21625
   macro avg       0.73      0.82      0.74     21625
weighted avg       0.86      0.76      0.78     21625

[[11793  4672]
 [  417  4743]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Month/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Month/svm_model.pkl

==============================
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Year/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Year/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7881
Precision: 0.5306
Recall:    0.9698
F1-score:  0.6859
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12039  4426]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Year/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Year/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8304, 'recall': 0.9203, 'f1_score': 0.8731}
XGBoost: {'accuracy': 0.9216, 'precision': 0.8046, 'recall': 0.8868, 'f1_score': 0.8437}
Random Forest: {'accuracy': 0.879, 'precision': 0.7287, 'recall': 0.7855, 'f1_score': 0.756}
Decision Tree: {'accuracy': 0.8745, 'precision': 0.7238, 'recall': 0.7663, 'f1_score': 0.7444}
SVM: {'accuracy': 0.8635, 'precision': 0.6794, 'recall': 0.8103, 'f1_score': 0.7391}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}

##################################################
Running experiment without RELEASE_MONTH feature
[Release_Month] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2972, Test Loss: 0.1839, F1: 0.8662, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0891, Test Loss: 0.2019, F1: 0.8666, AUC: 0.9822
Epoch [20/30] Train Loss: 0.0770, Test Loss: 0.2481, F1: 0.8620, AUC: 0.9805
Mejores resultados en la época:  3
f1-score 0.886408404464872
AUC según el mejor F1-score 0.9844537567355702
Confusion Matrix:
 [[15689   776]
 [  435  4725]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Month/confusion_matrix_param_160769.png
Accuracy:   0.9440
Precision:  0.8589
Recall:     0.9157
F1-score:   0.8864

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2918, Test Loss: 0.1765, F1: 0.8688, AUC: 0.9800
Epoch [10/30] Train Loss: 0.0882, Test Loss: 0.2078, F1: 0.8646, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0763, Test Loss: 0.2611, F1: 0.8533, AUC: 0.9803
Mejores resultados en la época:  4
f1-score 0.8878796480272495
AUC según el mejor F1-score 0.9839618511900978
Confusion Matrix:
 [[15748   717]
 [  468  4692]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Month/confusion_matrix_param_160769.png
Accuracy:   0.9452
Precision:  0.8674
Recall:     0.9093
F1-score:   0.8879

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2922, Test Loss: 0.1822, F1: 0.8666, AUC: 0.9799
Epoch [10/30] Train Loss: 0.0876, Test Loss: 0.1891, F1: 0.8700, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0601, Test Loss: 0.2368, F1: 0.8668, AUC: 0.9813
Mejores resultados en la época:  2
f1-score 0.8788737544565317
AUC según el mejor F1-score 0.9841087095718661
Confusion Matrix:
 [[15493   972]
 [  353  4807]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Month/confusion_matrix_param_160769.png
Accuracy:   0.9387
Precision:  0.8318
Recall:     0.9316
F1-score:   0.8789
Tiempo total para red 1: 287.02 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2243, Test Loss: 0.1761, F1: 0.8664, AUC: 0.9837
Epoch [10/30] Train Loss: 0.0040, Test Loss: 0.2290, F1: 0.9035, AUC: 0.9885
Epoch [20/30] Train Loss: 0.0025, Test Loss: 0.3118, F1: 0.9078, AUC: 0.9883
Mejores resultados en la época:  16
f1-score 0.9129447388342165
AUC según el mejor F1-score 0.9887257913779992
Confusion Matrix:
 [[15881   584]
 [  336  4824]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Month/confusion_matrix_param_5842945.png
Accuracy:   0.9575
Precision:  0.8920
Recall:     0.9349
F1-score:   0.9129

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2238, Test Loss: 0.1429, F1: 0.8867, AUC: 0.9825
Epoch [10/30] Train Loss: 0.0035, Test Loss: 0.3156, F1: 0.8818, AUC: 0.9875
Epoch [20/30] Train Loss: 0.0017, Test Loss: 0.3823, F1: 0.8769, AUC: 0.9886
Mejores resultados en la época:  13
f1-score 0.9146869289218044
AUC según el mejor F1-score 0.9875947923361043
Confusion Matrix:
 [[15983   482]
 [  405  4755]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Month/confusion_matrix_param_5842945.png
Accuracy:   0.9590
Precision:  0.9080
Recall:     0.9215
F1-score:   0.9147

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2269, Test Loss: 0.1795, F1: 0.8661, AUC: 0.9824
Epoch [10/30] Train Loss: 0.0025, Test Loss: 0.4035, F1: 0.8828, AUC: 0.9874
Epoch [20/30] Train Loss: 0.0022, Test Loss: 0.4039, F1: 0.9008, AUC: 0.9881
Mejores resultados en la época:  27
f1-score 0.9171367438241588
AUC según el mejor F1-score 0.9856695021386684
Confusion Matrix:
 [[16058   407]
 [  445  4715]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Month/confusion_matrix_param_5842945.png
Accuracy:   0.9606
Precision:  0.9205
Recall:     0.9138
F1-score:   0.9171
Tiempo total para red 6: 368.27 segundos
Saved on: outputs_ablation_remove_one_feature/1/Release_Month

==============================
Model: Logistic Regression
Accuracy:  0.9361
Precision: 0.8268
Recall:    0.9264
F1-score:  0.8738
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15464  1001]
 [  380  4780]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Month/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Month/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7647
Precision: 0.5038
Recall:    0.9192
F1-score:  0.6508
              precision    recall  f1-score   support

           0       0.97      0.72      0.82     16465
           1       0.50      0.92      0.65      5160

    accuracy                           0.76     21625
   macro avg       0.73      0.82      0.74     21625
weighted avg       0.86      0.76      0.78     21625

[[11793  4672]
 [  417  4743]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Month/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Month/svm_model.pkl

==============================
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:24:38] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:25:20] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_remove_one_feature.py:381: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Model: Decision Tree
Accuracy:  0.8704
Precision: 0.7096
Recall:    0.7734
F1-score:  0.7402
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14832  1633]
 [ 1169  3991]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Month/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Month/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8747
Precision: 0.7135
Recall:    0.7934
F1-score:  0.7513
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.71      0.79      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.87      0.88     21625

[[14821  1644]
 [ 1066  4094]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Month/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Month/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9222
Precision: 0.8026
Recall:    0.8938
F1-score:  0.8458
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15331  1134]
 [  548  4612]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Month/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Month/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Month/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Month/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8268, 'recall': 0.9264, 'f1_score': 0.8738}
XGBoost: {'accuracy': 0.9222, 'precision': 0.8026, 'recall': 0.8938, 'f1_score': 0.8458}
Random Forest: {'accuracy': 0.8747, 'precision': 0.7135, 'recall': 0.7934, 'f1_score': 0.7513}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.7096, 'recall': 0.7734, 'f1_score': 0.7402}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7647, 'precision': 0.5038, 'recall': 0.9192, 'f1_score': 0.6508}

##################################################
Running experiment without RELEASE_DAY feature
[Release_Day] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2961, Test Loss: 0.1770, F1: 0.8723, AUC: 0.9800
Epoch [10/30] Train Loss: 0.0776, Test Loss: 0.1967, F1: 0.8703, AUC: 0.9831
Epoch [20/30] Train Loss: 0.0506, Test Loss: 0.2587, F1: 0.8613, AUC: 0.9813
Mejores resultados en la época:  8
f1-score 0.8819782690146122
AUC según el mejor F1-score 0.9833739233092511
Confusion Matrix:
 [[15657   808]
 [  452  4708]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Day/confusion_matrix_param_160769.png
Accuracy:   0.9417
Precision:  0.8535
Recall:     0.9124
F1-score:   0.8820

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2955, Test Loss: 0.1843, F1: 0.8657, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0797, Test Loss: 0.1964, F1: 0.8708, AUC: 0.9831
Epoch [20/30] Train Loss: 0.0467, Test Loss: 0.2346, F1: 0.8725, AUC: 0.9822
Mejores resultados en la época:  2
f1-score 0.88712271738102
AUC según el mejor F1-score 0.9841722752279324
Confusion Matrix:
 [[15744   721]
 [  472  4688]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Day/confusion_matrix_param_160769.png
Accuracy:   0.9448
Precision:  0.8667
Recall:     0.9085
F1-score:   0.8871

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2973, Test Loss: 0.2012, F1: 0.8565, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0872, Test Loss: 0.2020, F1: 0.8637, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0618, Test Loss: 0.2570, F1: 0.8573, AUC: 0.9807
Mejores resultados en la época:  2
f1-score 0.8834663202820561
AUC según el mejor F1-score 0.9838941718044146
Confusion Matrix:
 [[15608   857]
 [  399  4761]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Day/confusion_matrix_param_160769.png
Accuracy:   0.9419
Precision:  0.8475
Recall:     0.9227
F1-score:   0.8835
Tiempo total para red 1: 276.87 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2235, Test Loss: 0.2035, F1: 0.8296, AUC: 0.9835
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.2916, F1: 0.9166, AUC: 0.9886
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.9006, F1: 0.9063, AUC: 0.9811
Mejores resultados en la época:  10
f1-score 0.9165534640015525
AUC según el mejor F1-score 0.9885947346614972
Confusion Matrix:
 [[16042   423]
 [  437  4723]]
Model: Decision Tree
Accuracy:  0.8704
Precision: 0.7096
Recall:    0.7734
F1-score:  0.7402
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14832  1633]
 [ 1169  3991]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Month/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Month/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8747
Precision: 0.7135
Recall:    0.7934
F1-score:  0.7513
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.71      0.79      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.87      0.88     21625

[[14821  1644]
 [ 1066  4094]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Month/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Month/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9222
Precision: 0.8026
Recall:    0.8938
F1-score:  0.8458
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15331  1134]
 [  548  4612]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Month/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Month/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Month/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Month/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8268, 'recall': 0.9264, 'f1_score': 0.8738}
XGBoost: {'accuracy': 0.9222, 'precision': 0.8026, 'recall': 0.8938, 'f1_score': 0.8458}
Random Forest: {'accuracy': 0.8747, 'precision': 0.7135, 'recall': 0.7934, 'f1_score': 0.7513}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.7096, 'recall': 0.7734, 'f1_score': 0.7402}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7647, 'precision': 0.5038, 'recall': 0.9192, 'f1_score': 0.6508}

##################################################
Running experiment without RELEASE_DAY feature
[Release_Day] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Numeric features shape: (108125, 22)

Splitting data (index-based split to avoid leakage)...
Concatenating TF-IDF and numeric features...
X_train_Numeric:  (86500, 5022)
X_train_Numeric:  (21625, 5022)
==================================================
Data antes del undersampling ...
X: (86500, 5022)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5022)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5022, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2964, Test Loss: 0.1757, F1: 0.8687, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0888, Test Loss: 0.2102, F1: 0.8649, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0758, Test Loss: 0.2493, F1: 0.8602, AUC: 0.9807
Mejores resultados en la época:  3
f1-score 0.8799116591515598
AUC según el mejor F1-score 0.9840753936586182
Confusion Matrix:
 [[15539   926]
 [  379  4781]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Day/confusion_matrix_param_160769.png
Accuracy:   0.9397
Precision:  0.8377
Recall:     0.9266
F1-score:   0.8799

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.2918, Test Loss: 0.1893, F1: 0.8612, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0865, Test Loss: 0.2063, F1: 0.8602, AUC: 0.9822
Epoch [20/30] Train Loss: 0.0569, Test Loss: 0.2534, F1: 0.8616, AUC: 0.9809
Mejores resultados en la época:  2
f1-score 0.88210094989756
AUC según el mejor F1-score 0.9839245745614965
Confusion Matrix:
 [[15623   842]
 [  424  4736]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Day/confusion_matrix_param_160769.png
Accuracy:   0.9415
Precision:  0.8490
Recall:     0.9178
F1-score:   0.8821

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.3014, Test Loss: 0.2092, F1: 0.8505, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0879, Test Loss: 0.1901, F1: 0.8686, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0635, Test Loss: 0.2417, F1: 0.8612, AUC: 0.9810
Mejores resultados en la época:  2
f1-score 0.88617192683158
AUC según el mejor F1-score 0.9840808786314406
Confusion Matrix:
 [[15749   716]
 [  485  4675]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Day/confusion_matrix_param_160769.png
Accuracy:   0.9445
Precision:  0.8672
Recall:     0.9060
F1-score:   0.8862
Tiempo total para red 1: 288.00 segundos

Entrenando red 6 con capas [5022, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2267, Test Loss: 0.1650, F1: 0.8788, AUC: 0.9839
Epoch [10/30] Train Loss: 0.0059, Test Loss: 0.3639, F1: 0.8798, AUC: 0.9880
Epoch [20/30] Train Loss: 0.0001, Test Loss: 0.4537, F1: 0.9038, AUC: 0.9877
Mejores resultados en la época:  17
f1-score 0.9183772740538962
AUC según el mejor F1-score 0.9886428458769718
Confusion Matrix:
 [[16066   399]
 [  440  4720]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:51:17] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:51:55] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Day/confusion_matrix_param_5842945.png
Accuracy:   0.9602
Precision:  0.9178
Recall:     0.9153
F1-score:   0.9166

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2216, Test Loss: 0.1879, F1: 0.8622, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0031, Test Loss: 0.3448, F1: 0.8971, AUC: 0.9882
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6396, F1: 0.8965, AUC: 0.9847
Mejores resultados en la época:  12
f1-score 0.9027071976346669
AUC según el mejor F1-score 0.987312769393381
Confusion Matrix:
 [[15687   778]
 [  275  4885]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Day/confusion_matrix_param_5842945.png
Accuracy:   0.9513
Precision:  0.8626
Recall:     0.9467
F1-score:   0.9027

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2248, Test Loss: 0.1845, F1: 0.8653, AUC: 0.9835
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.2718, F1: 0.9011, AUC: 0.9879
Epoch [20/30] Train Loss: 0.0022, Test Loss: 0.4002, F1: 0.9097, AUC: 0.9877
Mejores resultados en la época:  20
f1-score 0.9096901779828609
AUC según el mejor F1-score 0.987672099850046
Confusion Matrix:
 [[15836   629]
 [  330  4830]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Day/confusion_matrix_param_5842945.png
Accuracy:   0.9557
Precision:  0.8848
Recall:     0.9360
F1-score:   0.9097
Tiempo total para red 6: 367.57 segundos
Saved on: outputs_ablation_remove_one_feature/1/Release_Day

==============================
Model: Logistic Regression
Accuracy:  0.9360
Precision: 0.8264
Recall:    0.9266
F1-score:  0.8736
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15461  1004]
 [  379  4781]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Day/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Day/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7567
Precision: 0.4947
Recall:    0.9196
F1-score:  0.6433
              precision    recall  f1-score   support

           0       0.97      0.71      0.82     16465
           1       0.49      0.92      0.64      5160

    accuracy                           0.76     21625
   macro avg       0.73      0.81      0.73     21625
weighted avg       0.85      0.76      0.77     21625

[[11618  4847]
 [  415  4745]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Day/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Day/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8713
Precision: 0.7126
Recall:    0.7719
F1-score:  0.7411
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14859  1606]
 [ 1177  3983]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Day/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Day/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8755
Precision: 0.7159
Recall:    0.7926
F1-score:  0.7523
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.72      0.79      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.88      0.88     21625

[[14842  1623]
 [ 1070  4090]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Day/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Day/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9232
Precision: 0.8061
Recall:    0.8928
F1-score:  0.8473
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.81      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.89      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15357  1108]
 [  553  4607]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Day/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Day/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Day/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Day/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.936, 'precision': 0.8264, 'recall': 0.9266, 'f1_score': 0.8736}
XGBoost: {'accuracy': 0.9232, 'precision': 0.8061, 'recall': 0.8928, 'f1_score': 0.8473}
Random Forest: {'accuracy': 0.8755, 'precision': 0.7159, 'recall': 0.7926, 'f1_score': 0.7523}
Decision Tree: {'accuracy': 0.8713, 'precision': 0.7126, 'recall': 0.7719, 'f1_score': 0.7411}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7567, 'precision': 0.4947, 'recall': 0.9196, 'f1_score': 0.6433}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: EMOTION
MLP_5843969: {'accuracy': 0.9592138728323699, 'precision': 0.9199057714958775, 'recall': 0.9081395348837209, 'f1_score': 0.9139847864248098}
MLP_160801: {'accuracy': 0.942150289017341, 'precision': 0.8490801928915878, 'recall': 0.9213178294573643, 'f1_score': 0.8837252532763268}
Logistic Regression: {'accuracy': 0.9366, 'precision': 0.8275, 'recall': 0.9277, 'f1_score': 0.8747}
XGBoost: {'accuracy': 0.921, 'precision': 0.7993, 'recall': 0.8932, 'f1_score': 0.8437}
Decision Tree: {'accuracy': 0.864, 'precision': 0.6896, 'recall': 0.7822, 'f1_score': 0.733}
Random Forest: {'accuracy': 0.8535, 'precision': 0.6624, 'recall': 0.787, 'f1_score': 0.7193}
SVM: {'accuracy': 0.8118, 'precision': 0.5673, 'recall': 0.8899, 'f1_score': 0.6929}
Naive Bayes: {'accuracy': 0.7889, 'precision': 0.5315, 'recall': 0.9698, 'f1_score': 0.6867}


EMBEDDINGS TYPE: KEY
MLP_5843969: {'accuracy': 0.9571791907514451, 'precision': 0.8983816334211516, 'recall': 0.9251937984496124, 'f1_score': 0.9115906053083827}
MLP_160801: {'accuracy': 0.942335260115607, 'precision': 0.859452507808194, 'recall': 0.9065891472868217, 'f1_score': 0.8823917759124776}
Logistic Regression: {'accuracy': 0.936, 'precision': 0.8276, 'recall': 0.9246, 'f1_score': 0.8734}
XGBoost: {'accuracy': 0.9219, 'precision': 0.8036, 'recall': 0.8903, 'f1_score': 0.8447}
Random Forest: {'accuracy': 0.8677, 'precision': 0.695, 'recall': 0.794, 'f1_score': 0.7412}
Decision Tree: {'accuracy': 0.8706, 'precision': 0.7104, 'recall': 0.7725, 'f1_score': 0.7401}
Naive Bayes: {'accuracy': 0.7868, 'precision': 0.529, 'recall': 0.9696, 'f1_score': 0.6845}
SVM: {'accuracy': 0.7903, 'precision': 0.5365, 'recall': 0.8895, 'f1_score': 0.6693}


EMBEDDINGS TYPE: TIME SIGNATURE
MLP_5843969: {'accuracy': 0.9571329479768786, 'precision': 0.8958294370675145, 'recall': 0.9282945736434108, 'f1_score': 0.9117731036451889}
MLP_160801: {'accuracy': 0.9409942196531792, 'precision': 0.8475304223335719, 'recall': 0.9178294573643411, 'f1_score': 0.8812802381838482}
Logistic Regression: {'accuracy': 0.9359, 'precision': 0.8261, 'recall': 0.9262, 'f1_score': 0.8733}
XGBoost: {'accuracy': 0.9213, 'precision': 0.8015, 'recall': 0.8911, 'f1_score': 0.8439}
Decision Tree: {'accuracy': 0.8709, 'precision': 0.7116, 'recall': 0.7721, 'f1_score': 0.7406}
Random Forest: {'accuracy': 0.8635, 'precision': 0.6869, 'recall': 0.7866, 'f1_score': 0.7334}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7706, 'precision': 0.5108, 'recall': 0.9079, 'f1_score': 0.6538}


EMBEDDINGS TYPE: ARTIST(S)
MLP_5843969: {'accuracy': 0.9554219653179191, 'precision': 0.8824279985417426, 'recall': 0.9381782945736434, 'f1_score': 0.9094495585196318}
MLP_160801: {'accuracy': 0.9416878612716763, 'precision': 0.85297845373891, 'recall': 0.912984496124031, 'f1_score': 0.8819619956940934}
Logistic Regression: {'accuracy': 0.9353, 'precision': 0.8263, 'recall': 0.9229, 'f1_score': 0.8719}
XGBoost: {'accuracy': 0.9214, 'precision': 0.8007, 'recall': 0.8928, 'f1_score': 0.8442}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.7096, 'recall': 0.7734, 'f1_score': 0.7402}
Random Forest: {'accuracy': 0.8643, 'precision': 0.6865, 'recall': 0.794, 'f1_score': 0.7363}
SVM: {'accuracy': 0.7835, 'precision': 0.527, 'recall': 0.9054, 'f1_score': 0.6662}
Naive Bayes: {'accuracy': 0.7672, 'precision': 0.5063, 'recall': 0.9719, 'f1_score': 0.6658}


EMBEDDINGS TYPE: SONG
MLP_5843969: {'accuracy': 0.9569942196531792, 'precision': 0.8825976845151954, 'recall': 0.9455426356589147, 'f1_score': 0.9129865269461078}
MLP_160801: {'accuracy': 0.9424277456647399, 'precision': 0.8521316783594172, 'recall': 0.9180232558139535, 'f1_score': 0.883851105513574}
Logistic Regression: {'accuracy': 0.9383, 'precision': 0.8312, 'recall': 0.9304, 'f1_score': 0.878}
XGBoost: {'accuracy': 0.9219, 'precision': 0.8016, 'recall': 0.8942, 'f1_score': 0.8454}
Random Forest: {'accuracy': 0.8732, 'precision': 0.7102, 'recall': 0.7921, 'f1_score': 0.7489}
Decision Tree: {'accuracy': 0.8601, 'precision': 0.677, 'recall': 0.7913, 'f1_score': 0.7297}
SVM: {'accuracy': 0.8203, 'precision': 0.58, 'recall': 0.894, 'f1_score': 0.7036}
Naive Bayes: {'accuracy': 0.7985, 'precision': 0.5436, 'recall': 0.9711, 'f1_score': 0.697}


EMBEDDINGS TYPE: GENRE
MLP_5843969: {'accuracy': 0.96078612716763, 'precision': 0.9111365369946606, 'recall': 0.9259689922480621, 'f1_score': 0.9184928873510189}
MLP_160801: {'accuracy': 0.9439075144508671, 'precision': 0.8635107754650949, 'recall': 0.9085271317829458, 'f1_score': 0.8854471621494003}
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.8255, 'recall': 0.9248, 'f1_score': 0.8723}
XGBoost: {'accuracy': 0.9134, 'precision': 0.7786, 'recall': 0.8903, 'f1_score': 0.8307}
Random Forest: {'accuracy': 0.8759, 'precision': 0.7172, 'recall': 0.7926, 'f1_score': 0.753}
Decision Tree: {'accuracy': 0.8393, 'precision': 0.6313, 'recall': 0.7847, 'f1_score': 0.6997}
SVM: {'accuracy': 0.7968, 'precision': 0.546, 'recall': 0.8808, 'f1_score': 0.6741}
Naive Bayes: {'accuracy': 0.7754, 'precision': 0.5156, 'recall': 0.9717, 'f1_score': 0.6737}


EMBEDDINGS TYPE: ALBUM
MLP_5843969: {'accuracy': 0.9571329479768786, 'precision': 0.8928902914423612, 'recall': 0.9321705426356589, 'f1_score': 0.9121077083530862}
MLP_160801: {'accuracy': 0.9420115606936417, 'precision': 0.855349344978166, 'recall': 0.911046511627907, 'f1_score': 0.8823198198198198}
Logistic Regression: {'accuracy': 0.935, 'precision': 0.8254, 'recall': 0.9227, 'f1_score': 0.8713}
XGBoost: {'accuracy': 0.9197, 'precision': 0.7983, 'recall': 0.888, 'f1_score': 0.8407}
Random Forest: {'accuracy': 0.8706, 'precision': 0.7042, 'recall': 0.789, 'f1_score': 0.7442}
Decision Tree: {'accuracy': 0.8713, 'precision': 0.7128, 'recall': 0.7715, 'f1_score': 0.741}
Naive Bayes: {'accuracy': 0.7926, 'precision': 0.5362, 'recall': 0.9705, 'f1_score': 0.6908}
SVM: {'accuracy': 0.7669, 'precision': 0.5065, 'recall': 0.8973, 'f1_score': 0.6475}


EMBEDDINGS TYPE: SIMILAR ARTIST 1
MLP_5843969: {'accuracy': 0.9498265895953757, 'precision': 0.8843614412375024, 'recall': 0.9085271317829458, 'f1_score': 0.8962814262498805}
MLP_160801: {'accuracy': 0.9320693641618497, 'precision': 0.8234881682734444, 'recall': 0.9104651162790698, 'f1_score': 0.8647952139898758}
Logistic Regression: {'accuracy': 0.9277, 'precision': 0.8088, 'recall': 0.9126, 'f1_score': 0.8576}
XGBoost: {'accuracy': 0.9149, 'precision': 0.7869, 'recall': 0.8824, 'f1_score': 0.8319}
Decision Tree: {'accuracy': 0.8745, 'precision': 0.7227, 'recall': 0.7688, 'f1_score': 0.745}
Random Forest: {'accuracy': 0.8634, 'precision': 0.6865, 'recall': 0.7872, 'f1_score': 0.7334}
Naive Bayes: {'accuracy': 0.7514, 'precision': 0.4894, 'recall': 0.9692, 'f1_score': 0.6504}
SVM: {'accuracy': 0.7095, 'precision': 0.448, 'recall': 0.9359, 'f1_score': 0.6059}


EMBEDDINGS TYPE: SIMILAR SONG 1
MLP_5843969: {'accuracy': 0.9550520231213873, 'precision': 0.8884972170686456, 'recall': 0.9281007751937984, 'f1_score': 0.907867298578199}
MLP_160801: {'accuracy': 0.9440462427745665, 'precision': 0.8669639539204756, 'recall': 0.9042635658914728, 'f1_score': 0.8852210206791881}
Logistic Regression: {'accuracy': 0.9352, 'precision': 0.8254, 'recall': 0.9236, 'f1_score': 0.8718}
XGBoost: {'accuracy': 0.92, 'precision': 0.7986, 'recall': 0.8891, 'f1_score': 0.8414}
Decision Tree: {'accuracy': 0.8714, 'precision': 0.7133, 'recall': 0.7713, 'f1_score': 0.7412}
Random Forest: {'accuracy': 0.8664, 'precision': 0.6935, 'recall': 0.7886, 'f1_score': 0.738}
Naive Bayes: {'accuracy': 0.7886, 'precision': 0.5312, 'recall': 0.9721, 'f1_score': 0.687}
SVM: {'accuracy': 0.7384, 'precision': 0.4728, 'recall': 0.8397, 'f1_score': 0.605}


EMBEDDINGS TYPE: SIMILAR ARTIST 2
MLP_5843969: {'accuracy': 0.9454797687861272, 'precision': 0.8658334864914538, 'recall': 0.912984496124031, 'f1_score': 0.8887840769738704}
MLP_160801: {'accuracy': 0.9300346820809249, 'precision': 0.8187379828701276, 'recall': 0.9077519379844962, 'f1_score': 0.860950280305119}
Logistic Regression: {'accuracy': 0.9254, 'precision': 0.8028, 'recall': 0.9112, 'f1_score': 0.8536}
XGBoost: {'accuracy': 0.9157, 'precision': 0.7889, 'recall': 0.8833, 'f1_score': 0.8334}
Decision Tree: {'accuracy': 0.8697, 'precision': 0.709, 'recall': 0.7702, 'f1_score': 0.7383}
Random Forest: {'accuracy': 0.8641, 'precision': 0.6877, 'recall': 0.7886, 'f1_score': 0.7347}
SVM: {'accuracy': 0.8407, 'precision': 0.6591, 'recall': 0.6884, 'f1_score': 0.6734}
Naive Bayes: {'accuracy': 0.7479, 'precision': 0.4858, 'recall': 0.9667, 'f1_score': 0.6467}


EMBEDDINGS TYPE: SIMILAR SONG 2
MLP_5843969: {'accuracy': 0.952971098265896, 'precision': 0.8767048554282597, 'recall': 0.9343023255813954, 'f1_score': 0.90458767238953}
MLP_160801: {'accuracy': 0.9418265895953757, 'precision': 0.8529305354558611, 'recall': 0.9137596899224806, 'f1_score': 0.8822979041916168}
Logistic Regression: {'accuracy': 0.9349, 'precision': 0.8245, 'recall': 0.924, 'f1_score': 0.8714}
XGBoost: {'accuracy': 0.9206, 'precision': 0.8003, 'recall': 0.8891, 'f1_score': 0.8424}
Decision Tree: {'accuracy': 0.8792, 'precision': 0.7427, 'recall': 0.7556, 'f1_score': 0.7491}
Random Forest: {'accuracy': 0.8684, 'precision': 0.6983, 'recall': 0.7897, 'f1_score': 0.7412}
Naive Bayes: {'accuracy': 0.7905, 'precision': 0.5335, 'recall': 0.97, 'f1_score': 0.6884}
SVM: {'accuracy': 0.8254, 'precision': 0.6041, 'recall': 0.7787, 'f1_score': 0.6804}


EMBEDDINGS TYPE: SIMILAR ARTIST 3
MLP_5843969: {'accuracy': 0.9488554913294798, 'precision': 0.8828862863619191, 'recall': 0.9058139534883721, 'f1_score': 0.8942031758178688}
MLP_160801: {'accuracy': 0.9298034682080925, 'precision': 0.8209376101515685, 'recall': 0.9027131782945736, 'f1_score': 0.859885545504892}
Logistic Regression: {'accuracy': 0.9232, 'precision': 0.7986, 'recall': 0.907, 'f1_score': 0.8494}
XGBoost: {'accuracy': 0.9134, 'precision': 0.7856, 'recall': 0.8764, 'f1_score': 0.8285}
Random Forest: {'accuracy': 0.8641, 'precision': 0.6896, 'recall': 0.7828, 'f1_score': 0.7332}
Decision Tree: {'accuracy': 0.8599, 'precision': 0.6773, 'recall': 0.789, 'f1_score': 0.7289}
Naive Bayes: {'accuracy': 0.7434, 'precision': 0.4812, 'recall': 0.9663, 'f1_score': 0.6425}
SVM: {'accuracy': 0.7638, 'precision': 0.503, 'recall': 0.8519, 'f1_score': 0.6326}


EMBEDDINGS TYPE: SIMILAR SONG 3
MLP_5843969: {'accuracy': 0.9538034682080925, 'precision': 0.8857778601891341, 'recall': 0.9257751937984496, 'f1_score': 0.9053349758362551}
MLP_160801: {'accuracy': 0.9425202312138728, 'precision': 0.857978431730945, 'recall': 0.9096899224806202, 'f1_score': 0.8830777913648763}
Logistic Regression: {'accuracy': 0.9344, 'precision': 0.8241, 'recall': 0.9217, 'f1_score': 0.8702}
XGBoost: {'accuracy': 0.9204, 'precision': 0.7982, 'recall': 0.8917, 'f1_score': 0.8424}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.712, 'recall': 0.7671, 'f1_score': 0.7385}
Random Forest: {'accuracy': 0.8667, 'precision': 0.6945, 'recall': 0.7878, 'f1_score': 0.7382}
Naive Bayes: {'accuracy': 0.7891, 'precision': 0.5319, 'recall': 0.9705, 'f1_score': 0.6872}
SVM: {'accuracy': 0.7668, 'precision': 0.5063, 'recall': 0.9178, 'f1_score': 0.6526}


EMBEDDINGS TYPE: SONG_NORMALIZED
MLP_5843969: {'accuracy': 0.9592601156069365, 'precision': 0.8958371877890842, 'recall': 0.9383720930232559, 'f1_score': 0.9166114529105537}
MLP_160801: {'accuracy': 0.9452485549132948, 'precision': 0.8569120287253141, 'recall': 0.925, 'f1_score': 0.8896551724137931}
Logistic Regression: {'accuracy': 0.9381, 'precision': 0.8303, 'recall': 0.9308, 'f1_score': 0.8777}
XGBoost: {'accuracy': 0.9221, 'precision': 0.8026, 'recall': 0.893, 'f1_score': 0.8454}
Decision Tree: {'accuracy': 0.8707, 'precision': 0.7102, 'recall': 0.7738, 'f1_score': 0.7407}
Random Forest: {'accuracy': 0.8611, 'precision': 0.6799, 'recall': 0.7899, 'f1_score': 0.7308}
Naive Bayes: {'accuracy': 0.7991, 'precision': 0.5443, 'recall': 0.9711, 'f1_score': 0.6976}
SVM: {'accuracy': 0.7983, 'precision': 0.546, 'recall': 0.9176, 'f1_score': 0.6846}


EMBEDDINGS TYPE: ARTIST_NORMALIZED
MLP_5843969: {'accuracy': 0.9562543352601156, 'precision': 0.8844890510948905, 'recall': 0.9393410852713179, 'f1_score': 0.9110902255639097}
MLP_160801: {'accuracy': 0.9396531791907514, 'precision': 0.8379800105207785, 'recall': 0.9261627906976744, 'f1_score': 0.87986743993372}
Logistic Regression: {'accuracy': 0.9365, 'precision': 0.8284, 'recall': 0.9254, 'f1_score': 0.8742}
XGBoost: {'accuracy': 0.9212, 'precision': 0.8014, 'recall': 0.8901, 'f1_score': 0.8434}
Decision Tree: {'accuracy': 0.8703, 'precision': 0.7091, 'recall': 0.7738, 'f1_score': 0.7401}
Random Forest: {'accuracy': 0.8662, 'precision': 0.6931, 'recall': 0.7886, 'f1_score': 0.7377}
SVM: {'accuracy': 0.8018, 'precision': 0.5527, 'recall': 0.8895, 'f1_score': 0.6818}
Naive Bayes: {'accuracy': 0.7717, 'precision': 0.5113, 'recall': 0.9717, 'f1_score': 0.6701}


EMBEDDINGS TYPE: TEMPO
MLP_5842945: {'accuracy': 0.9561156069364162, 'precision': 0.8896909124560429, 'recall': 0.9315891472868217, 'f1_score': 0.9101580990248982}
MLP_160769: {'accuracy': 0.9390057803468208, 'precision': 0.8379377089565371, 'recall': 0.9228682170542636, 'f1_score': 0.8783546988840727}
Logistic Regression: {'accuracy': 0.9365, 'precision': 0.8275, 'recall': 0.9269, 'f1_score': 0.8744}
XGBoost: {'accuracy': 0.9214, 'precision': 0.8025, 'recall': 0.8897, 'f1_score': 0.8439}
Random Forest: {'accuracy': 0.8674, 'precision': 0.6952, 'recall': 0.7909, 'f1_score': 0.74}
Decision Tree: {'accuracy': 0.8697, 'precision': 0.7083, 'recall': 0.7717, 'f1_score': 0.7386}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7918, 'precision': 0.5392, 'recall': 0.8754, 'f1_score': 0.6674}


EMBEDDINGS TYPE: LENGTH
MLP_5842945: {'accuracy': 0.9577803468208093, 'precision': 0.9040913415794481, 'recall': 0.9207364341085271, 'f1_score': 0.9123379740758522}
MLP_160769: {'accuracy': 0.9435375722543352, 'precision': 0.8551848512173129, 'recall': 0.9189922480620155, 'f1_score': 0.8859411489957963}
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8266, 'recall': 0.9266, 'f1_score': 0.8737}
XGBoost: {'accuracy': 0.923, 'precision': 0.806, 'recall': 0.8921, 'f1_score': 0.8468}
Random Forest: {'accuracy': 0.869, 'precision': 0.7008, 'recall': 0.787, 'f1_score': 0.7414}
Decision Tree: {'accuracy': 0.8697, 'precision': 0.7082, 'recall': 0.7723, 'f1_score': 0.7389}
SVM: {'accuracy': 0.8702, 'precision': 0.7261, 'recall': 0.7326, 'f1_score': 0.7293}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}


EMBEDDINGS TYPE: LOUDNESS (DB)
MLP_5842945: {'accuracy': 0.9543583815028902, 'precision': 0.8777838131450298, 'recall': 0.9395348837209302, 'f1_score': 0.9076102218477956}
MLP_160769: {'accuracy': 0.9398843930635838, 'precision': 0.8431721194879089, 'recall': 0.9189922480620155, 'f1_score': 0.8794510385756676}
Logistic Regression: {'accuracy': 0.9364, 'precision': 0.8271, 'recall': 0.9271, 'f1_score': 0.8743}
XGBoost: {'accuracy': 0.9222, 'precision': 0.8031, 'recall': 0.8926, 'f1_score': 0.8455}
Random Forest: {'accuracy': 0.8723, 'precision': 0.7094, 'recall': 0.7876, 'f1_score': 0.7464}
Decision Tree: {'accuracy': 0.8718, 'precision': 0.7151, 'recall': 0.7692, 'f1_score': 0.7412}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.7542, 'precision': 0.4918, 'recall': 0.9112, 'f1_score': 0.6389}


EMBEDDINGS TYPE: POPULARITY
MLP_5842945: {'accuracy': 0.9581040462427746, 'precision': 0.896680343155539, 'recall': 0.931782945736434, 'f1_score': 0.9138946968256986}
MLP_160769: {'accuracy': 0.9398381502890173, 'precision': 0.847971145175834, 'recall': 0.9112403100775194, 'f1_score': 0.8784680056048575}
Logistic Regression: {'accuracy': 0.9359, 'precision': 0.8268, 'recall': 0.925, 'f1_score': 0.8731}
XGBoost: {'accuracy': 0.9214, 'precision': 0.8017, 'recall': 0.8911, 'f1_score': 0.8441}
Random Forest: {'accuracy': 0.868, 'precision': 0.6966, 'recall': 0.7919, 'f1_score': 0.7412}
Decision Tree: {'accuracy': 0.8706, 'precision': 0.7105, 'recall': 0.7725, 'f1_score': 0.7402}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7464, 'precision': 0.4838, 'recall': 0.9353, 'f1_score': 0.6377}


EMBEDDINGS TYPE: ENERGY
MLP_5842945: {'accuracy': 0.9577341040462428, 'precision': 0.8977144998126639, 'recall': 0.9286821705426357, 'f1_score': 0.9129357972947229}
MLP_160769: {'accuracy': 0.9420115606936417, 'precision': 0.8552200800291014, 'recall': 0.9112403100775194, 'f1_score': 0.8823419027960218}
Logistic Regression: {'accuracy': 0.9362, 'precision': 0.8274, 'recall': 0.926, 'f1_score': 0.8739}
XGBoost: {'accuracy': 0.9217, 'precision': 0.803, 'recall': 0.8903, 'f1_score': 0.8444}
Random Forest: {'accuracy': 0.8741, 'precision': 0.7129, 'recall': 0.7907, 'f1_score': 0.7498}
Decision Tree: {'accuracy': 0.8711, 'precision': 0.7122, 'recall': 0.7715, 'f1_score': 0.7407}
SVM: {'accuracy': 0.8418, 'precision': 0.6288, 'recall': 0.8221, 'f1_score': 0.7126}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}


EMBEDDINGS TYPE: DANCEABILITY
MLP_5842945: {'accuracy': 0.9561618497109826, 'precision': 0.8967596081386586, 'recall': 0.9224806201550387, 'f1_score': 0.9094382881161636}
MLP_160769: {'accuracy': 0.9393294797687861, 'precision': 0.8370707778556412, 'recall': 0.9259689922480621, 'f1_score': 0.8792786161207213}
Logistic Regression: {'accuracy': 0.9369, 'precision': 0.8296, 'recall': 0.9258, 'f1_score': 0.8751}
XGBoost: {'accuracy': 0.9197, 'precision': 0.7968, 'recall': 0.8907, 'f1_score': 0.8411}
Random Forest: {'accuracy': 0.8655, 'precision': 0.6897, 'recall': 0.793, 'f1_score': 0.7378}
Decision Tree: {'accuracy': 0.8598, 'precision': 0.6761, 'recall': 0.7917, 'f1_score': 0.7293}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.7461, 'precision': 0.4835, 'recall': 0.9399, 'f1_score': 0.6385}


EMBEDDINGS TYPE: POSITIVENESS
MLP_5842945: {'accuracy': 0.9573179190751445, 'precision': 0.8869406392694064, 'recall': 0.9410852713178295, 'f1_score': 0.9132110954395862}
MLP_160769: {'accuracy': 0.9411791907514451, 'precision': 0.8441926345609065, 'recall': 0.924031007751938, 'f1_score': 0.8823094004441154}
Logistic Regression: {'accuracy': 0.9362, 'precision': 0.8265, 'recall': 0.9271, 'f1_score': 0.8739}
XGBoost: {'accuracy': 0.9209, 'precision': 0.8005, 'recall': 0.8903, 'f1_score': 0.843}
Random Forest: {'accuracy': 0.8688, 'precision': 0.6988, 'recall': 0.7909, 'f1_score': 0.742}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.7098, 'recall': 0.7727, 'f1_score': 0.7399}
SVM: {'accuracy': 0.8278, 'precision': 0.5931, 'recall': 0.8868, 'f1_score': 0.7108}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}


EMBEDDINGS TYPE: SPEECHINESS
MLP_5842945: {'accuracy': 0.9561156069364162, 'precision': 0.8909935004642525, 'recall': 0.9298449612403101, 'f1_score': 0.9100047415836889}
MLP_160769: {'accuracy': 0.936, 'precision': 0.8293789253314724, 'recall': 0.9213178294573643, 'f1_score': 0.872934263679765}
Logistic Regression: {'accuracy': 0.9339, 'precision': 0.822, 'recall': 0.9227, 'f1_score': 0.8694}
XGBoost: {'accuracy': 0.92, 'precision': 0.8014, 'recall': 0.8837, 'f1_score': 0.8406}
Decision Tree: {'accuracy': 0.8627, 'precision': 0.7083, 'recall': 0.7215, 'f1_score': 0.7149}
Random Forest: {'accuracy': 0.8507, 'precision': 0.6569, 'recall': 0.7837, 'f1_score': 0.7147}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.7509, 'precision': 0.4884, 'recall': 0.9252, 'f1_score': 0.6393}


EMBEDDINGS TYPE: LIVENESS
MLP_5842945: {'accuracy': 0.9598612716763005, 'precision': 0.9119001919385796, 'recall': 0.9207364341085271, 'f1_score': 0.9162970106075217}
MLP_160769: {'accuracy': 0.9394682080924855, 'precision': 0.8406156023350433, 'recall': 0.9209302325581395, 'f1_score': 0.8789420142421159}
Logistic Regression: {'accuracy': 0.9364, 'precision': 0.8277, 'recall': 0.9264, 'f1_score': 0.8743}
XGBoost: {'accuracy': 0.9215, 'precision': 0.8004, 'recall': 0.8938, 'f1_score': 0.8445}
Random Forest: {'accuracy': 0.8728, 'precision': 0.7079, 'recall': 0.7948, 'f1_score': 0.7488}
SVM: {'accuracy': 0.863, 'precision': 0.6744, 'recall': 0.8236, 'f1_score': 0.7416}
Decision Tree: {'accuracy': 0.8703, 'precision': 0.7094, 'recall': 0.7731, 'f1_score': 0.7399}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}


EMBEDDINGS TYPE: ACOUSTICNESS
MLP_5842945: {'accuracy': 0.9581040462427746, 'precision': 0.9077837423312883, 'recall': 0.9176356589147286, 'f1_score': 0.9126831148804935}
MLP_160769: {'accuracy': 0.9394682080924855, 'precision': 0.8390561718612431, 'recall': 0.9234496124031008, 'f1_score': 0.8792324015130547}
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8267, 'recall': 0.9264, 'f1_score': 0.8737}
XGBoost: {'accuracy': 0.92, 'precision': 0.7981, 'recall': 0.8899, 'f1_score': 0.8415}
Random Forest: {'accuracy': 0.8745, 'precision': 0.7141, 'recall': 0.7905, 'f1_score': 0.7504}
Decision Tree: {'accuracy': 0.8691, 'precision': 0.7048, 'recall': 0.7767, 'f1_score': 0.739}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.8043, 'precision': 0.5568, 'recall': 0.8828, 'f1_score': 0.6829}


EMBEDDINGS TYPE: INSTRUMENTALNESS
MLP_5842945: {'accuracy': 0.9581040462427746, 'precision': 0.9117305458768873, 'recall': 0.9127906976744186, 'f1_score': 0.9122603137710633}
MLP_160769: {'accuracy': 0.9380346820809249, 'precision': 0.8408279800142755, 'recall': 0.9131782945736434, 'f1_score': 0.8755109624674842}
Logistic Regression: {'accuracy': 0.9347, 'precision': 0.8239, 'recall': 0.9236, 'f1_score': 0.8709}
XGBoost: {'accuracy': 0.9201, 'precision': 0.7993, 'recall': 0.8884, 'f1_score': 0.8415}
Random Forest: {'accuracy': 0.8733, 'precision': 0.7103, 'recall': 0.7922, 'f1_score': 0.7491}
Decision Tree: {'accuracy': 0.8742, 'precision': 0.7221, 'recall': 0.7684, 'f1_score': 0.7445}
Naive Bayes: {'accuracy': 0.788, 'precision': 0.5305, 'recall': 0.9698, 'f1_score': 0.6858}
SVM: {'accuracy': 0.7972, 'precision': 0.5454, 'recall': 0.9014, 'f1_score': 0.6796}


EMBEDDINGS TYPE: GOOD FOR PARTY
MLP_5842945: {'accuracy': 0.9571329479768786, 'precision': 0.8888480617306632, 'recall': 0.9375968992248062, 'f1_score': 0.9125719136093559}
MLP_160769: {'accuracy': 0.9437687861271676, 'precision': 0.8610399121201026, 'recall': 0.9114341085271318, 'f1_score': 0.885520617586142}
Logistic Regression: {'accuracy': 0.9365, 'precision': 0.8273, 'recall': 0.9273, 'f1_score': 0.8745}
XGBoost: {'accuracy': 0.9222, 'precision': 0.8032, 'recall': 0.8924, 'f1_score': 0.8455}
Random Forest: {'accuracy': 0.8777, 'precision': 0.7218, 'recall': 0.793, 'f1_score': 0.7557}
Decision Tree: {'accuracy': 0.8703, 'precision': 0.7091, 'recall': 0.7738, 'f1_score': 0.7401}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.764, 'precision': 0.5032, 'recall': 0.8593, 'f1_score': 0.6347}


EMBEDDINGS TYPE: GOOD FOR WORK/STUDY
MLP_5842945: {'accuracy': 0.955606936416185, 'precision': 0.8767491926803014, 'recall': 0.9470930232558139, 'f1_score': 0.9105645612073784}
MLP_160769: {'accuracy': 0.942242774566474, 'precision': 0.8497585405115364, 'recall': 0.9207364341085271, 'f1_score': 0.8838247604873966}
Logistic Regression: {'accuracy': 0.9359, 'precision': 0.826, 'recall': 0.9266, 'f1_score': 0.8734}
XGBoost: {'accuracy': 0.9218, 'precision': 0.8022, 'recall': 0.8921, 'f1_score': 0.8447}
Random Forest: {'accuracy': 0.8768, 'precision': 0.7214, 'recall': 0.7882, 'f1_score': 0.7533}
Decision Tree: {'accuracy': 0.8703, 'precision': 0.7095, 'recall': 0.7729, 'f1_score': 0.7398}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.7694, 'precision': 0.5093, 'recall': 0.9178, 'f1_score': 0.6551}


EMBEDDINGS TYPE: GOOD FOR RELAXATION/MEDITATION
MLP_5842945: {'accuracy': 0.9584739884393063, 'precision': 0.9016208066339992, 'recall': 0.9271317829457364, 'f1_score': 0.9141983565832219}
MLP_160769: {'accuracy': 0.943028901734104, 'precision': 0.8576110706482156, 'recall': 0.9127906976744186, 'f1_score': 0.8843409688321442}
Logistic Regression: {'accuracy': 0.9364, 'precision': 0.8278, 'recall': 0.926, 'f1_score': 0.8741}
XGBoost: {'accuracy': 0.9218, 'precision': 0.8023, 'recall': 0.8924, 'f1_score': 0.845}
Random Forest: {'accuracy': 0.8766, 'precision': 0.7199, 'recall': 0.7903, 'f1_score': 0.7534}
Decision Tree: {'accuracy': 0.8709, 'precision': 0.711, 'recall': 0.7736, 'f1_score': 0.741}
SVM: {'accuracy': 0.8134, 'precision': 0.5698, 'recall': 0.8895, 'f1_score': 0.6946}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}


EMBEDDINGS TYPE: GOOD FOR EXERCISE
MLP_5842945: {'accuracy': 0.9580578034682081, 'precision': 0.895333705149656, 'recall': 0.9333333333333333, 'f1_score': 0.9139387038618465}
MLP_160769: {'accuracy': 0.9400693641618497, 'precision': 0.8405005287275291, 'recall': 0.9242248062015503, 'f1_score': 0.8803765922097102}
Logistic Regression: {'accuracy': 0.9364, 'precision': 0.8277, 'recall': 0.9262, 'f1_score': 0.8742}
XGBoost: {'accuracy': 0.9213, 'precision': 0.802, 'recall': 0.8901, 'f1_score': 0.8438}
SVM: {'accuracy': 0.8729, 'precision': 0.7014, 'recall': 0.814, 'f1_score': 0.7535}
Random Forest: {'accuracy': 0.875, 'precision': 0.7148, 'recall': 0.7922, 'f1_score': 0.7515}
Decision Tree: {'accuracy': 0.8707, 'precision': 0.7103, 'recall': 0.7733, 'f1_score': 0.7405}
Naive Bayes: {'accuracy': 0.7877, 'precision': 0.5302, 'recall': 0.9698, 'f1_score': 0.6856}


EMBEDDINGS TYPE: GOOD FOR RUNNING
MLP_5842945: {'accuracy': 0.958936416184971, 'precision': 0.8982102908277405, 'recall': 0.9337209302325581, 'f1_score': 0.9156214367160775}
MLP_160769: {'accuracy': 0.9418728323699422, 'precision': 0.8480470840021401, 'recall': 0.9215116279069767, 'f1_score': 0.8832543884090276}
Logistic Regression: {'accuracy': 0.936, 'precision': 0.8266, 'recall': 0.926, 'f1_score': 0.8735}
XGBoost: {'accuracy': 0.9218, 'precision': 0.8016, 'recall': 0.8932, 'f1_score': 0.8449}
Random Forest: {'accuracy': 0.8747, 'precision': 0.7144, 'recall': 0.7915, 'f1_score': 0.7509}
Decision Tree: {'accuracy': 0.8709, 'precision': 0.7108, 'recall': 0.7736, 'f1_score': 0.7409}
SVM: {'accuracy': 0.825, 'precision': 0.5912, 'recall': 0.864, 'f1_score': 0.702}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}


EMBEDDINGS TYPE: GOOD FOR YOGA/STRETCHING
MLP_5842945: {'accuracy': 0.9591676300578035, 'precision': 0.9091256935144443, 'recall': 0.9209302325581395, 'f1_score': 0.9149898912101666}
MLP_160769: {'accuracy': 0.9417341040462428, 'precision': 0.8457446808510638, 'recall': 0.9244186046511628, 'f1_score': 0.8833333333333333}
Logistic Regression: {'accuracy': 0.9363, 'precision': 0.8275, 'recall': 0.9262, 'f1_score': 0.8741}
XGBoost: {'accuracy': 0.9222, 'precision': 0.8031, 'recall': 0.893, 'f1_score': 0.8457}
Random Forest: {'accuracy': 0.875, 'precision': 0.7157, 'recall': 0.7899, 'f1_score': 0.751}
Decision Tree: {'accuracy': 0.8705, 'precision': 0.7104, 'recall': 0.7721, 'f1_score': 0.74}
SVM: {'accuracy': 0.8434, 'precision': 0.6322, 'recall': 0.8215, 'f1_score': 0.7145}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}


EMBEDDINGS TYPE: GOOD FOR DRIVING
MLP_5842945: {'accuracy': 0.958150289017341, 'precision': 0.9060889482725711, 'recall': 0.9199612403100775, 'f1_score': 0.9129724011924224}
MLP_160769: {'accuracy': 0.9402543352601156, 'precision': 0.8396557780119424, 'recall': 0.9265503875968992, 'f1_score': 0.8809655426570849}
Logistic Regression: {'accuracy': 0.9362, 'precision': 0.8274, 'recall': 0.926, 'f1_score': 0.8739}
XGBoost: {'accuracy': 0.9215, 'precision': 0.8016, 'recall': 0.8919, 'f1_score': 0.8443}
Random Forest: {'accuracy': 0.8769, 'precision': 0.7212, 'recall': 0.7895, 'f1_score': 0.7538}
Decision Tree: {'accuracy': 0.8707, 'precision': 0.7102, 'recall': 0.7736, 'f1_score': 0.7406}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7729, 'precision': 0.5133, 'recall': 0.9304, 'f1_score': 0.6616}


EMBEDDINGS TYPE: GOOD FOR SOCIAL GATHERINGS
MLP_5842945: {'accuracy': 0.9576416184971098, 'precision': 0.9048073254483022, 'recall': 0.9191860465116279, 'f1_score': 0.911940011536243}
MLP_160769: {'accuracy': 0.9390982658959538, 'precision': 0.8357504805172112, 'recall': 0.9269379844961241, 'f1_score': 0.8789855738307452}
Logistic Regression: {'accuracy': 0.9363, 'precision': 0.827, 'recall': 0.9267, 'f1_score': 0.8741}
XGBoost: {'accuracy': 0.9219, 'precision': 0.8026, 'recall': 0.8922, 'f1_score': 0.8451}
Random Forest: {'accuracy': 0.8756, 'precision': 0.7172, 'recall': 0.7903, 'f1_score': 0.752}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.7097, 'recall': 0.7729, 'f1_score': 0.74}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.72, 'precision': 0.4574, 'recall': 0.9318, 'f1_score': 0.6136}


EMBEDDINGS TYPE: GOOD FOR MORNING ROUTINE
MLP_5842945: {'accuracy': 0.9594450867052023, 'precision': 0.9130183220829315, 'recall': 0.9174418604651163, 'f1_score': 0.9152247462542291}
MLP_160769: {'accuracy': 0.9426589595375723, 'precision': 0.8553299492385786, 'recall': 0.9143410852713179, 'f1_score': 0.8838516298239041}
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8268, 'recall': 0.9264, 'f1_score': 0.8738}
XGBoost: {'accuracy': 0.9214, 'precision': 0.8009, 'recall': 0.8924, 'f1_score': 0.8442}
Random Forest: {'accuracy': 0.8759, 'precision': 0.7177, 'recall': 0.7909, 'f1_score': 0.7525}
Decision Tree: {'accuracy': 0.8708, 'precision': 0.7105, 'recall': 0.774, 'f1_score': 0.7409}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7869, 'precision': 0.5319, 'recall': 0.8909, 'f1_score': 0.6661}


EMBEDDINGS TYPE: RELEASE_YEAR
MLP_5842945: {'accuracy': 0.9577341040462428, 'precision': 0.89198670605613, 'recall': 0.9362403100775194, 'f1_score': 0.9135779122541604}
MLP_160769: {'accuracy': 0.9376184971098266, 'precision': 0.8321422346173959, 'recall': 0.9251937984496124, 'f1_score': 0.8762044599431036}
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8304, 'recall': 0.9203, 'f1_score': 0.8731}
XGBoost: {'accuracy': 0.9216, 'precision': 0.8046, 'recall': 0.8868, 'f1_score': 0.8437}
Random Forest: {'accuracy': 0.879, 'precision': 0.7287, 'recall': 0.7855, 'f1_score': 0.756}
Decision Tree: {'accuracy': 0.8745, 'precision': 0.7238, 'recall': 0.7663, 'f1_score': 0.7444}
SVM: {'accuracy': 0.8635, 'precision': 0.6794, 'recall': 0.8103, 'f1_score': 0.7391}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}


EMBEDDINGS TYPE: RELEASE_MONTH
MLP_5842945: {'accuracy': 0.9550057803468208, 'precision': 0.8915279596035159, 'recall': 0.9238372093023256, 'f1_score': 0.9073950699533644}
MLP_160769: {'accuracy': 0.9427976878612717, 'precision': 0.8573510657678994, 'recall': 0.912015503875969, 'f1_score': 0.8838388581087426}
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8268, 'recall': 0.9264, 'f1_score': 0.8738}
XGBoost: {'accuracy': 0.9222, 'precision': 0.8026, 'recall': 0.8938, 'f1_score': 0.8458}
Random Forest: {'accuracy': 0.8747, 'precision': 0.7135, 'recall': 0.7934, 'f1_score': 0.7513}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.7096, 'recall': 0.7734, 'f1_score': 0.7402}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7647, 'precision': 0.5038, 'recall': 0.9192, 'f1_score': 0.6508}


EMBEDDINGS TYPE: RELEASE_DAY
MLP_5842945: {'accuracy': 0.9556531791907514, 'precision': 0.8847774317640593, 'recall': 0.936046511627907, 'f1_score': 0.9096901779828609}
MLP_160769: {'accuracy': 0.9419190751445087, 'precision': 0.8474546101815593, 'recall': 0.9226744186046512, 'f1_score': 0.8834663202820561}
Logistic Regression: {'accuracy': 0.936, 'precision': 0.8264, 'recall': 0.9266, 'f1_score': 0.8736}
XGBoost: {'accuracy': 0.9232, 'precision': 0.8061, 'recall': 0.8928, 'f1_score': 0.8473}
Random Forest: {'accuracy': 0.8755, 'precision': 0.7159, 'recall': 0.7926, 'f1_score': 0.7523}
Decision Tree: {'accuracy': 0.8713, 'precision': 0.7126, 'recall': 0.7719, 'f1_score': 0.7411}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7567, 'precision': 0.4947, 'recall': 0.9196, 'f1_score': 0.6433}
Feature:  emotion
Feature:  Key
Feature:  Time signature
Feature:  Artist(s)
Feature:  song
Feature:  Genre
Feature:  Album
Feature:  Similar Artist 1
Feature:  Similar Song 1
Feature:  Similar Artist 2
Feature:  Similar Song 2
Feature:  Similar Artist 3
Feature:  Similar Song 3
Feature:  song_normalized
Feature:  artist_normalized
Feature:  Tempo
Feature:  Length
Feature:  Loudness (db)
Feature:  Popularity
Feature:  Energy
Feature:  Danceability
Feature:  Positiveness
Feature:  Speechiness
Feature:  Liveness
Feature:  Acousticness
Feature:  Instrumentalness
Feature:  Good for Party
Feature:  Good for Work/Study
Feature:  Good for Relaxation/Meditation
Feature:  Good for Exercise
Feature:  Good for Running
Feature:  Good for Yoga/Stretching
Feature:  Good for Driving
Feature:  Good for Social Gatherings
Feature:  Good for Morning Routine
Feature:  Release_Year
Feature:  Release_Month
Feature:  Release_Day

 Mejores modelos por embedding:
                         embedding   best_model  ...    recall  f1_score
0                          emotion  MLP_5843969  ...  0.908140  0.913985
1                              Key  MLP_5843969  ...  0.925194  0.911591
2                   Time signature  MLP_5843969  ...  0.928295  0.911773
3                        Artist(s)  MLP_5843969  ...  0.938178  0.909450
4                             song  MLP_5843969  ...  0.945543  0.912987
5                            Genre  MLP_5843969  ...  0.925969  0.918493
6                            Album  MLP_5843969  ...  0.932171  0.912108
7                 Similar Artist 1  MLP_5843969  ...  0.908527  0.896281
8                   Similar Song 1  MLP_5843969  ...  0.928101  0.907867
9                 Similar Artist 2  MLP_5843969  ...  0.912984  0.888784
10                  Similar Song 2  MLP_5843969  ...  0.934302  0.904588
11                Similar Artist 3  MLP_5843969  ...  0.905814  0.894203
12                  Similar Song 3  MLP_5843969  ...  0.925775  0.905335
13                 song_normalized  MLP_5843969  ...  0.938372  0.916611
14               artist_normalized  MLP_5843969  ...  0.939341  0.911090
15                           Tempo  MLP_5842945  ...  0.931589  0.910158
16                          Length  MLP_5842945  ...  0.920736  0.912338
17                   Loudness (db)  MLP_5842945  ...  0.939535  0.907610
18                      Popularity  MLP_5842945  ...  0.931783  0.913895
19                          Energy  MLP_5842945  ...  0.928682  0.912936
20                    Danceability  MLP_5842945  ...  0.922481  0.909438
21                    Positiveness  MLP_5842945  ...  0.941085  0.913211
22                     Speechiness  MLP_5842945  ...  0.929845  0.910005
23                        Liveness  MLP_5842945  ...  0.920736  0.916297
24                    Acousticness  MLP_5842945  ...  0.917636  0.912683
25                Instrumentalness  MLP_5842945  ...  0.912791  0.912260
26                  Good for Party  MLP_5842945  ...  0.937597  0.912572
27             Good for Work/Study  MLP_5842945  ...  0.947093  0.910565
28  Good for Relaxation/Meditation  MLP_5842945  ...  0.927132  0.914198
29               Good for Exercise  MLP_5842945  ...  0.933333  0.913939
30                Good for Running  MLP_5842945  ...  0.933721  0.915621
31        Good for Yoga/Stretching  MLP_5842945  ...  0.920930  0.914990
32                Good for Driving  MLP_5842945  ...  0.919961  0.912972
33      Good for Social Gatherings  MLP_5842945  ...  0.919186  0.911940
34        Good for Morning Routine  MLP_5842945  ...  0.917442  0.915225
35                    Release_Year  MLP_5842945  ...  0.936240  0.913578
36                   Release_Month  MLP_5842945  ...  0.923837  0.907395
37                     Release_Day  MLP_5842945  ...  0.936047  0.909690

[38 rows x 8 columns]

 Mejor global: Genre con MLP_5843969 (f1=0.9185)

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
====================================

Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Day/confusion_matrix_param_5842945.png
Accuracy:   0.9612
Precision:  0.9221
Recall:     0.9147
F1-score:   0.9184

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2232, Test Loss: 0.1695, F1: 0.8813, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0036, Test Loss: 0.4435, F1: 0.9063, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8915, F1: 0.8984, AUC: 0.9805
Mejores resultados en la época:  11
f1-score 0.9094716801523084
AUC según el mejor F1-score 0.9877447698547778
Confusion Matrix:
 [[15897   568]
 [  383  4777]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Day/confusion_matrix_param_5842945.png
Accuracy:   0.9560
Precision:  0.8937
Recall:     0.9258
F1-score:   0.9095

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.2247, Test Loss: 0.1864, F1: 0.8530, AUC: 0.9831
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.4296, F1: 0.8938, AUC: 0.9872
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8086, F1: 0.8995, AUC: 0.9828
Mejores resultados en la época:  12
f1-score 0.9154698795180722
AUC según el mejor F1-score 0.9883500825099989
Confusion Matrix:
 [[15999   466]
 [  411  4749]]
Matriz de confusión guardada en: outputs_ablation_remove_one_feature/1/Release_Day/confusion_matrix_param_5842945.png
Accuracy:   0.9594
Precision:  0.9106
Recall:     0.9203
F1-score:   0.9155
Tiempo total para red 6: 368.69 segundos
Saved on: outputs_ablation_remove_one_feature/1/Release_Day

==============================
Model: Logistic Regression
Accuracy:  0.9360
Precision: 0.8264
Recall:    0.9266
F1-score:  0.8736
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15461  1004]
 [  379  4781]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Day/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Day/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7567
Precision: 0.4947
Recall:    0.9196
F1-score:  0.6433
              precision    recall  f1-score   support

           0       0.97      0.71      0.82     16465
           1       0.49      0.92      0.64      5160

    accuracy                           0.76     21625
   macro avg       0.73      0.81      0.73     21625
weighted avg       0.85      0.76      0.77     21625

[[11618  4847]
 [  415  4745]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Day/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Day/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8713
Precision: 0.7126
Recall:    0.7719
F1-score:  0.7411
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.71      0.77      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14859  1606]
 [ 1177  3983]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Day/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Day/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8755
Precision: 0.7159
Recall:    0.7926
F1-score:  0.7523
              precision    recall  f1-score   support

           0       0.93      0.90      0.92     16465
           1       0.72      0.79      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.88      0.88     21625

[[14842  1623]
 [ 1070  4090]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Day/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Day/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9232
Precision: 0.8061
Recall:    0.8928
F1-score:  0.8473
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.81      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.89      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15357  1108]
 [  553  4607]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Day/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Day/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7882
Precision: 0.5307
Recall:    0.9698
F1-score:  0.6860
              precision    recall  f1-score   support

           0       0.99      0.73      0.84     16465
           1       0.53      0.97      0.69      5160

    accuracy                           0.79     21625
   macro avg       0.76      0.85      0.76     21625
weighted avg       0.88      0.79      0.80     21625

[[12040  4425]
 [  156  5004]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_remove_one_feature/1/Release_Day/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_remove_one_feature/1/Release_Day/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.936, 'precision': 0.8264, 'recall': 0.9266, 'f1_score': 0.8736}
XGBoost: {'accuracy': 0.9232, 'precision': 0.8061, 'recall': 0.8928, 'f1_score': 0.8473}
Random Forest: {'accuracy': 0.8755, 'precision': 0.7159, 'recall': 0.7926, 'f1_score': 0.7523}
Decision Tree: {'accuracy': 0.8713, 'precision': 0.7126, 'recall': 0.7719, 'f1_score': 0.7411}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7567, 'precision': 0.4947, 'recall': 0.9196, 'f1_score': 0.6433}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: EMOTION
MLP_5843969: {'accuracy': 0.9551907514450867, 'precision': 0.885273028130171, 'recall': 0.9331395348837209, 'f1_score': 0.9085762807812058}
MLP_160801: {'accuracy': 0.9412716763005781, 'precision': 0.8467023172905526, 'recall': 0.9205426356589147, 'f1_score': 0.8820798514391829}
Logistic Regression: {'accuracy': 0.9366, 'precision': 0.8275, 'recall': 0.9277, 'f1_score': 0.8747}
XGBoost: {'accuracy': 0.921, 'precision': 0.7993, 'recall': 0.8932, 'f1_score': 0.8437}
Decision Tree: {'accuracy': 0.864, 'precision': 0.6896, 'recall': 0.7822, 'f1_score': 0.733}
Random Forest: {'accuracy': 0.8535, 'precision': 0.6624, 'recall': 0.787, 'f1_score': 0.7193}
SVM: {'accuracy': 0.8118, 'precision': 0.5673, 'recall': 0.8899, 'f1_score': 0.6929}
Naive Bayes: {'accuracy': 0.7889, 'precision': 0.5315, 'recall': 0.9698, 'f1_score': 0.6867}


EMBEDDINGS TYPE: KEY
MLP_5843969: {'accuracy': 0.958150289017341, 'precision': 0.896551724137931, 'recall': 0.9321705426356589, 'f1_score': 0.9140142517814727}
MLP_160801: {'accuracy': 0.9417341040462428, 'precision': 0.8505933117583603, 'recall': 0.916860465116279, 'f1_score': 0.8824846110800224}
Logistic Regression: {'accuracy': 0.936, 'precision': 0.8276, 'recall': 0.9246, 'f1_score': 0.8734}
XGBoost: {'accuracy': 0.9219, 'precision': 0.8036, 'recall': 0.8903, 'f1_score': 0.8447}
Random Forest: {'accuracy': 0.8677, 'precision': 0.695, 'recall': 0.794, 'f1_score': 0.7412}
Decision Tree: {'accuracy': 0.8706, 'precision': 0.7104, 'recall': 0.7725, 'f1_score': 0.7401}
Naive Bayes: {'accuracy': 0.7868, 'precision': 0.529, 'recall': 0.9696, 'f1_score': 0.6845}
SVM: {'accuracy': 0.7903, 'precision': 0.5365, 'recall': 0.8895, 'f1_score': 0.6693}


EMBEDDINGS TYPE: TIME SIGNATURE
MLP_5843969: {'accuracy': 0.9576878612716763, 'precision': 0.9039010466222646, 'recall': 0.9205426356589147, 'f1_score': 0.9121459433509361}
MLP_160801: {'accuracy': 0.9418265895953757, 'precision': 0.8529305354558611, 'recall': 0.9137596899224806, 'f1_score': 0.8822979041916168}
Logistic Regression: {'accuracy': 0.9359, 'precision': 0.8261, 'recall': 0.9262, 'f1_score': 0.8733}
XGBoost: {'accuracy': 0.9213, 'precision': 0.8015, 'recall': 0.8911, 'f1_score': 0.8439}
Decision Tree: {'accuracy': 0.8709, 'precision': 0.7116, 'recall': 0.7721, 'f1_score': 0.7406}
Random Forest: {'accuracy': 0.8635, 'precision': 0.6869, 'recall': 0.7866, 'f1_score': 0.7334}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7706, 'precision': 0.5108, 'recall': 0.9079, 'f1_score': 0.6538}


EMBEDDINGS TYPE: ARTIST(S)
MLP_5843969: {'accuracy': 0.9592601156069365, 'precision': 0.9252633671238323, 'recall': 0.9021317829457365, 'f1_score': 0.9135511726032775}
MLP_160801: {'accuracy': 0.9414566473988439, 'precision': 0.8513172140021653, 'recall': 0.9143410852713179, 'f1_score': 0.8817043543262941}
Logistic Regression: {'accuracy': 0.9353, 'precision': 0.8263, 'recall': 0.9229, 'f1_score': 0.8719}
XGBoost: {'accuracy': 0.9214, 'precision': 0.8007, 'recall': 0.8928, 'f1_score': 0.8442}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.7096, 'recall': 0.7734, 'f1_score': 0.7402}
Random Forest: {'accuracy': 0.8643, 'precision': 0.6865, 'recall': 0.794, 'f1_score': 0.7363}
SVM: {'accuracy': 0.7835, 'precision': 0.527, 'recall': 0.9054, 'f1_score': 0.6662}
Naive Bayes: {'accuracy': 0.7672, 'precision': 0.5063, 'recall': 0.9719, 'f1_score': 0.6658}


EMBEDDINGS TYPE: SONG
MLP_5843969: {'accuracy': 0.9610173410404624, 'precision': 0.9162970106075217, 'recall': 0.9207364341085271, 'f1_score': 0.918511358144031}
MLP_160801: {'accuracy': 0.9425202312138728, 'precision': 0.8489221450204881, 'recall': 0.9234496124031008, 'f1_score': 0.8846189547943933}
Logistic Regression: {'accuracy': 0.9383, 'precision': 0.8312, 'recall': 0.9304, 'f1_score': 0.878}
XGBoost: {'accuracy': 0.9219, 'precision': 0.8016, 'recall': 0.8942, 'f1_score': 0.8454}
Random Forest: {'accuracy': 0.8732, 'precision': 0.7102, 'recall': 0.7921, 'f1_score': 0.7489}
Decision Tree: {'accuracy': 0.8601, 'precision': 0.677, 'recall': 0.7913, 'f1_score': 0.7297}
SVM: {'accuracy': 0.8203, 'precision': 0.58, 'recall': 0.894, 'f1_score': 0.7036}
Naive Bayes: {'accuracy': 0.7985, 'precision': 0.5436, 'recall': 0.9711, 'f1_score': 0.697}


EMBEDDINGS TYPE: GENRE
MLP_5843969: {'accuracy': 0.9579653179190751, 'precision': 0.8915085651132806, 'recall': 0.937984496124031, 'f1_score': 0.9141561998300123}
MLP_160801: {'accuracy': 0.9410867052023122, 'precision': 0.845360824742268, 'recall': 0.9217054263565891, 'f1_score': 0.8818839236046727}
Logistic Regression: {'accuracy': 0.9354, 'precision': 0.8255, 'recall': 0.9248, 'f1_score': 0.8723}
XGBoost: {'accuracy': 0.9134, 'precision': 0.7786, 'recall': 0.8903, 'f1_score': 0.8307}
Random Forest: {'accuracy': 0.8759, 'precision': 0.7172, 'recall': 0.7926, 'f1_score': 0.753}
Decision Tree: {'accuracy': 0.8393, 'precision': 0.6313, 'recall': 0.7847, 'f1_score': 0.6997}
SVM: {'accuracy': 0.7968, 'precision': 0.546, 'recall': 0.8808, 'f1_score': 0.6741}
Naive Bayes: {'accuracy': 0.7754, 'precision': 0.5156, 'recall': 0.9717, 'f1_score': 0.6737}


EMBEDDINGS TYPE: ALBUM
MLP_5843969: {'accuracy': 0.9544971098265896, 'precision': 0.8811610076670318, 'recall': 0.9354651162790698, 'f1_score': 0.9075014100394811}
MLP_160801: {'accuracy': 0.9376184971098266, 'precision': 0.8324899668469726, 'recall': 0.9246124031007752, 'f1_score': 0.8761362592966669}
Logistic Regression: {'accuracy': 0.935, 'precision': 0.8254, 'recall': 0.9227, 'f1_score': 0.8713}
XGBoost: {'accuracy': 0.9197, 'precision': 0.7983, 'recall': 0.888, 'f1_score': 0.8407}
Random Forest: {'accuracy': 0.8706, 'precision': 0.7042, 'recall': 0.789, 'f1_score': 0.7442}
Decision Tree: {'accuracy': 0.8713, 'precision': 0.7128, 'recall': 0.7715, 'f1_score': 0.741}
Naive Bayes: {'accuracy': 0.7926, 'precision': 0.5362, 'recall': 0.9705, 'f1_score': 0.6908}
SVM: {'accuracy': 0.7669, 'precision': 0.5065, 'recall': 0.8973, 'f1_score': 0.6475}


EMBEDDINGS TYPE: SIMILAR ARTIST 1
MLP_5843969: {'accuracy': 0.9471907514450867, 'precision': 0.8652727272727273, 'recall': 0.9222868217054263, 'f1_score': 0.8928705440900563}
MLP_160801: {'accuracy': 0.9323468208092486, 'precision': 0.8315695067264574, 'recall': 0.8984496124031007, 'f1_score': 0.863716814159292}
Logistic Regression: {'accuracy': 0.9277, 'precision': 0.8088, 'recall': 0.9126, 'f1_score': 0.8576}
XGBoost: {'accuracy': 0.9149, 'precision': 0.7869, 'recall': 0.8824, 'f1_score': 0.8319}
Decision Tree: {'accuracy': 0.8745, 'precision': 0.7227, 'recall': 0.7688, 'f1_score': 0.745}
Random Forest: {'accuracy': 0.8634, 'precision': 0.6865, 'recall': 0.7872, 'f1_score': 0.7334}
Naive Bayes: {'accuracy': 0.7514, 'precision': 0.4894, 'recall': 0.9692, 'f1_score': 0.6504}
SVM: {'accuracy': 0.7095, 'precision': 0.448, 'recall': 0.9359, 'f1_score': 0.6059}


EMBEDDINGS TYPE: SIMILAR SONG 1
MLP_5843969: {'accuracy': 0.9514450867052023, 'precision': 0.8646202980837473, 'recall': 0.9443798449612403, 'f1_score': 0.9027417562060023}
MLP_160801: {'accuracy': 0.9391445086705202, 'precision': 0.8383802816901409, 'recall': 0.9228682170542636, 'f1_score': 0.8785977859778598}
Logistic Regression: {'accuracy': 0.9352, 'precision': 0.8254, 'recall': 0.9236, 'f1_score': 0.8718}
XGBoost: {'accuracy': 0.92, 'precision': 0.7986, 'recall': 0.8891, 'f1_score': 0.8414}
Decision Tree: {'accuracy': 0.8714, 'precision': 0.7133, 'recall': 0.7713, 'f1_score': 0.7412}
Random Forest: {'accuracy': 0.8664, 'precision': 0.6935, 'recall': 0.7886, 'f1_score': 0.738}
Naive Bayes: {'accuracy': 0.7886, 'precision': 0.5312, 'recall': 0.9721, 'f1_score': 0.687}
SVM: {'accuracy': 0.7384, 'precision': 0.4728, 'recall': 0.8397, 'f1_score': 0.605}


EMBEDDINGS TYPE: SIMILAR ARTIST 2
MLP_5843969: {'accuracy': 0.9475606936416185, 'precision': 0.8752796420581656, 'recall': 0.9098837209302325, 'f1_score': 0.8922462941847207}
MLP_160801: {'accuracy': 0.9308208092485549, 'precision': 0.8247075505140021, 'recall': 0.9017441860465116, 'f1_score': 0.8615071283095723}
Logistic Regression: {'accuracy': 0.9254, 'precision': 0.8028, 'recall': 0.9112, 'f1_score': 0.8536}
XGBoost: {'accuracy': 0.9157, 'precision': 0.7889, 'recall': 0.8833, 'f1_score': 0.8334}
Decision Tree: {'accuracy': 0.8697, 'precision': 0.709, 'recall': 0.7702, 'f1_score': 0.7383}
Random Forest: {'accuracy': 0.8641, 'precision': 0.6877, 'recall': 0.7886, 'f1_score': 0.7347}
SVM: {'accuracy': 0.8407, 'precision': 0.6591, 'recall': 0.6884, 'f1_score': 0.6734}
Naive Bayes: {'accuracy': 0.7479, 'precision': 0.4858, 'recall': 0.9667, 'f1_score': 0.6467}


EMBEDDINGS TYPE: SIMILAR SONG 2
MLP_5843969: {'accuracy': 0.9551445086705203, 'precision': 0.8865313653136532, 'recall': 0.9312015503875969, 'f1_score': 0.9083175803402647}
MLP_160801: {'accuracy': 0.9416416184971098, 'precision': 0.8515512265512265, 'recall': 0.914922480620155, 'f1_score': 0.8821001494768311}
Logistic Regression: {'accuracy': 0.9349, 'precision': 0.8245, 'recall': 0.924, 'f1_score': 0.8714}
XGBoost: {'accuracy': 0.9206, 'precision': 0.8003, 'recall': 0.8891, 'f1_score': 0.8424}
Decision Tree: {'accuracy': 0.8792, 'precision': 0.7427, 'recall': 0.7556, 'f1_score': 0.7491}
Random Forest: {'accuracy': 0.8684, 'precision': 0.6983, 'recall': 0.7897, 'f1_score': 0.7412}
Naive Bayes: {'accuracy': 0.7905, 'precision': 0.5335, 'recall': 0.97, 'f1_score': 0.6884}
SVM: {'accuracy': 0.8254, 'precision': 0.6041, 'recall': 0.7787, 'f1_score': 0.6804}


EMBEDDINGS TYPE: SIMILAR ARTIST 3
MLP_5843969: {'accuracy': 0.9491329479768786, 'precision': 0.8822975517890772, 'recall': 0.9079457364341085, 'f1_score': 0.894937917860554}
MLP_160801: {'accuracy': 0.9283699421965318, 'precision': 0.8105969378978153, 'recall': 0.9131782945736434, 'f1_score': 0.8588353230657068}
Logistic Regression: {'accuracy': 0.9232, 'precision': 0.7986, 'recall': 0.907, 'f1_score': 0.8494}
XGBoost: {'accuracy': 0.9134, 'precision': 0.7856, 'recall': 0.8764, 'f1_score': 0.8285}
Random Forest: {'accuracy': 0.8641, 'precision': 0.6896, 'recall': 0.7828, 'f1_score': 0.7332}
Decision Tree: {'accuracy': 0.8599, 'precision': 0.6773, 'recall': 0.789, 'f1_score': 0.7289}
Naive Bayes: {'accuracy': 0.7434, 'precision': 0.4812, 'recall': 0.9663, 'f1_score': 0.6425}
SVM: {'accuracy': 0.7638, 'precision': 0.503, 'recall': 0.8519, 'f1_score': 0.6326}


EMBEDDINGS TYPE: SIMILAR SONG 3
MLP_5843969: {'accuracy': 0.959121387283237, 'precision': 0.9170893484198205, 'recall': 0.911046511627907, 'f1_score': 0.9140579428349213}
MLP_160801: {'accuracy': 0.9386358381502891, 'precision': 0.8401064773735581, 'recall': 0.9174418604651163, 'f1_score': 0.8770727188513201}
Logistic Regression: {'accuracy': 0.9344, 'precision': 0.8241, 'recall': 0.9217, 'f1_score': 0.8702}
XGBoost: {'accuracy': 0.9204, 'precision': 0.7982, 'recall': 0.8917, 'f1_score': 0.8424}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.712, 'recall': 0.7671, 'f1_score': 0.7385}
Random Forest: {'accuracy': 0.8667, 'precision': 0.6945, 'recall': 0.7878, 'f1_score': 0.7382}
Naive Bayes: {'accuracy': 0.7891, 'precision': 0.5319, 'recall': 0.9705, 'f1_score': 0.6872}
SVM: {'accuracy': 0.7668, 'precision': 0.5063, 'recall': 0.9178, 'f1_score': 0.6526}


EMBEDDINGS TYPE: SONG_NORMALIZED
MLP_5843969: {'accuracy': 0.9596300578034682, 'precision': 0.8956987262322319, 'recall': 0.9403100775193799, 'f1_score': 0.9174624184551385}
MLP_160801: {'accuracy': 0.9415028901734104, 'precision': 0.8395815170008718, 'recall': 0.9331395348837209, 'f1_score': 0.8838916934373566}
Logistic Regression: {'accuracy': 0.9381, 'precision': 0.8303, 'recall': 0.9308, 'f1_score': 0.8777}
XGBoost: {'accuracy': 0.9221, 'precision': 0.8026, 'recall': 0.893, 'f1_score': 0.8454}
Decision Tree: {'accuracy': 0.8707, 'precision': 0.7102, 'recall': 0.7738, 'f1_score': 0.7407}
Random Forest: {'accuracy': 0.8611, 'precision': 0.6799, 'recall': 0.7899, 'f1_score': 0.7308}
Naive Bayes: {'accuracy': 0.7991, 'precision': 0.5443, 'recall': 0.9711, 'f1_score': 0.6976}
SVM: {'accuracy': 0.7983, 'precision': 0.546, 'recall': 0.9176, 'f1_score': 0.6846}


EMBEDDINGS TYPE: ARTIST_NORMALIZED
MLP_5843969: {'accuracy': 0.9575028901734104, 'precision': 0.8929034648879007, 'recall': 0.9339147286821705, 'f1_score': 0.9129487543809794}
MLP_160801: {'accuracy': 0.9436763005780346, 'precision': 0.861518708730741, 'recall': 0.9102713178294574, 'f1_score': 0.8852242744063324}
Logistic Regression: {'accuracy': 0.9365, 'precision': 0.8284, 'recall': 0.9254, 'f1_score': 0.8742}
XGBoost: {'accuracy': 0.9212, 'precision': 0.8014, 'recall': 0.8901, 'f1_score': 0.8434}
Decision Tree: {'accuracy': 0.8703, 'precision': 0.7091, 'recall': 0.7738, 'f1_score': 0.7401}
Random Forest: {'accuracy': 0.8662, 'precision': 0.6931, 'recall': 0.7886, 'f1_score': 0.7377}
SVM: {'accuracy': 0.8018, 'precision': 0.5527, 'recall': 0.8895, 'f1_score': 0.6818}
Naive Bayes: {'accuracy': 0.7717, 'precision': 0.5113, 'recall': 0.9717, 'f1_score': 0.6701}


EMBEDDINGS TYPE: TEMPO
MLP_5842945: {'accuracy': 0.9540809248554913, 'precision': 0.8802701222850885, 'recall': 0.9346899224806201, 'f1_score': 0.9066641601654291}
MLP_160769: {'accuracy': 0.9403468208092486, 'precision': 0.8434504792332268, 'recall': 0.9209302325581395, 'f1_score': 0.8804891606448026}
Logistic Regression: {'accuracy': 0.9365, 'precision': 0.8275, 'recall': 0.9269, 'f1_score': 0.8744}
XGBoost: {'accuracy': 0.9214, 'precision': 0.8025, 'recall': 0.8897, 'f1_score': 0.8439}
Random Forest: {'accuracy': 0.8674, 'precision': 0.6952, 'recall': 0.7909, 'f1_score': 0.74}
Decision Tree: {'accuracy': 0.8697, 'precision': 0.7083, 'recall': 0.7717, 'f1_score': 0.7386}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7918, 'precision': 0.5392, 'recall': 0.8754, 'f1_score': 0.6674}


EMBEDDINGS TYPE: LENGTH
MLP_5842945: {'accuracy': 0.9525549132947977, 'precision': 0.8698997852541159, 'recall': 0.9420542635658915, 'f1_score': 0.904540379605508}
MLP_160769: {'accuracy': 0.9403468208092486, 'precision': 0.8429634881247784, 'recall': 0.9217054263565891, 'f1_score': 0.8805776708017033}
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8266, 'recall': 0.9266, 'f1_score': 0.8737}
XGBoost: {'accuracy': 0.923, 'precision': 0.806, 'recall': 0.8921, 'f1_score': 0.8468}
Random Forest: {'accuracy': 0.869, 'precision': 0.7008, 'recall': 0.787, 'f1_score': 0.7414}
Decision Tree: {'accuracy': 0.8697, 'precision': 0.7082, 'recall': 0.7723, 'f1_score': 0.7389}
SVM: {'accuracy': 0.8702, 'precision': 0.7261, 'recall': 0.7326, 'f1_score': 0.7293}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}


EMBEDDINGS TYPE: LOUDNESS (DB)
MLP_5842945: {'accuracy': 0.9564393063583815, 'precision': 0.8804112554112554, 'recall': 0.9459302325581396, 'f1_score': 0.9119955156950673}
MLP_160769: {'accuracy': 0.940393063583815, 'precision': 0.8437222518202806, 'recall': 0.9207364341085271, 'f1_score': 0.8805486053192475}
Logistic Regression: {'accuracy': 0.9364, 'precision': 0.8271, 'recall': 0.9271, 'f1_score': 0.8743}
XGBoost: {'accuracy': 0.9222, 'precision': 0.8031, 'recall': 0.8926, 'f1_score': 0.8455}
Random Forest: {'accuracy': 0.8723, 'precision': 0.7094, 'recall': 0.7876, 'f1_score': 0.7464}
Decision Tree: {'accuracy': 0.8718, 'precision': 0.7151, 'recall': 0.7692, 'f1_score': 0.7412}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.7542, 'precision': 0.4918, 'recall': 0.9112, 'f1_score': 0.6389}


EMBEDDINGS TYPE: POPULARITY
MLP_5842945: {'accuracy': 0.9575028901734104, 'precision': 0.8917421023462035, 'recall': 0.9354651162790698, 'f1_score': 0.9130804880355623}
MLP_160769: {'accuracy': 0.9405780346820809, 'precision': 0.8448122441715608, 'recall': 0.9199612403100775, 'f1_score': 0.8807867149086186}
Logistic Regression: {'accuracy': 0.9359, 'precision': 0.8268, 'recall': 0.925, 'f1_score': 0.8731}
XGBoost: {'accuracy': 0.9214, 'precision': 0.8017, 'recall': 0.8911, 'f1_score': 0.8441}
Random Forest: {'accuracy': 0.868, 'precision': 0.6966, 'recall': 0.7919, 'f1_score': 0.7412}
Decision Tree: {'accuracy': 0.8706, 'precision': 0.7105, 'recall': 0.7725, 'f1_score': 0.7402}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7464, 'precision': 0.4838, 'recall': 0.9353, 'f1_score': 0.6377}


EMBEDDINGS TYPE: ENERGY
MLP_5842945: {'accuracy': 0.9548670520231214, 'precision': 0.8847002574475911, 'recall': 0.9323643410852713, 'f1_score': 0.9079071522928854}
MLP_160769: {'accuracy': 0.941364161849711, 'precision': 0.8452803406671399, 'recall': 0.9232558139534883, 'f1_score': 0.8825490922563912}
Logistic Regression: {'accuracy': 0.9362, 'precision': 0.8274, 'recall': 0.926, 'f1_score': 0.8739}
XGBoost: {'accuracy': 0.9217, 'precision': 0.803, 'recall': 0.8903, 'f1_score': 0.8444}
Random Forest: {'accuracy': 0.8741, 'precision': 0.7129, 'recall': 0.7907, 'f1_score': 0.7498}
Decision Tree: {'accuracy': 0.8711, 'precision': 0.7122, 'recall': 0.7715, 'f1_score': 0.7407}
SVM: {'accuracy': 0.8418, 'precision': 0.6288, 'recall': 0.8221, 'f1_score': 0.7126}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}


EMBEDDINGS TYPE: DANCEABILITY
MLP_5842945: {'accuracy': 0.9550057803468208, 'precision': 0.8827939294203694, 'recall': 0.9356589147286821, 'f1_score': 0.9084579922852574}
MLP_160769: {'accuracy': 0.9428439306358382, 'precision': 0.8522441651705566, 'recall': 0.9199612403100775, 'f1_score': 0.8848089468779125}
Logistic Regression: {'accuracy': 0.9369, 'precision': 0.8296, 'recall': 0.9258, 'f1_score': 0.8751}
XGBoost: {'accuracy': 0.9197, 'precision': 0.7968, 'recall': 0.8907, 'f1_score': 0.8411}
Random Forest: {'accuracy': 0.8655, 'precision': 0.6897, 'recall': 0.793, 'f1_score': 0.7378}
Decision Tree: {'accuracy': 0.8598, 'precision': 0.6761, 'recall': 0.7917, 'f1_score': 0.7293}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.7461, 'precision': 0.4835, 'recall': 0.9399, 'f1_score': 0.6385}


EMBEDDINGS TYPE: POSITIVENESS
MLP_5842945: {'accuracy': 0.9587052023121387, 'precision': 0.9000562535158447, 'recall': 0.9302325581395349, 'f1_score': 0.9148956447155246}
MLP_160769: {'accuracy': 0.9415491329479769, 'precision': 0.8457578984735534, 'recall': 0.9234496124031008, 'f1_score': 0.8828979062442097}
Logistic Regression: {'accuracy': 0.9362, 'precision': 0.8265, 'recall': 0.9271, 'f1_score': 0.8739}
XGBoost: {'accuracy': 0.9209, 'precision': 0.8005, 'recall': 0.8903, 'f1_score': 0.843}
Random Forest: {'accuracy': 0.8688, 'precision': 0.6988, 'recall': 0.7909, 'f1_score': 0.742}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.7098, 'recall': 0.7727, 'f1_score': 0.7399}
SVM: {'accuracy': 0.8278, 'precision': 0.5931, 'recall': 0.8868, 'f1_score': 0.7108}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}


EMBEDDINGS TYPE: SPEECHINESS
MLP_5842945: {'accuracy': 0.9548670520231214, 'precision': 0.8773448773448773, 'recall': 0.9426356589147287, 'f1_score': 0.9088191330343797}
MLP_160769: {'accuracy': 0.9389595375722544, 'precision': 0.8389830508474576, 'recall': 0.9209302325581395, 'f1_score': 0.8780487804878049}
Logistic Regression: {'accuracy': 0.9339, 'precision': 0.822, 'recall': 0.9227, 'f1_score': 0.8694}
XGBoost: {'accuracy': 0.92, 'precision': 0.8014, 'recall': 0.8837, 'f1_score': 0.8406}
Decision Tree: {'accuracy': 0.8627, 'precision': 0.7083, 'recall': 0.7215, 'f1_score': 0.7149}
Random Forest: {'accuracy': 0.8507, 'precision': 0.6569, 'recall': 0.7837, 'f1_score': 0.7147}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.7509, 'precision': 0.4884, 'recall': 0.9252, 'f1_score': 0.6393}


EMBEDDINGS TYPE: LIVENESS
MLP_5842945: {'accuracy': 0.9565317919075145, 'precision': 0.8881530537159676, 'recall': 0.9356589147286821, 'f1_score': 0.9112872782181956}
MLP_160769: {'accuracy': 0.9424277456647399, 'precision': 0.8539143012113541, 'recall': 0.9153100775193799, 'f1_score': 0.8835469086147226}
Logistic Regression: {'accuracy': 0.9364, 'precision': 0.8277, 'recall': 0.9264, 'f1_score': 0.8743}
XGBoost: {'accuracy': 0.9215, 'precision': 0.8004, 'recall': 0.8938, 'f1_score': 0.8445}
Random Forest: {'accuracy': 0.8728, 'precision': 0.7079, 'recall': 0.7948, 'f1_score': 0.7488}
SVM: {'accuracy': 0.863, 'precision': 0.6744, 'recall': 0.8236, 'f1_score': 0.7416}
Decision Tree: {'accuracy': 0.8703, 'precision': 0.7094, 'recall': 0.7731, 'f1_score': 0.7399}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}


EMBEDDINGS TYPE: ACOUSTICNESS
MLP_5842945: {'accuracy': 0.9521387283236994, 'precision': 0.8626692456479691, 'recall': 0.9507751937984497, 'f1_score': 0.9045819120494146}
MLP_160769: {'accuracy': 0.939606936416185, 'precision': 0.8363001745200698, 'recall': 0.9286821705426357, 'f1_score': 0.8800734618916437}
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8267, 'recall': 0.9264, 'f1_score': 0.8737}
XGBoost: {'accuracy': 0.92, 'precision': 0.7981, 'recall': 0.8899, 'f1_score': 0.8415}
Random Forest: {'accuracy': 0.8745, 'precision': 0.7141, 'recall': 0.7905, 'f1_score': 0.7504}
Decision Tree: {'accuracy': 0.8691, 'precision': 0.7048, 'recall': 0.7767, 'f1_score': 0.739}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.8043, 'precision': 0.5568, 'recall': 0.8828, 'f1_score': 0.6829}


EMBEDDINGS TYPE: INSTRUMENTALNESS
MLP_5842945: {'accuracy': 0.9575953757225434, 'precision': 0.8946976744186047, 'recall': 0.9319767441860465, 'f1_score': 0.9129568106312292}
MLP_160769: {'accuracy': 0.9402080924855492, 'precision': 0.8408249603384452, 'recall': 0.9244186046511628, 'f1_score': 0.8806424813071172}
Logistic Regression: {'accuracy': 0.9347, 'precision': 0.8239, 'recall': 0.9236, 'f1_score': 0.8709}
XGBoost: {'accuracy': 0.9201, 'precision': 0.7993, 'recall': 0.8884, 'f1_score': 0.8415}
Random Forest: {'accuracy': 0.8733, 'precision': 0.7103, 'recall': 0.7922, 'f1_score': 0.7491}
Decision Tree: {'accuracy': 0.8742, 'precision': 0.7221, 'recall': 0.7684, 'f1_score': 0.7445}
Naive Bayes: {'accuracy': 0.788, 'precision': 0.5305, 'recall': 0.9698, 'f1_score': 0.6858}
SVM: {'accuracy': 0.7972, 'precision': 0.5454, 'recall': 0.9014, 'f1_score': 0.6796}


EMBEDDINGS TYPE: GOOD FOR PARTY
MLP_5842945: {'accuracy': 0.9574104046242775, 'precision': 0.8952079060227485, 'recall': 0.9304263565891473, 'f1_score': 0.912477430390573}
MLP_160769: {'accuracy': 0.941364161849711, 'precision': 0.8481216457960644, 'recall': 0.9187984496124031, 'f1_score': 0.8820465116279069}
Logistic Regression: {'accuracy': 0.9365, 'precision': 0.8273, 'recall': 0.9273, 'f1_score': 0.8745}
XGBoost: {'accuracy': 0.9222, 'precision': 0.8032, 'recall': 0.8924, 'f1_score': 0.8455}
Random Forest: {'accuracy': 0.8777, 'precision': 0.7218, 'recall': 0.793, 'f1_score': 0.7557}
Decision Tree: {'accuracy': 0.8703, 'precision': 0.7091, 'recall': 0.7738, 'f1_score': 0.7401}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.764, 'precision': 0.5032, 'recall': 0.8593, 'f1_score': 0.6347}


EMBEDDINGS TYPE: GOOD FOR WORK/STUDY
MLP_5842945: {'accuracy': 0.9545433526011561, 'precision': 0.8807657247037375, 'recall': 0.9362403100775194, 'f1_score': 0.9076561766087365}
MLP_160769: {'accuracy': 0.9413179190751445, 'precision': 0.8447634237107922, 'recall': 0.9238372093023256, 'f1_score': 0.8825326298250485}
Logistic Regression: {'accuracy': 0.9359, 'precision': 0.826, 'recall': 0.9266, 'f1_score': 0.8734}
XGBoost: {'accuracy': 0.9218, 'precision': 0.8022, 'recall': 0.8921, 'f1_score': 0.8447}
Random Forest: {'accuracy': 0.8768, 'precision': 0.7214, 'recall': 0.7882, 'f1_score': 0.7533}
Decision Tree: {'accuracy': 0.8703, 'precision': 0.7095, 'recall': 0.7729, 'f1_score': 0.7398}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}
SVM: {'accuracy': 0.7694, 'precision': 0.5093, 'recall': 0.9178, 'f1_score': 0.6551}


EMBEDDINGS TYPE: GOOD FOR RELAXATION/MEDITATION
MLP_5842945: {'accuracy': 0.9552369942196531, 'precision': 0.8872875092387288, 'recall': 0.9306201550387597, 'f1_score': 0.9084373817631479}
MLP_160769: {'accuracy': 0.9418265895953757, 'precision': 0.8470295268587691, 'recall': 0.9228682170542636, 'f1_score': 0.8833240586162122}
Logistic Regression: {'accuracy': 0.9364, 'precision': 0.8278, 'recall': 0.926, 'f1_score': 0.8741}
XGBoost: {'accuracy': 0.9218, 'precision': 0.8023, 'recall': 0.8924, 'f1_score': 0.845}
Random Forest: {'accuracy': 0.8766, 'precision': 0.7199, 'recall': 0.7903, 'f1_score': 0.7534}
Decision Tree: {'accuracy': 0.8709, 'precision': 0.711, 'recall': 0.7736, 'f1_score': 0.741}
SVM: {'accuracy': 0.8134, 'precision': 0.5698, 'recall': 0.8895, 'f1_score': 0.6946}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}


EMBEDDINGS TYPE: GOOD FOR EXERCISE
MLP_5842945: {'accuracy': 0.9561156069364162, 'precision': 0.8865430512208555, 'recall': 0.9358527131782945, 'f1_score': 0.9105307815593476}
MLP_160769: {'accuracy': 0.9419653179190751, 'precision': 0.8522460761320585, 'recall': 0.9155038759689923, 'f1_score': 0.8827431561244511}
Logistic Regression: {'accuracy': 0.9364, 'precision': 0.8277, 'recall': 0.9262, 'f1_score': 0.8742}
XGBoost: {'accuracy': 0.9213, 'precision': 0.802, 'recall': 0.8901, 'f1_score': 0.8438}
SVM: {'accuracy': 0.8729, 'precision': 0.7014, 'recall': 0.814, 'f1_score': 0.7535}
Random Forest: {'accuracy': 0.875, 'precision': 0.7148, 'recall': 0.7922, 'f1_score': 0.7515}
Decision Tree: {'accuracy': 0.8707, 'precision': 0.7103, 'recall': 0.7733, 'f1_score': 0.7405}
Naive Bayes: {'accuracy': 0.7877, 'precision': 0.5302, 'recall': 0.9698, 'f1_score': 0.6856}


EMBEDDINGS TYPE: GOOD FOR RUNNING
MLP_5842945: {'accuracy': 0.9605549132947977, 'precision': 0.9245022669032131, 'recall': 0.9089147286821705, 'f1_score': 0.9166422359034496}
MLP_160769: {'accuracy': 0.9426589595375723, 'precision': 0.8593692702603594, 'recall': 0.9083333333333333, 'f1_score': 0.8831731675146034}
Logistic Regression: {'accuracy': 0.936, 'precision': 0.8266, 'recall': 0.926, 'f1_score': 0.8735}
XGBoost: {'accuracy': 0.9218, 'precision': 0.8016, 'recall': 0.8932, 'f1_score': 0.8449}
Random Forest: {'accuracy': 0.8747, 'precision': 0.7144, 'recall': 0.7915, 'f1_score': 0.7509}
Decision Tree: {'accuracy': 0.8709, 'precision': 0.7108, 'recall': 0.7736, 'f1_score': 0.7409}
SVM: {'accuracy': 0.825, 'precision': 0.5912, 'recall': 0.864, 'f1_score': 0.702}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}


EMBEDDINGS TYPE: GOOD FOR YOGA/STRETCHING
MLP_5842945: {'accuracy': 0.9570867052023121, 'precision': 0.8882568807339449, 'recall': 0.9381782945736434, 'f1_score': 0.9125353440150801}
MLP_160769: {'accuracy': 0.9397919075144509, 'precision': 0.8383023500526131, 'recall': 0.9263565891472868, 'f1_score': 0.8801325722703002}
Logistic Regression: {'accuracy': 0.9363, 'precision': 0.8275, 'recall': 0.9262, 'f1_score': 0.8741}
XGBoost: {'accuracy': 0.9222, 'precision': 0.8031, 'recall': 0.893, 'f1_score': 0.8457}
Random Forest: {'accuracy': 0.875, 'precision': 0.7157, 'recall': 0.7899, 'f1_score': 0.751}
Decision Tree: {'accuracy': 0.8705, 'precision': 0.7104, 'recall': 0.7721, 'f1_score': 0.74}
SVM: {'accuracy': 0.8434, 'precision': 0.6322, 'recall': 0.8215, 'f1_score': 0.7145}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}


EMBEDDINGS TYPE: GOOD FOR DRIVING
MLP_5842945: {'accuracy': 0.9597687861271677, 'precision': 0.9148936170212766, 'recall': 0.9166666666666666, 'f1_score': 0.9157792836398838}
MLP_160769: {'accuracy': 0.9412716763005781, 'precision': 0.8452254171104011, 'recall': 0.9228682170542636, 'f1_score': 0.8823420418751158}
Logistic Regression: {'accuracy': 0.9362, 'precision': 0.8274, 'recall': 0.926, 'f1_score': 0.8739}
XGBoost: {'accuracy': 0.9215, 'precision': 0.8016, 'recall': 0.8919, 'f1_score': 0.8443}
Random Forest: {'accuracy': 0.8769, 'precision': 0.7212, 'recall': 0.7895, 'f1_score': 0.7538}
Decision Tree: {'accuracy': 0.8707, 'precision': 0.7102, 'recall': 0.7736, 'f1_score': 0.7406}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7729, 'precision': 0.5133, 'recall': 0.9304, 'f1_score': 0.6616}


EMBEDDINGS TYPE: GOOD FOR SOCIAL GATHERINGS
MLP_5842945: {'accuracy': 0.9582890173410404, 'precision': 0.9069189602446484, 'recall': 0.9195736434108527, 'f1_score': 0.9132024634334103}
MLP_160769: {'accuracy': 0.9406242774566475, 'precision': 0.8426449787835927, 'recall': 0.9236434108527132, 'f1_score': 0.8812869822485208}
Logistic Regression: {'accuracy': 0.9363, 'precision': 0.827, 'recall': 0.9267, 'f1_score': 0.8741}
XGBoost: {'accuracy': 0.9219, 'precision': 0.8026, 'recall': 0.8922, 'f1_score': 0.8451}
Random Forest: {'accuracy': 0.8756, 'precision': 0.7172, 'recall': 0.7903, 'f1_score': 0.752}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.7097, 'recall': 0.7729, 'f1_score': 0.74}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.72, 'precision': 0.4574, 'recall': 0.9318, 'f1_score': 0.6136}


EMBEDDINGS TYPE: GOOD FOR MORNING ROUTINE
MLP_5842945: {'accuracy': 0.9542196531791908, 'precision': 0.8905957287373548, 'recall': 0.9213178294573643, 'f1_score': 0.9056963231091637}
MLP_160769: {'accuracy': 0.940300578034682, 'precision': 0.8400421866760415, 'recall': 0.9261627906976744, 'f1_score': 0.8810028574062125}
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8268, 'recall': 0.9264, 'f1_score': 0.8738}
XGBoost: {'accuracy': 0.9214, 'precision': 0.8009, 'recall': 0.8924, 'f1_score': 0.8442}
Random Forest: {'accuracy': 0.8759, 'precision': 0.7177, 'recall': 0.7909, 'f1_score': 0.7525}
Decision Tree: {'accuracy': 0.8708, 'precision': 0.7105, 'recall': 0.774, 'f1_score': 0.7409}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7869, 'precision': 0.5319, 'recall': 0.8909, 'f1_score': 0.6661}


EMBEDDINGS TYPE: RELEASE_YEAR
MLP_5842945: {'accuracy': 0.9569942196531792, 'precision': 0.8972577009767092, 'recall': 0.9257751937984496, 'f1_score': 0.9112933994658527}
MLP_160769: {'accuracy': 0.9418728323699422, 'precision': 0.8510523475445224, 'recall': 0.916860465116279, 'f1_score': 0.8827315980968374}
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8304, 'recall': 0.9203, 'f1_score': 0.8731}
XGBoost: {'accuracy': 0.9216, 'precision': 0.8046, 'recall': 0.8868, 'f1_score': 0.8437}
Random Forest: {'accuracy': 0.879, 'precision': 0.7287, 'recall': 0.7855, 'f1_score': 0.756}
Decision Tree: {'accuracy': 0.8745, 'precision': 0.7238, 'recall': 0.7663, 'f1_score': 0.7444}
SVM: {'accuracy': 0.8635, 'precision': 0.6794, 'recall': 0.8103, 'f1_score': 0.7391}
Naive Bayes: {'accuracy': 0.7881, 'precision': 0.5306, 'recall': 0.9698, 'f1_score': 0.6859}


EMBEDDINGS TYPE: RELEASE_MONTH
MLP_5842945: {'accuracy': 0.9606011560693641, 'precision': 0.9205388520109332, 'recall': 0.9137596899224806, 'f1_score': 0.9171367438241588}
MLP_160769: {'accuracy': 0.9387283236994219, 'precision': 0.8318048105208513, 'recall': 0.9315891472868217, 'f1_score': 0.8788737544565317}
Logistic Regression: {'accuracy': 0.9361, 'precision': 0.8268, 'recall': 0.9264, 'f1_score': 0.8738}
XGBoost: {'accuracy': 0.9222, 'precision': 0.8026, 'recall': 0.8938, 'f1_score': 0.8458}
Random Forest: {'accuracy': 0.8747, 'precision': 0.7135, 'recall': 0.7934, 'f1_score': 0.7513}
Decision Tree: {'accuracy': 0.8704, 'precision': 0.7096, 'recall': 0.7734, 'f1_score': 0.7402}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7647, 'precision': 0.5038, 'recall': 0.9192, 'f1_score': 0.6508}


EMBEDDINGS TYPE: RELEASE_DAY
MLP_5842945: {'accuracy': 0.9594450867052023, 'precision': 0.9106423777564717, 'recall': 0.9203488372093023, 'f1_score': 0.9154698795180722}
MLP_160769: {'accuracy': 0.9444624277456647, 'precision': 0.8671860508254499, 'recall': 0.9060077519379846, 'f1_score': 0.88617192683158}
Logistic Regression: {'accuracy': 0.936, 'precision': 0.8264, 'recall': 0.9266, 'f1_score': 0.8736}
XGBoost: {'accuracy': 0.9232, 'precision': 0.8061, 'recall': 0.8928, 'f1_score': 0.8473}
Random Forest: {'accuracy': 0.8755, 'precision': 0.7159, 'recall': 0.7926, 'f1_score': 0.7523}
Decision Tree: {'accuracy': 0.8713, 'precision': 0.7126, 'recall': 0.7719, 'f1_score': 0.7411}
Naive Bayes: {'accuracy': 0.7882, 'precision': 0.5307, 'recall': 0.9698, 'f1_score': 0.686}
SVM: {'accuracy': 0.7567, 'precision': 0.4947, 'recall': 0.9196, 'f1_score': 0.6433}
Feature:  emotion
Feature:  Key
Feature:  Time signature
Feature:  Artist(s)
Feature:  song
Feature:  Genre
Feature:  Album
Feature:  Similar Artist 1
Feature:  Similar Song 1
Feature:  Similar Artist 2
Feature:  Similar Song 2
Feature:  Similar Artist 3
Feature:  Similar Song 3
Feature:  song_normalized
Feature:  artist_normalized
Feature:  Tempo
Feature:  Length
Feature:  Loudness (db)
Feature:  Popularity
Feature:  Energy
Feature:  Danceability
Feature:  Positiveness
Feature:  Speechiness
Feature:  Liveness
Feature:  Acousticness
Feature:  Instrumentalness
Feature:  Good for Party
Feature:  Good for Work/Study
Feature:  Good for Relaxation/Meditation
Feature:  Good for Exercise
Feature:  Good for Running
Feature:  Good for Yoga/Stretching
Feature:  Good for Driving
Feature:  Good for Social Gatherings
Feature:  Good for Morning Routine
Feature:  Release_Year
Feature:  Release_Month
Feature:  Release_Day

 Mejores modelos por embedding:
                         embedding   best_model  ...    recall  f1_score
0                          emotion  MLP_5843969  ...  0.933140  0.908576
1                              Key  MLP_5843969  ...  0.932171  0.914014
2                   Time signature  MLP_5843969  ...  0.920543  0.912146
3                        Artist(s)  MLP_5843969  ...  0.902132  0.913551
4                             song  MLP_5843969  ...  0.920736  0.918511
5                            Genre  MLP_5843969  ...  0.937984  0.914156
6                            Album  MLP_5843969  ...  0.935465  0.907501
7                 Similar Artist 1  MLP_5843969  ...  0.922287  0.892871
8                   Similar Song 1  MLP_5843969  ...  0.944380  0.902742
9                 Similar Artist 2  MLP_5843969  ...  0.909884  0.892246
10                  Similar Song 2  MLP_5843969  ...  0.931202  0.908318
11                Similar Artist 3  MLP_5843969  ...  0.907946  0.894938
12                  Similar Song 3  MLP_5843969  ...  0.911047  0.914058
13                 song_normalized  MLP_5843969  ...  0.940310  0.917462
14               artist_normalized  MLP_5843969  ...  0.933915  0.912949
15                           Tempo  MLP_5842945  ...  0.934690  0.906664
16                          Length  MLP_5842945  ...  0.942054  0.904540
17                   Loudness (db)  MLP_5842945  ...  0.945930  0.911996
18                      Popularity  MLP_5842945  ...  0.935465  0.913080
19                          Energy  MLP_5842945  ...  0.932364  0.907907
20                    Danceability  MLP_5842945  ...  0.935659  0.908458
21                    Positiveness  MLP_5842945  ...  0.930233  0.914896
22                     Speechiness  MLP_5842945  ...  0.942636  0.908819
23                        Liveness  MLP_5842945  ...  0.935659  0.911287
24                    Acousticness  MLP_5842945  ...  0.950775  0.904582
25                Instrumentalness  MLP_5842945  ...  0.931977  0.912957
26                  Good for Party  MLP_5842945  ...  0.930426  0.912477
27             Good for Work/Study  MLP_5842945  ...  0.936240  0.907656
28  Good for Relaxation/Meditation  MLP_5842945  ...  0.930620  0.908437
29               Good for Exercise  MLP_5842945  ...  0.935853  0.910531
30                Good for Running  MLP_5842945  ...  0.908915  0.916642
31        Good for Yoga/Stretching  MLP_5842945  ...  0.938178  0.912535
32                Good for Driving  MLP_5842945  ...  0.916667  0.915779
33      Good for Social Gatherings  MLP_5842945  ...  0.919574  0.913202
34        Good for Morning Routine  MLP_5842945  ...  0.921318  0.905696
35                    Release_Year  MLP_5842945  ...  0.925775  0.911293
36                   Release_Month  MLP_5842945  ...  0.913760  0.917137
37                     Release_Day  MLP_5842945  ...  0.920349  0.915470

[38 rows x 8 columns]

 Mejor global: song con MLP_5843969 (f1=0.9185)

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
====================================

