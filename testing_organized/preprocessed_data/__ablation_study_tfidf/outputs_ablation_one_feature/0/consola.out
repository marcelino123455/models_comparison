/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 15 ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']

CAT_LOW 3 ['emotion', 'Key', 'Time signature']
CAT_HIGH 12 ['Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
Trainning one by one features for TF-IDF embbedings
len_COL_TF_IDF:  15
len_N_cols:  21
Total de columnas [36]:  ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized', 'Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']

==================================================
Loading and preprocessing main DataFrame...
==================================================
Processing 'Loudness (db)'...
Processing 'Length'...
Processing 'Release Date'...
New numeric columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Total de columnas [38]:  ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']

##################################################
Running experiment with EMOTION feature
[emotion] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}

Splitting data (index-based split to avoid leakage)...
==================================================
Data antes del undersampling ...
X: (86500, 7)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 7)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [7, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6675, Test Loss: 0.6526, F1: 0.3791, AUC: 0.6169
Epoch [10/30] Train Loss: 0.6620, Test Loss: 0.6499, F1: 0.3791, AUC: 0.6169
Epoch [20/30] Train Loss: 0.6621, Test Loss: 0.6652, F1: 0.3791, AUC: 0.6169
Mejores resultados en la época:  5
f1-score 0.37909654561558903
AUC según el mejor F1-score 0.6172095200766483
Confusion Matrix:
 [[14305  2160]
 [ 3448  1712]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/emotion/confusion_matrix_param_289.png
Accuracy:   0.7407
Precision:  0.4421
Recall:     0.3318
F1-score:   0.3791

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6657, Test Loss: 0.6528, F1: 0.3791, AUC: 0.6169
Epoch [10/30] Train Loss: 0.6620, Test Loss: 0.6605, F1: 0.3791, AUC: 0.6157
Epoch [20/30] Train Loss: 0.6621, Test Loss: 0.6601, F1: 0.3791, AUC: 0.6157
Mejores resultados en la época:  0
f1-score 0.37905457765969225
AUC según el mejor F1-score 0.6168976416970928
Confusion Matrix:
 [[14304  2161]
 [ 3448  1712]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/emotion/confusion_matrix_param_289.png
Accuracy:   0.7406
Precision:  0.4420
Recall:     0.3318
F1-score:   0.3791

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6681, Test Loss: 0.6492, F1: 0.3791, AUC: 0.6172
Epoch [10/30] Train Loss: 0.6619, Test Loss: 0.6603, F1: 0.3791, AUC: 0.6157
Epoch [20/30] Train Loss: 0.6620, Test Loss: 0.6552, F1: 0.3791, AUC: 0.6157
Mejores resultados en la época:  0
f1-score 0.37909654561558903
AUC según el mejor F1-score 0.6172095200766483
Confusion Matrix:
 [[14305  2160]
 [ 3448  1712]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/emotion/confusion_matrix_param_289.png
Accuracy:   0.7407
Precision:  0.4421
Recall:     0.3318
F1-score:   0.3791
Tiempo total para red 1: 111.32 segundos

Entrenando red 6 con capas [7, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6651, Test Loss: 0.6540, F1: 0.3791, AUC: 0.6172
Epoch [10/30] Train Loss: 0.6623, Test Loss: 0.6509, F1: 0.3791, AUC: 0.6169
Epoch [20/30] Train Loss: 0.6621, Test Loss: 0.6541, F1: 0.3791, AUC: 0.6157
Mejores resultados en la época:  0
f1-score 0.37909654561558903
AUC según el mejor F1-score 0.6172162056229211
Confusion Matrix:
 [[14305  2160]
 [ 3448  1712]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/emotion/confusion_matrix_param_707585.png
Accuracy:   0.7407
Precision:  0.4421
Recall:     0.3318
F1-score:   0.3791

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6647, Test Loss: 0.6595, F1: 0.3791, AUC: 0.6169
Epoch [10/30] Train Loss: 0.6625, Test Loss: 0.6556, F1: 0.3791, AUC: 0.6169
Epoch [20/30] Train Loss: 0.6622, Test Loss: 0.6494, F1: 0.3791, AUC: 0.6169
Mejores resultados en la época:  0
f1-score 0.37909654561558903
AUC según el mejor F1-score 0.6169281739277821
Confusion Matrix:
 [[14305  2160]
 [ 3448  1712]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/emotion/confusion_matrix_param_707585.png
Accuracy:   0.7407
Precision:  0.4421
Recall:     0.3318
F1-score:   0.3791

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6651, Test Loss: 0.6566, F1: 0.3791, AUC: 0.6172
Epoch [10/30] Train Loss: 0.6629, Test Loss: 0.6715, F1: 0.3791, AUC: 0.6169
Epoch [20/30] Train Loss: 0.6624, Test Loss: 0.6722, F1: 0.3791, AUC: 0.6169
Mejores resultados en la época:  0
f1-score 0.37909654561558903
AUC según el mejor F1-score 0.6172187833247409
Confusion Matrix:
 [[14305  2160]
 [ 3448  1712]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/emotion/confusion_matrix_param_707585.png
Accuracy:   0.7407
Precision:  0.4421
Recall:     0.3318
F1-score:   0.3791
Tiempo total para red 6: 173.93 segundos
Saved on: outputs_ablation_one_feature/0/emotion

==============================
Model: Logistic Regression
Accuracy:  0.7407
Precision: 0.4421
Recall:    0.3318
F1-score:  0.3791
              precision    recall  f1-score   support

           0       0.81      0.87      0.84     16465
           1       0.44      0.33      0.38      5160

    accuracy                           0.74     21625
   macro avg       0.62      0.60      0.61     21625
weighted avg       0.72      0.74      0.73     21625

[[14305  2160]
 [ 3448  1712]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [12:46:56] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/emotion/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/emotion/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.4716
Precision: 0.2047
Recall:    0.4211
F1-score:  0.2755
              precision    recall  f1-score   support

           0       0.73      0.49      0.58     16465
           1       0.20      0.42      0.28      5160

    accuracy                           0.47     21625
   macro avg       0.47      0.45      0.43     21625
weighted avg       0.60      0.47      0.51     21625

[[8025 8440]
 [2987 2173]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/emotion/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/emotion/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7407
Precision: 0.4421
Recall:    0.3318
F1-score:  0.3791
              precision    recall  f1-score   support

           0       0.81      0.87      0.84     16465
           1       0.44      0.33      0.38      5160

    accuracy                           0.74     21625
   macro avg       0.62      0.60      0.61     21625
weighted avg       0.72      0.74      0.73     21625

[[14305  2160]
 [ 3448  1712]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/emotion/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/emotion/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.7406
Precision: 0.4420
Recall:    0.3318
F1-score:  0.3791
              precision    recall  f1-score   support

           0       0.81      0.87      0.84     16465
           1       0.44      0.33      0.38      5160

    accuracy                           0.74     21625
   macro avg       0.62      0.60      0.61     21625
weighted avg       0.72      0.74      0.73     21625

[[14304  2161]
 [ 3448  1712]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/emotion/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/emotion/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.7407
Precision: 0.4421
Recall:    0.3318
F1-score:  0.3791
              precision    recall  f1-score   support

           0       0.81      0.87      0.84     16465
           1       0.44      0.33      0.38      5160

    accuracy                           0.74     21625
   macro avg       0.62      0.60      0.61     21625
weighted avg       0.72      0.74      0.73     21625

[[14305  2160]
 [ 3448  1712]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/emotion/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/emotion/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.5197
Precision: 0.2852
Recall:    0.6727
F1-score:  0.4006
              precision    recall  f1-score   support

           0       0.82      0.47      0.60     16465
           1       0.29      0.67      0.40      5160

    accuracy                           0.52     21625
   macro avg       0.55      0.57      0.50     21625
weighted avg       0.69      0.52      0.55     21625

[[7767 8698]
 [1689 3471]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/emotion/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/emotion/naive_bayes_model.pkl


Resumen de métricas:
Naive Bayes: {'accuracy': 0.5197, 'precision': 0.2852, 'recall': 0.6727, 'f1_score': 0.4006}
Logistic Regression: {'accuracy': 0.7407, 'precision': 0.4421, 'recall': 0.3318, 'f1_score': 0.3791}
Decision Tree: {'accuracy': 0.7407, 'precision': 0.4421, 'recall': 0.3318, 'f1_score': 0.3791}
Random Forest: {'accuracy': 0.7406, 'precision': 0.442, 'recall': 0.3318, 'f1_score': 0.3791}
XGBoost: {'accuracy': 0.7407, 'precision': 0.4421, 'recall': 0.3318, 'f1_score': 0.3791}
SVM: {'accuracy': 0.4716, 'precision': 0.2047, 'recall': 0.4211, 'f1_score': 0.2755}

##################################################
Running experiment with KEY feature
[Key] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}

Splitting data (index-based split to avoid leakage)...
==================================================
Data antes del undersampling ...
X: (86500, 2)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 2)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [2, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6893, Test Loss: 0.6899, F1: 0.3422, AUC: 0.5439
Epoch [10/30] Train Loss: 0.6890, Test Loss: 0.7036, F1: 0.3422, AUC: 0.5439
Epoch [20/30] Train Loss: 0.6890, Test Loss: 0.6958, F1: 0.3422, AUC: 0.5439
Mejores resultados en la época:  0
f1-score 0.34217344916933395
AUC según el mejor F1-score 0.5439280703488961
Confusion Matrix:
 [[10748  5717]
 [ 2915  2245]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Key/confusion_matrix_param_129.png
Accuracy:   0.6008
Precision:  0.2820
Recall:     0.4351
F1-score:   0.3422

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6892, Test Loss: 0.7126, F1: 0.3422, AUC: 0.5439
Epoch [10/30] Train Loss: 0.6891, Test Loss: 0.6988, F1: 0.3422, AUC: 0.5439
Epoch [20/30] Train Loss: 0.6891, Test Loss: 0.6852, F1: 0.3422, AUC: 0.5439
Mejores resultados en la época:  0
f1-score 0.34217344916933395
AUC según el mejor F1-score 0.5439280703488961
Confusion Matrix:
 [[10748  5717]
 [ 2915  2245]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Key/confusion_matrix_param_129.png
Accuracy:   0.6008
Precision:  0.2820
Recall:     0.4351
F1-score:   0.3422

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6903, Test Loss: 0.6864, F1: 0.3422, AUC: 0.5439
Epoch [10/30] Train Loss: 0.6891, Test Loss: 0.6901, F1: 0.3422, AUC: 0.5439
Epoch [20/30] Train Loss: 0.6891, Test Loss: 0.6878, F1: 0.3422, AUC: 0.5439
Mejores resultados en la época:  0
f1-score 0.34217344916933395
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [12:51:51] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
AUC según el mejor F1-score 0.5439280703488961
Confusion Matrix:
 [[10748  5717]
 [ 2915  2245]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Key/confusion_matrix_param_129.png
Accuracy:   0.6008
Precision:  0.2820
Recall:     0.4351
F1-score:   0.3422
Tiempo total para red 1: 108.60 segundos

Entrenando red 6 con capas [2, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6903, Test Loss: 0.6861, F1: 0.3422, AUC: 0.5439
Epoch [10/30] Train Loss: 0.6892, Test Loss: 0.6915, F1: 0.3422, AUC: 0.5439
Epoch [20/30] Train Loss: 0.6890, Test Loss: 0.6999, F1: 0.3422, AUC: 0.5439
Mejores resultados en la época:  0
f1-score 0.34217344916933395
AUC según el mejor F1-score 0.5439280703488961
Confusion Matrix:
 [[10748  5717]
 [ 2915  2245]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Key/confusion_matrix_param_702465.png
Accuracy:   0.6008
Precision:  0.2820
Recall:     0.4351
F1-score:   0.3422

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6898, Test Loss: 0.6890, F1: 0.3422, AUC: 0.5439
Epoch [10/30] Train Loss: 0.6891, Test Loss: 0.6830, F1: 0.3422, AUC: 0.5439
Epoch [20/30] Train Loss: 0.6890, Test Loss: 0.6778, F1: 0.3422, AUC: 0.5439
Mejores resultados en la época:  0
f1-score 0.34217344916933395
AUC según el mejor F1-score 0.5439280703488961
Confusion Matrix:
 [[10748  5717]
 [ 2915  2245]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Key/confusion_matrix_param_702465.png
Accuracy:   0.6008
Precision:  0.2820
Recall:     0.4351
F1-score:   0.3422

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6897, Test Loss: 0.6758, F1: 0.3422, AUC: 0.5443
Epoch [10/30] Train Loss: 0.6893, Test Loss: 0.6985, F1: 0.3422, AUC: 0.5439
Epoch [20/30] Train Loss: 0.6892, Test Loss: 0.6935, F1: 0.3422, AUC: 0.5439
Mejores resultados en la época:  0
f1-score 0.34217344916933395
AUC según el mejor F1-score 0.5442700219163507
Confusion Matrix:
 [[10748  5717]
 [ 2915  2245]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Key/confusion_matrix_param_702465.png
Accuracy:   0.6008
Precision:  0.2820
Recall:     0.4351
F1-score:   0.3422
Tiempo total para red 6: 172.02 segundos
Saved on: outputs_ablation_one_feature/0/Key

==============================
Model: Logistic Regression
Accuracy:  0.6008
Precision: 0.2820
Recall:    0.4351
F1-score:  0.3422
              precision    recall  f1-score   support

           0       0.79      0.65      0.71     16465
           1       0.28      0.44      0.34      5160

    accuracy                           0.60     21625
   macro avg       0.53      0.54      0.53     21625
weighted avg       0.67      0.60      0.62     21625

[[10748  5717]
 [ 2915  2245]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Key/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Key/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.2386
Precision: 0.2386
Recall:    1.0000
F1-score:  0.3853
              precision    recall  f1-score   support

           0       0.00      0.00      0.00     16465
           1       0.24      1.00      0.39      5160

    accuracy                           0.24     21625
   macro avg       0.12      0.50      0.19     21625
weighted avg       0.06      0.24      0.09     21625

[[    0 16465]
 [    0  5160]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Key/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Key/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6008
Precision: 0.2820
Recall:    0.4351
F1-score:  0.3422
              precision    recall  f1-score   support

           0       0.79      0.65      0.71     16465
           1       0.28      0.44      0.34      5160

    accuracy                           0.60     21625
   macro avg       0.53      0.54      0.53     21625
weighted avg       0.67      0.60      0.62     21625

[[10748  5717]
 [ 2915  2245]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Key/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Key/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.6008
Precision: 0.2820
Recall:    0.4351
F1-score:  0.3422
              precision    recall  f1-score   support

           0       0.79      0.65      0.71     16465
           1       0.28      0.44      0.34      5160

    accuracy                           0.60     21625
   macro avg       0.53      0.54      0.53     21625
weighted avg       0.67      0.60      0.62     21625

[[10748  5717]
 [ 2915  2245]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Key/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Key/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.6008
Precision: 0.2820
Recall:    0.4351
F1-score:  0.3422
              precision    recall  f1-score   support

           0       0.79      0.65      0.71     16465
           1       0.28      0.44      0.34      5160

    accuracy                           0.60     21625
   macro avg       0.53      0.54      0.53     21625
weighted avg       0.67      0.60      0.62     21625

[[10748  5717]
 [ 2915  2245]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Key/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Key/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6008
Precision: 0.2820
Recall:    0.4351
F1-score:  0.3422
              precision    recall  f1-score   support

           0       0.79      0.65      0.71     16465
           1       0.28      0.44      0.34      5160

    accuracy                           0.60     21625
   macro avg       0.53      0.54      0.53     21625
weighted avg       0.67      0.60      0.62     21625
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])

[[10748  5717]
 [ 2915  2245]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Key/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Key/naive_bayes_model.pkl


Resumen de métricas:
SVM: {'accuracy': 0.2386, 'precision': 0.2386, 'recall': 1.0, 'f1_score': 0.3853}
Logistic Regression: {'accuracy': 0.6008, 'precision': 0.282, 'recall': 0.4351, 'f1_score': 0.3422}
Decision Tree: {'accuracy': 0.6008, 'precision': 0.282, 'recall': 0.4351, 'f1_score': 0.3422}
Random Forest: {'accuracy': 0.6008, 'precision': 0.282, 'recall': 0.4351, 'f1_score': 0.3422}
XGBoost: {'accuracy': 0.6008, 'precision': 0.282, 'recall': 0.4351, 'f1_score': 0.3422}
Naive Bayes: {'accuracy': 0.6008, 'precision': 0.282, 'recall': 0.4351, 'f1_score': 0.3422}

##################################################
Running experiment with TIME SIGNATURE feature
[Time signature] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}

Splitting data (index-based split to avoid leakage)...
[Time signature] Vocabulario vacío (solo stopwords). Reintentando sin eliminar stopwords...
[Time signature] TF-IDF volvió a fallar. Usando One-Hot Encoding como respaldo...
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6935, Test Loss: 0.7053, F1: 0.3853, AUC: 0.5000
Epoch [10/30] Train Loss: 0.6932, Test Loss: 0.6996, F1: 0.3853, AUC: 0.5000
Epoch [20/30] Train Loss: 0.6932, Test Loss: 0.6959, F1: 0.3853, AUC: 0.5000
Mejores resultados en la época:  0
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Time signature/confusion_matrix_param_97.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6935, Test Loss: 0.6918, F1: 0.0000, AUC: 0.5000
Epoch [10/30] Train Loss: 0.6933, Test Loss: 0.6885, F1: 0.0000, AUC: 0.5000
Epoch [20/30] Train Loss: 0.6933, Test Loss: 0.7145, F1: 0.3853, AUC: 0.5000
Mejores resultados en la época:  1
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Time signature/confusion_matrix_param_97.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6934, Test Loss: 0.6939, F1: 0.3853, AUC: 0.5000
Epoch [10/30] Train Loss: 0.6931, Test Loss: 0.7042, F1: 0.3853, AUC: 0.5000
Epoch [20/30] Train Loss: 0.6933, Test Loss: 0.6947, F1: 0.3853, AUC: 0.5000
Mejores resultados en la época:  0
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Time signature/confusion_matrix_param_97.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853
Tiempo total para red 1: 106.43 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6934, Test Loss: 0.6959, F1: 0.3853, AUC: 0.5000
Epoch [10/30] Train Loss: 0.6932, Test Loss: 0.6967, F1: 0.3853, AUC: 0.5000
Epoch [20/30] Train Loss: 0.6932, Test Loss: 0.6952, F1: 0.3853, AUC: 0.5000
Mejores resultados en la época:  0
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Time signature/confusion_matrix_param_701441.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6933, Test Loss: 0.6932, F1: 0.3853, AUC: 0.5000
Epoch [10/30] Train Loss: 0.6932, Test Loss: 0.6925, F1: 0.0000, AUC: 0.5000
Epoch [20/30] Train Loss: 0.6932, Test Loss: 0.6934, F1: 0.3853, AUC: 0.5000
Mejores resultados en la época:  0
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Time signature/confusion_matrix_param_701441.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6932, Test Loss: 0.6884, F1: 0.0000, AUC: 0.5000
Epoch [10/30] Train Loss: 0.6932, Test Loss: 0.6957, F1: 0.3853, AUC: 0.5000
Epoch [20/30] Train Loss: 0.6932, Test Loss: 0.6933, F1: 0.3853, AUC: 0.5000
Mejores resultados en la época:  1
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Time signature/confusion_matrix_param_701441.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853
Tiempo total para red 6: 169.41 segundos
Saved on: outputs_ablation_one_feature/0/Time signature

==============================
Model: Logistic Regression
Accuracy:  0.7614
Precision: 0.0000
Recall:    0.0000
F1-score:  0.0000
              precision    recall  f1-score   support

           0       0.76      1.00      0.86     16465
           1       0.00      0.00      0.00      5160

    accuracy                           0.76     21625
   macro avg       0.38      0.50      0.43     21625
weighted avg       0.58      0.76      0.66     21625

[[16465     0]
 [ 5160     0]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Time signature/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Time signature/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.2386
Precision: 0.2386
Recall:    1.0000
F1-score:  0.3853
              precision    recall  f1-score   support

           0       0.00      0.00      0.00     16465
           1       0.24      1.00      0.39      5160

    accuracy                           0.24     21625
   macro avg       0.12      0.50      0.19     21625
weighted avg       0.06      0.24      0.09     21625

[[    0 16465]
 [    0  5160]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Time signature/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Time signature/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7614
Precision: 0.0000
Recall:    0.0000
F1-score:  0.0000
              precision    recall  f1-score   support

           0       0.76      1.00      0.86     16465
           1       0.00      0.00      0.00      5160

    accuracy                           0.76     21625
   macro avg       0.38      0.50      0.43     21625
weighted avg       0.58      0.76      0.66     21625

[[16465     0]
 [ 5160     0]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [12:56:36] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/naive_bayes.py:513: RuntimeWarning: divide by zero encountered in log
  n_ij = -0.5 * np.sum(np.log(2.0 * np.pi * self.var_[i, :]))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/naive_bayes.py:514: RuntimeWarning: invalid value encountered in divide
  n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) / (self.var_[i, :]), 1)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Confusion matrix saved as: outputs_ablation_one_feature/0/Time signature/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Time signature/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.7614
Precision: 0.0000
Recall:    0.0000
F1-score:  0.0000
              precision    recall  f1-score   support

           0       0.76      1.00      0.86     16465
           1       0.00      0.00      0.00      5160

    accuracy                           0.76     21625
   macro avg       0.38      0.50      0.43     21625
weighted avg       0.58      0.76      0.66     21625

[[16465     0]
 [ 5160     0]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Time signature/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Time signature/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.7614
Precision: 0.0000
Recall:    0.0000
F1-score:  0.0000
              precision    recall  f1-score   support

           0       0.76      1.00      0.86     16465
           1       0.00      0.00      0.00      5160

    accuracy                           0.76     21625
   macro avg       0.38      0.50      0.43     21625
weighted avg       0.58      0.76      0.66     21625

[[16465     0]
 [ 5160     0]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Time signature/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Time signature/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7614
Precision: 0.0000
Recall:    0.0000
F1-score:  0.0000
              precision    recall  f1-score   support

           0       0.76      1.00      0.86     16465
           1       0.00      0.00      0.00      5160

    accuracy                           0.76     21625
   macro avg       0.38      0.50      0.43     21625
weighted avg       0.58      0.76      0.66     21625

[[16465     0]
 [ 5160     0]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Time signature/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Time signature/naive_bayes_model.pkl


Resumen de métricas:
SVM: {'accuracy': 0.2386, 'precision': 0.2386, 'recall': 1.0, 'f1_score': 0.3853}
Logistic Regression: {'accuracy': 0.7614, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}
Decision Tree: {'accuracy': 0.7614, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}
Random Forest: {'accuracy': 0.7614, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}
XGBoost: {'accuracy': 0.7614, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}
Naive Bayes: {'accuracy': 0.7614, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}

##################################################
Running experiment with ARTIST(S) feature
[Artist(s)] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}

Splitting data (index-based split to avoid leakage)...
==================================================
Data antes del undersampling ...
X: (86500, 5000)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5000)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5000, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4786, Test Loss: 0.3514, F1: 0.7346, AUC: 0.9146
Epoch [10/30] Train Loss: 0.2899, Test Loss: 0.3462, F1: 0.7271, AUC: 0.9220
Epoch [20/30] Train Loss: 0.2852, Test Loss: 0.3601, F1: 0.7281, AUC: 0.9214
Mejores resultados en la época:  0
f1-score 0.7346142592411977
AUC según el mejor F1-score 0.9145540516999884
Confusion Matrix:
 [[15138  1327]
 [ 1394  3766]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Artist(s)/confusion_matrix_param_160065.png
Accuracy:   0.8742
Precision:  0.7394
Recall:     0.7298
F1-score:   0.7346

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4766, Test Loss: 0.3497, F1: 0.7314, AUC: 0.9141
Epoch [10/30] Train Loss: 0.2898, Test Loss: 0.3364, F1: 0.7331, AUC: 0.9210
Epoch [20/30] Train Loss: 0.2852, Test Loss: 0.3595, F1: 0.7269, AUC: 0.9214
Mejores resultados en la época:  10
f1-score 0.7331317331317331
AUC según el mejor F1-score 0.9210264608742529
Confusion Matrix:
 [[14922  1543]
 [ 1281  3879]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Artist(s)/confusion_matrix_param_160065.png
Accuracy:   0.8694
Precision:  0.7154
Recall:     0.7517
F1-score:   0.7331

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4770, Test Loss: 0.3643, F1: 0.7258, AUC: 0.9138
Epoch [10/30] Train Loss: 0.2897, Test Loss: 0.3542, F1: 0.7231, AUC: 0.9215
Epoch [20/30] Train Loss: 0.2854, Test Loss: 0.3720, F1: 0.7212, AUC: 0.9208
Mejores resultados en la época:  2
f1-score 0.7358015119209149
AUC según el mejor F1-score 0.9208953806170947
Confusion Matrix:
 [[15103  1362]
 [ 1364  3796]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Artist(s)/confusion_matrix_param_160065.png
Accuracy:   0.8739
Precision:  0.7359
Recall:     0.7357
F1-score:   0.7358
Tiempo total para red 1: 289.57 segundos

Entrenando red 6 con capas [5000, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4017, Test Loss: 0.3249, F1: 0.7356, AUC: 0.9192
Epoch [10/30] Train Loss: 0.2723, Test Loss: 0.4020, F1: 0.7300, AUC: 0.9226
Epoch [20/30] Train Loss: 0.2674, Test Loss: 0.4089, F1: 0.7290, AUC: 0.9229
Mejores resultados en la época:  19
f1-score 0.7356824009877482
AUC según el mejor F1-score 0.9224234045908988
Confusion Matrix:
 [[14969  1496]
 [ 1287  3873]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Artist(s)/confusion_matrix_param_5820417.png
Accuracy:   0.8713
Precision:  0.7214
Recall:     0.7506
F1-score:   0.7357

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4043, Test Loss: 0.3189, F1: 0.7349, AUC: 0.9199
Epoch [10/30] Train Loss: 0.2723, Test Loss: 0.3726, F1: 0.7298, AUC: 0.9227
Epoch [20/30] Train Loss: 0.2660, Test Loss: 0.5357, F1: 0.7327, AUC: 0.9205
Mejores resultados en la época:  17
f1-score 0.7402031930333817
AUC según el mejor F1-score 0.9231052008371057
Confusion Matrix:
 [[15115  1350]
 [ 1335  3825]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Artist(s)/confusion_matrix_param_5820417.png
Accuracy:   0.8758
Precision:  0.7391
Recall:     0.7413
F1-score:   0.7402

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4032, Test Loss: 0.3327, F1: 0.7290, AUC: 0.9200
Epoch [10/30] Train Loss: 0.2729, Test Loss: 0.3642, F1: 0.7311, AUC: 0.9234
Epoch [20/30] Train Loss: 0.2661, Test Loss: 0.4354, F1: 0.7277, AUC: 0.9223
Mejores resultados en la época:  28
f1-score 0.738499951983098
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [13:13:03] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
AUC según el mejor F1-score 0.9225633596753273
Confusion Matrix:
 [[15057  1408]
 [ 1315  3845]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Artist(s)/confusion_matrix_param_5820417.png
Accuracy:   0.8741
Precision:  0.7320
Recall:     0.7452
F1-score:   0.7385
Tiempo total para red 6: 358.73 segundos
Saved on: outputs_ablation_one_feature/0/Artist(s)

==============================
Model: Logistic Regression
Accuracy:  0.8710
Precision: 0.7241
Recall:    0.7424
F1-score:  0.7331
              precision    recall  f1-score   support

           0       0.92      0.91      0.91     16465
           1       0.72      0.74      0.73      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.82     21625
weighted avg       0.87      0.87      0.87     21625

[[15005  1460]
 [ 1329  3831]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Artist(s)/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Artist(s)/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.4839
Precision: 0.2312
Recall:    0.5002
F1-score:  0.3163
              precision    recall  f1-score   support

           0       0.75      0.48      0.59     16465
           1       0.23      0.50      0.32      5160

    accuracy                           0.48     21625
   macro avg       0.49      0.49      0.45     21625
weighted avg       0.63      0.48      0.52     21625

[[7884 8581]
 [2579 2581]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Artist(s)/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Artist(s)/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7941
Precision: 0.9585
Recall:    0.1432
F1-score:  0.2492
              precision    recall  f1-score   support

           0       0.79      1.00      0.88     16465
           1       0.96      0.14      0.25      5160

    accuracy                           0.79     21625
   macro avg       0.87      0.57      0.56     21625
weighted avg       0.83      0.79      0.73     21625

[[16433    32]
 [ 4421   739]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Artist(s)/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Artist(s)/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8428
Precision: 0.9356
Recall:    0.3663
F1-score:  0.5265
              precision    recall  f1-score   support

           0       0.83      0.99      0.91     16465
           1       0.94      0.37      0.53      5160

    accuracy                           0.84     21625
   macro avg       0.88      0.68      0.72     21625
weighted avg       0.86      0.84      0.82     21625

[[16335   130]
 [ 3270  1890]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Artist(s)/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Artist(s)/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8748
Precision: 0.8838
Recall:    0.5471
F1-score:  0.6758
              precision    recall  f1-score   support

           0       0.87      0.98      0.92     16465
           1       0.88      0.55      0.68      5160

    accuracy                           0.87     21625
   macro avg       0.88      0.76      0.80     21625
weighted avg       0.88      0.87      0.86     21625

[[16094   371]
 [ 2337  2823]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Artist(s)/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Artist(s)/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.5963
Precision: 0.3697
Recall:    0.9816
F1-score:  0.5371
              precision    recall  f1-score   support

           0       0.99      0.48      0.64     16465
           1       0.37      0.98      0.54      5160

    accuracy                           0.60     21625
   macro avg       0.68      0.73      0.59     21625
weighted avg       0.84      0.60      0.62     21625

[[7831 8634]
 [  95 5065]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Artist(s)/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Artist(s)/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.871, 'precision': 0.7241, 'recall': 0.7424, 'f1_score': 0.7331}
XGBoost: {'accuracy': 0.8748, 'precision': 0.8838, 'recall': 0.5471, 'f1_score': 0.6758}
Naive Bayes: {'accuracy': 0.5963, 'precision': 0.3697, 'recall': 0.9816, 'f1_score': 0.5371}
Random Forest: {'accuracy': 0.8428, 'precision': 0.9356, 'recall': 0.3663, 'f1_score': 0.5265}
SVM: {'accuracy': 0.4839, 'precision': 0.2312, 'recall': 0.5002, 'f1_score': 0.3163}
Decision Tree: {'accuracy': 0.7941, 'precision': 0.9585, 'recall': 0.1432, 'f1_score': 0.2492}

##################################################
Running experiment with SONG feature
[song] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}

Splitting data (index-based split to avoid leakage)...
==================================================
Data antes del undersampling ...
X: (86500, 5000)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5000)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5000, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6616, Test Loss: 0.6124, F1: 0.4491, AUC: 0.6790
Epoch [10/30] Train Loss: 0.5559, Test Loss: 0.6840, F1: 0.4472, AUC: 0.6718
Epoch [20/30] Train Loss: 0.5440, Test Loss: 0.7303, F1: 0.4477, AUC: 0.6709
Mejores resultados en la época:  3
f1-score 0.44986276887725796
AUC según el mejor F1-score 0.6758952687989793
Confusion Matrix:
 [[9482 6983]
 [1636 3524]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [13:30:23] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Matriz de confusión guardada en: outputs_ablation_one_feature/0/song/confusion_matrix_param_160065.png
Accuracy:   0.6014
Precision:  0.3354
Recall:     0.6829
F1-score:   0.4499

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6622, Test Loss: 0.6153, F1: 0.4496, AUC: 0.6782
Epoch [10/30] Train Loss: 0.5573, Test Loss: 0.6706, F1: 0.4481, AUC: 0.6732
Epoch [20/30] Train Loss: 0.5479, Test Loss: 0.7163, F1: 0.4478, AUC: 0.6705
Mejores resultados en la época:  1
f1-score 0.4512485541266925
AUC según el mejor F1-score 0.6790994757495934
Confusion Matrix:
 [[10244  6221]
 [ 1844  3316]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/song/confusion_matrix_param_160065.png
Accuracy:   0.6271
Precision:  0.3477
Recall:     0.6426
F1-score:   0.4512

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6632, Test Loss: 0.6240, F1: 0.4523, AUC: 0.6796
Epoch [10/30] Train Loss: 0.5514, Test Loss: 0.6628, F1: 0.4495, AUC: 0.6728
Epoch [20/30] Train Loss: 0.5261, Test Loss: 0.7296, F1: 0.4478, AUC: 0.6694
Mejores resultados en la época:  0
f1-score 0.45225723733045414
AUC según el mejor F1-score 0.6795732785306865
Confusion Matrix:
 [[10157  6308]
 [ 1809  3351]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/song/confusion_matrix_param_160065.png
Accuracy:   0.6246
Precision:  0.3469
Recall:     0.6494
F1-score:   0.4523
Tiempo total para red 1: 289.76 segundos

Entrenando red 6 con capas [5000, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6569, Test Loss: 0.6646, F1: 0.4496, AUC: 0.6775
Epoch [10/30] Train Loss: 0.3559, Test Loss: 1.4418, F1: 0.4362, AUC: 0.6489
Epoch [20/30] Train Loss: 0.3366, Test Loss: 1.4061, F1: 0.4426, AUC: 0.6573
Mejores resultados en la época:  1
f1-score 0.45320401801177695
AUC según el mejor F1-score 0.6778745730313538
Confusion Matrix:
 [[10461  6004]
 [ 1889  3271]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/song/confusion_matrix_param_5820417.png
Accuracy:   0.6350
Precision:  0.3527
Recall:     0.6339
F1-score:   0.4532

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6566, Test Loss: 0.6134, F1: 0.4440, AUC: 0.6794
Epoch [10/30] Train Loss: 0.3512, Test Loss: 1.2639, F1: 0.4410, AUC: 0.6518
Epoch [20/30] Train Loss: 0.3346, Test Loss: 1.6041, F1: 0.4392, AUC: 0.6492
Mejores resultados en la época:  2
f1-score 0.4471777142477229
AUC según el mejor F1-score 0.673594787627973
Confusion Matrix:
 [[9947 6518]
 [1797 3363]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/song/confusion_matrix_param_5820417.png
Accuracy:   0.6155
Precision:  0.3404
Recall:     0.6517
F1-score:   0.4472

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6546, Test Loss: 0.6312, F1: 0.4441, AUC: 0.6745
Epoch [10/30] Train Loss: 0.3532, Test Loss: 1.2089, F1: 0.4445, AUC: 0.6549
Epoch [20/30] Train Loss: 0.3379, Test Loss: 1.1439, F1: 0.4414, AUC: 0.6572
Mejores resultados en la época:  2
f1-score 0.4494293585202676
AUC según el mejor F1-score 0.6730893108943802
Confusion Matrix:
 [[9805 6660]
 [1734 3426]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/song/confusion_matrix_param_5820417.png
Accuracy:   0.6118
Precision:  0.3397
Recall:     0.6640
F1-score:   0.4494
Tiempo total para red 6: 360.33 segundos
Saved on: outputs_ablation_one_feature/0/song

==============================
Model: Logistic Regression
Accuracy:  0.6149
Precision: 0.3423
Recall:    0.6665
F1-score:  0.4523
              precision    recall  f1-score   support

           0       0.85      0.60      0.70     16465
           1       0.34      0.67      0.45      5160

    accuracy                           0.61     21625
   macro avg       0.60      0.63      0.58     21625
weighted avg       0.73      0.61      0.64     21625

[[9858 6607]
 [1721 3439]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/song/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/song/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7090
Precision: 0.3164
Recall:    0.1890
F1-score:  0.2366
              precision    recall  f1-score   support

           0       0.77      0.87      0.82     16465
           1       0.32      0.19      0.24      5160

    accuracy                           0.71     21625
   macro avg       0.55      0.53      0.53     21625
weighted avg       0.67      0.71      0.68     21625

[[14358  2107]
 [ 4185   975]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/song/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/song/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7662
Precision: 0.5907
Recall:    0.0663
F1-score:  0.1192
              precision    recall  f1-score   support

           0       0.77      0.99      0.87     16465
           1       0.59      0.07      0.12      5160

    accuracy                           0.77     21625
   macro avg       0.68      0.53      0.49     21625
weighted avg       0.73      0.77      0.69     21625

[[16228   237]
 [ 4818   342]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/song/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/song/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.4775
Precision: 0.2889
Recall:    0.8143
F1-score:  0.4265
              precision    recall  f1-score   support

           0       0.86      0.37      0.52     16465
           1       0.29      0.81      0.43      5160

    accuracy                           0.48     21625
   macro avg       0.58      0.59      0.47     21625
weighted avg       0.73      0.48      0.50     21625

[[ 6124 10341]
 [  958  4202]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/song/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/song/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.5016
Precision: 0.2961
Recall:    0.7907
F1-score:  0.4309
              precision    recall  f1-score   support

           0       0.86      0.41      0.56     16465
           1       0.30      0.79      0.43      5160

    accuracy                           0.50     21625
   macro avg       0.58      0.60      0.49     21625
weighted avg       0.73      0.50      0.53     21625

[[6767 9698]
 [1080 4080]]
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/song/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/song/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.3456
Precision: 0.2568
Recall:    0.9203
F1-score:  0.4016
              precision    recall  f1-score   support

           0       0.87      0.17      0.28     16465
           1       0.26      0.92      0.40      5160

    accuracy                           0.35     21625
   macro avg       0.56      0.54      0.34     21625
weighted avg       0.72      0.35      0.31     21625

[[ 2724 13741]
 [  411  4749]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/song/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/song/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.6149, 'precision': 0.3423, 'recall': 0.6665, 'f1_score': 0.4523}
XGBoost: {'accuracy': 0.5016, 'precision': 0.2961, 'recall': 0.7907, 'f1_score': 0.4309}
Random Forest: {'accuracy': 0.4775, 'precision': 0.2889, 'recall': 0.8143, 'f1_score': 0.4265}
Naive Bayes: {'accuracy': 0.3456, 'precision': 0.2568, 'recall': 0.9203, 'f1_score': 0.4016}
SVM: {'accuracy': 0.709, 'precision': 0.3164, 'recall': 0.189, 'f1_score': 0.2366}
Decision Tree: {'accuracy': 0.7662, 'precision': 0.5907, 'recall': 0.0663, 'f1_score': 0.1192}

##################################################
Running experiment with GENRE feature
[Genre] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}

Splitting data (index-based split to avoid leakage)...
==================================================
Data antes del undersampling ...
X: (86500, 734)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 734)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [734, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4975, Test Loss: 0.4468, F1: 0.6313, AUC: 0.8664
Epoch [10/30] Train Loss: 0.4254, Test Loss: 0.4408, F1: 0.6324, AUC: 0.8737
Epoch [20/30] Train Loss: 0.4200, Test Loss: 0.4327, F1: 0.6363, AUC: 0.8759
Mejores resultados en la época:  26
f1-score 0.6419865935405241
AUC según el mejor F1-score 0.8765802548040593
Confusion Matrix:
 [[12711  3754]
 [  946  4214]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Genre/confusion_matrix_param_23553.png
Accuracy:   0.7827
Precision:  0.5289
Recall:     0.8167
F1-score:   0.6420

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.5000, Test Loss: 0.4405, F1: 0.6326, AUC: 0.8655
Epoch [10/30] Train Loss: 0.4248, Test Loss: 0.4339, F1: 0.6367, AUC: 0.8743
Epoch [20/30] Train Loss: 0.4185, Test Loss: 0.4193, F1: 0.6374, AUC: 0.8769
Mejores resultados en la época:  28
f1-score 0.6452812645281265
AUC según el mejor F1-score 0.8774823209674267
Confusion Matrix:
 [[12883  3582]
 [  996  4164]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Genre/confusion_matrix_param_23553.png
Accuracy:   0.7883
Precision:  0.5376
Recall:     0.8070
F1-score:   0.6453

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4979, Test Loss: 0.4250, F1: 0.6341, AUC: 0.8666
Epoch [10/30] Train Loss: 0.4282, Test Loss: 0.4203, F1: 0.6311, AUC: 0.8729
Epoch [20/30] Train Loss: 0.4226, Test Loss: 0.4162, F1: 0.6361, AUC: 0.8742
Mejores resultados en la época:  27
f1-score 0.6403857930189835
AUC según el mejor F1-score 0.8766254057820559
Confusion Matrix:
 [[12744  3721]
 [  977  4183]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Genre/confusion_matrix_param_23553.png
Accuracy:   0.7828
Precision:  0.5292
Recall:     0.8107
F1-score:   0.6404
Tiempo total para red 1: 132.91 segundos

Entrenando red 6 con capas [734, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4633, Test Loss: 0.4261, F1: 0.6398, AUC: 0.8728
Epoch [10/30] Train Loss: 0.4081, Test Loss: 0.4202, F1: 0.6426, AUC: 0.8802
Epoch [20/30] Train Loss: 0.4041, Test Loss: 0.4186, F1: 0.6441, AUC: 0.8812
Mejores resultados en la época:  8
f1-score 0.6474610125220865
AUC según el mejor F1-score 0.8801930863447718
Confusion Matrix:
 [[12822  3643]
 [  946  4214]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Genre/confusion_matrix_param_1452033.png
Accuracy:   0.7878
Precision:  0.5363
Recall:     0.8167
F1-score:   0.6475

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4609, Test Loss: 0.4356, F1: 0.6350, AUC: 0.8718
Epoch [10/30] Train Loss: 0.4082, Test Loss: 0.4156, F1: 0.6448, AUC: 0.8808
Epoch [20/30] Train Loss: 0.4033, Test Loss: 0.4386, F1: 0.6444, AUC: 0.8814
Mejores resultados en la época:  26
f1-score 0.6475986540226369
AUC según el mejor F1-score 0.881536133729758
Confusion Matrix:
 [[12783  3682]
 [  926  4234]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Genre/confusion_matrix_param_1452033.png
Accuracy:   0.7869
Precision:  0.5349
Recall:     0.8205
F1-score:   0.6476

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4623, Test Loss: 0.4095, F1: 0.6369, AUC: 0.8738
Epoch [10/30] Train Loss: 0.4082, Test Loss: 0.4109, F1: 0.6388, AUC: 0.8810
Epoch [20/30] Train Loss: 0.4040, Test Loss: 0.4310, F1: 0.6465, AUC: 0.8808
Mejores resultados en la época:  23
f1-score 0.6481196054254007
AUC según el mejor F1-score 0.8811571232847689
Confusion Matrix:
 [[12854  3611]
 [  955  4205]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Genre/confusion_matrix_param_1452033.png
Accuracy:   0.7889
Precision:  0.5380
Recall:     0.8149
F1-score:   0.6481
Tiempo total para red 6: 198.07 segundos
Saved on: outputs_ablation_one_feature/0/Genre

==============================
Model: Logistic Regression
Accuracy:  0.7791
Precision: 0.5245
Recall:    0.7967
F1-score:  0.6326
              precision    recall  f1-score   support

           0       0.92      0.77      0.84     16465
           1       0.52      0.80      0.63      5160

    accuracy                           0.78     21625
   macro avg       0.72      0.79      0.74     21625
weighted avg       0.83      0.78      0.79     21625

[[12738  3727]
 [ 1049  4111]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Genre/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Genre/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7406
Precision: 0.4556
Recall:    0.4463
F1-score:  0.4509
              precision    recall  f1-score   support

           0       0.83      0.83      0.83     16465
           1       0.46      0.45      0.45      5160

    accuracy                           0.74     21625
   macro avg       0.64      0.64      0.64     21625
weighted avg       0.74      0.74      0.74     21625

[[13713  2752]
 [ 2857  2303]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Genre/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Genre/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7918
Precision: 0.5453
Recall:    0.7671
F1-score:  0.6375
              precision    recall  f1-score   support

           0       0.92      0.80      0.85     16465
           1       0.55      0.77      0.64      5160

    accuracy                           0.79     21625
   macro avg       0.73      0.78      0.75     21625
weighted avg       0.83      0.79      0.80     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [13:38:10] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[13165  3300]
 [ 1202  3958]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Genre/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Genre/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.7915
Precision: 0.5462
Recall:    0.7461
F1-score:  0.6307
              precision    recall  f1-score   support

           0       0.91      0.81      0.85     16465
           1       0.55      0.75      0.63      5160

    accuracy                           0.79     21625
   macro avg       0.73      0.78      0.74     21625
weighted avg       0.82      0.79      0.80     21625

[[13266  3199]
 [ 1310  3850]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Genre/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Genre/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.7875
Precision: 0.5363
Recall:    0.8099
F1-score:  0.6453
              precision    recall  f1-score   support

           0       0.93      0.78      0.85     16465
           1       0.54      0.81      0.65      5160

    accuracy                           0.79     21625
   macro avg       0.73      0.80      0.75     21625
weighted avg       0.84      0.79      0.80     21625

[[12851  3614]
 [  981  4179]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Genre/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Genre/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.3943
Precision: 0.2804
Recall:    0.9822
F1-score:  0.4362
              precision    recall  f1-score   support

           0       0.97      0.21      0.35     16465
           1       0.28      0.98      0.44      5160

    accuracy                           0.39     21625
   macro avg       0.63      0.60      0.39     21625
weighted avg       0.81      0.39      0.37     21625

[[ 3458 13007]
 [   92  5068]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Genre/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Genre/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.7875, 'precision': 0.5363, 'recall': 0.8099, 'f1_score': 0.6453}
Decision Tree: {'accuracy': 0.7918, 'precision': 0.5453, 'recall': 0.7671, 'f1_score': 0.6375}
Logistic Regression: {'accuracy': 0.7791, 'precision': 0.5245, 'recall': 0.7967, 'f1_score': 0.6326}
Random Forest: {'accuracy': 0.7915, 'precision': 0.5462, 'recall': 0.7461, 'f1_score': 0.6307}
SVM: {'accuracy': 0.7406, 'precision': 0.4556, 'recall': 0.4463, 'f1_score': 0.4509}
Naive Bayes: {'accuracy': 0.3943, 'precision': 0.2804, 'recall': 0.9822, 'f1_score': 0.4362}

##################################################
Running experiment with ALBUM feature
[Album] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}

Splitting data (index-based split to avoid leakage)...
==================================================
Data antes del undersampling ...
X: (86500, 5000)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5000)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5000, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.5607, Test Loss: 0.4568, F1: 0.6172, AUC: 0.8529
Epoch [10/30] Train Loss: 0.3846, Test Loss: 0.4480, F1: 0.6256, AUC: 0.8604
Epoch [20/30] Train Loss: 0.3789, Test Loss: 0.4856, F1: 0.5935, AUC: 0.8608
Mejores resultados en la época:  21
f1-score 0.627877002393666
AUC según el mejor F1-score 0.8608050492352817
Confusion Matrix:
 [[14173  2292]
 [ 1750  3410]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Album/confusion_matrix_param_160065.png
Accuracy:   0.8131
Precision:  0.5980
Recall:     0.6609
F1-score:   0.6279

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.5577, Test Loss: 0.4676, F1: 0.6144, AUC: 0.8534
Epoch [10/30] Train Loss: 0.3806, Test Loss: 0.4591, F1: 0.5986, AUC: 0.8628
Epoch [20/30] Train Loss: 0.3734, Test Loss: 0.4854, F1: 0.5980, AUC: 0.8624
Mejores resultados en la época:  24
f1-score 0.6304008806531511
AUC según el mejor F1-score 0.862607839744631
Confusion Matrix:
 [[14160  2305]
 [ 1724  3436]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Album/confusion_matrix_param_160065.png
Accuracy:   0.8137
Precision:  0.5985
Recall:     0.6659
F1-score:   0.6304

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.5570, Test Loss: 0.4474, F1: 0.6202, AUC: 0.8527
Epoch [10/30] Train Loss: 0.3807, Test Loss: 0.4691, F1: 0.5954, AUC: 0.8622
Epoch [20/30] Train Loss: 0.3740, Test Loss: 0.4874, F1: 0.5959, AUC: 0.8625
Mejores resultados en la época:  3
f1-score 0.6311444652908067
AUC según el mejor F1-score 0.8631577023849039
Confusion Matrix:
 [[14329  2136]
 [ 1796  3364]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Album/confusion_matrix_param_160065.png
Accuracy:   0.8182
Precision:  0.6116
Recall:     0.6519
F1-score:   0.6311
Tiempo total para red 1: 290.03 segundos

Entrenando red 6 con capas [5000, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.5023, Test Loss: 0.4385, F1: 0.5912, AUC: 0.8652
Epoch [10/30] Train Loss: 0.2686, Test Loss: 0.6634, F1: 0.6151, AUC: 0.8777
Epoch [20/30] Train Loss: 0.2605, Test Loss: 0.6797, F1: 0.6118, AUC: 0.8768
Mejores resultados en la época:  9
f1-score 0.6569289757166332
AUC según el mejor F1-score 0.8766384649609108
Confusion Matrix:
 [[14269  2196]
 [ 1562  3598]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Album/confusion_matrix_param_5820417.png
Accuracy:   0.8262
Precision:  0.6210
Recall:     0.6973
F1-score:   0.6569

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.5017, Test Loss: 0.4576, F1: 0.6355, AUC: 0.8641
Epoch [10/30] Train Loss: 0.2717, Test Loss: 0.5601, F1: 0.6188, AUC: 0.8781
Epoch [20/30] Train Loss: 0.2612, Test Loss: 0.6483, F1: 0.6237, AUC: 0.8781
Mejores resultados en la época:  19
f1-score 0.6630534631636965
AUC según el mejor F1-score 0.8778569469652564
Confusion Matrix:
 [[14348  2117]
 [ 1551  3609]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Album/confusion_matrix_param_5820417.png
Accuracy:   0.8304
Precision:  0.6303
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [13:55:03] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Recall:     0.6994
F1-score:   0.6631

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.5025, Test Loss: 0.4571, F1: 0.5944, AUC: 0.8626
Epoch [10/30] Train Loss: 0.2712, Test Loss: 0.5745, F1: 0.6104, AUC: 0.8755
Epoch [20/30] Train Loss: 0.2629, Test Loss: 0.6990, F1: 0.6110, AUC: 0.8791
Mejores resultados en la época:  24
f1-score 0.6585006446859458
AUC según el mejor F1-score 0.8770835363714904
Confusion Matrix:
 [[14342  2123]
 [ 1585  3575]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Album/confusion_matrix_param_5820417.png
Accuracy:   0.8285
Precision:  0.6274
Recall:     0.6928
F1-score:   0.6585
Tiempo total para red 6: 361.67 segundos
Saved on: outputs_ablation_one_feature/0/Album

==============================
Model: Logistic Regression
Accuracy:  0.8114
Precision: 0.5942
Recall:    0.6614
F1-score:  0.6260
              precision    recall  f1-score   support

           0       0.89      0.86      0.87     16465
           1       0.59      0.66      0.63      5160

    accuracy                           0.81     21625
   macro avg       0.74      0.76      0.75     21625
weighted avg       0.82      0.81      0.81     21625

[[14134  2331]
 [ 1747  3413]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Album/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Album/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.6893
Precision: 0.2476
Recall:    0.1481
F1-score:  0.1853
              precision    recall  f1-score   support

           0       0.76      0.86      0.81     16465
           1       0.25      0.15      0.19      5160

    accuracy                           0.69     21625
   macro avg       0.51      0.50      0.50     21625
weighted avg       0.64      0.69      0.66     21625

[[14143  2322]
 [ 4396   764]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Album/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Album/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.3127
Precision: 0.2553
Recall:    0.9812
F1-score:  0.4052
              precision    recall  f1-score   support

           0       0.95      0.10      0.19     16465
           1       0.26      0.98      0.41      5160

    accuracy                           0.31     21625
   macro avg       0.60      0.54      0.30     21625
weighted avg       0.78      0.31      0.24     21625

[[ 1700 14765]
 [   97  5063]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Album/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Album/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8125
Precision: 0.7036
Recall:    0.3703
F1-score:  0.4853
              precision    recall  f1-score   support

           0       0.83      0.95      0.89     16465
           1       0.70      0.37      0.49      5160

    accuracy                           0.81     21625
   macro avg       0.77      0.66      0.69     21625
weighted avg       0.80      0.81      0.79     21625

[[15660   805]
 [ 3249  1911]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Album/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Album/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8240
Precision: 0.6954
Recall:    0.4669
F1-score:  0.5587
              precision    recall  f1-score   support

           0       0.85      0.94      0.89     16465
           1       0.70      0.47      0.56      5160

    accuracy                           0.82     21625
   macro avg       0.77      0.70      0.72     21625
weighted avg       0.81      0.82      0.81     21625

[[15410  1055]
 [ 2751  2409]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Album/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Album/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.4695
Precision: 0.3065
Recall:    0.9684
F1-score:  0.4656
              precision    recall  f1-score   support

           0       0.97      0.31      0.47     16465
           1       0.31      0.97      0.47      5160

    accuracy                           0.47     21625
   macro avg       0.64      0.64      0.47     21625
weighted avg       0.81      0.47      0.47     21625

[[ 5156 11309]
 [  163  4997]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Album/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Album/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.8114, 'precision': 0.5942, 'recall': 0.6614, 'f1_score': 0.626}
XGBoost: {'accuracy': 0.824, 'precision': 0.6954, 'recall': 0.4669, 'f1_score': 0.5587}
Random Forest: {'accuracy': 0.8125, 'precision': 0.7036, 'recall': 0.3703, 'f1_score': 0.4853}
Naive Bayes: {'accuracy': 0.4695, 'precision': 0.3065, 'recall': 0.9684, 'f1_score': 0.4656}
Decision Tree: {'accuracy': 0.3127, 'precision': 0.2553, 'recall': 0.9812, 'f1_score': 0.4052}
SVM: {'accuracy': 0.6893, 'precision': 0.2476, 'recall': 0.1481, 'f1_score': 0.1853}

##################################################
Running experiment with SIMILAR ARTIST 1 feature
[Similar Artist 1] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}

Splitting data (index-based split to avoid leakage)...
==================================================
Data antes del undersampling ...
X: (86500, 5000)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5000)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5000, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [14:12:27] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [0/30] Train Loss: 0.4708, Test Loss: 0.3413, F1: 0.7475, AUC: 0.9213
Epoch [10/30] Train Loss: 0.2787, Test Loss: 0.3264, F1: 0.7481, AUC: 0.9278
Epoch [20/30] Train Loss: 0.2737, Test Loss: 0.3493, F1: 0.7409, AUC: 0.9275
Mejores resultados en la época:  9
f1-score 0.7523145938722917
AUC según el mejor F1-score 0.9280203132319671
Confusion Matrix:
 [[15089  1376]
 [ 1219  3941]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Artist 1/confusion_matrix_param_160065.png
Accuracy:   0.8800
Precision:  0.7412
Recall:     0.7638
F1-score:   0.7523

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4709, Test Loss: 0.3455, F1: 0.7477, AUC: 0.9224
Epoch [10/30] Train Loss: 0.2779, Test Loss: 0.3364, F1: 0.7436, AUC: 0.9279
Epoch [20/30] Train Loss: 0.2730, Test Loss: 0.3512, F1: 0.7429, AUC: 0.9275
Mejores resultados en la época:  21
f1-score 0.7506186940795736
AUC según el mejor F1-score 0.927472892934743
Confusion Matrix:
 [[15062  1403]
 [ 1217  3943]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Artist 1/confusion_matrix_param_160065.png
Accuracy:   0.8788
Precision:  0.7376
Recall:     0.7641
F1-score:   0.7506

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4658, Test Loss: 0.3393, F1: 0.7511, AUC: 0.9222
Epoch [10/30] Train Loss: 0.2785, Test Loss: 0.3449, F1: 0.7404, AUC: 0.9281
Epoch [20/30] Train Loss: 0.2733, Test Loss: 0.3532, F1: 0.7428, AUC: 0.9273
Mejores resultados en la época:  0
f1-score 0.7511373535959733
AUC según el mejor F1-score 0.9221860618130543
Confusion Matrix:
 [[15174  1291]
 [ 1280  3880]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Artist 1/confusion_matrix_param_160065.png
Accuracy:   0.8811
Precision:  0.7503
Recall:     0.7519
F1-score:   0.7511
Tiempo total para red 1: 288.23 segundos

Entrenando red 6 con capas [5000, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3969, Test Loss: 0.3153, F1: 0.7564, AUC: 0.9261
Epoch [10/30] Train Loss: 0.2463, Test Loss: 0.4302, F1: 0.7512, AUC: 0.9295
Epoch [20/30] Train Loss: 0.2358, Test Loss: 0.4673, F1: 0.7544, AUC: 0.9299
Mejores resultados en la época:  25
f1-score 0.7607960946301164
AUC según el mejor F1-score 0.9315565729042342
Confusion Matrix:
 [[15025  1440]
 [ 1108  4052]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Artist 1/confusion_matrix_param_5820417.png
Accuracy:   0.8822
Precision:  0.7378
Recall:     0.7853
F1-score:   0.7608

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3954, Test Loss: 0.3171, F1: 0.7556, AUC: 0.9259
Epoch [10/30] Train Loss: 0.2453, Test Loss: 0.3966, F1: 0.7519, AUC: 0.9302
Epoch [20/30] Train Loss: 0.2356, Test Loss: 0.4549, F1: 0.7539, AUC: 0.9297
Mejores resultados en la época:  26
f1-score 0.7628826408214183
AUC según el mejor F1-score 0.9307405360678159
Confusion Matrix:
 [[15179  1286]
 [ 1185  3975]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Artist 1/confusion_matrix_param_5820417.png
Accuracy:   0.8857
Precision:  0.7556
Recall:     0.7703
F1-score:   0.7629

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3917, Test Loss: 0.3289, F1: 0.7483, AUC: 0.9262
Epoch [10/30] Train Loss: 0.2451, Test Loss: 0.3810, F1: 0.7542, AUC: 0.9303
Epoch [20/30] Train Loss: 0.2361, Test Loss: 0.4730, F1: 0.7493, AUC: 0.9306
Mejores resultados en la época:  28
f1-score 0.762945914844649
AUC según el mejor F1-score 0.9305478146032106
Confusion Matrix:
 [[15175  1290]
 [ 1182  3978]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Artist 1/confusion_matrix_param_5820417.png
Accuracy:   0.8857
Precision:  0.7551
Recall:     0.7709
F1-score:   0.7629
Tiempo total para red 6: 359.20 segundos
Saved on: outputs_ablation_one_feature/0/Similar Artist 1

==============================
Model: Logistic Regression
Accuracy:  0.8803
Precision: 0.7424
Recall:    0.7630
F1-score:  0.7526
              precision    recall  f1-score   support

           0       0.93      0.92      0.92     16465
           1       0.74      0.76      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.84      0.84     21625
weighted avg       0.88      0.88      0.88     21625

[[15099  1366]
 [ 1223  3937]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Artist 1/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Artist 1/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.4856
Precision: 0.2210
Recall:    0.4578
F1-score:  0.2981
              precision    recall  f1-score   support

           0       0.74      0.49      0.59     16465
           1       0.22      0.46      0.30      5160

    accuracy                           0.49     21625
   macro avg       0.48      0.48      0.45     21625
weighted avg       0.62      0.49      0.52     21625

[[8139 8326]
 [2798 2362]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Artist 1/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Artist 1/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7960
Precision: 0.9539
Recall:    0.1525
F1-score:  0.2630
              precision    recall  f1-score   support

           0       0.79      1.00      0.88     16465
           1       0.95      0.15      0.26      5160

    accuracy                           0.80     21625
   macro avg       0.87      0.58      0.57     21625
weighted avg       0.83      0.80      0.73     21625

[[16427    38]
 [ 4373   787]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Artist 1/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Artist 1/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8425
Precision: 0.9156
Recall:    0.3742
F1-score:  0.5313
              precision    recall  f1-score   support

           0       0.83      0.99      0.91     16465
           1       0.92      0.37      0.53      5160

    accuracy                           0.84     21625
   macro avg       0.88      0.68      0.72     21625
weighted avg       0.85      0.84      0.82     21625

[[16287   178]
 [ 3229  1931]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Artist 1/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Artist 1/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8734
Precision: 0.8637
Recall:    0.5574
F1-score:  0.6775
              precision    recall  f1-score   support

           0       0.88      0.97      0.92     16465
           1       0.86      0.56      0.68      5160

    accuracy                           0.87     21625
   macro avg       0.87      0.76      0.80     21625
weighted avg       0.87      0.87      0.86     21625
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(

[[16011   454]
 [ 2284  2876]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Artist 1/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Artist 1/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6032
Precision: 0.3733
Recall:    0.9767
F1-score:  0.5402
              precision    recall  f1-score   support

           0       0.99      0.49      0.65     16465
           1       0.37      0.98      0.54      5160

    accuracy                           0.60     21625
   macro avg       0.68      0.73      0.60     21625
weighted avg       0.84      0.60      0.62     21625

[[8004 8461]
 [ 120 5040]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Artist 1/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Artist 1/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.8803, 'precision': 0.7424, 'recall': 0.763, 'f1_score': 0.7526}
XGBoost: {'accuracy': 0.8734, 'precision': 0.8637, 'recall': 0.5574, 'f1_score': 0.6775}
Naive Bayes: {'accuracy': 0.6032, 'precision': 0.3733, 'recall': 0.9767, 'f1_score': 0.5402}
Random Forest: {'accuracy': 0.8425, 'precision': 0.9156, 'recall': 0.3742, 'f1_score': 0.5313}
SVM: {'accuracy': 0.4856, 'precision': 0.221, 'recall': 0.4578, 'f1_score': 0.2981}
Decision Tree: {'accuracy': 0.796, 'precision': 0.9539, 'recall': 0.1525, 'f1_score': 0.263}

##################################################
Running experiment with SIMILAR SONG 1 feature
[Similar Song 1] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}

Splitting data (index-based split to avoid leakage)...
==================================================
Data antes del undersampling ...
X: (86500, 5000)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5000)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5000, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6372, Test Loss: 0.5974, F1: 0.4944, AUC: 0.7355
Epoch [10/30] Train Loss: 0.5049, Test Loss: 0.6313, F1: 0.4940, AUC: 0.7407
Epoch [20/30] Train Loss: 0.4973, Test Loss: 0.6501, F1: 0.4938, AUC: 0.7401
Mejores resultados en la época:  2
f1-score 0.5
AUC según el mejor F1-score 0.743571070417164
Confusion Matrix:
 [[10765  5700]
 [ 1540  3620]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Song 1/confusion_matrix_param_160065.png
Accuracy:   0.6652
Precision:  0.3884
Recall:     0.7016
F1-score:   0.5000

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6358, Test Loss: 0.5866, F1: 0.4920, AUC: 0.7349
Epoch [10/30] Train Loss: 0.5018, Test Loss: 0.6388, F1: 0.4942, AUC: 0.7418
Epoch [20/30] Train Loss: 0.4905, Test Loss: 0.6535, F1: 0.4977, AUC: 0.7430
Mejores resultados en la época:  26
f1-score 0.5011823619418556
AUC según el mejor F1-score 0.7429598725979703
Confusion Matrix:
 [[10850  5615]
 [ 1557  3603]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Song 1/confusion_matrix_param_160065.png
Accuracy:   0.6683
Precision:  0.3909
Recall:     0.6983
F1-score:   0.5012

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6354, Test Loss: 0.5696, F1: 0.4965, AUC: 0.7356
Epoch [10/30] Train Loss: 0.5029, Test Loss: 0.6240, F1: 0.4967, AUC: 0.7418
Epoch [20/30] Train Loss: 0.4935, Test Loss: 0.6652, F1: 0.4973, AUC: 0.7418
Mejores resultados en la época:  1
f1-score 0.5005131011835534
AUC según el mejor F1-score 0.7423292360821756
Confusion Matrix:
 [[10666  5799]
 [ 1502  3658]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Song 1/confusion_matrix_param_160065.png
Accuracy:   0.6624
Precision:  0.3868
Recall:     0.7089
F1-score:   0.5005
Tiempo total para red 1: 290.59 segundos

Entrenando red 6 con capas [5000, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6153, Test Loss: 0.5447, F1: 0.4988, AUC: 0.7458
Epoch [10/30] Train Loss: 0.3179, Test Loss: 0.9156, F1: 0.5200, AUC: 0.7655
Epoch [20/30] Train Loss: 0.3069, Test Loss: 1.0055, F1: 0.5141, AUC: 0.7673
Mejores resultados en la época:  18
f1-score 0.5204790577648383
AUC según el mejor F1-score 0.7728458181201845
Confusion Matrix:
 [[10445  6020]
 [ 1227  3933]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Song 1/confusion_matrix_param_5820417.png
Accuracy:   0.6649
Precision:  0.3952
Recall:     0.7622
F1-score:   0.5205

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6155, Test Loss: 0.5891, F1: 0.4996, AUC: 0.7448
Epoch [10/30] Train Loss: 0.3193, Test Loss: 1.2482, F1: 0.5086, AUC: 0.7600
Epoch [20/30] Train Loss: 0.3079, Test Loss: 1.2675, F1: 0.5074, AUC: 0.7610
Mejores resultados en la época:  9
f1-score 0.5214144829529445
AUC según el mejor F1-score 0.7661313874627176
Confusion Matrix:
 [[11130  5335]
 [ 1459  3701]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Song 1/confusion_matrix_param_5820417.png
Accuracy:   0.6858
Precision:  0.4096
Recall:     0.7172
F1-score:   0.5214

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6149, Test Loss: 0.6408, F1: 0.4992, AUC: 0.7408
Epoch [10/30] Train Loss: 0.3188, Test Loss: 1.0321, F1: 0.5166, AUC: 0.7625
Epoch [20/30] Train Loss: 0.3071, Test Loss: 1.1272, F1: 0.5183, AUC: 0.7699
Mejores resultados en la época:  26
f1-score 0.5213501423342822
AUC según el mejor F1-score 0.7670334948222328
Confusion Matrix:
 [[10717  5748]
 [ 1314  3846]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Song 1/confusion_matrix_param_5820417.png
Accuracy:   0.6734
Precision:  0.4009
Recall:     0.7453
F1-score:   0.5214
Tiempo total para red 6: 359.67 segundos
Saved on: outputs_ablation_one_feature/0/Similar Song 1

==============================
Model: Logistic Regression
Accuracy:  0.6640
Precision: 0.3876
Recall:    0.7037
F1-score:  0.4998
              precision    recall  f1-score   support

           0       0.88      0.65      0.75     16465
           1       0.39      0.70      0.50      5160

    accuracy                           0.66     21625
   macro avg       0.63      0.68      0.62     21625
weighted avg       0.76      0.66      0.69     21625

[[10727  5738]
 [ 1529  3631]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Song 1/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Song 1/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7056
Precision: 0.3104
Recall:    0.1915
F1-score:  0.2368
              precision    recall  f1-score   support

           0       0.77      0.87      0.82     16465
           1       0.31      0.19      0.24      5160

    accuracy                           0.71     21625
   macro avg       0.54      0.53      0.53     21625
weighted avg       0.66      0.71      0.68     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [14:29:53] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[14270  2195]
 [ 4172   988]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Song 1/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Song 1/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7703
Precision: 0.6338
Recall:    0.0886
F1-score:  0.1554
              precision    recall  f1-score   support

           0       0.78      0.98      0.87     16465
           1       0.63      0.09      0.16      5160

    accuracy                           0.77     21625
   macro avg       0.70      0.54      0.51     21625
weighted avg       0.74      0.77      0.70     21625

[[16201   264]
 [ 4703   457]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Song 1/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Song 1/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.5109
Precision: 0.3043
Recall:    0.8157
F1-score:  0.4432
              precision    recall  f1-score   support

           0       0.88      0.42      0.56     16465
           1       0.30      0.82      0.44      5160

    accuracy                           0.51     21625
   macro avg       0.59      0.62      0.50     21625
weighted avg       0.74      0.51      0.54     21625

[[6840 9625]
 [ 951 4209]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Song 1/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Song 1/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.5247
Precision: 0.3125
Recall:    0.8264
F1-score:  0.4535
              precision    recall  f1-score   support

           0       0.89      0.43      0.58     16465
           1       0.31      0.83      0.45      5160

    accuracy                           0.52     21625
   macro avg       0.60      0.63      0.52     21625
weighted avg       0.75      0.52      0.55     21625

[[7083 9382]
 [ 896 4264]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Song 1/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Song 1/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.3762
Precision: 0.2710
Recall:    0.9552
F1-score:  0.4222
              precision    recall  f1-score   support

           0       0.93      0.19      0.32     16465
           1       0.27      0.96      0.42      5160

    accuracy                           0.38     21625
   macro avg       0.60      0.58      0.37     21625
weighted avg       0.77      0.38      0.35     21625

[[ 3207 13258]
 [  231  4929]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Song 1/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Song 1/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.664, 'precision': 0.3876, 'recall': 0.7037, 'f1_score': 0.4998}
XGBoost: {'accuracy': 0.5247, 'precision': 0.3125, 'recall': 0.8264, 'f1_score': 0.4535}
Random Forest: {'accuracy': 0.5109, 'precision': 0.3043, 'recall': 0.8157, 'f1_score': 0.4432}
Naive Bayes: {'accuracy': 0.3762, 'precision': 0.271, 'recall': 0.9552, 'f1_score': 0.4222}
SVM: {'accuracy': 0.7056, 'precision': 0.3104, 'recall': 0.1915, 'f1_score': 0.2368}
Decision Tree: {'accuracy': 0.7703, 'precision': 0.6338, 'recall': 0.0886, 'f1_score': 0.1554}

##################################################
Running experiment with SIMILAR ARTIST 2 feature
[Similar Artist 2] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}

Splitting data (index-based split to avoid leakage)...
==================================================
Data antes del undersampling ...
X: (86500, 5000)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5000)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5000, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4695, Test Loss: 0.3285, F1: 0.7517, AUC: 0.9225
Epoch [10/30] Train Loss: 0.2768, Test Loss: 0.3272, F1: 0.7485, AUC: 0.9290
Epoch [20/30] Train Loss: 0.2712, Test Loss: 0.3581, F1: 0.7407, AUC: 0.9283
Mejores resultados en la época:  0
f1-score 0.7517437862265448
AUC según el mejor F1-score 0.9224688969084058
Confusion Matrix:
 [[15272  1193]
 [ 1334  3826]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Artist 2/confusion_matrix_param_160065.png
Accuracy:   0.8831
Precision:  0.7623
Recall:     0.7415
F1-score:   0.7517

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4652, Test Loss: 0.3336, F1: 0.7478, AUC: 0.9225
Epoch [10/30] Train Loss: 0.2770, Test Loss: 0.3408, F1: 0.7442, AUC: 0.9294
Epoch [20/30] Train Loss: 0.2721, Test Loss: 0.3494, F1: 0.7442, AUC: 0.9283
Mejores resultados en la época:  2
f1-score 0.75
AUC según el mejor F1-score 0.9299573325611998
Confusion Matrix:
 [[14940  1525]
 [ 1149  4011]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Artist 2/confusion_matrix_param_160065.png
Accuracy:   0.8763
Precision:  0.7245
Recall:     0.7773
F1-score:   0.7500

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4682, Test Loss: 0.3470, F1: 0.7418, AUC: 0.9231
Epoch [10/30] Train Loss: 0.2765, Test Loss: 0.3566, F1: 0.7325, AUC: 0.9286
Epoch [20/30] Train Loss: 0.2714, Test Loss: 0.3652, F1: 0.7371, AUC: 0.9279
Mejores resultados en la época:  7
f1-score 0.7474953617810761
AUC según el mejor F1-score 0.9295438703663161
Confusion Matrix:
 [[14874  1591]
 [ 1131  4029]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Artist 2/confusion_matrix_param_160065.png
Accuracy:   0.8741
Precision:  0.7169
Recall:     0.7808
F1-score:   0.7475
Tiempo total para red 1: 289.13 segundos

Entrenando red 6 con capas [5000, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3951, Test Loss: 0.3398, F1: 0.7559, AUC: 0.9287
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [14:47:23] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [10/30] Train Loss: 0.2393, Test Loss: 0.3589, F1: 0.7499, AUC: 0.9320
Epoch [20/30] Train Loss: 0.2294, Test Loss: 0.4823, F1: 0.7522, AUC: 0.9311
Mejores resultados en la época:  8
f1-score 0.7611221309152735
AUC según el mejor F1-score 0.9326672504749327
Confusion Matrix:
 [[15067  1398]
 [ 1131  4029]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Artist 2/confusion_matrix_param_5820417.png
Accuracy:   0.8831
Precision:  0.7424
Recall:     0.7808
F1-score:   0.7611

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3925, Test Loss: 0.3048, F1: 0.7519, AUC: 0.9265
Epoch [10/30] Train Loss: 0.2375, Test Loss: 0.3823, F1: 0.7619, AUC: 0.9331
Epoch [20/30] Train Loss: 0.2283, Test Loss: 0.4000, F1: 0.7525, AUC: 0.9334
Mejores resultados en la época:  13
f1-score 0.765012807134048
AUC según el mejor F1-score 0.9317430031285532
Confusion Matrix:
 [[15116  1349]
 [ 1128  4032]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Artist 2/confusion_matrix_param_5820417.png
Accuracy:   0.8855
Precision:  0.7493
Recall:     0.7814
F1-score:   0.7650

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4003, Test Loss: 0.3324, F1: 0.7449, AUC: 0.9274
Epoch [10/30] Train Loss: 0.2395, Test Loss: 0.3738, F1: 0.7524, AUC: 0.9331
Epoch [20/30] Train Loss: 0.2262, Test Loss: 0.5138, F1: 0.7517, AUC: 0.9324
Mejores resultados en la época:  26
f1-score 0.7623976663216335
AUC según el mejor F1-score 0.9324226042085986
Confusion Matrix:
 [[15049  1416]
 [ 1109  4051]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Artist 2/confusion_matrix_param_5820417.png
Accuracy:   0.8832
Precision:  0.7410
Recall:     0.7851
F1-score:   0.7624
Tiempo total para red 6: 360.07 segundos
Saved on: outputs_ablation_one_feature/0/Similar Artist 2

==============================
Model: Logistic Regression
Accuracy:  0.8801
Precision: 0.7401
Recall:    0.7667
F1-score:  0.7532
              precision    recall  f1-score   support

           0       0.93      0.92      0.92     16465
           1       0.74      0.77      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.84      0.84     21625
weighted avg       0.88      0.88      0.88     21625

[[15076  1389]
 [ 1204  3956]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Artist 2/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Artist 2/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.4860
Precision: 0.2185
Recall:    0.4481
F1-score:  0.2938
              precision    recall  f1-score   support

           0       0.74      0.50      0.60     16465
           1       0.22      0.45      0.29      5160

    accuracy                           0.49     21625
   macro avg       0.48      0.47      0.44     21625
weighted avg       0.62      0.49      0.52     21625

[[8198 8267]
 [2848 2312]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Artist 2/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Artist 2/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7959
Precision: 0.9388
Recall:    0.1547
F1-score:  0.2656
              precision    recall  f1-score   support

           0       0.79      1.00      0.88     16465
           1       0.94      0.15      0.27      5160

    accuracy                           0.80     21625
   macro avg       0.86      0.58      0.57     21625
weighted avg       0.83      0.80      0.73     21625

[[16413    52]
 [ 4362   798]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Artist 2/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Artist 2/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8427
Precision: 0.9066
Recall:    0.3800
F1-score:  0.5356
              precision    recall  f1-score   support

           0       0.84      0.99      0.91     16465
           1       0.91      0.38      0.54      5160

    accuracy                           0.84     21625
   macro avg       0.87      0.68      0.72     21625
weighted avg       0.85      0.84      0.82     21625

[[16263   202]
 [ 3199  1961]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Artist 2/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Artist 2/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8748
Precision: 0.8608
Recall:    0.5671
F1-score:  0.6837
              precision    recall  f1-score   support

           0       0.88      0.97      0.92     16465
           1       0.86      0.57      0.68      5160

    accuracy                           0.87     21625
   macro avg       0.87      0.77      0.80     21625
weighted avg       0.87      0.87      0.87     21625

[[15992   473]
 [ 2234  2926]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Artist 2/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Artist 2/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6035
Precision: 0.3736
Recall:    0.9785
F1-score:  0.5408
              precision    recall  f1-score   support

           0       0.99      0.49      0.65     16465
           1       0.37      0.98      0.54      5160

    accuracy                           0.60     21625
   macro avg       0.68      0.73      0.60     21625
weighted avg       0.84      0.60      0.62     21625

[[8001 8464]
 [ 111 5049]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Artist 2/conf_matrix_naive_bayes.png
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Modelo guardado como: outputs_ablation_one_feature/0/Similar Artist 2/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.8801, 'precision': 0.7401, 'recall': 0.7667, 'f1_score': 0.7532}
XGBoost: {'accuracy': 0.8748, 'precision': 0.8608, 'recall': 0.5671, 'f1_score': 0.6837}
Naive Bayes: {'accuracy': 0.6035, 'precision': 0.3736, 'recall': 0.9785, 'f1_score': 0.5408}
Random Forest: {'accuracy': 0.8427, 'precision': 0.9066, 'recall': 0.38, 'f1_score': 0.5356}
SVM: {'accuracy': 0.486, 'precision': 0.2185, 'recall': 0.4481, 'f1_score': 0.2938}
Decision Tree: {'accuracy': 0.7959, 'precision': 0.9388, 'recall': 0.1547, 'f1_score': 0.2656}

##################################################
Running experiment with SIMILAR SONG 2 feature
[Similar Song 2] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}

Splitting data (index-based split to avoid leakage)...
==================================================
Data antes del undersampling ...
X: (86500, 5000)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5000)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5000, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6344, Test Loss: 0.5616, F1: 0.5051, AUC: 0.7481
Epoch [10/30] Train Loss: 0.4980, Test Loss: 0.6029, F1: 0.5072, AUC: 0.7519
Epoch [20/30] Train Loss: 0.4896, Test Loss: 0.6341, F1: 0.5069, AUC: 0.7520
Mejores resultados en la época:  11
f1-score 0.5085896370185647
AUC según el mejor F1-score 0.7522578078470423
Confusion Matrix:
 [[10860  5605]
 [ 1489  3671]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Song 2/confusion_matrix_param_160065.png
Accuracy:   0.6720
Precision:  0.3958
Recall:     0.7114
F1-score:   0.5086

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6349, Test Loss: 0.5756, F1: 0.5033, AUC: 0.7470
Epoch [10/30] Train Loss: 0.4921, Test Loss: 0.6137, F1: 0.5094, AUC: 0.7532
Epoch [20/30] Train Loss: 0.4747, Test Loss: 0.6564, F1: 0.5103, AUC: 0.7565
Mejores resultados en la época:  27
f1-score 0.5123601712470653
AUC según el mejor F1-score 0.7574235281793422
Confusion Matrix:
 [[10853  5612]
 [ 1450  3710]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Song 2/confusion_matrix_param_160065.png
Accuracy:   0.6734
Precision:  0.3980
Recall:     0.7190
F1-score:   0.5124

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6352, Test Loss: 0.5667, F1: 0.5030, AUC: 0.7464
Epoch [10/30] Train Loss: 0.4984, Test Loss: 0.6292, F1: 0.5048, AUC: 0.7509
Epoch [20/30] Train Loss: 0.4897, Test Loss: 0.6284, F1: 0.5072, AUC: 0.7519
Mejores resultados en la época:  23
f1-score 0.50914483440435
AUC según el mejor F1-score 0.752392631068487
Confusion Matrix:
 [[11069  5396]
 [ 1555  3605]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Song 2/confusion_matrix_param_160065.png
Accuracy:   0.6786
Precision:  0.4005
Recall:     0.6986
F1-score:   0.5091
Tiempo total para red 1: 291.68 segundos

Entrenando red 6 con capas [5000, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6143, Test Loss: 0.5566, F1: 0.5084, AUC: 0.7532
Epoch [10/30] Train Loss: 0.3172, Test Loss: 1.0458, F1: 0.5307, AUC: 0.7789
Epoch [20/30] Train Loss: 0.3081, Test Loss: 1.0377, F1: 0.5319, AUC: 0.7848
Mejores resultados en la época:  15
f1-score 0.5364657814096017
AUC según el mejor F1-score 0.7847718910444282
Confusion Matrix:
 [[10879  5586]
 [ 1221  3939]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Song 2/confusion_matrix_param_5820417.png
Accuracy:   0.6852
Precision:  0.4135
Recall:     0.7634
F1-score:   0.5365

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6151, Test Loss: 0.5900, F1: 0.5070, AUC: 0.7513
Epoch [10/30] Train Loss: 0.3185, Test Loss: 0.8727, F1: 0.5322, AUC: 0.7827
Epoch [20/30] Train Loss: 0.3089, Test Loss: 1.4743, F1: 0.5167, AUC: 0.7704
Mejores resultados en la época:  27
f1-score 0.5359052453468697
AUC según el mejor F1-score 0.7859701633956926
Confusion Matrix:
 [[10809  5656]
 [ 1201  3959]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Song 2/confusion_matrix_param_5820417.png
Accuracy:   0.6829
Precision:  0.4118
Recall:     0.7672
F1-score:   0.5359

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6131, Test Loss: 0.6144, F1: 0.5017, AUC: 0.7514
Epoch [10/30] Train Loss: 0.3194, Test Loss: 0.8059, F1: 0.5319, AUC: 0.7832
Epoch [20/30] Train Loss: 0.3088, Test Loss: 0.9834, F1: 0.5342, AUC: 0.7853
Mejores resultados en la época:  16
f1-score 0.5382890534755724
AUC según el mejor F1-score 0.7870958363641928
Confusion Matrix:
 [[10869  5596]
 [ 1199  3961]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Song 2/confusion_matrix_param_5820417.png
Accuracy:   0.6858
Precision:  0.4145
Recall:     0.7676
F1-score:   0.5383
Tiempo total para red 6: 360.77 segundos
Saved on: outputs_ablation_one_feature/0/Similar Song 2

==============================
Model: Logistic Regression
Accuracy:  0.6668
Precision: 0.3911
Recall:    0.7124
F1-score:  0.5050
              precision    recall  f1-score   support

           0       0.88      0.65      0.75     16465
           1       0.39      0.71      0.51      5160

    accuracy                           0.67     21625
   macro avg       0.63      0.68      0.63     21625
weighted avg       0.76      0.67      0.69     21625

[[10743  5722]
 [ 1484  3676]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Song 2/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Song 2/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7083
Precision: 0.3145
Recall:    0.1888
F1-score:  0.2359
              precision    recall  f1-score   support

           0       0.77      0.87      0.82     16465
           1       0.31      0.19      0.24      5160

    accuracy                           0.71     21625
   macro avg       0.54      0.53      0.53     21625
weighted avg       0.66      0.71      0.68     21625

[[14342  2123]
 [ 4186   974]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Song 2/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Song 2/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7699
Precision: 0.6439
Recall:    0.0802
F1-score:  0.1427
              precision    recall  f1-score   support

           0       0.77      0.99      0.87     16465
           1       0.64      0.08      0.14      5160

    accuracy                           0.77     21625
   macro avg       0.71      0.53      0.50     21625
weighted avg       0.74      0.77      0.69     21625

[[16236   229]
 [ 4746   414]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Song 2/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Song 2/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.5209
Precision: 0.3116
Recall:    0.8335
F1-score:  0.4536
              precision    recall  f1-score   support

           0       0.89      0.42      0.57     16465
           1       0.31      0.83      0.45      5160

    accuracy                           0.52     21625
   macro avg       0.60      0.63      0.51     21625
weighted avg       0.75      0.52      0.54     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [15:04:54] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[6963 9502]
 [ 859 4301]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Song 2/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Song 2/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.5342
Precision: 0.3178
Recall:    0.8304
F1-score:  0.4597
              precision    recall  f1-score   support

           0       0.89      0.44      0.59     16465
           1       0.32      0.83      0.46      5160

    accuracy                           0.53     21625
   macro avg       0.61      0.64      0.53     21625
weighted avg       0.76      0.53      0.56     21625

[[7268 9197]
 [ 875 4285]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Song 2/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Song 2/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.3772
Precision: 0.2716
Recall:    0.9574
F1-score:  0.4232
              precision    recall  f1-score   support

           0       0.94      0.20      0.32     16465
           1       0.27      0.96      0.42      5160

    accuracy                           0.38     21625
   macro avg       0.60      0.58      0.37     21625
weighted avg       0.78      0.38      0.35     21625

[[ 3217 13248]
 [  220  4940]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Song 2/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Song 2/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.6668, 'precision': 0.3911, 'recall': 0.7124, 'f1_score': 0.505}
XGBoost: {'accuracy': 0.5342, 'precision': 0.3178, 'recall': 0.8304, 'f1_score': 0.4597}
Random Forest: {'accuracy': 0.5209, 'precision': 0.3116, 'recall': 0.8335, 'f1_score': 0.4536}
Naive Bayes: {'accuracy': 0.3772, 'precision': 0.2716, 'recall': 0.9574, 'f1_score': 0.4232}
SVM: {'accuracy': 0.7083, 'precision': 0.3145, 'recall': 0.1888, 'f1_score': 0.2359}
Decision Tree: {'accuracy': 0.7699, 'precision': 0.6439, 'recall': 0.0802, 'f1_score': 0.1427}

##################################################
Running experiment with SIMILAR ARTIST 3 feature
[Similar Artist 3] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}

Splitting data (index-based split to avoid leakage)...
==================================================
Data antes del undersampling ...
X: (86500, 5000)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5000)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5000, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4658, Test Loss: 0.3525, F1: 0.7430, AUC: 0.9196
Epoch [10/30] Train Loss: 0.2707, Test Loss: 0.3455, F1: 0.7383, AUC: 0.9268
Epoch [20/30] Train Loss: 0.2657, Test Loss: 0.3609, F1: 0.7344, AUC: 0.9258
Mejores resultados en la época:  13
f1-score 0.7498808956646021
AUC según el mejor F1-score 0.9262849019649386
Confusion Matrix:
 [[15065  1400]
 [ 1225  3935]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Artist 3/confusion_matrix_param_160065.png
Accuracy:   0.8786
Precision:  0.7376
Recall:     0.7626
F1-score:   0.7499

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4633, Test Loss: 0.3348, F1: 0.7504, AUC: 0.9201
Epoch [10/30] Train Loss: 0.2706, Test Loss: 0.3320, F1: 0.7427, AUC: 0.9267
Epoch [20/30] Train Loss: 0.2655, Test Loss: 0.3554, F1: 0.7379, AUC: 0.9257
Mejores resultados en la época:  0
f1-score 0.7504135448087963
AUC según el mejor F1-score 0.920081780238561
Confusion Matrix:
 [[15204  1261]
 [ 1304  3856]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Artist 3/confusion_matrix_param_160065.png
Accuracy:   0.8814
Precision:  0.7536
Recall:     0.7473
F1-score:   0.7504

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4663, Test Loss: 0.3314, F1: 0.7498, AUC: 0.9204
Epoch [10/30] Train Loss: 0.2707, Test Loss: 0.3440, F1: 0.7379, AUC: 0.9266
Epoch [20/30] Train Loss: 0.2656, Test Loss: 0.3481, F1: 0.7423, AUC: 0.9259
Mejores resultados en la época:  15
f1-score 0.7506451304597151
AUC según el mejor F1-score 0.926193393550331
Confusion Matrix:
 [[15089  1376]
 [ 1233  3927]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Artist 3/confusion_matrix_param_160065.png
Accuracy:   0.8794
Precision:  0.7405
Recall:     0.7610
F1-score:   0.7506
Tiempo total para red 1: 289.47 segundos

Entrenando red 6 con capas [5000, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3874, Test Loss: 0.2955, F1: 0.7577, AUC: 0.9251
Epoch [10/30] Train Loss: 0.2317, Test Loss: 0.3808, F1: 0.7572, AUC: 0.9325
Epoch [20/30] Train Loss: 0.2234, Test Loss: 0.4270, F1: 0.7585, AUC: 0.9320
Mejores resultados en la época:  28
f1-score 0.7705426356589147
AUC según el mejor F1-score 0.9325076271725082
Confusion Matrix:
 [[15281  1184]
 [ 1184  3976]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Artist 3/confusion_matrix_param_5820417.png
Accuracy:   0.8905
Precision:  0.7705
Recall:     0.7705
F1-score:   0.7705

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3888, Test Loss: 0.3264, F1: 0.7393, AUC: 0.9254
Epoch [10/30] Train Loss: 0.2315, Test Loss: 0.3919, F1: 0.7560, AUC: 0.9314
Epoch [20/30] Train Loss: 0.2228, Test Loss: 0.3972, F1: 0.7626, AUC: 0.9321
Mejores resultados en la época:  8
f1-score 0.7626243305279266
AUC según el mejor F1-score 0.932270443294091
Confusion Matrix:
 [[15156  1309]
 [ 1173  3987]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Artist 3/confusion_matrix_param_5820417.png
Accuracy:   0.8852
Precision:  0.7528
Recall:     0.7727
F1-score:   0.7626

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.3899, Test Loss: 0.3101, F1: 0.7526, AUC: 0.9238
Epoch [10/30] Train Loss: 0.2322, Test Loss: 0.3663, F1: 0.7569, AUC: 0.9317
Epoch [20/30] Train Loss: 0.2225, Test Loss: 0.4672, F1: 0.7549, AUC: 0.9317
Mejores resultados en la época:  7
f1-score 0.7633130375954383
AUC según el mejor F1-score 0.9318865128520211
Confusion Matrix:
 [[15227  1238]
 [ 1211  3949]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Artist 3/confusion_matrix_param_5820417.png
Accuracy:   0.8868
Precision:  0.7613
Recall:     0.7653
F1-score:   0.7633
Tiempo total para red 6: 359.80 segundos
Saved on: outputs_ablation_one_feature/0/Similar Artist 3

==============================
Model: Logistic Regression
Accuracy:  0.8819
Precision: 0.7513
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [15:22:15] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Recall:    0.7550
F1-score:  0.7531
              precision    recall  f1-score   support

           0       0.92      0.92      0.92     16465
           1       0.75      0.76      0.75      5160

    accuracy                           0.88     21625
   macro avg       0.84      0.84      0.84     21625
weighted avg       0.88      0.88      0.88     21625

[[15175  1290]
 [ 1264  3896]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Artist 3/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Artist 3/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.4792
Precision: 0.2143
Recall:    0.4436
F1-score:  0.2890
              precision    recall  f1-score   support

           0       0.74      0.49      0.59     16465
           1       0.21      0.44      0.29      5160

    accuracy                           0.48     21625
   macro avg       0.48      0.47      0.44     21625
weighted avg       0.61      0.48      0.52     21625

[[8073 8392]
 [2871 2289]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Artist 3/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Artist 3/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7974
Precision: 0.9545
Recall:    0.1585
F1-score:  0.2719
              precision    recall  f1-score   support

           0       0.79      1.00      0.88     16465
           1       0.95      0.16      0.27      5160

    accuracy                           0.80     21625
   macro avg       0.87      0.58      0.58     21625
weighted avg       0.83      0.80      0.74     21625

[[16426    39]
 [ 4342   818]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Artist 3/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Artist 3/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8407
Precision: 0.9047
Recall:    0.3715
F1-score:  0.5267
              precision    recall  f1-score   support

           0       0.83      0.99      0.90     16465
           1       0.90      0.37      0.53      5160

    accuracy                           0.84     21625
   macro avg       0.87      0.68      0.72     21625
weighted avg       0.85      0.84      0.81     21625

[[16263   202]
 [ 3243  1917]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Artist 3/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Artist 3/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8770
Precision: 0.8687
Recall:    0.5707
F1-score:  0.6889
              precision    recall  f1-score   support

           0       0.88      0.97      0.92     16465
           1       0.87      0.57      0.69      5160

    accuracy                           0.88     21625
   macro avg       0.87      0.77      0.81     21625
weighted avg       0.88      0.88      0.87     21625

[[16020   445]
 [ 2215  2945]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Artist 3/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Artist 3/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6059
Precision: 0.3749
Recall:    0.9764
F1-score:  0.5418
              precision    recall  f1-score   support

           0       0.99      0.49      0.65     16465
           1       0.37      0.98      0.54      5160

    accuracy                           0.61     21625
   macro avg       0.68      0.73      0.60     21625
weighted avg       0.84      0.61      0.63     21625

[[8065 8400]
 [ 122 5038]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Artist 3/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Artist 3/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.8819, 'precision': 0.7513, 'recall': 0.755, 'f1_score': 0.7531}
XGBoost: {'accuracy': 0.877, 'precision': 0.8687, 'recall': 0.5707, 'f1_score': 0.6889}
Naive Bayes: {'accuracy': 0.6059, 'precision': 0.3749, 'recall': 0.9764, 'f1_score': 0.5418}
Random Forest: {'accuracy': 0.8407, 'precision': 0.9047, 'recall': 0.3715, 'f1_score': 0.5267}
SVM: {'accuracy': 0.4792, 'precision': 0.2143, 'recall': 0.4436, 'f1_score': 0.289}
Decision Tree: {'accuracy': 0.7974, 'precision': 0.9545, 'recall': 0.1585, 'f1_score': 0.2719}

##################################################
Running experiment with SIMILAR SONG 3 feature
[Similar Song 3] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}

Splitting data (index-based split to avoid leakage)...
==================================================
Data antes del undersampling ...
X: (86500, 5000)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5000)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5000, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6349, Test Loss: 0.5980, F1: 0.4893, AUC: 0.7367
Epoch [10/30] Train Loss: 0.4933, Test Loss: 0.6415, F1: 0.4999, AUC: 0.7475
Epoch [20/30] Train Loss: 0.4796, Test Loss: 0.6569, F1: 0.5036, AUC: 0.7497
Mejores resultados en la época:  27
f1-score 0.5070343275182893
AUC según el mejor F1-score 0.7513254978260204
Confusion Matrix:
 [[11013  5452]
 [ 1556  3604]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Song 3/confusion_matrix_param_160065.png
Accuracy:   0.6759
Precision:  0.3980
Recall:     0.6984
F1-score:   0.5070

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6348, Test Loss: 0.5884, F1: 0.4919, AUC: 0.7380
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [15:39:44] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [10/30] Train Loss: 0.4967, Test Loss: 0.6283, F1: 0.5004, AUC: 0.7457
Epoch [20/30] Train Loss: 0.4883, Test Loss: 0.6391, F1: 0.5003, AUC: 0.7464
Mejores resultados en la época:  29
f1-score 0.5037406483790524
AUC según el mejor F1-score 0.7472345967603349
Confusion Matrix:
 [[10825  5640]
 [ 1524  3636]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Song 3/confusion_matrix_param_160065.png
Accuracy:   0.6687
Precision:  0.3920
Recall:     0.7047
F1-score:   0.5037

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6346, Test Loss: 0.5678, F1: 0.4944, AUC: 0.7373
Epoch [10/30] Train Loss: 0.4969, Test Loss: 0.6188, F1: 0.5021, AUC: 0.7462
Epoch [20/30] Train Loss: 0.4883, Test Loss: 0.6590, F1: 0.5025, AUC: 0.7464
Mejores resultados en la época:  28
f1-score 0.5041959935029778
AUC según el mejor F1-score 0.7471028397093199
Confusion Matrix:
 [[10574  5891]
 [ 1435  3725]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Song 3/confusion_matrix_param_160065.png
Accuracy:   0.6612
Precision:  0.3874
Recall:     0.7219
F1-score:   0.5042
Tiempo total para red 1: 291.77 segundos

Entrenando red 6 con capas [5000, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6154, Test Loss: 0.5618, F1: 0.4957, AUC: 0.7441
Epoch [10/30] Train Loss: 0.3176, Test Loss: 1.0019, F1: 0.5230, AUC: 0.7741
Epoch [20/30] Train Loss: 0.3092, Test Loss: 1.0268, F1: 0.5276, AUC: 0.7792
Mejores resultados en la época:  18
f1-score 0.5355831685964417
AUC según el mejor F1-score 0.7852718769200349
Confusion Matrix:
 [[11254  5211]
 [ 1367  3793]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Song 3/confusion_matrix_param_5820417.png
Accuracy:   0.6958
Precision:  0.4213
Recall:     0.7351
F1-score:   0.5356

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6146, Test Loss: 0.5673, F1: 0.4975, AUC: 0.7434
Epoch [10/30] Train Loss: 0.3181, Test Loss: 0.8929, F1: 0.5330, AUC: 0.7831
Epoch [20/30] Train Loss: 0.3080, Test Loss: 1.3009, F1: 0.5281, AUC: 0.7793
Mejores resultados en la época:  16
f1-score 0.5348788198103267
AUC según el mejor F1-score 0.7850341810323518
Confusion Matrix:
 [[11197  5268]
 [ 1353  3807]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Song 3/confusion_matrix_param_5820417.png
Accuracy:   0.6938
Precision:  0.4195
Recall:     0.7378
F1-score:   0.5349

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6138, Test Loss: 0.5562, F1: 0.4985, AUC: 0.7464
Epoch [10/30] Train Loss: 0.3187, Test Loss: 0.9194, F1: 0.5315, AUC: 0.7804
Epoch [20/30] Train Loss: 0.3073, Test Loss: 1.1462, F1: 0.5274, AUC: 0.7792
Mejores resultados en la época:  22
f1-score 0.5337541163556532
AUC según el mejor F1-score 0.7840856220736021
Confusion Matrix:
 [[10939  5526]
 [ 1270  3890]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Similar Song 3/confusion_matrix_param_5820417.png
Accuracy:   0.6857
Precision:  0.4131
Recall:     0.7539
F1-score:   0.5338
Tiempo total para red 6: 361.76 segundos
Saved on: outputs_ablation_one_feature/0/Similar Song 3

==============================
Model: Logistic Regression
Accuracy:  0.6651
Precision: 0.3878
Recall:    0.6979
F1-score:  0.4986
              precision    recall  f1-score   support

           0       0.87      0.65      0.75     16465
           1       0.39      0.70      0.50      5160

    accuracy                           0.67     21625
   macro avg       0.63      0.68      0.62     21625
weighted avg       0.76      0.67      0.69     21625

[[10781  5684]
 [ 1559  3601]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Song 3/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Song 3/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.6757
Precision: 0.2852
Recall:    0.2382
F1-score:  0.2596
              precision    recall  f1-score   support

           0       0.77      0.81      0.79     16465
           1       0.29      0.24      0.26      5160

    accuracy                           0.68     21625
   macro avg       0.53      0.53      0.53     21625
weighted avg       0.66      0.68      0.67     21625

[[13384  3081]
 [ 3931  1229]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Song 3/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Song 3/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7671
Precision: 0.5853
Recall:    0.0818
F1-score:  0.1435
              precision    recall  f1-score   support

           0       0.77      0.98      0.87     16465
           1       0.59      0.08      0.14      5160

    accuracy                           0.77     21625
   macro avg       0.68      0.53      0.50     21625
weighted avg       0.73      0.77      0.69     21625

[[16166   299]
 [ 4738   422]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Song 3/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Song 3/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.5330
Precision: 0.3133
Recall:    0.8027
F1-score:  0.4507
              precision    recall  f1-score   support

           0       0.88      0.45      0.59     16465
           1       0.31      0.80      0.45      5160

    accuracy                           0.53     21625
   macro avg       0.60      0.63      0.52     21625
weighted avg       0.74      0.53      0.56     21625

[[7385 9080]
 [1018 4142]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Song 3/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Song 3/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.5336
Precision: 0.3152
Recall:    0.8143
F1-score:  0.4545
              precision    recall  f1-score   support

           0       0.88      0.45      0.59     16465
           1       0.32      0.81      0.45      5160

    accuracy                           0.53     21625
   macro avg       0.60      0.63      0.52     21625
weighted avg       0.75      0.53      0.56     21625

[[7337 9128]
 [ 958 4202]]
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Song 3/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Song 3/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.3780
Precision: 0.2723
Recall:    0.9605
F1-score:  0.4243
              precision    recall  f1-score   support

           0       0.94      0.20      0.32     16465
           1       0.27      0.96      0.42      5160

    accuracy                           0.38     21625
   macro avg       0.61      0.58      0.37     21625
weighted avg       0.78      0.38      0.35     21625

[[ 3218 13247]
 [  204  4956]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Similar Song 3/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Similar Song 3/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.6651, 'precision': 0.3878, 'recall': 0.6979, 'f1_score': 0.4986}
XGBoost: {'accuracy': 0.5336, 'precision': 0.3152, 'recall': 0.8143, 'f1_score': 0.4545}
Random Forest: {'accuracy': 0.533, 'precision': 0.3133, 'recall': 0.8027, 'f1_score': 0.4507}
Naive Bayes: {'accuracy': 0.378, 'precision': 0.2723, 'recall': 0.9605, 'f1_score': 0.4243}
SVM: {'accuracy': 0.6757, 'precision': 0.2852, 'recall': 0.2382, 'f1_score': 0.2596}
Decision Tree: {'accuracy': 0.7671, 'precision': 0.5853, 'recall': 0.0818, 'f1_score': 0.1435}

##################################################
Running experiment with SONG_NORMALIZED feature
[song_normalized] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}

Splitting data (index-based split to avoid leakage)...
==================================================
Data antes del undersampling ...
X: (86500, 5000)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5000)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5000, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6629, Test Loss: 0.6274, F1: 0.4515, AUC: 0.6794
Epoch [10/30] Train Loss: 0.5513, Test Loss: 0.6852, F1: 0.4499, AUC: 0.6728
Epoch [20/30] Train Loss: 0.5208, Test Loss: 0.7525, F1: 0.4474, AUC: 0.6691
Mejores resultados en la época:  1
f1-score 0.4531897265948633
AUC según el mejor F1-score 0.6802487423404591
Confusion Matrix:
 [[10423  6042]
 [ 1878  3282]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/song_normalized/confusion_matrix_param_160065.png
Accuracy:   0.6338
Precision:  0.3520
Recall:     0.6360
F1-score:   0.4532

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6637, Test Loss: 0.6393, F1: 0.4516, AUC: 0.6779
Epoch [10/30] Train Loss: 0.5547, Test Loss: 0.6846, F1: 0.4463, AUC: 0.6726
Epoch [20/30] Train Loss: 0.5385, Test Loss: 0.7188, F1: 0.4496, AUC: 0.6716
Mejores resultados en la época:  19
f1-score 0.4524290135917104
AUC según el mejor F1-score 0.6729807767004005
Confusion Matrix:
 [[10125  6340]
 [ 1798  3362]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/song_normalized/confusion_matrix_param_160065.png
Accuracy:   0.6237
Precision:  0.3465
Recall:     0.6516
F1-score:   0.4524

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6629, Test Loss: 0.6314, F1: 0.4507, AUC: 0.6791
Epoch [10/30] Train Loss: 0.5538, Test Loss: 0.6853, F1: 0.4466, AUC: 0.6719
Epoch [20/30] Train Loss: 0.5330, Test Loss: 0.7476, F1: 0.4466, AUC: 0.6693
Mejores resultados en la época:  3
f1-score 0.45282010157711844
AUC según el mejor F1-score 0.6765246812006676
Confusion Matrix:
 [[10049  6416]
 [ 1772  3388]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/song_normalized/confusion_matrix_param_160065.png
Accuracy:   0.6214
Precision:  0.3456
Recall:     0.6566
F1-score:   0.4528
Tiempo total para red 1: 292.69 segundos

Entrenando red 6 con capas [5000, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6562, Test Loss: 0.6238, F1: 0.4498, AUC: 0.6801
Epoch [10/30] Train Loss: 0.3559, Test Loss: 1.4646, F1: 0.4390, AUC: 0.6496
Epoch [20/30] Train Loss: 0.3355, Test Loss: 1.3618, F1: 0.4344, AUC: 0.6465
Mejores resultados en la época:  3
f1-score 0.4505696936617316
AUC según el mejor F1-score 0.6738263570599604
Confusion Matrix:
 [[10270  6195]
 [ 1858  3302]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/song_normalized/confusion_matrix_param_5820417.png
Accuracy:   0.6276
Precision:  0.3477
Recall:     0.6399
F1-score:   0.4506

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6559, Test Loss: 0.6223, F1: 0.4473, AUC: 0.6777
Epoch [10/30] Train Loss: 0.3542, Test Loss: 1.2962, F1: 0.4377, AUC: 0.6492
Epoch [20/30] Train Loss: 0.3371, Test Loss: 1.7256, F1: 0.4419, AUC: 0.6484
Mejores resultados en la época:  2
f1-score 0.44922028567684447
AUC según el mejor F1-score 0.6754523984397254
Confusion Matrix:
 [[9791 6674]
 [1732 3428]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/song_normalized/confusion_matrix_param_5820417.png
Accuracy:   0.6113
Precision:  0.3393
Recall:     0.6643
F1-score:   0.4492

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6543, Test Loss: 0.6238, F1: 0.4424, AUC: 0.6776
Epoch [10/30] Train Loss: 0.3550, Test Loss: 1.4824, F1: 0.4411, AUC: 0.6540
Epoch [20/30] Train Loss: 0.3360, Test Loss: 1.4285, F1: 0.4448, AUC: 0.6547
Mejores resultados en la época:  1
f1-score 0.44958924570575054
AUC según el mejor F1-score 0.6786514794125195
Confusion Matrix:
 [[10207  6258]
 [ 1849  3311]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/song_normalized/confusion_matrix_param_5820417.png
Accuracy:   0.6251
Precision:  0.3460
Recall:     0.6417
F1-score:   0.4496
Tiempo total para red 6: 361.53 segundos
Saved on: outputs_ablation_one_feature/0/song_normalized

==============================
Model: Logistic Regression
Accuracy:  0.6154
Precision: 0.3424
Recall:    0.6651
F1-score:  0.4521
              precision    recall  f1-score   support

           0       0.85      0.60      0.70     16465
           1       0.34      0.67      0.45      5160

    accuracy                           0.62     21625
   macro avg       0.60      0.63      0.58     21625
weighted avg       0.73      0.62      0.64     21625

[[9875 6590]
 [1728 3432]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/song_normalized/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/song_normalized/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7092
Precision: 0.3173
Recall:    0.1899
F1-score:  0.2376
              precision    recall  f1-score   support

           0       0.77      0.87      0.82     16465
           1       0.32      0.19      0.24      5160

    accuracy                           0.71     21625
   macro avg       0.55      0.53      0.53     21625
weighted avg       0.67      0.71      0.68     21625

[[14356  2109]
 [ 4180   980]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/song_normalized/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/song_normalized/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7668
Precision: 0.6032
Recall:    0.0663
F1-score:  0.1194
              precision    recall  f1-score   support

           0       0.77      0.99      0.87     16465
           1       0.60      0.07      0.12      5160

    accuracy                           0.77     21625
   macro avg       0.69      0.53      0.49     21625
weighted avg       0.73      0.77      0.69     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [15:57:21] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[16240   225]
 [ 4818   342]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/song_normalized/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/song_normalized/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.4826
Precision: 0.2909
Recall:    0.8130
F1-score:  0.4285
              precision    recall  f1-score   support

           0       0.87      0.38      0.53     16465
           1       0.29      0.81      0.43      5160

    accuracy                           0.48     21625
   macro avg       0.58      0.60      0.48     21625
weighted avg       0.73      0.48      0.50     21625

[[ 6241 10224]
 [  965  4195]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/song_normalized/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/song_normalized/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.5072
Precision: 0.2980
Recall:    0.7860
F1-score:  0.4322
              precision    recall  f1-score   support

           0       0.86      0.42      0.56     16465
           1       0.30      0.79      0.43      5160

    accuracy                           0.51     21625
   macro avg       0.58      0.60      0.50     21625
weighted avg       0.73      0.51      0.53     21625

[[6912 9553]
 [1104 4056]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/song_normalized/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/song_normalized/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.3445
Precision: 0.2568
Recall:    0.9227
F1-score:  0.4018
              precision    recall  f1-score   support

           0       0.87      0.16      0.27     16465
           1       0.26      0.92      0.40      5160

    accuracy                           0.34     21625
   macro avg       0.56      0.54      0.34     21625
weighted avg       0.72      0.34      0.31     21625

[[ 2688 13777]
 [  399  4761]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/song_normalized/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/song_normalized/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.6154, 'precision': 0.3424, 'recall': 0.6651, 'f1_score': 0.4521}
XGBoost: {'accuracy': 0.5072, 'precision': 0.298, 'recall': 0.786, 'f1_score': 0.4322}
Random Forest: {'accuracy': 0.4826, 'precision': 0.2909, 'recall': 0.813, 'f1_score': 0.4285}
Naive Bayes: {'accuracy': 0.3445, 'precision': 0.2568, 'recall': 0.9227, 'f1_score': 0.4018}
SVM: {'accuracy': 0.7092, 'precision': 0.3173, 'recall': 0.1899, 'f1_score': 0.2376}
Decision Tree: {'accuracy': 0.7668, 'precision': 0.6032, 'recall': 0.0663, 'f1_score': 0.1194}

##################################################
Running experiment with ARTIST_NORMALIZED feature
[artist_normalized] in COL_TF_IDF
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}

Splitting data (index-based split to avoid leakage)...
==================================================
Data antes del undersampling ...
X: (86500, 5000)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5000)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5000, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4767, Test Loss: 0.3483, F1: 0.7326, AUC: 0.9135
Epoch [10/30] Train Loss: 0.2896, Test Loss: 0.3385, F1: 0.7279, AUC: 0.9211
Epoch [20/30] Train Loss: 0.2851, Test Loss: 0.3600, F1: 0.7267, AUC: 0.9214
Mejores resultados en la época:  0
f1-score 0.7326324064066031
AUC según el mejor F1-score 0.9135204992031486
Confusion Matrix:
 [[15176  1289]
 [ 1432  3728]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/artist_normalized/confusion_matrix_param_160065.png
Accuracy:   0.8742
Precision:  0.7431
Recall:     0.7225
F1-score:   0.7326

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4798, Test Loss: 0.3438, F1: 0.7352, AUC: 0.9126
Epoch [10/30] Train Loss: 0.2896, Test Loss: 0.3388, F1: 0.7302, AUC: 0.9218
Epoch [20/30] Train Loss: 0.2853, Test Loss: 0.3575, F1: 0.7307, AUC: 0.9212
Mejores resultados en la época:  0
f1-score 0.7352098765432099
AUC según el mejor F1-score 0.9125885364068013
Confusion Matrix:
 [[15222  1243]
 [ 1438  3722]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/artist_normalized/confusion_matrix_param_160065.png
Accuracy:   0.8760
Precision:  0.7496
Recall:     0.7213
F1-score:   0.7352

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.4788, Test Loss: 0.3525, F1: 0.7292, AUC: 0.9137
Epoch [10/30] Train Loss: 0.2899, Test Loss: 0.3506, F1: 0.7230, AUC: 0.9209
Epoch [20/30] Train Loss: 0.2853, Test Loss: 0.3644, F1: 0.7259, AUC: 0.9209
Mejores resultados en la época:  3
f1-score 0.7313489073097211
AUC según el mejor F1-score 0.9208937739673302
Confusion Matrix:
 [[14891  1574]
 [ 1278  3882]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/artist_normalized/confusion_matrix_param_160065.png
Accuracy:   0.8681
Precision:  0.7115
Recall:     0.7523
F1-score:   0.7313
Tiempo total para red 1: 290.59 segundos

Entrenando red 6 con capas [5000, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4038, Test Loss: 0.3292, F1: 0.7326, AUC: 0.9192
Epoch [10/30] Train Loss: 0.2726, Test Loss: 0.3819, F1: 0.7372, AUC: 0.9229
Epoch [20/30] Train Loss: 0.2665, Test Loss: 0.4171, F1: 0.7379, AUC: 0.9226
Mejores resultados en la época:  4
f1-score 0.7406623026569118
AUC según el mejor F1-score 0.923444015612163
Confusion Matrix:
 [[15084  1381]
 [ 1313  3847]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/artist_normalized/confusion_matrix_param_5820417.png
Accuracy:   0.8754
Precision:  0.7358
Recall:     0.7455
F1-score:   0.7407

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4025, Test Loss: 0.2986, F1: 0.7351, AUC: 0.9184
Epoch [10/30] Train Loss: 0.2718, Test Loss: 0.4314, F1: 0.7271, AUC: 0.9225
Epoch [20/30] Train Loss: 0.2666, Test Loss: 0.4800, F1: 0.7266, AUC: 0.9208
Mejores resultados en la época:  21
f1-score 0.7437694704049844
AUC según el mejor F1-score 0.9226940515116632
Confusion Matrix:
 [[15173  1292]
 [ 1340  3820]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [16:15:01] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Matriz de confusión guardada en: outputs_ablation_one_feature/0/artist_normalized/confusion_matrix_param_5820417.png
Accuracy:   0.8783
Precision:  0.7473
Recall:     0.7403
F1-score:   0.7438

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.4032, Test Loss: 0.3204, F1: 0.7341, AUC: 0.9194
Epoch [10/30] Train Loss: 0.2732, Test Loss: 0.3841, F1: 0.7245, AUC: 0.9227
Epoch [20/30] Train Loss: 0.2660, Test Loss: 0.3956, F1: 0.7265, AUC: 0.9226
Mejores resultados en la época:  9
f1-score 0.7376403955073437
AUC según el mejor F1-score 0.9222441483814622
Confusion Matrix:
 [[15050  1415]
 [ 1318  3842]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/artist_normalized/confusion_matrix_param_5820417.png
Accuracy:   0.8736
Precision:  0.7308
Recall:     0.7446
F1-score:   0.7376
Tiempo total para red 6: 362.99 segundos
Saved on: outputs_ablation_one_feature/0/artist_normalized

==============================
Model: Logistic Regression
Accuracy:  0.8710
Precision: 0.7241
Recall:    0.7424
F1-score:  0.7331
              precision    recall  f1-score   support

           0       0.92      0.91      0.91     16465
           1       0.72      0.74      0.73      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.83      0.82     21625
weighted avg       0.87      0.87      0.87     21625

[[15005  1460]
 [ 1329  3831]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/artist_normalized/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/artist_normalized/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.4839
Precision: 0.2312
Recall:    0.5002
F1-score:  0.3163
              precision    recall  f1-score   support

           0       0.75      0.48      0.59     16465
           1       0.23      0.50      0.32      5160

    accuracy                           0.48     21625
   macro avg       0.49      0.49      0.45     21625
weighted avg       0.63      0.48      0.52     21625

[[7884 8581]
 [2579 2581]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/artist_normalized/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/artist_normalized/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7941
Precision: 0.9585
Recall:    0.1432
F1-score:  0.2492
              precision    recall  f1-score   support

           0       0.79      1.00      0.88     16465
           1       0.96      0.14      0.25      5160

    accuracy                           0.79     21625
   macro avg       0.87      0.57      0.56     21625
weighted avg       0.83      0.79      0.73     21625

[[16433    32]
 [ 4421   739]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/artist_normalized/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/artist_normalized/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8428
Precision: 0.9356
Recall:    0.3663
F1-score:  0.5265
              precision    recall  f1-score   support

           0       0.83      0.99      0.91     16465
           1       0.94      0.37      0.53      5160

    accuracy                           0.84     21625
   macro avg       0.88      0.68      0.72     21625
weighted avg       0.86      0.84      0.82     21625

[[16335   130]
 [ 3270  1890]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/artist_normalized/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/artist_normalized/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8748
Precision: 0.8838
Recall:    0.5471
F1-score:  0.6758
              precision    recall  f1-score   support

           0       0.87      0.98      0.92     16465
           1       0.88      0.55      0.68      5160

    accuracy                           0.87     21625
   macro avg       0.88      0.76      0.80     21625
weighted avg       0.88      0.87      0.86     21625

[[16094   371]
 [ 2337  2823]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/artist_normalized/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/artist_normalized/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.5963
Precision: 0.3697
Recall:    0.9816
F1-score:  0.5371
              precision    recall  f1-score   support

           0       0.99      0.48      0.64     16465
           1       0.37      0.98      0.54      5160

    accuracy                           0.60     21625
   macro avg       0.68      0.73      0.59     21625
weighted avg       0.84      0.60      0.62     21625

[[7831 8634]
 [  95 5065]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/artist_normalized/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/artist_normalized/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.871, 'precision': 0.7241, 'recall': 0.7424, 'f1_score': 0.7331}
XGBoost: {'accuracy': 0.8748, 'precision': 0.8838, 'recall': 0.5471, 'f1_score': 0.6758}
Naive Bayes: {'accuracy': 0.5963, 'precision': 0.3697, 'recall': 0.9816, 'f1_score': 0.5371}
Random Forest: {'accuracy': 0.8428, 'precision': 0.9356, 'recall': 0.3663, 'f1_score': 0.5265}
SVM: {'accuracy': 0.4839, 'precision': 0.2312, 'recall': 0.5002, 'f1_score': 0.3163}
Decision Tree: {'accuracy': 0.7941, 'precision': 0.9585, 'recall': 0.1432, 'f1_score': 0.2492}

##################################################
Running experiment with TEMPO feature
[Tempo] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [16:20:28] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6928, Test Loss: 0.7072, F1: 0.3738, AUC: 0.5263
Epoch [10/30] Train Loss: 0.6891, Test Loss: 0.6868, F1: 0.3237, AUC: 0.5809
Epoch [20/30] Train Loss: 0.6865, Test Loss: 0.6964, F1: 0.3869, AUC: 0.5836
Mejores resultados en la época:  27
f1-score 0.3948931446017208
AUC según el mejor F1-score 0.5862297874043366
Confusion Matrix:
 [[7167 9298]
 [1603 3557]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Tempo/confusion_matrix_param_97.png
Accuracy:   0.4959
Precision:  0.2767
Recall:     0.6893
F1-score:   0.3949

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6924, Test Loss: 0.6878, F1: 0.3189, AUC: 0.5433
Epoch [10/30] Train Loss: 0.6894, Test Loss: 0.6761, F1: 0.3072, AUC: 0.5619
Epoch [20/30] Train Loss: 0.6879, Test Loss: 0.6937, F1: 0.3578, AUC: 0.5667
Mejores resultados en la época:  26
f1-score 0.39876305383757477
AUC según el mejor F1-score 0.5685262607786777
Confusion Matrix:
 [[ 5832 10633]
 [ 1227  3933]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Tempo/confusion_matrix_param_97.png
Accuracy:   0.4516
Precision:  0.2700
Recall:     0.7622
F1-score:   0.3988

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6924, Test Loss: 0.6939, F1: 0.3425, AUC: 0.5232
Epoch [10/30] Train Loss: 0.6890, Test Loss: 0.6981, F1: 0.3749, AUC: 0.5727
Epoch [20/30] Train Loss: 0.6864, Test Loss: 0.6806, F1: 0.3602, AUC: 0.5833
Mejores resultados en la época:  29
f1-score 0.3940074906367041
AUC según el mejor F1-score 0.5892447686777449
Confusion Matrix:
 [[7689 8776]
 [1741 3419]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Tempo/confusion_matrix_param_97.png
Accuracy:   0.5137
Precision:  0.2804
Recall:     0.6626
F1-score:   0.3940
Tiempo total para red 1: 106.03 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6928, Test Loss: 0.6859, F1: 0.0000, AUC: 0.5232
Epoch [10/30] Train Loss: 0.6824, Test Loss: 0.6888, F1: 0.3858, AUC: 0.5748
Epoch [20/30] Train Loss: 0.6818, Test Loss: 0.6785, F1: 0.3539, AUC: 0.5691
Mejores resultados en la época:  3
f1-score 0.4007492482870804
AUC según el mejor F1-score 0.575057639295946
Confusion Matrix:
 [[ 5403 11062]
 [ 1095  4065]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Tempo/confusion_matrix_param_701441.png
Accuracy:   0.4378
Precision:  0.2687
Recall:     0.7878
F1-score:   0.4007

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6932, Test Loss: 0.6874, F1: 0.0000, AUC: 0.5223
Epoch [10/30] Train Loss: 0.6815, Test Loss: 0.6601, F1: 0.3643, AUC: 0.5901
Epoch [20/30] Train Loss: 0.6800, Test Loss: 0.6761, F1: 0.3940, AUC: 0.5929
Mejores resultados en la época:  19
f1-score 0.40308880308880307
AUC según el mejor F1-score 0.5857143882842863
Confusion Matrix:
 [[7149 9316]
 [1506 3654]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Tempo/confusion_matrix_param_701441.png
Accuracy:   0.4996
Precision:  0.2817
Recall:     0.7081
F1-score:   0.4031

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6928, Test Loss: 0.7105, F1: 0.3439, AUC: 0.5555
Epoch [10/30] Train Loss: 0.6826, Test Loss: 0.6945, F1: 0.3838, AUC: 0.5762
Epoch [20/30] Train Loss: 0.6818, Test Loss: 0.6641, F1: 0.3633, AUC: 0.5766
Mejores resultados en la época:  24
f1-score 0.39384615384615385
AUC según el mejor F1-score 0.5802208348929018
Confusion Matrix:
 [[6748 9717]
 [1512 3648]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Tempo/confusion_matrix_param_701441.png
Accuracy:   0.4807
Precision:  0.2730
Recall:     0.7070
F1-score:   0.3938
Tiempo total para red 6: 170.41 segundos
Saved on: outputs_ablation_one_feature/0/Tempo

==============================
Model: Logistic Regression
Accuracy:  0.5292
Precision: 0.2583
Recall:    0.5200
F1-score:  0.3451
              precision    recall  f1-score   support

           0       0.78      0.53      0.63     16465
           1       0.26      0.52      0.35      5160

    accuracy                           0.53     21625
   macro avg       0.52      0.53      0.49     21625
weighted avg       0.66      0.53      0.56     21625

[[8760 7705]
 [2477 2683]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Tempo/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Tempo/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.5848
Precision: 0.2518
Recall:    0.3754
F1-score:  0.3014
              precision    recall  f1-score   support

           0       0.77      0.65      0.70     16465
           1       0.25      0.38      0.30      5160

    accuracy                           0.58     21625
   macro avg       0.51      0.51      0.50     21625
weighted avg       0.65      0.58      0.61     21625

[[10710  5755]
 [ 3223  1937]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Tempo/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Tempo/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.5961
Precision: 0.3088
Recall:    0.5595
F1-score:  0.3980
              precision    recall  f1-score   support

           0       0.81      0.61      0.70     16465
           1       0.31      0.56      0.40      5160

    accuracy                           0.60     21625
   macro avg       0.56      0.58      0.55     21625
weighted avg       0.69      0.60      0.62     21625

[[10004  6461]
 [ 2273  2887]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Tempo/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Tempo/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.5932
Precision: 0.3079
Recall:    0.5647
F1-score:  0.3985
              precision    recall  f1-score   support

           0       0.82      0.60      0.69     16465
           1       0.31      0.56      0.40      5160

    accuracy                           0.59     21625
   macro avg       0.56      0.58      0.55     21625
weighted avg       0.69      0.59      0.62     21625

[[9915 6550]
 [2246 2914]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Tempo/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Tempo/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.5851
Precision: 0.3063
Recall:    0.5843
F1-score:  0.4019
              precision    recall  f1-score   support

           0       0.82      0.59      0.68     16465
           1       0.31      0.58      0.40      5160

    accuracy                           0.59     21625
   macro avg       0.56      0.58      0.54     21625
weighted avg       0.70      0.59      0.62     21625
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(

[[9638 6827]
 [2145 3015]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Tempo/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Tempo/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6289
Precision: 0.2855
Recall:    0.3694
F1-score:  0.3220
              precision    recall  f1-score   support

           0       0.78      0.71      0.74     16465
           1       0.29      0.37      0.32      5160

    accuracy                           0.63     21625
   macro avg       0.53      0.54      0.53     21625
weighted avg       0.66      0.63      0.64     21625

[[11694  4771]
 [ 3254  1906]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Tempo/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Tempo/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.5851, 'precision': 0.3063, 'recall': 0.5843, 'f1_score': 0.4019}
Random Forest: {'accuracy': 0.5932, 'precision': 0.3079, 'recall': 0.5647, 'f1_score': 0.3985}
Decision Tree: {'accuracy': 0.5961, 'precision': 0.3088, 'recall': 0.5595, 'f1_score': 0.398}
Logistic Regression: {'accuracy': 0.5292, 'precision': 0.2583, 'recall': 0.52, 'f1_score': 0.3451}
Naive Bayes: {'accuracy': 0.6289, 'precision': 0.2855, 'recall': 0.3694, 'f1_score': 0.322}
SVM: {'accuracy': 0.5848, 'precision': 0.2518, 'recall': 0.3754, 'f1_score': 0.3014}

##################################################
Running experiment with LENGTH feature
[Length] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6918, Test Loss: 0.6875, F1: 0.3933, AUC: 0.6073
Epoch [10/30] Train Loss: 0.6734, Test Loss: 0.6723, F1: 0.3935, AUC: 0.6073
Epoch [20/30] Train Loss: 0.6721, Test Loss: 0.6862, F1: 0.3902, AUC: 0.6073
Mejores resultados en la época:  5
f1-score 0.3936463278823047
AUC según el mejor F1-score 0.6073307662248084
Confusion Matrix:
 [[7922 8543]
 [1802 3358]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Length/confusion_matrix_param_97.png
Accuracy:   0.5216
Precision:  0.2822
Recall:     0.6508
F1-score:   0.3936

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6922, Test Loss: 0.7027, F1: 0.3901, AUC: 0.6073
Epoch [10/30] Train Loss: 0.6739, Test Loss: 0.6511, F1: 0.3730, AUC: 0.6073
Epoch [20/30] Train Loss: 0.6723, Test Loss: 0.6642, F1: 0.3948, AUC: 0.6073
Mejores resultados en la época:  1
f1-score 0.3951253343901714
AUC según el mejor F1-score 0.6073307662248084
Confusion Matrix:
 [[ 5427 11038]
 [ 1172  3988]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Length/confusion_matrix_param_97.png
Accuracy:   0.4354
Precision:  0.2654
Recall:     0.7729
F1-score:   0.3951

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6911, Test Loss: 0.6846, F1: 0.3703, AUC: 0.6073
Epoch [10/30] Train Loss: 0.6739, Test Loss: 0.6447, F1: 0.3726, AUC: 0.6073
Epoch [20/30] Train Loss: 0.6723, Test Loss: 0.6581, F1: 0.3880, AUC: 0.6073
Mejores resultados en la época:  3
f1-score 0.3947795310426191
AUC según el mejor F1-score 0.6073307662248084
Confusion Matrix:
 [[10740  5725]
 [ 2483  2677]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Length/confusion_matrix_param_97.png
Accuracy:   0.6204
Precision:  0.3186
Recall:     0.5188
F1-score:   0.3948
Tiempo total para red 1: 105.84 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6899, Test Loss: 0.6909, F1: 0.3929, AUC: 0.6073
Epoch [10/30] Train Loss: 0.6719, Test Loss: 0.6655, F1: 0.3734, AUC: 0.6067
Epoch [20/30] Train Loss: 0.6711, Test Loss: 0.6817, F1: 0.3948, AUC: 0.6076
Mejores resultados en la época:  16
f1-score 0.3947795310426191
AUC según el mejor F1-score 0.6069106125984883
Confusion Matrix:
 [[10740  5725]
 [ 2483  2677]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Length/confusion_matrix_param_701441.png
Accuracy:   0.6204
Precision:  0.3186
Recall:     0.5188
F1-score:   0.3948

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6931, Test Loss: 0.6810, F1: 0.0000, AUC: 0.5000
Epoch [10/30] Train Loss: 0.6722, Test Loss: 0.6939, F1: 0.3929, AUC: 0.6073
Epoch [20/30] Train Loss: 0.6715, Test Loss: 0.6748, F1: 0.3948, AUC: 0.6078
Mejores resultados en la época:  19
f1-score 0.39483890214797135
AUC según el mejor F1-score 0.6067745829184292
Confusion Matrix:
 [[10864  5601]
 [ 2513  2647]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Length/confusion_matrix_param_701441.png
Accuracy:   0.6248
Precision:  0.3209
Recall:     0.5130
F1-score:   0.3948

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6855, Test Loss: 0.6640, F1: 0.3497, AUC: 0.6073
Epoch [10/30] Train Loss: 0.6718, Test Loss: 0.6532, F1: 0.3654, AUC: 0.6073
Epoch [20/30] Train Loss: 0.6705, Test Loss: 0.6704, F1: 0.3730, AUC: 0.6087
Mejores resultados en la época:  2
f1-score 0.39483890214797135
AUC según el mejor F1-score 0.6073308486171042
Confusion Matrix:
 [[10864  5601]
 [ 2513  2647]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Length/confusion_matrix_param_701441.png
Accuracy:   0.6248
Precision:  0.3209
Recall:     0.5130
F1-score:   0.3948
Tiempo total para red 6: 170.23 segundos
Saved on: outputs_ablation_one_feature/0/Length

==============================
Model: Logistic Regression
Accuracy:  0.5532
Precision: 0.2894
Recall:    0.5994
F1-score:  0.3903
              precision    recall  f1-score   support

           0       0.81      0.54      0.65     16465
           1       0.29      0.60      0.39      5160

    accuracy                           0.55     21625
   macro avg       0.55      0.57      0.52     21625
weighted avg       0.69      0.55      0.59     21625

[[8870 7595]
 [2067 3093]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Length/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Length/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.6065
Precision: 0.1801
Recall:    0.1828
F1-score:  0.1814
              precision    recall  f1-score   support

           0       0.74      0.74      0.74     16465
           1       0.18      0.18      0.18      5160

    accuracy                           0.61     21625
   macro avg       0.46      0.46      0.46     21625
weighted avg       0.61      0.61      0.61     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [16:25:13] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[12172  4293]
 [ 4217   943]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Length/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Length/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6470
Precision: 0.3401
Recall:    0.5097
F1-score:  0.4080
              precision    recall  f1-score   support

           0       0.82      0.69      0.75     16465
           1       0.34      0.51      0.41      5160

    accuracy                           0.65     21625
   macro avg       0.58      0.60      0.58     21625
weighted avg       0.70      0.65      0.67     21625

[[11362  5103]
 [ 2530  2630]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Length/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Length/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.6295
Precision: 0.3364
Recall:    0.5682
F1-score:  0.4226
              precision    recall  f1-score   support

           0       0.83      0.65      0.73     16465
           1       0.34      0.57      0.42      5160

    accuracy                           0.63     21625
   macro avg       0.58      0.61      0.57     21625
weighted avg       0.71      0.63      0.65     21625

[[10680  5785]
 [ 2228  2932]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Length/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Length/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.6283
Precision: 0.3344
Recall:    0.5628
F1-score:  0.4195
              precision    recall  f1-score   support

           0       0.83      0.65      0.73     16465
           1       0.33      0.56      0.42      5160

    accuracy                           0.63     21625
   macro avg       0.58      0.61      0.57     21625
weighted avg       0.71      0.63      0.65     21625

[[10684  5781]
 [ 2256  2904]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Length/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Length/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.4646
Precision: 0.2688
Recall:    0.7231
F1-score:  0.3919
              precision    recall  f1-score   support

           0       0.82      0.38      0.52     16465
           1       0.27      0.72      0.39      5160

    accuracy                           0.46     21625
   macro avg       0.54      0.55      0.46     21625
weighted avg       0.69      0.46      0.49     21625

[[ 6317 10148]
 [ 1429  3731]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Length/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Length/naive_bayes_model.pkl


Resumen de métricas:
Random Forest: {'accuracy': 0.6295, 'precision': 0.3364, 'recall': 0.5682, 'f1_score': 0.4226}
XGBoost: {'accuracy': 0.6283, 'precision': 0.3344, 'recall': 0.5628, 'f1_score': 0.4195}
Decision Tree: {'accuracy': 0.647, 'precision': 0.3401, 'recall': 0.5097, 'f1_score': 0.408}
Naive Bayes: {'accuracy': 0.4646, 'precision': 0.2688, 'recall': 0.7231, 'f1_score': 0.3919}
Logistic Regression: {'accuracy': 0.5532, 'precision': 0.2894, 'recall': 0.5994, 'f1_score': 0.3903}
SVM: {'accuracy': 0.6065, 'precision': 0.1801, 'recall': 0.1828, 'f1_score': 0.1814}

##################################################
Running experiment with LOUDNESS (DB) feature
[Loudness (db)] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6885, Test Loss: 0.7066, F1: 0.4150, AUC: 0.5747
Epoch [10/30] Train Loss: 0.6723, Test Loss: 0.6725, F1: 0.4173, AUC: 0.5747
Epoch [20/30] Train Loss: 0.6717, Test Loss: 0.6826, F1: 0.4166, AUC: 0.5747
Mejores resultados en la época:  9
f1-score 0.4178160067230214
AUC según el mejor F1-score 0.5746504624561849
Confusion Matrix:
 [[ 5622 10843]
 [  934  4226]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Loudness (db)/confusion_matrix_param_97.png
Accuracy:   0.4554
Precision:  0.2804
Recall:     0.8190
F1-score:   0.4178

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6890, Test Loss: 0.6879, F1: 0.4019, AUC: 0.5747
Epoch [10/30] Train Loss: 0.6727, Test Loss: 0.6779, F1: 0.4151, AUC: 0.5747
Epoch [20/30] Train Loss: 0.6720, Test Loss: 0.6600, F1: 0.4137, AUC: 0.5747
Mejores resultados en la época:  4
f1-score 0.4173644338118022
AUC según el mejor F1-score 0.5746504624561849
Confusion Matrix:
 [[ 5748 10717]
 [  973  4187]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Loudness (db)/confusion_matrix_param_97.png
Accuracy:   0.4594
Precision:  0.2809
Recall:     0.8114
F1-score:   0.4174

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6900, Test Loss: 0.6810, F1: 0.3617, AUC: 0.5747
Epoch [10/30] Train Loss: 0.6727, Test Loss: 0.6608, F1: 0.3979, AUC: 0.5747
Epoch [20/30] Train Loss: 0.6719, Test Loss: 0.6827, F1: 0.4161, AUC: 0.5747
Mejores resultados en la época:  8
f1-score 0.4176626493974096
AUC según el mejor F1-score 0.5746504624561849
Confusion Matrix:
 [[ 5804 10661]
 [  984  4176]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Loudness (db)/confusion_matrix_param_97.png
Accuracy:   0.4615
Precision:  0.2815
Recall:     0.8093
F1-score:   0.4177
Tiempo total para red 1: 105.92 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6790, Test Loss: 0.6682, F1: 0.4161, AUC: 0.5747
Epoch [10/30] Train Loss: 0.6718, Test Loss: 0.6621, F1: 0.4163, AUC: 0.6025
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [16:30:02] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [20/30] Train Loss: 0.6710, Test Loss: 0.6628, F1: 0.4149, AUC: 0.6063
Mejores resultados en la época:  18
f1-score 0.4179695230556105
AUC según el mejor F1-score 0.6037650807326793
Confusion Matrix:
 [[ 5637 10828]
 [  936  4224]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Loudness (db)/confusion_matrix_param_701441.png
Accuracy:   0.4560
Precision:  0.2806
Recall:     0.8186
F1-score:   0.4180

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6831, Test Loss: 0.6732, F1: 0.4160, AUC: 0.5747
Epoch [10/30] Train Loss: 0.6716, Test Loss: 0.6860, F1: 0.4160, AUC: 0.5947
Epoch [20/30] Train Loss: 0.6710, Test Loss: 0.6961, F1: 0.4180, AUC: 0.6068
Mejores resultados en la época:  20
f1-score 0.4179635393508226
AUC según el mejor F1-score 0.6067546734087106
Confusion Matrix:
 [[ 5614 10851]
 [  930  4230]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Loudness (db)/confusion_matrix_param_701441.png
Accuracy:   0.4552
Precision:  0.2805
Recall:     0.8198
F1-score:   0.4180

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6773, Test Loss: 0.6743, F1: 0.4180, AUC: 0.5747
Epoch [10/30] Train Loss: 0.6715, Test Loss: 0.6864, F1: 0.4161, AUC: 0.6036
Epoch [20/30] Train Loss: 0.6706, Test Loss: 0.6765, F1: 0.4166, AUC: 0.6058
Mejores resultados en la época:  0
f1-score 0.4179635393508226
AUC según el mejor F1-score 0.5746504565710209
Confusion Matrix:
 [[ 5614 10851]
 [  930  4230]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Loudness (db)/confusion_matrix_param_701441.png
Accuracy:   0.4552
Precision:  0.2805
Recall:     0.8198
F1-score:   0.4180
Tiempo total para red 6: 170.34 segundos
Saved on: outputs_ablation_one_feature/0/Loudness (db)

==============================
Model: Logistic Regression
Accuracy:  0.5150
Precision: 0.2808
Recall:    0.6614
F1-score:  0.3942
              precision    recall  f1-score   support

           0       0.82      0.47      0.60     16465
           1       0.28      0.66      0.39      5160

    accuracy                           0.51     21625
   macro avg       0.55      0.57      0.49     21625
weighted avg       0.69      0.51      0.55     21625

[[7723 8742]
 [1747 3413]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Loudness (db)/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Loudness (db)/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.4718
Precision: 0.2283
Recall:    0.5101
F1-score:  0.3155
              precision    recall  f1-score   support

           0       0.75      0.46      0.57     16465
           1       0.23      0.51      0.32      5160

    accuracy                           0.47     21625
   macro avg       0.49      0.48      0.44     21625
weighted avg       0.63      0.47      0.51     21625

[[7570 8895]
 [2528 2632]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Loudness (db)/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Loudness (db)/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.5778
Precision: 0.3137
Recall:    0.6475
F1-score:  0.4226
              precision    recall  f1-score   support

           0       0.83      0.56      0.67     16465
           1       0.31      0.65      0.42      5160

    accuracy                           0.58     21625
   macro avg       0.57      0.60      0.54     21625
weighted avg       0.71      0.58      0.61     21625

[[9154 7311]
 [1819 3341]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Loudness (db)/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Loudness (db)/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.6053
Precision: 0.3359
Recall:    0.6694
F1-score:  0.4473
              precision    recall  f1-score   support

           0       0.85      0.59      0.69     16465
           1       0.34      0.67      0.45      5160

    accuracy                           0.61     21625
   macro avg       0.59      0.63      0.57     21625
weighted avg       0.73      0.61      0.63     21625

[[9635 6830]
 [1706 3454]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Loudness (db)/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Loudness (db)/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.5785
Precision: 0.3127
Recall:    0.6395
F1-score:  0.4200
              precision    recall  f1-score   support

           0       0.83      0.56      0.67     16465
           1       0.31      0.64      0.42      5160

    accuracy                           0.58     21625
   macro avg       0.57      0.60      0.54     21625
weighted avg       0.71      0.58      0.61     21625

[[9211 7254]
 [1860 3300]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Loudness (db)/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Loudness (db)/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.4188
Precision: 0.2723
Recall:    0.8585
F1-score:  0.4135
              precision    recall  f1-score   support

           0       0.86      0.28      0.42     16465
           1       0.27      0.86      0.41      5160

    accuracy                           0.42     21625
   macro avg       0.57      0.57      0.42     21625
weighted avg       0.72      0.42      0.42     21625

[[ 4627 11838]
 [  730  4430]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Loudness (db)/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Loudness (db)/naive_bayes_model.pkl


Resumen de métricas:
Random Forest: {'accuracy': 0.6053, 'precision': 0.3359, 'recall': 0.6694, 'f1_score': 0.4473}
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Decision Tree: {'accuracy': 0.5778, 'precision': 0.3137, 'recall': 0.6475, 'f1_score': 0.4226}
XGBoost: {'accuracy': 0.5785, 'precision': 0.3127, 'recall': 0.6395, 'f1_score': 0.42}
Naive Bayes: {'accuracy': 0.4188, 'precision': 0.2723, 'recall': 0.8585, 'f1_score': 0.4135}
Logistic Regression: {'accuracy': 0.515, 'precision': 0.2808, 'recall': 0.6614, 'f1_score': 0.3942}
SVM: {'accuracy': 0.4718, 'precision': 0.2283, 'recall': 0.5101, 'f1_score': 0.3155}

##################################################
Running experiment with POPULARITY feature
[Popularity] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6880, Test Loss: 0.7022, F1: 0.3667, AUC: 0.5650
Epoch [10/30] Train Loss: 0.6860, Test Loss: 0.6979, F1: 0.3580, AUC: 0.5650
Epoch [20/30] Train Loss: 0.6860, Test Loss: 0.6777, F1: 0.3463, AUC: 0.5626
Mejores resultados en la época:  0
f1-score 0.36669585887651696
AUC según el mejor F1-score 0.5649875352227064
Confusion Matrix:
 [[8570 7895]
 [2229 2931]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Popularity/confusion_matrix_param_97.png
Accuracy:   0.5318
Precision:  0.2707
Recall:     0.5680
F1-score:   0.3667

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6901, Test Loss: 0.6944, F1: 0.3646, AUC: 0.5650
Epoch [10/30] Train Loss: 0.6861, Test Loss: 0.6948, F1: 0.3573, AUC: 0.5650
Epoch [20/30] Train Loss: 0.6860, Test Loss: 0.6755, F1: 0.3402, AUC: 0.5676
Mejores resultados en la época:  0
f1-score 0.36455033741908827
AUC según el mejor F1-score 0.5649875352227064
Confusion Matrix:
 [[9750 6715]
 [2513 2647]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Popularity/confusion_matrix_param_97.png
Accuracy:   0.5733
Precision:  0.2827
Recall:     0.5130
F1-score:   0.3646

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6879, Test Loss: 0.6827, F1: 0.3573, AUC: 0.5650
Epoch [10/30] Train Loss: 0.6860, Test Loss: 0.6944, F1: 0.3573, AUC: 0.5650
Epoch [20/30] Train Loss: 0.6859, Test Loss: 0.6874, F1: 0.3531, AUC: 0.5626
Mejores resultados en la época:  0
f1-score 0.3572866656341954
AUC según el mejor F1-score 0.5649875352227064
Confusion Matrix:
 [[11018  5447]
 [ 2853  2307]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Popularity/confusion_matrix_param_97.png
Accuracy:   0.6162
Precision:  0.2975
Recall:     0.4471
F1-score:   0.3573
Tiempo total para red 1: 105.91 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6879, Test Loss: 0.6934, F1: 0.3625, AUC: 0.5664
Epoch [10/30] Train Loss: 0.6862, Test Loss: 0.7078, F1: 0.3646, AUC: 0.5650
Epoch [20/30] Train Loss: 0.6862, Test Loss: 0.6883, F1: 0.3531, AUC: 0.5616
Mejores resultados en la época:  1
f1-score 0.36455033741908827
AUC según el mejor F1-score 0.5649867112997502
Confusion Matrix:
 [[9750 6715]
 [2513 2647]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Popularity/confusion_matrix_param_701441.png
Accuracy:   0.5733
Precision:  0.2827
Recall:     0.5130
F1-score:   0.3646

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6877, Test Loss: 0.7044, F1: 0.3580, AUC: 0.5646
Epoch [10/30] Train Loss: 0.6861, Test Loss: 0.7006, F1: 0.3686, AUC: 0.5654
Epoch [20/30] Train Loss: 0.6859, Test Loss: 0.6822, F1: 0.3463, AUC: 0.5641
Mejores resultados en la época:  10
f1-score 0.3686429512516469
AUC según el mejor F1-score 0.5653635324637414
Confusion Matrix:
 [[9243 7222]
 [2362 2798]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Popularity/confusion_matrix_param_701441.png
Accuracy:   0.5568
Precision:  0.2792
Recall:     0.5422
F1-score:   0.3686

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6883, Test Loss: 0.6584, F1: 0.3345, AUC: 0.5650
Epoch [10/30] Train Loss: 0.6860, Test Loss: 0.6675, F1: 0.3345, AUC: 0.5650
Epoch [20/30] Train Loss: 0.6859, Test Loss: 0.6851, F1: 0.3531, AUC: 0.5658
Mejores resultados en la época:  1
f1-score 0.37774862240881657
AUC según el mejor F1-score 0.5649875352227064
Confusion Matrix:
 [[ 6169 10296]
 [ 1561  3599]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Popularity/confusion_matrix_param_701441.png
Accuracy:   0.4517
Precision:  0.2590
Recall:     0.6975
F1-score:   0.3777
Tiempo total para red 6: 170.14 segundos
Saved on: outputs_ablation_one_feature/0/Popularity

==============================
Model: Logistic Regression
Accuracy:  0.6012
Precision: 0.2907
Recall:    0.4659
F1-score:  0.3580
              precision    recall  f1-score   support

           0       0.79      0.64      0.71     16465
           1       0.29      0.47      0.36      5160

    accuracy                           0.60     21625
   macro avg       0.54      0.55      0.53     21625
weighted avg       0.67      0.60      0.63     21625

[[10598  5867]
 [ 2756  2404]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Popularity/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Popularity/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.4862
Precision: 0.2324
Recall:    0.5008
F1-score:  0.3175
              precision    recall  f1-score   support

           0       0.75      0.48      0.59     16465
           1       0.23      0.50      0.32      5160

    accuracy                           0.49     21625
   macro avg       0.49      0.49      0.45     21625
weighted avg       0.63      0.49      0.52     21625

[[7931 8534]
 [2576 2584]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Popularity/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Popularity/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6302
Precision: 0.3023
Recall:    0.4205
F1-score:  0.3518
              precision    recall  f1-score   support

           0       0.79      0.70      0.74     16465
           1       0.30      0.42      0.35      5160

    accuracy                           0.63     21625
   macro avg       0.55      0.56      0.55     21625
weighted avg       0.68      0.63      0.65     21625

[[11457  5008]
 [ 2990  2170]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Popularity/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Popularity/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.6302
Precision: 0.3023
Recall:    0.4205
F1-score:  0.3518
              precision    recall  f1-score   support

           0       0.79      0.70      0.74     16465
           1       0.30      0.42      0.35      5160

    accuracy                           0.63     21625
   macro avg       0.55      0.56      0.55     21625
weighted avg       0.68      0.63      0.65     21625

[[11457  5008]
 [ 2990  2170]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [16:34:45] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Popularity/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Popularity/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.6301
Precision: 0.3022
Recall:    0.4203
F1-score:  0.3516
              precision    recall  f1-score   support

           0       0.79      0.70      0.74     16465
           1       0.30      0.42      0.35      5160

    accuracy                           0.63     21625
   macro avg       0.55      0.56      0.55     21625
weighted avg       0.68      0.63      0.65     21625

[[11457  5008]
 [ 2991  2169]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Popularity/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Popularity/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6466
Precision: 0.3068
Recall:    0.3818
F1-score:  0.3402
              precision    recall  f1-score   support

           0       0.79      0.73      0.76     16465
           1       0.31      0.38      0.34      5160

    accuracy                           0.65     21625
   macro avg       0.55      0.56      0.55     21625
weighted avg       0.67      0.65      0.66     21625

[[12013  4452]
 [ 3190  1970]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Popularity/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Popularity/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.6012, 'precision': 0.2907, 'recall': 0.4659, 'f1_score': 0.358}
Decision Tree: {'accuracy': 0.6302, 'precision': 0.3023, 'recall': 0.4205, 'f1_score': 0.3518}
Random Forest: {'accuracy': 0.6302, 'precision': 0.3023, 'recall': 0.4205, 'f1_score': 0.3518}
XGBoost: {'accuracy': 0.6301, 'precision': 0.3022, 'recall': 0.4203, 'f1_score': 0.3516}
Naive Bayes: {'accuracy': 0.6466, 'precision': 0.3068, 'recall': 0.3818, 'f1_score': 0.3402}
SVM: {'accuracy': 0.4862, 'precision': 0.2324, 'recall': 0.5008, 'f1_score': 0.3175}

##################################################
Running experiment with ENERGY feature
[Energy] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6879, Test Loss: 0.7013, F1: 0.4107, AUC: 0.5387
Epoch [10/30] Train Loss: 0.6669, Test Loss: 0.6801, F1: 0.4133, AUC: 0.6132
Epoch [20/30] Train Loss: 0.6655, Test Loss: 0.6640, F1: 0.4041, AUC: 0.6149
Mejores resultados en la época:  12
f1-score 0.41776027996500437
AUC según el mejor F1-score 0.6128030977149084
Confusion Matrix:
 [[7157 9308]
 [1340 3820]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Energy/confusion_matrix_param_97.png
Accuracy:   0.5076
Precision:  0.2910
Recall:     0.7403
F1-score:   0.4178

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6848, Test Loss: 0.6778, F1: 0.4037, AUC: 0.6016
Epoch [10/30] Train Loss: 0.6654, Test Loss: 0.6826, F1: 0.4178, AUC: 0.6154
Epoch [20/30] Train Loss: 0.6651, Test Loss: 0.6613, F1: 0.4053, AUC: 0.6157
Mejores resultados en la época:  9
f1-score 0.41776027996500437
AUC según el mejor F1-score 0.6151700282723276
Confusion Matrix:
 [[7157 9308]
 [1340 3820]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Energy/confusion_matrix_param_97.png
Accuracy:   0.5076
Precision:  0.2910
Recall:     0.7403
F1-score:   0.4178

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6892, Test Loss: 0.6908, F1: 0.4037, AUC: 0.5387
Epoch [10/30] Train Loss: 0.6654, Test Loss: 0.6771, F1: 0.4125, AUC: 0.6134
Epoch [20/30] Train Loss: 0.6652, Test Loss: 0.6657, F1: 0.4110, AUC: 0.6156
Mejores resultados en la época:  24
f1-score 0.41776027996500437
AUC según el mejor F1-score 0.614918002010372
Confusion Matrix:
 [[7157 9308]
 [1340 3820]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Energy/confusion_matrix_param_97.png
Accuracy:   0.5076
Precision:  0.2910
Recall:     0.7403
F1-score:   0.4178
Tiempo total para red 1: 105.98 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6749, Test Loss: 0.7494, F1: 0.4105, AUC: 0.6093
Epoch [10/30] Train Loss: 0.6659, Test Loss: 0.6617, F1: 0.4155, AUC: 0.6161
Epoch [20/30] Train Loss: 0.6651, Test Loss: 0.6905, F1: 0.4182, AUC: 0.6149
Mejores resultados en la época:  20
f1-score 0.4182286667003486
AUC según el mejor F1-score 0.6149284776022429
Confusion Matrix:
 [[ 5971 10494]
 [ 1021  4139]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Energy/confusion_matrix_param_701441.png
Accuracy:   0.4675
Precision:  0.2829
Recall:     0.8021
F1-score:   0.4182

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6732, Test Loss: 0.6628, F1: 0.4060, AUC: 0.6100
Epoch [10/30] Train Loss: 0.6656, Test Loss: 0.6837, F1: 0.4169, AUC: 0.6160
Epoch [20/30] Train Loss: 0.6649, Test Loss: 0.6536, F1: 0.4159, AUC: 0.6165
Mejores resultados en la época:  16
f1-score 0.41776027996500437
AUC según el mejor F1-score 0.6153687584893489
Confusion Matrix:
 [[7157 9308]
 [1340 3820]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Energy/confusion_matrix_param_701441.png
Accuracy:   0.5076
Precision:  0.2910
Recall:     0.7403
F1-score:   0.4178

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6747, Test Loss: 0.6771, F1: 0.3938, AUC: 0.6004
Epoch [10/30] Train Loss: 0.6653, Test Loss: 0.6862, F1: 0.4148, AUC: 0.6155
Epoch [20/30] Train Loss: 0.6653, Test Loss: 0.6677, F1: 0.4166, AUC: 0.6156
Mejores resultados en la época:  15
f1-score 0.4180218608371101
AUC según el mejor F1-score 0.6155729854495204
Confusion Matrix:
 [[6790 9675]
 [1240 3920]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Energy/confusion_matrix_param_701441.png
Accuracy:   0.4953
Precision:  0.2883
Recall:     0.7597
F1-score:   0.4180
Tiempo total para red 6: 170.74 segundos
Saved on: outputs_ablation_one_feature/0/Energy

==============================
Model: Logistic Regression
Accuracy:  0.4998
Precision: 0.2541
Recall:    0.5665
F1-score:  0.3509
              precision    recall  f1-score   support

           0       0.78      0.48      0.59     16465
           1       0.25      0.57      0.35      5160

    accuracy                           0.50     21625
   macro avg       0.52      0.52      0.47     21625
weighted avg       0.65      0.50      0.54     21625

[[7886 8579]
 [2237 2923]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [16:39:30] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Energy/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Energy/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.6189
Precision: 0.1790
Recall:    0.1665
F1-score:  0.1725
              precision    recall  f1-score   support

           0       0.74      0.76      0.75     16465
           1       0.18      0.17      0.17      5160

    accuracy                           0.62     21625
   macro avg       0.46      0.46      0.46     21625
weighted avg       0.61      0.62      0.61     21625

[[12525  3940]
 [ 4301   859]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Energy/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Energy/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.5676
Precision: 0.3089
Recall:    0.6562
F1-score:  0.4200
              precision    recall  f1-score   support

           0       0.83      0.54      0.66     16465
           1       0.31      0.66      0.42      5160

    accuracy                           0.57     21625
   macro avg       0.57      0.60      0.54     21625
weighted avg       0.71      0.57      0.60     21625

[[8889 7576]
 [1774 3386]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Energy/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Energy/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.5676
Precision: 0.3089
Recall:    0.6562
F1-score:  0.4200
              precision    recall  f1-score   support

           0       0.83      0.54      0.66     16465
           1       0.31      0.66      0.42      5160

    accuracy                           0.57     21625
   macro avg       0.57      0.60      0.54     21625
weighted avg       0.71      0.57      0.60     21625

[[8889 7576]
 [1774 3386]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Energy/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Energy/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.5751
Precision: 0.3118
Recall:    0.6467
F1-score:  0.4208
              precision    recall  f1-score   support

           0       0.83      0.55      0.66     16465
           1       0.31      0.65      0.42      5160

    accuracy                           0.58     21625
   macro avg       0.57      0.60      0.54     21625
weighted avg       0.71      0.58      0.61     21625

[[9100 7365]
 [1823 3337]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Energy/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Energy/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.4839
Precision: 0.2830
Recall:    0.7583
F1-score:  0.4122
              precision    recall  f1-score   support

           0       0.84      0.40      0.54     16465
           1       0.28      0.76      0.41      5160

    accuracy                           0.48     21625
   macro avg       0.56      0.58      0.48     21625
weighted avg       0.71      0.48      0.51     21625

[[6552 9913]
 [1247 3913]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Energy/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Energy/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.5751, 'precision': 0.3118, 'recall': 0.6467, 'f1_score': 0.4208}
Decision Tree: {'accuracy': 0.5676, 'precision': 0.3089, 'recall': 0.6562, 'f1_score': 0.42}
Random Forest: {'accuracy': 0.5676, 'precision': 0.3089, 'recall': 0.6562, 'f1_score': 0.42}
Naive Bayes: {'accuracy': 0.4839, 'precision': 0.283, 'recall': 0.7583, 'f1_score': 0.4122}
Logistic Regression: {'accuracy': 0.4998, 'precision': 0.2541, 'recall': 0.5665, 'f1_score': 0.3509}
SVM: {'accuracy': 0.6189, 'precision': 0.179, 'recall': 0.1665, 'f1_score': 0.1725}

##################################################
Running experiment with DANCEABILITY feature
[Danceability] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6732, Test Loss: 0.6761, F1: 0.4397, AUC: 0.6679
Epoch [10/30] Train Loss: 0.6462, Test Loss: 0.6567, F1: 0.4421, AUC: 0.6679
Epoch [20/30] Train Loss: 0.6460, Test Loss: 0.6640, F1: 0.4421, AUC: 0.6679
Mejores resultados en la época:  4
f1-score 0.4441869974103857
AUC según el mejor F1-score 0.6678505497920183
Confusion Matrix:
 [[10210  6255]
 [ 1901  3259]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Danceability/confusion_matrix_param_97.png
Accuracy:   0.6228
Precision:  0.3425
Recall:     0.6316
F1-score:   0.4442

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6624, Test Loss: 0.6604, F1: 0.4428, AUC: 0.6679
Epoch [10/30] Train Loss: 0.6464, Test Loss: 0.6350, F1: 0.4433, AUC: 0.6679
Epoch [20/30] Train Loss: 0.6460, Test Loss: 0.6555, F1: 0.4421, AUC: 0.6679
Mejores resultados en la época:  1
f1-score 0.4441869974103857
AUC según el mejor F1-score 0.6678505497920183
Confusion Matrix:
 [[10210  6255]
 [ 1901  3259]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Danceability/confusion_matrix_param_97.png
Accuracy:   0.6228
Precision:  0.3425
Recall:     0.6316
F1-score:   0.4442

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6650, Test Loss: 0.6355, F1: 0.4389, AUC: 0.6679
Epoch [10/30] Train Loss: 0.6462, Test Loss: 0.6533, F1: 0.4421, AUC: 0.6679
Epoch [20/30] Train Loss: 0.6458, Test Loss: 0.6340, F1: 0.4389, AUC: 0.6679
Mejores resultados en la época:  1
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [16:44:14] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
f1-score 0.4441869974103857
AUC según el mejor F1-score 0.6678505497920183
Confusion Matrix:
 [[10210  6255]
 [ 1901  3259]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Danceability/confusion_matrix_param_97.png
Accuracy:   0.6228
Precision:  0.3425
Recall:     0.6316
F1-score:   0.4442
Tiempo total para red 1: 106.04 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6542, Test Loss: 0.6538, F1: 0.4433, AUC: 0.6679
Epoch [10/30] Train Loss: 0.6469, Test Loss: 0.6229, F1: 0.4341, AUC: 0.6678
Epoch [20/30] Train Loss: 0.6462, Test Loss: 0.6296, F1: 0.4389, AUC: 0.6682
Mejores resultados en la época:  4
f1-score 0.4441869974103857
AUC según el mejor F1-score 0.6678509499831684
Confusion Matrix:
 [[10210  6255]
 [ 1901  3259]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Danceability/confusion_matrix_param_701441.png
Accuracy:   0.6228
Precision:  0.3425
Recall:     0.6316
F1-score:   0.4442

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6520, Test Loss: 0.6528, F1: 0.4389, AUC: 0.6678
Epoch [10/30] Train Loss: 0.6469, Test Loss: 0.6133, F1: 0.4341, AUC: 0.6679
Epoch [20/30] Train Loss: 0.6463, Test Loss: 0.6707, F1: 0.4428, AUC: 0.6679
Mejores resultados en la época:  1
f1-score 0.44329896907216493
AUC según el mejor F1-score 0.667851250126531
Confusion Matrix:
 [[10602  5863]
 [ 2021  3139]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Danceability/confusion_matrix_param_701441.png
Accuracy:   0.6354
Precision:  0.3487
Recall:     0.6083
F1-score:   0.4433

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6520, Test Loss: 0.6332, F1: 0.4373, AUC: 0.6679
Epoch [10/30] Train Loss: 0.6468, Test Loss: 0.6476, F1: 0.4389, AUC: 0.6679
Epoch [20/30] Train Loss: 0.6461, Test Loss: 0.6582, F1: 0.4433, AUC: 0.6690
Mejores resultados en la época:  15
f1-score 0.4441869974103857
AUC según el mejor F1-score 0.6678513089781708
Confusion Matrix:
 [[10210  6255]
 [ 1901  3259]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Danceability/confusion_matrix_param_701441.png
Accuracy:   0.6228
Precision:  0.3425
Recall:     0.6316
F1-score:   0.4442
Tiempo total para red 6: 170.53 segundos
Saved on: outputs_ablation_one_feature/0/Danceability

==============================
Model: Logistic Regression
Accuracy:  0.6112
Precision: 0.3361
Recall:    0.6455
F1-score:  0.4421
              precision    recall  f1-score   support

           0       0.84      0.60      0.70     16465
           1       0.34      0.65      0.44      5160

    accuracy                           0.61     21625
   macro avg       0.59      0.62      0.57     21625
weighted avg       0.72      0.61      0.64     21625

[[9886 6579]
 [1829 3331]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Danceability/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Danceability/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.4773
Precision: 0.1559
Recall:    0.2696
F1-score:  0.1975
              precision    recall  f1-score   support

           0       0.70      0.54      0.61     16465
           1       0.16      0.27      0.20      5160

    accuracy                           0.48     21625
   macro avg       0.43      0.41      0.40     21625
weighted avg       0.57      0.48      0.51     21625

[[8931 7534]
 [3769 1391]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Danceability/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Danceability/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6401
Precision: 0.3530
Recall:    0.6099
F1-score:  0.4471
              precision    recall  f1-score   support

           0       0.84      0.65      0.73     16465
           1       0.35      0.61      0.45      5160

    accuracy                           0.64     21625
   macro avg       0.60      0.63      0.59     21625
weighted avg       0.73      0.64      0.66     21625

[[10696  5769]
 [ 2013  3147]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Danceability/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Danceability/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.6401
Precision: 0.3530
Recall:    0.6099
F1-score:  0.4471
              precision    recall  f1-score   support

           0       0.84      0.65      0.73     16465
           1       0.35      0.61      0.45      5160

    accuracy                           0.64     21625
   macro avg       0.60      0.63      0.59     21625
weighted avg       0.73      0.64      0.66     21625

[[10696  5769]
 [ 2013  3147]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Danceability/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Danceability/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.6401
Precision: 0.3530
Recall:    0.6099
F1-score:  0.4471
              precision    recall  f1-score   support

           0       0.84      0.65      0.73     16465
           1       0.35      0.61      0.45      5160

    accuracy                           0.64     21625
   macro avg       0.60      0.63      0.59     21625
weighted avg       0.73      0.64      0.66     21625

[[10696  5769]
 [ 2013  3147]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Danceability/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Danceability/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6003
Precision: 0.3317
Recall:    0.6655
F1-score:  0.4428
              precision    recall  f1-score   support

           0       0.85      0.58      0.69     16465
           1       0.33      0.67      0.44      5160

    accuracy                           0.60     21625
   macro avg       0.59      0.62      0.57     21625
weighted avg       0.72      0.60      0.63     21625
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(

[[9547 6918]
 [1726 3434]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Danceability/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Danceability/naive_bayes_model.pkl


Resumen de métricas:
Decision Tree: {'accuracy': 0.6401, 'precision': 0.353, 'recall': 0.6099, 'f1_score': 0.4471}
Random Forest: {'accuracy': 0.6401, 'precision': 0.353, 'recall': 0.6099, 'f1_score': 0.4471}
XGBoost: {'accuracy': 0.6401, 'precision': 0.353, 'recall': 0.6099, 'f1_score': 0.4471}
Naive Bayes: {'accuracy': 0.6003, 'precision': 0.3317, 'recall': 0.6655, 'f1_score': 0.4428}
Logistic Regression: {'accuracy': 0.6112, 'precision': 0.3361, 'recall': 0.6455, 'f1_score': 0.4421}
SVM: {'accuracy': 0.4773, 'precision': 0.1559, 'recall': 0.2696, 'f1_score': 0.1975}

##################################################
Running experiment with POSITIVENESS feature
[Positiveness] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6946, Test Loss: 0.6996, F1: 0.3721, AUC: 0.5220
Epoch [10/30] Train Loss: 0.6884, Test Loss: 0.6806, F1: 0.3721, AUC: 0.5421
Epoch [20/30] Train Loss: 0.6879, Test Loss: 0.6800, F1: 0.3732, AUC: 0.5444
Mejores resultados en la época:  25
f1-score 0.384103562214276
AUC según el mejor F1-score 0.5457859459930272
Confusion Matrix:
 [[ 3775 12690]
 [  917  4243]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Positiveness/confusion_matrix_param_97.png
Accuracy:   0.3708
Precision:  0.2506
Recall:     0.8223
F1-score:   0.3841

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6921, Test Loss: 0.6911, F1: 0.3628, AUC: 0.5220
Epoch [10/30] Train Loss: 0.6881, Test Loss: 0.6972, F1: 0.3768, AUC: 0.5445
Epoch [20/30] Train Loss: 0.6880, Test Loss: 0.6890, F1: 0.3743, AUC: 0.5467
Mejores resultados en la época:  24
f1-score 0.38771373171161644
AUC según el mejor F1-score 0.5460994015965273
Confusion Matrix:
 [[ 3332 13133]
 [  761  4399]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Positiveness/confusion_matrix_param_97.png
Accuracy:   0.3575
Precision:  0.2509
Recall:     0.8525
F1-score:   0.3877

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6932, Test Loss: 0.6912, F1: 0.3520, AUC: 0.5220
Epoch [10/30] Train Loss: 0.6883, Test Loss: 0.6878, F1: 0.3655, AUC: 0.5448
Epoch [20/30] Train Loss: 0.6879, Test Loss: 0.6967, F1: 0.3811, AUC: 0.5457
Mejores resultados en la época:  14
f1-score 0.38645454545454544
AUC según el mejor F1-score 0.5466342217576866
Confusion Matrix:
 [[ 3876 12589]
 [  909  4251]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Positiveness/confusion_matrix_param_97.png
Accuracy:   0.3758
Precision:  0.2524
Recall:     0.8238
F1-score:   0.3865
Tiempo total para red 1: 106.06 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6902, Test Loss: 0.6861, F1: 0.3795, AUC: 0.5406
Epoch [10/30] Train Loss: 0.6861, Test Loss: 0.6927, F1: 0.3863, AUC: 0.5449
Epoch [20/30] Train Loss: 0.6856, Test Loss: 0.6928, F1: 0.3790, AUC: 0.5448
Mejores resultados en la época:  5
f1-score 0.386586016723003
AUC según el mejor F1-score 0.5461459002770734
Confusion Matrix:
 [[ 3487 12978]
 [  814  4346]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Positiveness/confusion_matrix_param_701441.png
Accuracy:   0.3622
Precision:  0.2509
Recall:     0.8422
F1-score:   0.3866

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6909, Test Loss: 0.6795, F1: 0.3697, AUC: 0.5220
Epoch [10/30] Train Loss: 0.6860, Test Loss: 0.6753, F1: 0.3579, AUC: 0.5470
Epoch [20/30] Train Loss: 0.6854, Test Loss: 0.7002, F1: 0.3795, AUC: 0.5463
Mejores resultados en la época:  11
f1-score 0.38850415512465375
AUC según el mejor F1-score 0.5482083147950667
Confusion Matrix:
 [[ 3009 13456]
 [  672  4488]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Positiveness/confusion_matrix_param_701441.png
Accuracy:   0.3467
Precision:  0.2501
Recall:     0.8698
F1-score:   0.3885

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6905, Test Loss: 0.6955, F1: 0.3863, AUC: 0.5395
Epoch [10/30] Train Loss: 0.6870, Test Loss: 0.6931, F1: 0.3877, AUC: 0.5219
Epoch [20/30] Train Loss: 0.6864, Test Loss: 0.6849, F1: 0.3810, AUC: 0.5321
Mejores resultados en la época:  7
f1-score 0.3907039407207814
AUC según el mejor F1-score 0.5285693166383001
Confusion Matrix:
 [[ 2513 13952]
 [  520  4640]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Positiveness/confusion_matrix_param_701441.png
Accuracy:   0.3308
Precision:  0.2496
Recall:     0.8992
F1-score:   0.3907
Tiempo total para red 6: 170.38 segundos
Saved on: outputs_ablation_one_feature/0/Positiveness

==============================
Model: Logistic Regression
Accuracy:  0.5088
Precision: 0.2445
Recall:    0.5066
F1-score:  0.3298
              precision    recall  f1-score   support

           0       0.77      0.51      0.61     16465
           1       0.24      0.51      0.33      5160

    accuracy                           0.51     21625
   macro avg       0.51      0.51      0.47     21625
weighted avg       0.64      0.51      0.54     21625

[[8388 8077]
 [2546 2614]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Positiveness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Positiveness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.6183
Precision: 0.2237
Recall:    0.2426
F1-score:  0.2328
              precision    recall  f1-score   support

           0       0.76      0.74      0.75     16465
           1       0.22      0.24      0.23      5160

    accuracy                           0.62     21625
   macro avg       0.49      0.49      0.49     21625
weighted avg       0.63      0.62      0.62     21625

[[12119  4346]
 [ 3908  1252]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Positiveness/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Positiveness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.5388
Precision: 0.2811
Recall:    0.5990
F1-score:  0.3827
              precision    recall  f1-score   support

           0       0.81      0.52      0.63     16465
           1       0.28      0.60      0.38      5160

    accuracy                           0.54     21625
   macro avg       0.54      0.56      0.51     21625
weighted avg       0.68      0.54      0.57     21625

[[8561 7904]
 [2069 3091]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Positiveness/conf_matrix_decision_tree.png
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [16:48:59] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Modelo guardado como: outputs_ablation_one_feature/0/Positiveness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.5302
Precision: 0.2812
Recall:    0.6225
F1-score:  0.3874
              precision    recall  f1-score   support

           0       0.81      0.50      0.62     16465
           1       0.28      0.62      0.39      5160

    accuracy                           0.53     21625
   macro avg       0.55      0.56      0.50     21625
weighted avg       0.68      0.53      0.56     21625

[[8254 8211]
 [1948 3212]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Positiveness/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Positiveness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.5274
Precision: 0.2802
Recall:    0.6254
F1-score:  0.3870
              precision    recall  f1-score   support

           0       0.81      0.50      0.62     16465
           1       0.28      0.63      0.39      5160

    accuracy                           0.53     21625
   macro avg       0.54      0.56      0.50     21625
weighted avg       0.68      0.53      0.56     21625

[[8177 8288]
 [1933 3227]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Positiveness/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Positiveness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.4378
Precision: 0.2512
Recall:    0.6847
F1-score:  0.3676
              precision    recall  f1-score   support

           0       0.78      0.36      0.49     16465
           1       0.25      0.68      0.37      5160

    accuracy                           0.44     21625
   macro avg       0.52      0.52      0.43     21625
weighted avg       0.66      0.44      0.46     21625

[[ 5935 10530]
 [ 1627  3533]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Positiveness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Positiveness/naive_bayes_model.pkl


Resumen de métricas:
Random Forest: {'accuracy': 0.5302, 'precision': 0.2812, 'recall': 0.6225, 'f1_score': 0.3874}
XGBoost: {'accuracy': 0.5274, 'precision': 0.2802, 'recall': 0.6254, 'f1_score': 0.387}
Decision Tree: {'accuracy': 0.5388, 'precision': 0.2811, 'recall': 0.599, 'f1_score': 0.3827}
Naive Bayes: {'accuracy': 0.4378, 'precision': 0.2512, 'recall': 0.6847, 'f1_score': 0.3676}
Logistic Regression: {'accuracy': 0.5088, 'precision': 0.2445, 'recall': 0.5066, 'f1_score': 0.3298}
SVM: {'accuracy': 0.6183, 'precision': 0.2237, 'recall': 0.2426, 'f1_score': 0.2328}

##################################################
Running experiment with SPEECHINESS feature
[Speechiness] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6342, Test Loss: 0.5744, F1: 0.6017, AUC: 0.8151
Epoch [10/30] Train Loss: 0.5301, Test Loss: 0.5152, F1: 0.5931, AUC: 0.8151
Epoch [20/30] Train Loss: 0.5278, Test Loss: 0.5252, F1: 0.5885, AUC: 0.8151
Mejores resultados en la época:  0
f1-score 0.6016685093979295
AUC según el mejor F1-score 0.8150658961809994
Confusion Matrix:
 [[14669  1796]
 [ 2167  2993]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Speechiness/confusion_matrix_param_97.png
Accuracy:   0.8167
Precision:  0.6250
Recall:     0.5800
F1-score:   0.6017

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6245, Test Loss: 0.5549, F1: 0.6017, AUC: 0.8151
Epoch [10/30] Train Loss: 0.5301, Test Loss: 0.5167, F1: 0.5931, AUC: 0.8151
Epoch [20/30] Train Loss: 0.5277, Test Loss: 0.5194, F1: 0.5885, AUC: 0.8151
Mejores resultados en la época:  0
f1-score 0.6016685093979295
AUC según el mejor F1-score 0.8150658961809994
Confusion Matrix:
 [[14669  1796]
 [ 2167  2993]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Speechiness/confusion_matrix_param_97.png
Accuracy:   0.8167
Precision:  0.6250
Recall:     0.5800
F1-score:   0.6017

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6463, Test Loss: 0.5766, F1: 0.6009, AUC: 0.8151
Epoch [10/30] Train Loss: 0.5311, Test Loss: 0.5176, F1: 0.5931, AUC: 0.8151
Epoch [20/30] Train Loss: 0.5282, Test Loss: 0.5121, F1: 0.5931, AUC: 0.8151
Mejores resultados en la época:  0
f1-score 0.6008977972648502
AUC según el mejor F1-score 0.8150658961809994
Confusion Matrix:
 [[14924  1541]
 [ 2282  2878]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Speechiness/confusion_matrix_param_97.png
Accuracy:   0.8232
Precision:  0.6513
Recall:     0.5578
F1-score:   0.6009
Tiempo total para red 1: 105.92 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.5485, Test Loss: 0.5223, F1: 0.5991, AUC: 0.8151
Epoch [10/30] Train Loss: 0.5305, Test Loss: 0.5212, F1: 0.5931, AUC: 0.8152
Epoch [20/30] Train Loss: 0.5274, Test Loss: 0.5186, F1: 0.5885, AUC: 0.8152
Mejores resultados en la época:  0
f1-score 0.5990749306197964
AUC según el mejor F1-score 0.8150835516729167
Confusion Matrix:
 [[14053  2412]
 [ 1922  3238]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Speechiness/confusion_matrix_param_701441.png
Accuracy:   0.7996
Precision:  0.5731
Recall:     0.6275
F1-score:   0.5991

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.5498, Test Loss: 0.4738, F1: 0.5991, AUC: 0.8151
Epoch [10/30] Train Loss: 0.5294, Test Loss: 0.5570, F1: 0.5734, AUC: 0.8152
Epoch [20/30] Train Loss: 0.5275, Test Loss: 0.5241, F1: 0.5885, AUC: 0.8152
Mejores resultados en la época:  0
f1-score 0.5990749306197964
AUC según el mejor F1-score 0.8150628829770455
Confusion Matrix:
 [[14053  2412]
 [ 1922  3238]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Speechiness/confusion_matrix_param_701441.png
Accuracy:   0.7996
Precision:  0.5731
Recall:     0.6275
F1-score:   0.5991

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.5500, Test Loss: 0.5216, F1: 0.5931, AUC: 0.8151
Epoch [10/30] Train Loss: 0.5302, Test Loss: 0.5335, F1: 0.5885, AUC: 0.8152
Epoch [20/30] Train Loss: 0.5281, Test Loss: 0.5262, F1: 0.5885, AUC: 0.8152
Mejores resultados en la época:  4
f1-score 0.6016685093979295
AUC según el mejor F1-score 0.8152306748870638
Confusion Matrix:
 [[14669  1796]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [16:53:42] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
 [ 2167  2993]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Speechiness/confusion_matrix_param_701441.png
Accuracy:   0.8167
Precision:  0.6250
Recall:     0.5800
F1-score:   0.6017
Tiempo total para red 6: 170.19 segundos
Saved on: outputs_ablation_one_feature/0/Speechiness

==============================
Model: Logistic Regression
Accuracy:  0.8080
Precision: 0.5978
Recall:    0.5979
F1-score:  0.5978
              precision    recall  f1-score   support

           0       0.87      0.87      0.87     16465
           1       0.60      0.60      0.60      5160

    accuracy                           0.81     21625
   macro avg       0.74      0.74      0.74     21625
weighted avg       0.81      0.81      0.81     21625

[[14389  2076]
 [ 2075  3085]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Speechiness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Speechiness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.2900
Precision: 0.1772
Recall:    0.5422
F1-score:  0.2671
              precision    recall  f1-score   support

           0       0.60      0.21      0.31     16465
           1       0.18      0.54      0.27      5160

    accuracy                           0.29     21625
   macro avg       0.39      0.38      0.29     21625
weighted avg       0.50      0.29      0.30     21625

[[ 3474 12991]
 [ 2362  2798]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Speechiness/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Speechiness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7674
Precision: 0.5091
Recall:    0.6961
F1-score:  0.5881
              precision    recall  f1-score   support

           0       0.89      0.79      0.84     16465
           1       0.51      0.70      0.59      5160

    accuracy                           0.77     21625
   macro avg       0.70      0.74      0.71     21625
weighted avg       0.80      0.77      0.78     21625

[[13002  3463]
 [ 1568  3592]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Speechiness/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Speechiness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.7675
Precision: 0.5094
Recall:    0.6965
F1-score:  0.5884
              precision    recall  f1-score   support

           0       0.89      0.79      0.84     16465
           1       0.51      0.70      0.59      5160

    accuracy                           0.77     21625
   macro avg       0.70      0.74      0.71     21625
weighted avg       0.80      0.77      0.78     21625

[[13003  3462]
 [ 1566  3594]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Speechiness/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Speechiness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.7675
Precision: 0.5094
Recall:    0.6965
F1-score:  0.5884
              precision    recall  f1-score   support

           0       0.89      0.79      0.84     16465
           1       0.51      0.70      0.59      5160

    accuracy                           0.77     21625
   macro avg       0.70      0.74      0.71     21625
weighted avg       0.80      0.77      0.78     21625

[[13003  3462]
 [ 1566  3594]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Speechiness/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Speechiness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8243
Precision: 0.6773
Recall:    0.5035
F1-score:  0.5776
              precision    recall  f1-score   support

           0       0.86      0.92      0.89     16465
           1       0.68      0.50      0.58      5160

    accuracy                           0.82     21625
   macro avg       0.77      0.71      0.73     21625
weighted avg       0.81      0.82      0.81     21625

[[15227  1238]
 [ 2562  2598]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Speechiness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Speechiness/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.808, 'precision': 0.5978, 'recall': 0.5979, 'f1_score': 0.5978}
Random Forest: {'accuracy': 0.7675, 'precision': 0.5094, 'recall': 0.6965, 'f1_score': 0.5884}
XGBoost: {'accuracy': 0.7675, 'precision': 0.5094, 'recall': 0.6965, 'f1_score': 0.5884}
Decision Tree: {'accuracy': 0.7674, 'precision': 0.5091, 'recall': 0.6961, 'f1_score': 0.5881}
Naive Bayes: {'accuracy': 0.8243, 'precision': 0.6773, 'recall': 0.5035, 'f1_score': 0.5776}
SVM: {'accuracy': 0.29, 'precision': 0.1772, 'recall': 0.5422, 'f1_score': 0.2671}

##################################################
Running experiment with LIVENESS feature
[Liveness] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6929, Test Loss: 0.6862, F1: 0.2654, AUC: 0.5412
Epoch [10/30] Train Loss: 0.6913, Test Loss: 0.6771, F1: 0.3061, AUC: 0.5412
Epoch [20/30] Train Loss: 0.6908, Test Loss: 0.6872, F1: 0.3417, AUC: 0.5419
Mejores resultados en la época:  5
f1-score 0.3737981387217852
AUC según el mejor F1-score 0.5411776389663769
Confusion Matrix:
 [[ 5811 10654]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [16:58:26] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
 [ 1525  3635]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Liveness/confusion_matrix_param_97.png
Accuracy:   0.4368
Precision:  0.2544
Recall:     0.7045
F1-score:   0.3738

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6927, Test Loss: 0.6923, F1: 0.3053, AUC: 0.5412
Epoch [10/30] Train Loss: 0.6914, Test Loss: 0.7034, F1: 0.3665, AUC: 0.5428
Epoch [20/30] Train Loss: 0.6908, Test Loss: 0.6696, F1: 0.2990, AUC: 0.5430
Mejores resultados en la época:  21
f1-score 0.36685288640595903
AUC según el mejor F1-score 0.5428373375989003
Confusion Matrix:
 [[7593 8872]
 [2008 3152]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Liveness/confusion_matrix_param_97.png
Accuracy:   0.4969
Precision:  0.2621
Recall:     0.6109
F1-score:   0.3669

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6934, Test Loss: 0.6856, F1: 0.2737, AUC: 0.5412
Epoch [10/30] Train Loss: 0.6912, Test Loss: 0.6896, F1: 0.3329, AUC: 0.5422
Epoch [20/30] Train Loss: 0.6908, Test Loss: 0.6960, F1: 0.3533, AUC: 0.5427
Mejores resultados en la época:  26
f1-score 0.37441788264514125
AUC según el mejor F1-score 0.5424993997132748
Confusion Matrix:
 [[ 5917 10548]
 [ 1542  3618]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Liveness/confusion_matrix_param_97.png
Accuracy:   0.4409
Precision:  0.2554
Recall:     0.7012
F1-score:   0.3744
Tiempo total para red 1: 106.01 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6931, Test Loss: 0.6952, F1: 0.3853, AUC: 0.5412
Epoch [10/30] Train Loss: 0.6909, Test Loss: 0.7059, F1: 0.3854, AUC: 0.5423
Epoch [20/30] Train Loss: 0.6908, Test Loss: 0.7048, F1: 0.3738, AUC: 0.5415
Mejores resultados en la época:  10
f1-score 0.38537660429540643
AUC según el mejor F1-score 0.5422577254547466
Confusion Matrix:
 [[ 2943 13522]
 [  701  4459]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Liveness/confusion_matrix_param_701441.png
Accuracy:   0.3423
Precision:  0.2480
Recall:     0.8641
F1-score:   0.3854

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6932, Test Loss: 0.6860, F1: 0.2590, AUC: 0.5412
Epoch [10/30] Train Loss: 0.6912, Test Loss: 0.6824, F1: 0.3533, AUC: 0.5435
Epoch [20/30] Train Loss: 0.6906, Test Loss: 0.7026, F1: 0.3742, AUC: 0.5435
Mejores resultados en la época:  29
f1-score 0.3849412586801764
AUC según el mejor F1-score 0.5415019821232259
Confusion Matrix:
 [[ 3135 13330]
 [  753  4407]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Liveness/confusion_matrix_param_701441.png
Accuracy:   0.3488
Precision:  0.2485
Recall:     0.8541
F1-score:   0.3849

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6933, Test Loss: 0.6866, F1: 0.1781, AUC: 0.5412
Epoch [10/30] Train Loss: 0.6910, Test Loss: 0.6940, F1: 0.3661, AUC: 0.5425
Epoch [20/30] Train Loss: 0.6907, Test Loss: 0.6972, F1: 0.3744, AUC: 0.5438
Mejores resultados en la época:  3
f1-score 0.3858958837772397
AUC según el mejor F1-score 0.5421220488845261
Confusion Matrix:
 [[  293 16172]
 [   60  5100]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Liveness/confusion_matrix_param_701441.png
Accuracy:   0.2494
Precision:  0.2398
Recall:     0.9884
F1-score:   0.3859
Tiempo total para red 6: 170.26 segundos
Saved on: outputs_ablation_one_feature/0/Liveness

==============================
Model: Logistic Regression
Accuracy:  0.6125
Precision: 0.2667
Recall:    0.3568
F1-score:  0.3053
              precision    recall  f1-score   support

           0       0.77      0.69      0.73     16465
           1       0.27      0.36      0.31      5160

    accuracy                           0.61     21625
   macro avg       0.52      0.52      0.52     21625
weighted avg       0.65      0.61      0.63     21625

[[11404  5061]
 [ 3319  1841]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Liveness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Liveness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.6705
Precision: 0.2715
Recall:    0.2262
F1-score:  0.2467
              precision    recall  f1-score   support

           0       0.77      0.81      0.79     16465
           1       0.27      0.23      0.25      5160

    accuracy                           0.67     21625
   macro avg       0.52      0.52      0.52     21625
weighted avg       0.65      0.67      0.66     21625

[[13333  3132]
 [ 3993  1167]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Liveness/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Liveness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.5963
Precision: 0.2781
Recall:    0.4333
F1-score:  0.3388
              precision    recall  f1-score   support

           0       0.78      0.65      0.71     16465
           1       0.28      0.43      0.34      5160

    accuracy                           0.60     21625
   macro avg       0.53      0.54      0.52     21625
weighted avg       0.66      0.60      0.62     21625

[[10660  5805]
 [ 2924  2236]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Liveness/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Liveness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.5943
Precision: 0.2782
Recall:    0.4391
F1-score:  0.3406
              precision    recall  f1-score   support

           0       0.79      0.64      0.71     16465
           1       0.28      0.44      0.34      5160

    accuracy                           0.59     21625
   macro avg       0.53      0.54      0.52     21625
weighted avg       0.66      0.59      0.62     21625

[[10585  5880]
 [ 2894  2266]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Liveness/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Liveness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.5725
Precision: 0.2740
Recall:    0.4798
F1-score:  0.3488
              precision    recall  f1-score   support

           0       0.79      0.60      0.68     16465
           1       0.27      0.48      0.35      5160

    accuracy                           0.57     21625
   macro avg       0.53      0.54      0.52     21625
weighted avg       0.66      0.57      0.60     21625

[[9904 6561]
 [2684 2476]]
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Liveness/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Liveness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6271
Precision: 0.2606
Recall:    0.3064
F1-score:  0.2816
              precision    recall  f1-score   support

           0       0.77      0.73      0.75     16465
           1       0.26      0.31      0.28      5160

    accuracy                           0.63     21625
   macro avg       0.52      0.52      0.51     21625
weighted avg       0.65      0.63      0.64     21625

[[11979  4486]
 [ 3579  1581]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Liveness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Liveness/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.5725, 'precision': 0.274, 'recall': 0.4798, 'f1_score': 0.3488}
Random Forest: {'accuracy': 0.5943, 'precision': 0.2782, 'recall': 0.4391, 'f1_score': 0.3406}
Decision Tree: {'accuracy': 0.5963, 'precision': 0.2781, 'recall': 0.4333, 'f1_score': 0.3388}
Logistic Regression: {'accuracy': 0.6125, 'precision': 0.2667, 'recall': 0.3568, 'f1_score': 0.3053}
Naive Bayes: {'accuracy': 0.6271, 'precision': 0.2606, 'recall': 0.3064, 'f1_score': 0.2816}
SVM: {'accuracy': 0.6705, 'precision': 0.2715, 'recall': 0.2262, 'f1_score': 0.2467}

##################################################
Running experiment with ACOUSTICNESS feature
[Acousticness] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6781, Test Loss: 0.6962, F1: 0.4106, AUC: 0.5943
Epoch [10/30] Train Loss: 0.6703, Test Loss: 0.6672, F1: 0.3960, AUC: 0.5970
Epoch [20/30] Train Loss: 0.6696, Test Loss: 0.6737, F1: 0.4070, AUC: 0.5981
Mejores resultados en la época:  6
f1-score 0.41076527030189564
AUC según el mejor F1-score 0.5972363976205105
Confusion Matrix:
 [[ 4648 11817]
 [  772  4388]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Acousticness/confusion_matrix_param_97.png
Accuracy:   0.4178
Precision:  0.2708
Recall:     0.8504
F1-score:   0.4108

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6783, Test Loss: 0.6891, F1: 0.4036, AUC: 0.5624
Epoch [10/30] Train Loss: 0.6712, Test Loss: 0.6743, F1: 0.4070, AUC: 0.5976
Epoch [20/30] Train Loss: 0.6703, Test Loss: 0.6549, F1: 0.3834, AUC: 0.5971
Mejores resultados en la época:  26
f1-score 0.41076527030189564
AUC según el mejor F1-score 0.5971682827326935
Confusion Matrix:
 [[ 4648 11817]
 [  772  4388]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Acousticness/confusion_matrix_param_97.png
Accuracy:   0.4178
Precision:  0.2708
Recall:     0.8504
F1-score:   0.4108

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6771, Test Loss: 0.6812, F1: 0.4050, AUC: 0.5624
Epoch [10/30] Train Loss: 0.6705, Test Loss: 0.6754, F1: 0.4073, AUC: 0.5973
Epoch [20/30] Train Loss: 0.6696, Test Loss: 0.6777, F1: 0.3971, AUC: 0.5980
Mejores resultados en la época:  28
f1-score 0.4106188107303919
AUC según el mejor F1-score 0.5997240093503485
Confusion Matrix:
 [[ 4532 11933]
 [  744  4416]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Acousticness/confusion_matrix_param_97.png
Accuracy:   0.4138
Precision:  0.2701
Recall:     0.8558
F1-score:   0.4106
Tiempo total para red 1: 106.06 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6754, Test Loss: 0.6551, F1: 0.4108, AUC: 0.5624
Epoch [10/30] Train Loss: 0.6720, Test Loss: 0.6787, F1: 0.4037, AUC: 0.5814
Epoch [20/30] Train Loss: 0.6695, Test Loss: 0.6607, F1: 0.3719, AUC: 0.5987
Mejores resultados en la época:  2
f1-score 0.4134446397188049
AUC según el mejor F1-score 0.5986642561035035
Confusion Matrix:
 [[ 3570 12895]
 [  455  4705]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Acousticness/confusion_matrix_param_701441.png
Accuracy:   0.3827
Precision:  0.2673
Recall:     0.9118
F1-score:   0.4134

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6752, Test Loss: 0.6616, F1: 0.4120, AUC: 0.5624
Epoch [10/30] Train Loss: 0.6723, Test Loss: 0.6763, F1: 0.4115, AUC: 0.5626
Epoch [20/30] Train Loss: 0.6695, Test Loss: 0.6672, F1: 0.3884, AUC: 0.6009
Mejores resultados en la época:  9
f1-score 0.4138987352851079
AUC según el mejor F1-score 0.562432291188497
Confusion Matrix:
 [[ 3503 12962]
 [  431  4729]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Acousticness/confusion_matrix_param_701441.png
Accuracy:   0.3807
Precision:  0.2673
Recall:     0.9165
F1-score:   0.4139

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6755, Test Loss: 0.6728, F1: 0.4100, AUC: 0.5624
Epoch [10/30] Train Loss: 0.6703, Test Loss: 0.6869, F1: 0.4100, AUC: 0.5949
Epoch [20/30] Train Loss: 0.6694, Test Loss: 0.6779, F1: 0.4035, AUC: 0.5973
Mejores resultados en la época:  9
f1-score 0.41146263182026593
AUC según el mejor F1-score 0.5959178737137975
Confusion Matrix:
 [[ 4302 12163]
 [  673  4487]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Acousticness/confusion_matrix_param_701441.png
Accuracy:   0.4064
Precision:  0.2695
Recall:     0.8696
F1-score:   0.4115
Tiempo total para red 6: 170.64 segundos
Saved on: outputs_ablation_one_feature/0/Acousticness

==============================
Model: Logistic Regression
Accuracy:  0.4837
Precision: 0.2766
Recall:    0.7205
F1-score:  0.3997
              precision    recall  f1-score   support

           0       0.82      0.41      0.55     16465
           1       0.28      0.72      0.40      5160

    accuracy                           0.48     21625
   macro avg       0.55      0.56      0.47     21625
weighted avg       0.69      0.48      0.51     21625

[[6741 9724]
 [1442 3718]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Acousticness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Acousticness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.6571
Precision: 0.1678
Recall:    0.1105
F1-score:  0.1332
              precision    recall  f1-score   support

           0       0.75      0.83      0.79     16465
           1       0.17      0.11      0.13      5160

    accuracy                           0.66     21625
   macro avg       0.46      0.47      0.46     21625
weighted avg       0.61      0.66      0.63     21625

[[13639  2826]
 [ 4590   570]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Acousticness/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Acousticness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.5638
Precision: 0.2957
Recall:    0.5990
F1-score:  0.3959
              precision    recall  f1-score   support

           0       0.81      0.55      0.66     16465
           1       0.30      0.60      0.40      5160

    accuracy                           0.56     21625
   macro avg       0.56      0.58      0.53     21625
weighted avg       0.69      0.56      0.60     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:03:10] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])

[[9102 7363]
 [2069 3091]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Acousticness/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Acousticness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.5501
Precision: 0.2938
Recall:    0.6312
F1-score:  0.4010
              precision    recall  f1-score   support

           0       0.82      0.52      0.64     16465
           1       0.29      0.63      0.40      5160

    accuracy                           0.55     21625
   macro avg       0.56      0.58      0.52     21625
weighted avg       0.69      0.55      0.58     21625

[[8638 7827]
 [1903 3257]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Acousticness/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Acousticness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.5436
Precision: 0.2931
Recall:    0.6465
F1-score:  0.4034
              precision    recall  f1-score   support

           0       0.82      0.51      0.63     16465
           1       0.29      0.65      0.40      5160

    accuracy                           0.54     21625
   macro avg       0.56      0.58      0.52     21625
weighted avg       0.70      0.54      0.58     21625

[[8420 8045]
 [1824 3336]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Acousticness/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Acousticness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.4277
Precision: 0.2705
Recall:    0.8240
F1-score:  0.4073
              precision    recall  f1-score   support

           0       0.85      0.30      0.45     16465
           1       0.27      0.82      0.41      5160

    accuracy                           0.43     21625
   macro avg       0.56      0.56      0.43     21625
weighted avg       0.71      0.43      0.44     21625

[[ 4996 11469]
 [  908  4252]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Acousticness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Acousticness/naive_bayes_model.pkl


Resumen de métricas:
Naive Bayes: {'accuracy': 0.4277, 'precision': 0.2705, 'recall': 0.824, 'f1_score': 0.4073}
XGBoost: {'accuracy': 0.5436, 'precision': 0.2931, 'recall': 0.6465, 'f1_score': 0.4034}
Random Forest: {'accuracy': 0.5501, 'precision': 0.2938, 'recall': 0.6312, 'f1_score': 0.401}
Logistic Regression: {'accuracy': 0.4837, 'precision': 0.2766, 'recall': 0.7205, 'f1_score': 0.3997}
Decision Tree: {'accuracy': 0.5638, 'precision': 0.2957, 'recall': 0.599, 'f1_score': 0.3959}
SVM: {'accuracy': 0.6571, 'precision': 0.1678, 'recall': 0.1105, 'f1_score': 0.1332}

##################################################
Running experiment with INSTRUMENTALNESS feature
[Instrumentalness] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6797, Test Loss: 0.6624, F1: 0.4173, AUC: 0.5841
Epoch [10/30] Train Loss: 0.6737, Test Loss: 0.6848, F1: 0.4155, AUC: 0.5841
Epoch [20/30] Train Loss: 0.6719, Test Loss: 0.6633, F1: 0.4221, AUC: 0.5841
Mejores resultados en la época:  8
f1-score 0.4220971017168174
AUC según el mejor F1-score 0.5841186319583236
Confusion Matrix:
 [[ 4530 11935]
 [  587  4573]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Instrumentalness/confusion_matrix_param_97.png
Accuracy:   0.4209
Precision:  0.2770
Recall:     0.8862
F1-score:   0.4221

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6795, Test Loss: 0.6741, F1: 0.4145, AUC: 0.5841
Epoch [10/30] Train Loss: 0.6720, Test Loss: 0.6691, F1: 0.4173, AUC: 0.5841
Epoch [20/30] Train Loss: 0.6705, Test Loss: 0.6665, F1: 0.4221, AUC: 0.5841
Mejores resultados en la época:  13
f1-score 0.4220971017168174
AUC según el mejor F1-score 0.5841186319583236
Confusion Matrix:
 [[ 4530 11935]
 [  587  4573]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Instrumentalness/confusion_matrix_param_97.png
Accuracy:   0.4209
Precision:  0.2770
Recall:     0.8862
F1-score:   0.4221

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6823, Test Loss: 0.6893, F1: 0.4104, AUC: 0.5841
Epoch [10/30] Train Loss: 0.6734, Test Loss: 0.6662, F1: 0.4173, AUC: 0.5841
Epoch [20/30] Train Loss: 0.6715, Test Loss: 0.6651, F1: 0.4221, AUC: 0.5841
Mejores resultados en la época:  16
f1-score 0.4220971017168174
AUC según el mejor F1-score 0.5841186319583236
Confusion Matrix:
 [[ 4530 11935]
 [  587  4573]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Instrumentalness/confusion_matrix_param_97.png
Accuracy:   0.4209
Precision:  0.2770
Recall:     0.8862
F1-score:   0.4221
Tiempo total para red 1: 105.80 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6791, Test Loss: 0.7064, F1: 0.4073, AUC: 0.5305
Epoch [10/30] Train Loss: 0.6703, Test Loss: 0.6685, F1: 0.4173, AUC: 0.5604
Epoch [20/30] Train Loss: 0.6691, Test Loss: 0.6691, F1: 0.4221, AUC: 0.5843
Mejores resultados en la época:  4
f1-score 0.4220971017168174
AUC según el mejor F1-score 0.5841944976070923
Confusion Matrix:
 [[ 4530 11935]
 [  587  4573]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Instrumentalness/confusion_matrix_param_701441.png
Accuracy:   0.4209
Precision:  0.2770
Recall:     0.8862
F1-score:   0.4221

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6786, Test Loss: 0.6313, F1: 0.0000, AUC: 0.5841
Epoch [10/30] Train Loss: 0.6703, Test Loss: 0.6803, F1: 0.4173, AUC: 0.5844
Epoch [20/30] Train Loss: 0.6694, Test Loss: 0.6874, F1: 0.4221, AUC: 0.5843
Mejores resultados en la época:  3
f1-score 0.4220971017168174
AUC según el mejor F1-score 0.5841186672693075
Confusion Matrix:
 [[ 4530 11935]
 [  587  4573]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:07:53] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Instrumentalness/confusion_matrix_param_701441.png
Accuracy:   0.4209
Precision:  0.2770
Recall:     0.8862
F1-score:   0.4221

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6792, Test Loss: 0.6766, F1: 0.4087, AUC: 0.5249
Epoch [10/30] Train Loss: 0.6729, Test Loss: 0.6638, F1: 0.4173, AUC: 0.5843
Epoch [20/30] Train Loss: 0.6698, Test Loss: 0.6793, F1: 0.4173, AUC: 0.5843
Mejores resultados en la época:  13
f1-score 0.4220971017168174
AUC según el mejor F1-score 0.5843327518791328
Confusion Matrix:
 [[ 4530 11935]
 [  587  4573]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Instrumentalness/confusion_matrix_param_701441.png
Accuracy:   0.4209
Precision:  0.2770
Recall:     0.8862
F1-score:   0.4221
Tiempo total para red 6: 170.40 segundos
Saved on: outputs_ablation_one_feature/0/Instrumentalness

==============================
Model: Logistic Regression
Accuracy:  0.3608
Precision: 0.2645
Recall:    0.9430
F1-score:  0.4132
              precision    recall  f1-score   support

           0       0.91      0.18      0.30     16465
           1       0.26      0.94      0.41      5160

    accuracy                           0.36     21625
   macro avg       0.59      0.56      0.36     21625
weighted avg       0.76      0.36      0.33     21625

[[ 2936 13529]
 [  294  4866]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Instrumentalness/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Instrumentalness/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7026
Precision: 0.1146
Recall:    0.0366
F1-score:  0.0555
              precision    recall  f1-score   support

           0       0.75      0.91      0.82     16465
           1       0.11      0.04      0.06      5160

    accuracy                           0.70     21625
   macro avg       0.43      0.47      0.44     21625
weighted avg       0.60      0.70      0.64     21625

[[15005  1460]
 [ 4971   189]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Instrumentalness/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Instrumentalness/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.4204
Precision: 0.2774
Recall:    0.8903
F1-score:  0.4230
              precision    recall  f1-score   support

           0       0.89      0.27      0.42     16465
           1       0.28      0.89      0.42      5160

    accuracy                           0.42     21625
   macro avg       0.58      0.58      0.42     21625
weighted avg       0.74      0.42      0.42     21625

[[ 4498 11967]
 [  566  4594]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Instrumentalness/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Instrumentalness/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.4197
Precision: 0.2773
Recall:    0.8911
F1-score:  0.4229
              precision    recall  f1-score   support

           0       0.89      0.27      0.42     16465
           1       0.28      0.89      0.42      5160

    accuracy                           0.42     21625
   macro avg       0.58      0.58      0.42     21625
weighted avg       0.74      0.42      0.42     21625

[[ 4479 11986]
 [  562  4598]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Instrumentalness/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Instrumentalness/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.4204
Precision: 0.2774
Recall:    0.8903
F1-score:  0.4230
              precision    recall  f1-score   support

           0       0.89      0.27      0.42     16465
           1       0.28      0.89      0.42      5160

    accuracy                           0.42     21625
   macro avg       0.58      0.58      0.42     21625
weighted avg       0.74      0.42      0.42     21625

[[ 4498 11967]
 [  566  4594]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Instrumentalness/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Instrumentalness/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.3286
Precision: 0.2573
Recall:    0.9614
F1-score:  0.4060
              precision    recall  f1-score   support

           0       0.92      0.13      0.23     16465
           1       0.26      0.96      0.41      5160

    accuracy                           0.33     21625
   macro avg       0.59      0.55      0.32     21625
weighted avg       0.76      0.33      0.27     21625

[[ 2146 14319]
 [  199  4961]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Instrumentalness/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Instrumentalness/naive_bayes_model.pkl


Resumen de métricas:
Decision Tree: {'accuracy': 0.4204, 'precision': 0.2774, 'recall': 0.8903, 'f1_score': 0.423}
XGBoost: {'accuracy': 0.4204, 'precision': 0.2774, 'recall': 0.8903, 'f1_score': 0.423}
Random Forest: {'accuracy': 0.4197, 'precision': 0.2773, 'recall': 0.8911, 'f1_score': 0.4229}
Logistic Regression: {'accuracy': 0.3608, 'precision': 0.2645, 'recall': 0.943, 'f1_score': 0.4132}
Naive Bayes: {'accuracy': 0.3286, 'precision': 0.2573, 'recall': 0.9614, 'f1_score': 0.406}
SVM: {'accuracy': 0.7026, 'precision': 0.1146, 'recall': 0.0366, 'f1_score': 0.0555}

##################################################
Running experiment with GOOD FOR PARTY feature
[Good for Party] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:12:35] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6862, Test Loss: 0.6812, F1: 0.2935, AUC: 0.5479
Epoch [10/30] Train Loss: 0.6860, Test Loss: 0.6854, F1: 0.2935, AUC: 0.5479
Epoch [20/30] Train Loss: 0.6859, Test Loss: 0.6810, F1: 0.2935, AUC: 0.5479
Mejores resultados en la época:  0
f1-score 0.2935424948963146
AUC según el mejor F1-score 0.5479124734873363
Confusion Matrix:
 [[13684  2781]
 [ 3794  1366]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Party/confusion_matrix_param_97.png
Accuracy:   0.6960
Precision:  0.3294
Recall:     0.2647
F1-score:   0.2935

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6878, Test Loss: 0.6936, F1: 0.2935, AUC: 0.5479
Epoch [10/30] Train Loss: 0.6860, Test Loss: 0.6973, F1: 0.2935, AUC: 0.5479
Epoch [20/30] Train Loss: 0.6860, Test Loss: 0.6915, F1: 0.2935, AUC: 0.5479
Mejores resultados en la época:  0
f1-score 0.2935424948963146
AUC según el mejor F1-score 0.5479124734873363
Confusion Matrix:
 [[13684  2781]
 [ 3794  1366]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Party/confusion_matrix_param_97.png
Accuracy:   0.6960
Precision:  0.3294
Recall:     0.2647
F1-score:   0.2935

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6874, Test Loss: 0.6795, F1: 0.2935, AUC: 0.5479
Epoch [10/30] Train Loss: 0.6859, Test Loss: 0.6763, F1: 0.2935, AUC: 0.5479
Epoch [20/30] Train Loss: 0.6859, Test Loss: 0.6853, F1: 0.2935, AUC: 0.5479
Mejores resultados en la época:  0
f1-score 0.2935424948963146
AUC según el mejor F1-score 0.5479124734873363
Confusion Matrix:
 [[13684  2781]
 [ 3794  1366]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Party/confusion_matrix_param_97.png
Accuracy:   0.6960
Precision:  0.3294
Recall:     0.2647
F1-score:   0.2935
Tiempo total para red 1: 105.77 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6872, Test Loss: 0.6754, F1: 0.2935, AUC: 0.5479
Epoch [10/30] Train Loss: 0.6861, Test Loss: 0.6882, F1: 0.2935, AUC: 0.5479
Epoch [20/30] Train Loss: 0.6861, Test Loss: 0.6896, F1: 0.2935, AUC: 0.5479
Mejores resultados en la época:  0
f1-score 0.2935424948963146
AUC según el mejor F1-score 0.5479124734873363
Confusion Matrix:
 [[13684  2781]
 [ 3794  1366]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Party/confusion_matrix_param_701441.png
Accuracy:   0.6960
Precision:  0.3294
Recall:     0.2647
F1-score:   0.2935

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6870, Test Loss: 0.6817, F1: 0.2935, AUC: 0.5479
Epoch [10/30] Train Loss: 0.6861, Test Loss: 0.6897, F1: 0.2935, AUC: 0.5479
Epoch [20/30] Train Loss: 0.6859, Test Loss: 0.6879, F1: 0.2935, AUC: 0.5479
Mejores resultados en la época:  0
f1-score 0.2935424948963146
AUC según el mejor F1-score 0.5479124734873363
Confusion Matrix:
 [[13684  2781]
 [ 3794  1366]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Party/confusion_matrix_param_701441.png
Accuracy:   0.6960
Precision:  0.3294
Recall:     0.2647
F1-score:   0.2935

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6876, Test Loss: 0.6835, F1: 0.2935, AUC: 0.5479
Epoch [10/30] Train Loss: 0.6862, Test Loss: 0.6883, F1: 0.2935, AUC: 0.5479
Epoch [20/30] Train Loss: 0.6860, Test Loss: 0.6873, F1: 0.2935, AUC: 0.5479
Mejores resultados en la época:  9
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5479124734873363
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Party/confusion_matrix_param_701441.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853
Tiempo total para red 6: 170.12 segundos
Saved on: outputs_ablation_one_feature/0/Good for Party

==============================
Model: Logistic Regression
Accuracy:  0.6960
Precision: 0.3294
Recall:    0.2647
F1-score:  0.2935
              precision    recall  f1-score   support

           0       0.78      0.83      0.81     16465
           1       0.33      0.26      0.29      5160

    accuracy                           0.70     21625
   macro avg       0.56      0.55      0.55     21625
weighted avg       0.67      0.70      0.68     21625

[[13684  2781]
 [ 3794  1366]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Party/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Party/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.3040
Precision: 0.2171
Recall:    0.7353
F1-score:  0.3352
              precision    recall  f1-score   support

           0       0.67      0.17      0.27     16465
           1       0.22      0.74      0.34      5160

    accuracy                           0.30     21625
   macro avg       0.44      0.45      0.30     21625
weighted avg       0.56      0.30      0.29     21625

[[ 2781 13684]
 [ 1366  3794]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Party/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Party/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6960
Precision: 0.3294
Recall:    0.2647
F1-score:  0.2935
              precision    recall  f1-score   support

           0       0.78      0.83      0.81     16465
           1       0.33      0.26      0.29      5160

    accuracy                           0.70     21625
   macro avg       0.56      0.55      0.55     21625
weighted avg       0.67      0.70      0.68     21625

[[13684  2781]
 [ 3794  1366]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Party/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Party/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.6960
Precision: 0.3294
Recall:    0.2647
F1-score:  0.2935
              precision    recall  f1-score   support

           0       0.78      0.83      0.81     16465
           1       0.33      0.26      0.29      5160

    accuracy                           0.70     21625
   macro avg       0.56      0.55      0.55     21625
weighted avg       0.67      0.70      0.68     21625

[[13684  2781]
 [ 3794  1366]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Party/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Party/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.6960
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
Precision: 0.3294
Recall:    0.2647
F1-score:  0.2935
              precision    recall  f1-score   support

           0       0.78      0.83      0.81     16465
           1       0.33      0.26      0.29      5160

    accuracy                           0.70     21625
   macro avg       0.56      0.55      0.55     21625
weighted avg       0.67      0.70      0.68     21625

[[13684  2781]
 [ 3794  1366]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Party/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Party/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6960
Precision: 0.3294
Recall:    0.2647
F1-score:  0.2935
              precision    recall  f1-score   support

           0       0.78      0.83      0.81     16465
           1       0.33      0.26      0.29      5160

    accuracy                           0.70     21625
   macro avg       0.56      0.55      0.55     21625
weighted avg       0.67      0.70      0.68     21625

[[13684  2781]
 [ 3794  1366]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Party/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Party/naive_bayes_model.pkl


Resumen de métricas:
SVM: {'accuracy': 0.304, 'precision': 0.2171, 'recall': 0.7353, 'f1_score': 0.3352}
Logistic Regression: {'accuracy': 0.696, 'precision': 0.3294, 'recall': 0.2647, 'f1_score': 0.2935}
Decision Tree: {'accuracy': 0.696, 'precision': 0.3294, 'recall': 0.2647, 'f1_score': 0.2935}
Random Forest: {'accuracy': 0.696, 'precision': 0.3294, 'recall': 0.2647, 'f1_score': 0.2935}
XGBoost: {'accuracy': 0.696, 'precision': 0.3294, 'recall': 0.2647, 'f1_score': 0.2935}
Naive Bayes: {'accuracy': 0.696, 'precision': 0.3294, 'recall': 0.2647, 'f1_score': 0.2935}

##################################################
Running experiment with GOOD FOR WORK/STUDY feature
[Good for Work/Study] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6816, Test Loss: 0.6827, F1: 0.4028, AUC: 0.5375
Epoch [10/30] Train Loss: 0.6796, Test Loss: 0.6773, F1: 0.4028, AUC: 0.5375
Epoch [20/30] Train Loss: 0.6797, Test Loss: 0.6852, F1: 0.4028, AUC: 0.5375
Mejores resultados en la época:  0
f1-score 0.40281309599491416
AUC según el mejor F1-score 0.5375228932878527
Confusion Matrix:
 [[ 1526 14939]
 [   91  5069]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Work/Study/confusion_matrix_param_97.png
Accuracy:   0.3050
Precision:  0.2533
Recall:     0.9824
F1-score:   0.4028

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6829, Test Loss: 0.6889, F1: 0.4028, AUC: 0.5375
Epoch [10/30] Train Loss: 0.6795, Test Loss: 0.6727, F1: 0.4028, AUC: 0.5375
Epoch [20/30] Train Loss: 0.6796, Test Loss: 0.6913, F1: 0.4028, AUC: 0.5375
Mejores resultados en la época:  0
f1-score 0.40281309599491416
AUC según el mejor F1-score 0.5375228932878527
Confusion Matrix:
 [[ 1526 14939]
 [   91  5069]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Work/Study/confusion_matrix_param_97.png
Accuracy:   0.3050
Precision:  0.2533
Recall:     0.9824
F1-score:   0.4028

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6836, Test Loss: 0.6895, F1: 0.4028, AUC: 0.5375
Epoch [10/30] Train Loss: 0.6796, Test Loss: 0.6803, F1: 0.4028, AUC: 0.5375
Epoch [20/30] Train Loss: 0.6796, Test Loss: 0.6782, F1: 0.4028, AUC: 0.5375
Mejores resultados en la época:  0
f1-score 0.40281309599491416
AUC según el mejor F1-score 0.5375228932878527
Confusion Matrix:
 [[ 1526 14939]
 [   91  5069]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Work/Study/confusion_matrix_param_97.png
Accuracy:   0.3050
Precision:  0.2533
Recall:     0.9824
F1-score:   0.4028
Tiempo total para red 1: 105.73 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6814, Test Loss: 0.6887, F1: 0.4028, AUC: 0.5375
Epoch [10/30] Train Loss: 0.6800, Test Loss: 0.6682, F1: 0.4028, AUC: 0.5375
Epoch [20/30] Train Loss: 0.6798, Test Loss: 0.6788, F1: 0.4028, AUC: 0.5375
Mejores resultados en la época:  0
f1-score 0.40281309599491416
AUC según el mejor F1-score 0.5375228932878527
Confusion Matrix:
 [[ 1526 14939]
 [   91  5069]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Work/Study/confusion_matrix_param_701441.png
Accuracy:   0.3050
Precision:  0.2533
Recall:     0.9824
F1-score:   0.4028

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6813, Test Loss: 0.6979, F1: 0.4028, AUC: 0.5375
Epoch [10/30] Train Loss: 0.6798, Test Loss: 0.6851, F1: 0.4028, AUC: 0.5375
Epoch [20/30] Train Loss: 0.6797, Test Loss: 0.6894, F1: 0.4028, AUC: 0.5375
Mejores resultados en la época:  0
f1-score 0.40281309599491416
AUC según el mejor F1-score 0.5375228932878527
Confusion Matrix:
 [[ 1526 14939]
 [   91  5069]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Work/Study/confusion_matrix_param_701441.png
Accuracy:   0.3050
Precision:  0.2533
Recall:     0.9824
F1-score:   0.4028

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6822, Test Loss: 0.6664, F1: 0.4028, AUC: 0.5375
Epoch [10/30] Train Loss: 0.6801, Test Loss: 0.6812, F1: 0.4028, AUC: 0.5375
Epoch [20/30] Train Loss: 0.6799, Test Loss: 0.6887, F1: 0.4028, AUC: 0.5375
Mejores resultados en la época:  0
f1-score 0.40281309599491416
AUC según el mejor F1-score 0.5375228932878527
Confusion Matrix:
 [[ 1526 14939]
 [   91  5069]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Work/Study/confusion_matrix_param_701441.png
Accuracy:   0.3050
Precision:  0.2533
Recall:     0.9824
F1-score:   0.4028
Tiempo total para red 6: 169.96 segundos
Saved on: outputs_ablation_one_feature/0/Good for Work/Study

==============================
Model: Logistic Regression
Accuracy:  0.3050
Precision: 0.2533
Recall:    0.9824
F1-score:  0.4028
              precision    recall  f1-score   support

           0       0.94      0.09      0.17     16465
           1       0.25      0.98      0.40      5160

    accuracy                           0.30     21625
   macro avg       0.60      0.54      0.29     21625
weighted avg       0.78      0.30      0.22     21625

[[ 1526 14939]
 [   91  5069]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:17:16] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Work/Study/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Work/Study/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.6950
Precision: 0.0563
Recall:    0.0176
F1-score:  0.0269
              precision    recall  f1-score   support

           0       0.75      0.91      0.82     16465
           1       0.06      0.02      0.03      5160

    accuracy                           0.70     21625
   macro avg       0.40      0.46      0.42     21625
weighted avg       0.58      0.70      0.63     21625

[[14939  1526]
 [ 5069    91]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Work/Study/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Work/Study/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.3050
Precision: 0.2533
Recall:    0.9824
F1-score:  0.4028
              precision    recall  f1-score   support

           0       0.94      0.09      0.17     16465
           1       0.25      0.98      0.40      5160

    accuracy                           0.30     21625
   macro avg       0.60      0.54      0.29     21625
weighted avg       0.78      0.30      0.22     21625

[[ 1526 14939]
 [   91  5069]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Work/Study/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Work/Study/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.3050
Precision: 0.2533
Recall:    0.9824
F1-score:  0.4028
              precision    recall  f1-score   support

           0       0.94      0.09      0.17     16465
           1       0.25      0.98      0.40      5160

    accuracy                           0.30     21625
   macro avg       0.60      0.54      0.29     21625
weighted avg       0.78      0.30      0.22     21625

[[ 1526 14939]
 [   91  5069]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Work/Study/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Work/Study/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.3050
Precision: 0.2533
Recall:    0.9824
F1-score:  0.4028
              precision    recall  f1-score   support

           0       0.94      0.09      0.17     16465
           1       0.25      0.98      0.40      5160

    accuracy                           0.30     21625
   macro avg       0.60      0.54      0.29     21625
weighted avg       0.78      0.30      0.22     21625

[[ 1526 14939]
 [   91  5069]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Work/Study/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Work/Study/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.3050
Precision: 0.2533
Recall:    0.9824
F1-score:  0.4028
              precision    recall  f1-score   support

           0       0.94      0.09      0.17     16465
           1       0.25      0.98      0.40      5160

    accuracy                           0.30     21625
   macro avg       0.60      0.54      0.29     21625
weighted avg       0.78      0.30      0.22     21625

[[ 1526 14939]
 [   91  5069]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Work/Study/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Work/Study/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.305, 'precision': 0.2533, 'recall': 0.9824, 'f1_score': 0.4028}
Decision Tree: {'accuracy': 0.305, 'precision': 0.2533, 'recall': 0.9824, 'f1_score': 0.4028}
Random Forest: {'accuracy': 0.305, 'precision': 0.2533, 'recall': 0.9824, 'f1_score': 0.4028}
XGBoost: {'accuracy': 0.305, 'precision': 0.2533, 'recall': 0.9824, 'f1_score': 0.4028}
Naive Bayes: {'accuracy': 0.305, 'precision': 0.2533, 'recall': 0.9824, 'f1_score': 0.4028}
SVM: {'accuracy': 0.695, 'precision': 0.0563, 'recall': 0.0176, 'f1_score': 0.0269}

##################################################
Running experiment with GOOD FOR RELAXATION/MEDITATION feature
[Good for Relaxation/Meditation] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6891, Test Loss: 0.6650, F1: 0.0000, AUC: 0.5167
Epoch [10/30] Train Loss: 0.6856, Test Loss: 0.7001, F1: 0.3931, AUC: 0.5167
Epoch [20/30] Train Loss: 0.6857, Test Loss: 0.6986, F1: 0.3931, AUC: 0.5167
Mejores resultados en la época:  1
f1-score 0.39305385556915545
AUC según el mejor F1-score 0.5166656661887914
Confusion Matrix:
 [[  619 15846]
 [   22  5138]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Relaxation/Meditation/confusion_matrix_param_97.png
Accuracy:   0.2662
Precision:  0.2449
Recall:     0.9957
F1-score:   0.3931

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6872, Test Loss: 0.6964, F1: 0.3931, AUC: 0.5167
Epoch [10/30] Train Loss: 0.6856, Test Loss: 0.6826, F1: 0.3931, AUC: 0.5167
Epoch [20/30] Train Loss: 0.6856, Test Loss: 0.6827, F1: 0.3931, AUC: 0.5167
Mejores resultados en la época:  0
f1-score 0.39305385556915545
AUC según el mejor F1-score 0.5166656661887914
Confusion Matrix:
 [[  619 15846]
 [   22  5138]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Relaxation/Meditation/confusion_matrix_param_97.png
Accuracy:   0.2662
Precision:  0.2449
Recall:     0.9957
F1-score:   0.3931

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6879, Test Loss: 0.6798, F1: 0.3931, AUC: 0.5167
Epoch [10/30] Train Loss: 0.6857, Test Loss: 0.6816, F1: 0.3931, AUC: 0.5167
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:21:57] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [20/30] Train Loss: 0.6856, Test Loss: 0.6983, F1: 0.3931, AUC: 0.5167
Mejores resultados en la época:  0
f1-score 0.39305385556915545
AUC según el mejor F1-score 0.5166656661887914
Confusion Matrix:
 [[  619 15846]
 [   22  5138]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Relaxation/Meditation/confusion_matrix_param_97.png
Accuracy:   0.2662
Precision:  0.2449
Recall:     0.9957
F1-score:   0.3931
Tiempo total para red 1: 105.72 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6869, Test Loss: 0.6993, F1: 0.3931, AUC: 0.5167
Epoch [10/30] Train Loss: 0.6860, Test Loss: 0.6925, F1: 0.3931, AUC: 0.5167
Epoch [20/30] Train Loss: 0.6858, Test Loss: 0.6868, F1: 0.3931, AUC: 0.5167
Mejores resultados en la época:  0
f1-score 0.39305385556915545
AUC según el mejor F1-score 0.5166656661887914
Confusion Matrix:
 [[  619 15846]
 [   22  5138]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Relaxation/Meditation/confusion_matrix_param_701441.png
Accuracy:   0.2662
Precision:  0.2449
Recall:     0.9957
F1-score:   0.3931

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6868, Test Loss: 0.6905, F1: 0.3931, AUC: 0.5167
Epoch [10/30] Train Loss: 0.6858, Test Loss: 0.6877, F1: 0.3931, AUC: 0.5167
Epoch [20/30] Train Loss: 0.6858, Test Loss: 0.6899, F1: 0.3931, AUC: 0.5167
Mejores resultados en la época:  0
f1-score 0.39305385556915545
AUC según el mejor F1-score 0.5166656661887914
Confusion Matrix:
 [[  619 15846]
 [   22  5138]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Relaxation/Meditation/confusion_matrix_param_701441.png
Accuracy:   0.2662
Precision:  0.2449
Recall:     0.9957
F1-score:   0.3931

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6866, Test Loss: 0.6922, F1: 0.3931, AUC: 0.5162
Epoch [10/30] Train Loss: 0.6857, Test Loss: 0.6831, F1: 0.3931, AUC: 0.5167
Epoch [20/30] Train Loss: 0.6858, Test Loss: 0.6853, F1: 0.3931, AUC: 0.5167
Mejores resultados en la época:  0
f1-score 0.39305385556915545
AUC según el mejor F1-score 0.516229092954988
Confusion Matrix:
 [[  619 15846]
 [   22  5138]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Relaxation/Meditation/confusion_matrix_param_701441.png
Accuracy:   0.2662
Precision:  0.2449
Recall:     0.9957
F1-score:   0.3931
Tiempo total para red 6: 170.22 segundos
Saved on: outputs_ablation_one_feature/0/Good for Relaxation/Meditation

==============================
Model: Logistic Regression
Accuracy:  0.2662
Precision: 0.2449
Recall:    0.9957
F1-score:  0.3931
              precision    recall  f1-score   support

           0       0.97      0.04      0.07     16465
           1       0.24      1.00      0.39      5160

    accuracy                           0.27     21625
   macro avg       0.61      0.52      0.23     21625
weighted avg       0.79      0.27      0.15     21625

[[  619 15846]
 [   22  5138]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Relaxation/Meditation/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Relaxation/Meditation/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.2386
Precision: 0.2386
Recall:    1.0000
F1-score:  0.3853
              precision    recall  f1-score   support

           0       0.00      0.00      0.00     16465
           1       0.24      1.00      0.39      5160

    accuracy                           0.24     21625
   macro avg       0.12      0.50      0.19     21625
weighted avg       0.06      0.24      0.09     21625

[[    0 16465]
 [    0  5160]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Relaxation/Meditation/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Relaxation/Meditation/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.2662
Precision: 0.2449
Recall:    0.9957
F1-score:  0.3931
              precision    recall  f1-score   support

           0       0.97      0.04      0.07     16465
           1       0.24      1.00      0.39      5160

    accuracy                           0.27     21625
   macro avg       0.61      0.52      0.23     21625
weighted avg       0.79      0.27      0.15     21625

[[  619 15846]
 [   22  5138]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Relaxation/Meditation/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Relaxation/Meditation/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.2662
Precision: 0.2449
Recall:    0.9957
F1-score:  0.3931
              precision    recall  f1-score   support

           0       0.97      0.04      0.07     16465
           1       0.24      1.00      0.39      5160

    accuracy                           0.27     21625
   macro avg       0.61      0.52      0.23     21625
weighted avg       0.79      0.27      0.15     21625

[[  619 15846]
 [   22  5138]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Relaxation/Meditation/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Relaxation/Meditation/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.2662
Precision: 0.2449
Recall:    0.9957
F1-score:  0.3931
              precision    recall  f1-score   support

           0       0.97      0.04      0.07     16465
           1       0.24      1.00      0.39      5160

    accuracy                           0.27     21625
   macro avg       0.61      0.52      0.23     21625
weighted avg       0.79      0.27      0.15     21625

[[  619 15846]
 [   22  5138]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Relaxation/Meditation/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Relaxation/Meditation/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.2662
Precision: 0.2449
Recall:    0.9957
F1-score:  0.3931
              precision    recall  f1-score   support

           0       0.97      0.04      0.07     16465
           1       0.24      1.00      0.39      5160

    accuracy                           0.27     21625
   macro avg       0.61      0.52      0.23     21625
weighted avg       0.79      0.27      0.15     21625

[[  619 15846]
 [   22  5138]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Relaxation/Meditation/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Relaxation/Meditation/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.2662, 'precision': 0.2449, 'recall': 0.9957, 'f1_score': 0.3931}
Decision Tree: {'accuracy': 0.2662, 'precision': 0.2449, 'recall': 0.9957, 'f1_score': 0.3931}
Random Forest: {'accuracy': 0.2662, 'precision': 0.2449, 'recall': 0.9957, 'f1_score': 0.3931}
XGBoost: {'accuracy': 0.2662, 'precision': 0.2449, 'recall': 0.9957, 'f1_score': 0.3931}
Naive Bayes: {'accuracy': 0.2662, 'precision': 0.2449, 'recall': 0.9957, 'f1_score': 0.3931}
SVM: {'accuracy': 0.2386, 'precision': 0.2386, 'recall': 1.0, 'f1_score': 0.3853}

##################################################
Running experiment with GOOD FOR EXERCISE feature
[Good for Exercise] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6868, Test Loss: 0.6843, F1: 0.3062, AUC: 0.5525
Epoch [10/30] Train Loss: 0.6854, Test Loss: 0.6897, F1: 0.3062, AUC: 0.5525
Epoch [20/30] Train Loss: 0.6852, Test Loss: 0.6865, F1: 0.3062, AUC: 0.5525
Mejores resultados en la época:  0
f1-score 0.30616509926854757
AUC según el mejor F1-score 0.5525252355831138
Confusion Matrix:
 [[13520  2945]
 [ 3695  1465]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Exercise/confusion_matrix_param_97.png
Accuracy:   0.6929
Precision:  0.3322
Recall:     0.2839
F1-score:   0.3062

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6902, Test Loss: 0.6706, F1: 0.3062, AUC: 0.5525
Epoch [10/30] Train Loss: 0.6854, Test Loss: 0.6919, F1: 0.3062, AUC: 0.5525
Epoch [20/30] Train Loss: 0.6853, Test Loss: 0.7076, F1: 0.3062, AUC: 0.5525
Mejores resultados en la época:  0
f1-score 0.30616509926854757
AUC según el mejor F1-score 0.5525252355831138
Confusion Matrix:
 [[13520  2945]
 [ 3695  1465]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Exercise/confusion_matrix_param_97.png
Accuracy:   0.6929
Precision:  0.3322
Recall:     0.2839
F1-score:   0.3062

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6861, Test Loss: 0.6989, F1: 0.3062, AUC: 0.5525
Epoch [10/30] Train Loss: 0.6853, Test Loss: 0.6705, F1: 0.3062, AUC: 0.5525
Epoch [20/30] Train Loss: 0.6852, Test Loss: 0.6776, F1: 0.3062, AUC: 0.5525
Mejores resultados en la época:  0
f1-score 0.30616509926854757
AUC según el mejor F1-score 0.5525252355831138
Confusion Matrix:
 [[13520  2945]
 [ 3695  1465]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Exercise/confusion_matrix_param_97.png
Accuracy:   0.6929
Precision:  0.3322
Recall:     0.2839
F1-score:   0.3062
Tiempo total para red 1: 106.23 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6869, Test Loss: 0.6944, F1: 0.3062, AUC: 0.5525
Epoch [10/30] Train Loss: 0.6853, Test Loss: 0.6910, F1: 0.3062, AUC: 0.5525
Epoch [20/30] Train Loss: 0.6853, Test Loss: 0.6862, F1: 0.3062, AUC: 0.5525
Mejores resultados en la época:  0
f1-score 0.30616509926854757
AUC según el mejor F1-score 0.5525252355831138
Confusion Matrix:
 [[13520  2945]
 [ 3695  1465]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Exercise/confusion_matrix_param_701441.png
Accuracy:   0.6929
Precision:  0.3322
Recall:     0.2839
F1-score:   0.3062

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6866, Test Loss: 0.6827, F1: 0.3062, AUC: 0.5525
Epoch [10/30] Train Loss: 0.6857, Test Loss: 0.6800, F1: 0.3062, AUC: 0.5525
Epoch [20/30] Train Loss: 0.6854, Test Loss: 0.6918, F1: 0.3062, AUC: 0.5525
Mejores resultados en la época:  0
f1-score 0.30616509926854757
AUC según el mejor F1-score 0.5525252355831138
Confusion Matrix:
 [[13520  2945]
 [ 3695  1465]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Exercise/confusion_matrix_param_701441.png
Accuracy:   0.6929
Precision:  0.3322
Recall:     0.2839
F1-score:   0.3062

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6864, Test Loss: 0.6924, F1: 0.3062, AUC: 0.5525
Epoch [10/30] Train Loss: 0.6853, Test Loss: 0.6825, F1: 0.3062, AUC: 0.5525
Epoch [20/30] Train Loss: 0.6852, Test Loss: 0.6859, F1: 0.3062, AUC: 0.5525
Mejores resultados en la época:  0
f1-score 0.30616509926854757
AUC según el mejor F1-score 0.5525252355831138
Confusion Matrix:
 [[13520  2945]
 [ 3695  1465]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Exercise/confusion_matrix_param_701441.png
Accuracy:   0.6929
Precision:  0.3322
Recall:     0.2839
F1-score:   0.3062
Tiempo total para red 6: 170.18 segundos
Saved on: outputs_ablation_one_feature/0/Good for Exercise

==============================
Model: Logistic Regression
Accuracy:  0.6929
Precision: 0.3322
Recall:    0.2839
F1-score:  0.3062
              precision    recall  f1-score   support

           0       0.79      0.82      0.80     16465
           1       0.33      0.28      0.31      5160

    accuracy                           0.69     21625
   macro avg       0.56      0.55      0.55     21625
weighted avg       0.68      0.69      0.68     21625

[[13520  2945]
 [ 3695  1465]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Exercise/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Exercise/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7614
Precision: 0.0000
Recall:    0.0000
F1-score:  0.0000
              precision    recall  f1-score   support

           0       0.76      1.00      0.86     16465
           1       0.00      0.00      0.00      5160

    accuracy                           0.76     21625
   macro avg       0.38      0.50      0.43     21625
weighted avg       0.58      0.76      0.66     21625

[[16465     0]
 [ 5160     0]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Exercise/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Exercise/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6929
Precision: 0.3322
Recall:    0.2839
F1-score:  0.3062
              precision    recall  f1-score   support

           0       0.79      0.82      0.80     16465
           1       0.33      0.28      0.31      5160

    accuracy                           0.69     21625
   macro avg       0.56      0.55      0.55     21625
weighted avg       0.68      0.69      0.68     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:26:39] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[13520  2945]
 [ 3695  1465]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Exercise/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Exercise/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.6929
Precision: 0.3322
Recall:    0.2839
F1-score:  0.3062
              precision    recall  f1-score   support

           0       0.79      0.82      0.80     16465
           1       0.33      0.28      0.31      5160

    accuracy                           0.69     21625
   macro avg       0.56      0.55      0.55     21625
weighted avg       0.68      0.69      0.68     21625

[[13520  2945]
 [ 3695  1465]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Exercise/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Exercise/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.6929
Precision: 0.3322
Recall:    0.2839
F1-score:  0.3062
              precision    recall  f1-score   support

           0       0.79      0.82      0.80     16465
           1       0.33      0.28      0.31      5160

    accuracy                           0.69     21625
   macro avg       0.56      0.55      0.55     21625
weighted avg       0.68      0.69      0.68     21625

[[13520  2945]
 [ 3695  1465]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Exercise/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Exercise/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6929
Precision: 0.3322
Recall:    0.2839
F1-score:  0.3062
              precision    recall  f1-score   support

           0       0.79      0.82      0.80     16465
           1       0.33      0.28      0.31      5160

    accuracy                           0.69     21625
   macro avg       0.56      0.55      0.55     21625
weighted avg       0.68      0.69      0.68     21625

[[13520  2945]
 [ 3695  1465]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Exercise/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Exercise/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.6929, 'precision': 0.3322, 'recall': 0.2839, 'f1_score': 0.3062}
Decision Tree: {'accuracy': 0.6929, 'precision': 0.3322, 'recall': 0.2839, 'f1_score': 0.3062}
Random Forest: {'accuracy': 0.6929, 'precision': 0.3322, 'recall': 0.2839, 'f1_score': 0.3062}
XGBoost: {'accuracy': 0.6929, 'precision': 0.3322, 'recall': 0.2839, 'f1_score': 0.3062}
Naive Bayes: {'accuracy': 0.6929, 'precision': 0.3322, 'recall': 0.2839, 'f1_score': 0.3062}
SVM: {'accuracy': 0.7614, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}

##################################################
Running experiment with GOOD FOR RUNNING feature
[Good for Running] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6870, Test Loss: 0.6909, F1: 0.1769, AUC: 0.5314
Epoch [10/30] Train Loss: 0.6866, Test Loss: 0.6882, F1: 0.1769, AUC: 0.5314
Epoch [20/30] Train Loss: 0.6865, Test Loss: 0.6741, F1: 0.1769, AUC: 0.5314
Mejores resultados en la época:  0
f1-score 0.17688319609637085
AUC según el mejor F1-score 0.5313609794796103
Confusion Matrix:
 [[15647   818]
 [ 4580   580]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Running/confusion_matrix_param_97.png
Accuracy:   0.7504
Precision:  0.4149
Recall:     0.1124
F1-score:   0.1769

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6874, Test Loss: 0.6960, F1: 0.1769, AUC: 0.5314
Epoch [10/30] Train Loss: 0.6865, Test Loss: 0.6826, F1: 0.1769, AUC: 0.5314
Epoch [20/30] Train Loss: 0.6866, Test Loss: 0.6865, F1: 0.1769, AUC: 0.5314
Mejores resultados en la época:  0
f1-score 0.17688319609637085
AUC según el mejor F1-score 0.5313609794796103
Confusion Matrix:
 [[15647   818]
 [ 4580   580]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Running/confusion_matrix_param_97.png
Accuracy:   0.7504
Precision:  0.4149
Recall:     0.1124
F1-score:   0.1769

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6882, Test Loss: 0.6939, F1: 0.1769, AUC: 0.5314
Epoch [10/30] Train Loss: 0.6867, Test Loss: 0.6782, F1: 0.1769, AUC: 0.5314
Epoch [20/30] Train Loss: 0.6866, Test Loss: 0.6872, F1: 0.1769, AUC: 0.5314
Mejores resultados en la época:  0
f1-score 0.17688319609637085
AUC según el mejor F1-score 0.5313609794796103
Confusion Matrix:
 [[15647   818]
 [ 4580   580]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Running/confusion_matrix_param_97.png
Accuracy:   0.7504
Precision:  0.4149
Recall:     0.1124
F1-score:   0.1769
Tiempo total para red 1: 105.81 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6885, Test Loss: 0.6542, F1: 0.1769, AUC: 0.5314
Epoch [10/30] Train Loss: 0.6866, Test Loss: 0.6850, F1: 0.1769, AUC: 0.5314
Epoch [20/30] Train Loss: 0.6866, Test Loss: 0.6843, F1: 0.1769, AUC: 0.5314
Mejores resultados en la época:  0
f1-score 0.17688319609637085
AUC según el mejor F1-score 0.5313609794796103
Confusion Matrix:
 [[15647   818]
 [ 4580   580]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Running/confusion_matrix_param_701441.png
Accuracy:   0.7504
Precision:  0.4149
Recall:     0.1124
F1-score:   0.1769

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6877, Test Loss: 0.6887, F1: 0.1769, AUC: 0.5314
Epoch [10/30] Train Loss: 0.6867, Test Loss: 0.6911, F1: 0.1769, AUC: 0.5314
Epoch [20/30] Train Loss: 0.6867, Test Loss: 0.6881, F1: 0.1769, AUC: 0.5314
Mejores resultados en la época:  7
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5313609794796103
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:31:20] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Running/confusion_matrix_param_701441.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6880, Test Loss: 0.6810, F1: 0.1769, AUC: 0.5314
Epoch [10/30] Train Loss: 0.6866, Test Loss: 0.6935, F1: 0.1769, AUC: 0.5314
Epoch [20/30] Train Loss: 0.6867, Test Loss: 0.7016, F1: 0.1769, AUC: 0.5314
Mejores resultados en la época:  0
f1-score 0.17688319609637085
AUC según el mejor F1-score 0.5313589667535317
Confusion Matrix:
 [[15647   818]
 [ 4580   580]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Running/confusion_matrix_param_701441.png
Accuracy:   0.7504
Precision:  0.4149
Recall:     0.1124
F1-score:   0.1769
Tiempo total para red 6: 170.16 segundos
Saved on: outputs_ablation_one_feature/0/Good for Running

==============================
Model: Logistic Regression
Accuracy:  0.7504
Precision: 0.4149
Recall:    0.1124
F1-score:  0.1769
              precision    recall  f1-score   support

           0       0.77      0.95      0.85     16465
           1       0.41      0.11      0.18      5160

    accuracy                           0.75     21625
   macro avg       0.59      0.53      0.51     21625
weighted avg       0.69      0.75      0.69     21625

[[15647   818]
 [ 4580   580]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Running/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Running/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.2496
Precision: 0.2264
Recall:    0.8876
F1-score:  0.3608
              precision    recall  f1-score   support

           0       0.59      0.05      0.09     16465
           1       0.23      0.89      0.36      5160

    accuracy                           0.25     21625
   macro avg       0.41      0.47      0.23     21625
weighted avg       0.50      0.25      0.16     21625

[[  818 15647]
 [  580  4580]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Running/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Running/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7504
Precision: 0.4149
Recall:    0.1124
F1-score:  0.1769
              precision    recall  f1-score   support

           0       0.77      0.95      0.85     16465
           1       0.41      0.11      0.18      5160

    accuracy                           0.75     21625
   macro avg       0.59      0.53      0.51     21625
weighted avg       0.69      0.75      0.69     21625

[[15647   818]
 [ 4580   580]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Running/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Running/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.7504
Precision: 0.4149
Recall:    0.1124
F1-score:  0.1769
              precision    recall  f1-score   support

           0       0.77      0.95      0.85     16465
           1       0.41      0.11      0.18      5160

    accuracy                           0.75     21625
   macro avg       0.59      0.53      0.51     21625
weighted avg       0.69      0.75      0.69     21625

[[15647   818]
 [ 4580   580]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Running/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Running/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.7504
Precision: 0.4149
Recall:    0.1124
F1-score:  0.1769
              precision    recall  f1-score   support

           0       0.77      0.95      0.85     16465
           1       0.41      0.11      0.18      5160

    accuracy                           0.75     21625
   macro avg       0.59      0.53      0.51     21625
weighted avg       0.69      0.75      0.69     21625

[[15647   818]
 [ 4580   580]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Running/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Running/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7504
Precision: 0.4149
Recall:    0.1124
F1-score:  0.1769
              precision    recall  f1-score   support

           0       0.77      0.95      0.85     16465
           1       0.41      0.11      0.18      5160

    accuracy                           0.75     21625
   macro avg       0.59      0.53      0.51     21625
weighted avg       0.69      0.75      0.69     21625

[[15647   818]
 [ 4580   580]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Running/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Running/naive_bayes_model.pkl


Resumen de métricas:
SVM: {'accuracy': 0.2496, 'precision': 0.2264, 'recall': 0.8876, 'f1_score': 0.3608}
Logistic Regression: {'accuracy': 0.7504, 'precision': 0.4149, 'recall': 0.1124, 'f1_score': 0.1769}
Decision Tree: {'accuracy': 0.7504, 'precision': 0.4149, 'recall': 0.1124, 'f1_score': 0.1769}
Random Forest: {'accuracy': 0.7504, 'precision': 0.4149, 'recall': 0.1124, 'f1_score': 0.1769}
XGBoost: {'accuracy': 0.7504, 'precision': 0.4149, 'recall': 0.1124, 'f1_score': 0.1769}
Naive Bayes: {'accuracy': 0.7504, 'precision': 0.4149, 'recall': 0.1124, 'f1_score': 0.1769}

##################################################
Running experiment with GOOD FOR YOGA/STRETCHING feature
[Good for Yoga/Stretching] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6898, Test Loss: 0.6921, F1: 0.3913, AUC: 0.5128
Epoch [10/30] Train Loss: 0.6873, Test Loss: 0.6804, F1: 0.0000, AUC: 0.5128
Epoch [20/30] Train Loss: 0.6872, Test Loss: 0.6890, F1: 0.3913, AUC: 0.5128
Mejores resultados en la época:  0
f1-score 0.3912878211950737
AUC según el mejor F1-score 0.5128308050668908
Confusion Matrix:
 [[  464 16001]
 [   13  5147]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Yoga/Stretching/confusion_matrix_param_97.png
Accuracy:   0.2595
Precision:  0.2434
Recall:     0.9975
F1-score:   0.3913

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6896, Test Loss: 0.6798, F1: 0.0000, AUC: 0.5128
Epoch [10/30] Train Loss: 0.6871, Test Loss: 0.6903, F1: 0.3913, AUC: 0.5128
Epoch [20/30] Train Loss: 0.6871, Test Loss: 0.6823, F1: 0.3913, AUC: 0.5128
Mejores resultados en la época:  1
f1-score 0.3912878211950737
AUC según el mejor F1-score 0.5128308050668908
Confusion Matrix:
 [[  464 16001]
 [   13  5147]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Yoga/Stretching/confusion_matrix_param_97.png
Accuracy:   0.2595
Precision:  0.2434
Recall:     0.9975
F1-score:   0.3913

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6906, Test Loss: 0.6682, F1: 0.0000, AUC: 0.5128
Epoch [10/30] Train Loss: 0.6873, Test Loss: 0.6861, F1: 0.3913, AUC: 0.5128
Epoch [20/30] Train Loss: 0.6872, Test Loss: 0.6786, F1: 0.0000, AUC: 0.5128
Mejores resultados en la época:  1
f1-score 0.3912878211950737
AUC según el mejor F1-score 0.5128308050668908
Confusion Matrix:
 [[  464 16001]
 [   13  5147]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Yoga/Stretching/confusion_matrix_param_97.png
Accuracy:   0.2595
Precision:  0.2434
Recall:     0.9975
F1-score:   0.3913
Tiempo total para red 1: 105.89 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6882, Test Loss: 0.6908, F1: 0.3913, AUC: 0.5128
Epoch [10/30] Train Loss: 0.6873, Test Loss: 0.6898, F1: 0.3913, AUC: 0.5128
Epoch [20/30] Train Loss: 0.6873, Test Loss: 0.6839, F1: 0.3913, AUC: 0.5128
Mejores resultados en la época:  0
f1-score 0.3912878211950737
AUC según el mejor F1-score 0.5128308050668908
Confusion Matrix:
 [[  464 16001]
 [   13  5147]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Yoga/Stretching/confusion_matrix_param_701441.png
Accuracy:   0.2595
Precision:  0.2434
Recall:     0.9975
F1-score:   0.3913

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6883, Test Loss: 0.6862, F1: 0.3913, AUC: 0.5128
Epoch [10/30] Train Loss: 0.6874, Test Loss: 0.6852, F1: 0.3913, AUC: 0.5128
Epoch [20/30] Train Loss: 0.6874, Test Loss: 0.6864, F1: 0.3913, AUC: 0.5128
Mejores resultados en la época:  0
f1-score 0.3912878211950737
AUC según el mejor F1-score 0.5128308050668908
Confusion Matrix:
 [[  464 16001]
 [   13  5147]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Yoga/Stretching/confusion_matrix_param_701441.png
Accuracy:   0.2595
Precision:  0.2434
Recall:     0.9975
F1-score:   0.3913

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6884, Test Loss: 0.6915, F1: 0.3913, AUC: 0.5128
Epoch [10/30] Train Loss: 0.6874, Test Loss: 0.6875, F1: 0.3913, AUC: 0.5128
Epoch [20/30] Train Loss: 0.6872, Test Loss: 0.6831, F1: 0.3913, AUC: 0.5128
Mejores resultados en la época:  0
f1-score 0.3912878211950737
AUC según el mejor F1-score 0.5128308050668908
Confusion Matrix:
 [[  464 16001]
 [   13  5147]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Yoga/Stretching/confusion_matrix_param_701441.png
Accuracy:   0.2595
Precision:  0.2434
Recall:     0.9975
F1-score:   0.3913
Tiempo total para red 6: 170.03 segundos
Saved on: outputs_ablation_one_feature/0/Good for Yoga/Stretching

==============================
Model: Logistic Regression
Accuracy:  0.2595
Precision: 0.2434
Recall:    0.9975
F1-score:  0.3913
              precision    recall  f1-score   support

           0       0.97      0.03      0.05     16465
           1       0.24      1.00      0.39      5160

    accuracy                           0.26     21625
   macro avg       0.61      0.51      0.22     21625
weighted avg       0.80      0.26      0.14     21625

[[  464 16001]
 [   13  5147]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Yoga/Stretching/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Yoga/Stretching/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.2386
Precision: 0.2386
Recall:    1.0000
F1-score:  0.3853
              precision    recall  f1-score   support

           0       0.00      0.00      0.00     16465
           1       0.24      1.00      0.39      5160

    accuracy                           0.24     21625
   macro avg       0.12      0.50      0.19     21625
weighted avg       0.06      0.24      0.09     21625

[[    0 16465]
 [    0  5160]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Yoga/Stretching/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Yoga/Stretching/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.2595
Precision: 0.2434
Recall:    0.9975
F1-score:  0.3913
              precision    recall  f1-score   support

           0       0.97      0.03      0.05     16465
           1       0.24      1.00      0.39      5160

    accuracy                           0.26     21625
   macro avg       0.61      0.51      0.22     21625
weighted avg       0.80      0.26      0.14     21625

[[  464 16001]
 [   13  5147]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Yoga/Stretching/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Yoga/Stretching/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.2595
Precision: 0.2434
Recall:    0.9975
F1-score:  0.3913
              precision    recall  f1-score   support

           0       0.97      0.03      0.05     16465
           1       0.24      1.00      0.39      5160

    accuracy                           0.26     21625
   macro avg       0.61      0.51      0.22     21625
weighted avg       0.80      0.26      0.14     21625

[[  464 16001]
 [   13  5147]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:36:01] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Yoga/Stretching/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Yoga/Stretching/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.2595
Precision: 0.2434
Recall:    0.9975
F1-score:  0.3913
              precision    recall  f1-score   support

           0       0.97      0.03      0.05     16465
           1       0.24      1.00      0.39      5160

    accuracy                           0.26     21625
   macro avg       0.61      0.51      0.22     21625
weighted avg       0.80      0.26      0.14     21625

[[  464 16001]
 [   13  5147]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Yoga/Stretching/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Yoga/Stretching/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.2595
Precision: 0.2434
Recall:    0.9975
F1-score:  0.3913
              precision    recall  f1-score   support

           0       0.97      0.03      0.05     16465
           1       0.24      1.00      0.39      5160

    accuracy                           0.26     21625
   macro avg       0.61      0.51      0.22     21625
weighted avg       0.80      0.26      0.14     21625

[[  464 16001]
 [   13  5147]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Yoga/Stretching/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Yoga/Stretching/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.2595, 'precision': 0.2434, 'recall': 0.9975, 'f1_score': 0.3913}
Decision Tree: {'accuracy': 0.2595, 'precision': 0.2434, 'recall': 0.9975, 'f1_score': 0.3913}
Random Forest: {'accuracy': 0.2595, 'precision': 0.2434, 'recall': 0.9975, 'f1_score': 0.3913}
XGBoost: {'accuracy': 0.2595, 'precision': 0.2434, 'recall': 0.9975, 'f1_score': 0.3913}
Naive Bayes: {'accuracy': 0.2595, 'precision': 0.2434, 'recall': 0.9975, 'f1_score': 0.3913}
SVM: {'accuracy': 0.2386, 'precision': 0.2386, 'recall': 1.0, 'f1_score': 0.3853}

##################################################
Running experiment with GOOD FOR DRIVING feature
[Good for Driving] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6922, Test Loss: 0.6827, F1: 0.1317, AUC: 0.5145
Epoch [10/30] Train Loss: 0.6914, Test Loss: 0.6971, F1: 0.1317, AUC: 0.5145
Epoch [20/30] Train Loss: 0.6914, Test Loss: 0.6892, F1: 0.1317, AUC: 0.5145
Mejores resultados en la época:  3
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5144646148630992
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Driving/confusion_matrix_param_97.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6924, Test Loss: 0.6820, F1: 0.1317, AUC: 0.5145
Epoch [10/30] Train Loss: 0.6915, Test Loss: 0.6845, F1: 0.1317, AUC: 0.5145
Epoch [20/30] Train Loss: 0.6914, Test Loss: 0.6899, F1: 0.1317, AUC: 0.5145
Mejores resultados en la época:  3
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5144646148630992
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Driving/confusion_matrix_param_97.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6921, Test Loss: 0.6906, F1: 0.1317, AUC: 0.5145
Epoch [10/30] Train Loss: 0.6914, Test Loss: 0.6926, F1: 0.1317, AUC: 0.5145
Epoch [20/30] Train Loss: 0.6915, Test Loss: 0.6818, F1: 0.1317, AUC: 0.5145
Mejores resultados en la época:  7
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5144646148630992
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Driving/confusion_matrix_param_97.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853
Tiempo total para red 1: 105.93 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6920, Test Loss: 0.6971, F1: 0.1317, AUC: 0.5145
Epoch [10/30] Train Loss: 0.6915, Test Loss: 0.6944, F1: 0.1317, AUC: 0.5145
Epoch [20/30] Train Loss: 0.6915, Test Loss: 0.6912, F1: 0.1317, AUC: 0.5145
Mejores resultados en la época:  0
f1-score 0.1317050548771062
AUC según el mejor F1-score 0.5144570936235425
Confusion Matrix:
 [[15582   883]
 [ 4734   426]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Driving/confusion_matrix_param_701441.png
Accuracy:   0.7403
Precision:  0.3254
Recall:     0.0826
F1-score:   0.1317

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6920, Test Loss: 0.6926, F1: 0.1317, AUC: 0.5145
Epoch [10/30] Train Loss: 0.6915, Test Loss: 0.6860, F1: 0.1317, AUC: 0.5145
Epoch [20/30] Train Loss: 0.6915, Test Loss: 0.6871, F1: 0.1317, AUC: 0.5145
Mejores resultados en la época:  0
f1-score 0.1317050548771062
AUC según el mejor F1-score 0.5144646148630992
Confusion Matrix:
 [[15582   883]
 [ 4734   426]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Driving/confusion_matrix_param_701441.png
Accuracy:   0.7403
Precision:  0.3254
Recall:     0.0826
F1-score:   0.1317

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6919, Test Loss: 0.7213, F1: 0.1317, AUC: 0.5145
Epoch [10/30] Train Loss: 0.6916, Test Loss: 0.6917, F1: 0.1317, AUC: 0.5145
Epoch [20/30] Train Loss: 0.6915, Test Loss: 0.6953, F1: 0.1317, AUC: 0.5145
Mejores resultados en la época:  1
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5144646148630992
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Driving/confusion_matrix_param_701441.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853
Tiempo total para red 6: 170.11 segundos
Saved on: outputs_ablation_one_feature/0/Good for Driving

==============================
Model: Logistic Regression
Accuracy:  0.7403
Precision: 0.3254
Recall:    0.0826
F1-score:  0.1317
              precision    recall  f1-score   support

           0       0.77      0.95      0.85     16465
           1       0.33      0.08      0.13      5160

    accuracy                           0.74     21625
   macro avg       0.55      0.51      0.49     21625
weighted avg       0.66      0.74      0.68     21625

[[15582   883]
 [ 4734   426]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:40:42] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Driving/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Driving/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7403
Precision: 0.3254
Recall:    0.0826
F1-score:  0.1317
              precision    recall  f1-score   support

           0       0.77      0.95      0.85     16465
           1       0.33      0.08      0.13      5160

    accuracy                           0.74     21625
   macro avg       0.55      0.51      0.49     21625
weighted avg       0.66      0.74      0.68     21625

[[15582   883]
 [ 4734   426]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Driving/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Driving/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7403
Precision: 0.3254
Recall:    0.0826
F1-score:  0.1317
              precision    recall  f1-score   support

           0       0.77      0.95      0.85     16465
           1       0.33      0.08      0.13      5160

    accuracy                           0.74     21625
   macro avg       0.55      0.51      0.49     21625
weighted avg       0.66      0.74      0.68     21625

[[15582   883]
 [ 4734   426]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Driving/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Driving/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.7403
Precision: 0.3254
Recall:    0.0826
F1-score:  0.1317
              precision    recall  f1-score   support

           0       0.77      0.95      0.85     16465
           1       0.33      0.08      0.13      5160

    accuracy                           0.74     21625
   macro avg       0.55      0.51      0.49     21625
weighted avg       0.66      0.74      0.68     21625

[[15582   883]
 [ 4734   426]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Driving/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Driving/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.7403
Precision: 0.3254
Recall:    0.0826
F1-score:  0.1317
              precision    recall  f1-score   support

           0       0.77      0.95      0.85     16465
           1       0.33      0.08      0.13      5160

    accuracy                           0.74     21625
   macro avg       0.55      0.51      0.49     21625
weighted avg       0.66      0.74      0.68     21625

[[15582   883]
 [ 4734   426]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Driving/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Driving/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7403
Precision: 0.3254
Recall:    0.0826
F1-score:  0.1317
              precision    recall  f1-score   support

           0       0.77      0.95      0.85     16465
           1       0.33      0.08      0.13      5160

    accuracy                           0.74     21625
   macro avg       0.55      0.51      0.49     21625
weighted avg       0.66      0.74      0.68     21625

[[15582   883]
 [ 4734   426]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Driving/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Driving/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.7403, 'precision': 0.3254, 'recall': 0.0826, 'f1_score': 0.1317}
SVM: {'accuracy': 0.7403, 'precision': 0.3254, 'recall': 0.0826, 'f1_score': 0.1317}
Decision Tree: {'accuracy': 0.7403, 'precision': 0.3254, 'recall': 0.0826, 'f1_score': 0.1317}
Random Forest: {'accuracy': 0.7403, 'precision': 0.3254, 'recall': 0.0826, 'f1_score': 0.1317}
XGBoost: {'accuracy': 0.7403, 'precision': 0.3254, 'recall': 0.0826, 'f1_score': 0.1317}
Naive Bayes: {'accuracy': 0.7403, 'precision': 0.3254, 'recall': 0.0826, 'f1_score': 0.1317}

##################################################
Running experiment with GOOD FOR SOCIAL GATHERINGS feature
[Good for Social Gatherings] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6931, Test Loss: 0.6886, F1: 0.0790, AUC: 0.5059
Epoch [10/30] Train Loss: 0.6930, Test Loss: 0.6912, F1: 0.0790, AUC: 0.5059
Epoch [20/30] Train Loss: 0.6929, Test Loss: 0.6921, F1: 0.0790, AUC: 0.5059
Mejores resultados en la época:  2
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5059477526912856
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Social Gatherings/confusion_matrix_param_97.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6934, Test Loss: 0.7004, F1: 0.3853, AUC: 0.5059
Epoch [10/30] Train Loss: 0.6930, Test Loss: 0.6991, F1: 0.3853, AUC: 0.5059
Epoch [20/30] Train Loss: 0.6930, Test Loss: 0.6770, F1: 0.0790, AUC: 0.5059
Mejores resultados en la época:  0
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5059477526912856
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Social Gatherings/confusion_matrix_param_97.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6931, Test Loss: 0.6914, F1: 0.0790, AUC: 0.5059
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:45:23] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [10/30] Train Loss: 0.6930, Test Loss: 0.6884, F1: 0.0790, AUC: 0.5059
Epoch [20/30] Train Loss: 0.6928, Test Loss: 0.6926, F1: 0.0790, AUC: 0.5059
Mejores resultados en la época:  1
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5059477526912856
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Social Gatherings/confusion_matrix_param_97.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853
Tiempo total para red 1: 105.76 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6933, Test Loss: 0.6925, F1: 0.0790, AUC: 0.5059
Epoch [10/30] Train Loss: 0.6929, Test Loss: 0.6894, F1: 0.0790, AUC: 0.5059
Epoch [20/30] Train Loss: 0.6929, Test Loss: 0.6946, F1: 0.0790, AUC: 0.5059
Mejores resultados en la época:  1
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5059426149431376
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Social Gatherings/confusion_matrix_param_701441.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6932, Test Loss: 0.6912, F1: 0.0790, AUC: 0.5059
Epoch [10/30] Train Loss: 0.6929, Test Loss: 0.6956, F1: 0.3853, AUC: 0.5059
Epoch [20/30] Train Loss: 0.6929, Test Loss: 0.6924, F1: 0.0790, AUC: 0.5059
Mejores resultados en la época:  2
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5059477526912856
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Social Gatherings/confusion_matrix_param_701441.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6932, Test Loss: 0.6949, F1: 0.3853, AUC: 0.5059
Epoch [10/30] Train Loss: 0.6929, Test Loss: 0.6958, F1: 0.3853, AUC: 0.5059
Epoch [20/30] Train Loss: 0.6929, Test Loss: 0.6896, F1: 0.0790, AUC: 0.5059
Mejores resultados en la época:  0
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5059477526912856
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Social Gatherings/confusion_matrix_param_701441.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853
Tiempo total para red 6: 169.88 segundos
Saved on: outputs_ablation_one_feature/0/Good for Social Gatherings

==============================
Model: Logistic Regression
Accuracy:  0.7466
Precision: 0.2978
Recall:    0.0455
F1-score:  0.0790
              precision    recall  f1-score   support

           0       0.76      0.97      0.85     16465
           1       0.30      0.05      0.08      5160

    accuracy                           0.75     21625
   macro avg       0.53      0.51      0.47     21625
weighted avg       0.65      0.75      0.67     21625

[[15911   554]
 [ 4925   235]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Social Gatherings/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Social Gatherings/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7614
Precision: 0.0000
Recall:    0.0000
F1-score:  0.0000
              precision    recall  f1-score   support

           0       0.76      1.00      0.86     16465
           1       0.00      0.00      0.00      5160

    accuracy                           0.76     21625
   macro avg       0.38      0.50      0.43     21625
weighted avg       0.58      0.76      0.66     21625

[[16465     0]
 [ 5160     0]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Social Gatherings/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Social Gatherings/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7466
Precision: 0.2978
Recall:    0.0455
F1-score:  0.0790
              precision    recall  f1-score   support

           0       0.76      0.97      0.85     16465
           1       0.30      0.05      0.08      5160

    accuracy                           0.75     21625
   macro avg       0.53      0.51      0.47     21625
weighted avg       0.65      0.75      0.67     21625

[[15911   554]
 [ 4925   235]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Social Gatherings/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Social Gatherings/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.7466
Precision: 0.2978
Recall:    0.0455
F1-score:  0.0790
              precision    recall  f1-score   support

           0       0.76      0.97      0.85     16465
           1       0.30      0.05      0.08      5160

    accuracy                           0.75     21625
   macro avg       0.53      0.51      0.47     21625
weighted avg       0.65      0.75      0.67     21625

[[15911   554]
 [ 4925   235]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Social Gatherings/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Social Gatherings/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.7466
Precision: 0.2978
Recall:    0.0455
F1-score:  0.0790
              precision    recall  f1-score   support

           0       0.76      0.97      0.85     16465
           1       0.30      0.05      0.08      5160

    accuracy                           0.75     21625
   macro avg       0.53      0.51      0.47     21625
weighted avg       0.65      0.75      0.67     21625

[[15911   554]
 [ 4925   235]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Social Gatherings/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Social Gatherings/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7466
Precision: 0.2978
Recall:    0.0455
F1-score:  0.0790
              precision    recall  f1-score   support

           0       0.76      0.97      0.85     16465
           1       0.30      0.05      0.08      5160

    accuracy                           0.75     21625
   macro avg       0.53      0.51      0.47     21625
weighted avg       0.65      0.75      0.67     21625

[[15911   554]
 [ 4925   235]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Social Gatherings/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Social Gatherings/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.7466, 'precision': 0.2978, 'recall': 0.0455, 'f1_score': 0.079}
Decision Tree: {'accuracy': 0.7466, 'precision': 0.2978, 'recall': 0.0455, 'f1_score': 0.079}
Random Forest: {'accuracy': 0.7466, 'precision': 0.2978, 'recall': 0.0455, 'f1_score': 0.079}
XGBoost: {'accuracy': 0.7466, 'precision': 0.2978, 'recall': 0.0455, 'f1_score': 0.079}
Naive Bayes: {'accuracy': 0.7466, 'precision': 0.2978, 'recall': 0.0455, 'f1_score': 0.079}
SVM: {'accuracy': 0.7614, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}

##################################################
Running experiment with GOOD FOR MORNING ROUTINE feature
[Good for Morning Routine] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6938, Test Loss: 0.6952, F1: 0.3853, AUC: 0.5016
Epoch [10/30] Train Loss: 0.6932, Test Loss: 0.6818, F1: 0.0000, AUC: 0.5016
Epoch [20/30] Train Loss: 0.6932, Test Loss: 0.6826, F1: 0.0000, AUC: 0.5016
Mejores resultados en la época:  0
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5016186554989795
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Morning Routine/confusion_matrix_param_97.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6935, Test Loss: 0.6856, F1: 0.1144, AUC: 0.5016
Epoch [10/30] Train Loss: 0.6933, Test Loss: 0.6876, F1: 0.0000, AUC: 0.5016
Epoch [20/30] Train Loss: 0.6932, Test Loss: 0.6918, F1: 0.1144, AUC: 0.5016
Mejores resultados en la época:  1
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5016186554989795
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Morning Routine/confusion_matrix_param_97.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6935, Test Loss: 0.7052, F1: 0.3853, AUC: 0.4984
Epoch [10/30] Train Loss: 0.6933, Test Loss: 0.6994, F1: 0.3786, AUC: 0.4984
Epoch [20/30] Train Loss: 0.6933, Test Loss: 0.6864, F1: 0.0000, AUC: 0.5016
Mejores resultados en la época:  0
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.4983813445010205
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Morning Routine/confusion_matrix_param_97.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853
Tiempo total para red 1: 105.78 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6933, Test Loss: 0.6961, F1: 0.3853, AUC: 0.5000
Epoch [10/30] Train Loss: 0.6932, Test Loss: 0.6946, F1: 0.3853, AUC: 0.5000
Epoch [20/30] Train Loss: 0.6932, Test Loss: 0.6944, F1: 0.3853, AUC: 0.5000
Mejores resultados en la época:  0
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Morning Routine/confusion_matrix_param_701441.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6935, Test Loss: 0.6936, F1: 0.3853, AUC: 0.5016
Epoch [10/30] Train Loss: 0.6932, Test Loss: 0.6951, F1: 0.3853, AUC: 0.5000
Epoch [20/30] Train Loss: 0.6932, Test Loss: 0.6940, F1: 0.3853, AUC: 0.5000
Mejores resultados en la época:  0
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5016186554989795
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Morning Routine/confusion_matrix_param_701441.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6932, Test Loss: 0.6927, F1: 0.0000, AUC: 0.4984
Epoch [10/30] Train Loss: 0.6932, Test Loss: 0.6966, F1: 0.3853, AUC: 0.5000
Epoch [20/30] Train Loss: 0.6932, Test Loss: 0.6927, F1: 0.0000, AUC: 0.5000
Mejores resultados en la época:  1
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.4983813445010205
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Good for Morning Routine/confusion_matrix_param_701441.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853
Tiempo total para red 6: 169.43 segundos
Saved on: outputs_ablation_one_feature/0/Good for Morning Routine

==============================
Model: Logistic Regression
Accuracy:  0.7249
Precision: 0.2468
Recall:    0.0744
F1-score:  0.1144
              precision    recall  f1-score   support

           0       0.76      0.93      0.84     16465
           1       0.25      0.07      0.11      5160

    accuracy                           0.72     21625
   macro avg       0.50      0.50      0.48     21625
weighted avg       0.64      0.72      0.66     21625

[[15293  1172]
 [ 4776   384]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Morning Routine/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Morning Routine/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7614
Precision: 0.0000
Recall:    0.0000
F1-score:  0.0000
              precision    recall  f1-score   support

           0       0.76      1.00      0.86     16465
           1       0.00      0.00      0.00      5160

    accuracy                           0.76     21625
   macro avg       0.38      0.50      0.43     21625
weighted avg       0.58      0.76      0.66     21625

[[16465     0]
 [ 5160     0]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Morning Routine/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Morning Routine/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7249
Precision: 0.2468
Recall:    0.0744
F1-score:  0.1144
              precision    recall  f1-score   support

           0       0.76      0.93      0.84     16465
           1       0.25      0.07      0.11      5160

    accuracy                           0.72     21625
   macro avg       0.50      0.50      0.48     21625
weighted avg       0.64      0.72      0.66     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:50:04] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[15293  1172]
 [ 4776   384]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Morning Routine/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Morning Routine/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.7249
Precision: 0.2468
Recall:    0.0744
F1-score:  0.1144
              precision    recall  f1-score   support

           0       0.76      0.93      0.84     16465
           1       0.25      0.07      0.11      5160

    accuracy                           0.72     21625
   macro avg       0.50      0.50      0.48     21625
weighted avg       0.64      0.72      0.66     21625

[[15293  1172]
 [ 4776   384]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Morning Routine/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Morning Routine/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.7249
Precision: 0.2468
Recall:    0.0744
F1-score:  0.1144
              precision    recall  f1-score   support

           0       0.76      0.93      0.84     16465
           1       0.25      0.07      0.11      5160

    accuracy                           0.72     21625
   macro avg       0.50      0.50      0.48     21625
weighted avg       0.64      0.72      0.66     21625

[[15293  1172]
 [ 4776   384]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Morning Routine/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Morning Routine/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7249
Precision: 0.2468
Recall:    0.0744
F1-score:  0.1144
              precision    recall  f1-score   support

           0       0.76      0.93      0.84     16465
           1       0.25      0.07      0.11      5160

    accuracy                           0.72     21625
   macro avg       0.50      0.50      0.48     21625
weighted avg       0.64      0.72      0.66     21625

[[15293  1172]
 [ 4776   384]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Good for Morning Routine/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Good for Morning Routine/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.7249, 'precision': 0.2468, 'recall': 0.0744, 'f1_score': 0.1144}
Decision Tree: {'accuracy': 0.7249, 'precision': 0.2468, 'recall': 0.0744, 'f1_score': 0.1144}
Random Forest: {'accuracy': 0.7249, 'precision': 0.2468, 'recall': 0.0744, 'f1_score': 0.1144}
XGBoost: {'accuracy': 0.7249, 'precision': 0.2468, 'recall': 0.0744, 'f1_score': 0.1144}
Naive Bayes: {'accuracy': 0.7249, 'precision': 0.2468, 'recall': 0.0744, 'f1_score': 0.1144}
SVM: {'accuracy': 0.7614, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}

##################################################
Running experiment with RELEASE_YEAR feature
[Release_Year] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6836, Test Loss: 0.6833, F1: 0.4391, AUC: 0.6537
Epoch [10/30] Train Loss: 0.6319, Test Loss: 0.6388, F1: 0.4575, AUC: 0.6537
Epoch [20/30] Train Loss: 0.6316, Test Loss: 0.6403, F1: 0.4575, AUC: 0.6537
Mejores resultados en la época:  16
f1-score 0.459931155262999
AUC según el mejor F1-score 0.6537300993180272
Confusion Matrix:
 [[8874 7591]
 [1352 3808]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Release_Year/confusion_matrix_param_97.png
Accuracy:   0.5865
Precision:  0.3341
Recall:     0.7380
F1-score:   0.4599

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6690, Test Loss: 0.6669, F1: 0.4478, AUC: 0.6537
Epoch [10/30] Train Loss: 0.6310, Test Loss: 0.6156, F1: 0.4587, AUC: 0.6537
Epoch [20/30] Train Loss: 0.6306, Test Loss: 0.6459, F1: 0.4543, AUC: 0.6537
Mejores resultados en la época:  1
f1-score 0.45873913804163996
AUC según el mejor F1-score 0.6537300993180272
Confusion Matrix:
 [[8411 8054]
 [1227 3933]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Release_Year/confusion_matrix_param_97.png
Accuracy:   0.5708
Precision:  0.3281
Recall:     0.7622
F1-score:   0.4587

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6937, Test Loss: 0.6889, F1: 0.4543, AUC: 0.6537
Epoch [10/30] Train Loss: 0.6322, Test Loss: 0.6225, F1: 0.4587, AUC: 0.6537
Epoch [20/30] Train Loss: 0.6317, Test Loss: 0.6505, F1: 0.4543, AUC: 0.6537
Mejores resultados en la época:  3
f1-score 0.45873913804163996
AUC según el mejor F1-score 0.6537300993180272
Confusion Matrix:
 [[8411 8054]
 [1227 3933]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Release_Year/confusion_matrix_param_97.png
Accuracy:   0.5708
Precision:  0.3281
Recall:     0.7622
F1-score:   0.4587
Tiempo total para red 1: 105.90 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6356, Test Loss: 0.6847, F1: 0.4454, AUC: 0.6531
Epoch [10/30] Train Loss: 0.6151, Test Loss: 0.5969, F1: 0.4599, AUC: 0.6859
Epoch [20/30] Train Loss: 0.6135, Test Loss: 0.6258, F1: 0.4582, AUC: 0.6853
Mejores resultados en la época:  5
f1-score 0.459931155262999
AUC según el mejor F1-score 0.6831582967864651
Confusion Matrix:
 [[8874 7591]
 [1352 3808]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Release_Year/confusion_matrix_param_701441.png
Accuracy:   0.5865
Precision:  0.3341
Recall:     0.7380
F1-score:   0.4599

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6392, Test Loss: 0.6138, F1: 0.4599, AUC: 0.6537
Epoch [10/30] Train Loss: 0.6170, Test Loss: 0.6163, F1: 0.4436, AUC: 0.6499
Epoch [20/30] Train Loss: 0.6131, Test Loss: 0.6353, F1: 0.4599, AUC: 0.6838
Mejores resultados en la época:  0
f1-score 0.459931155262999
AUC según el mejor F1-score 0.6537294166390064
Confusion Matrix:
 [[8874 7591]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:54:47] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
 [1352 3808]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Release_Year/confusion_matrix_param_701441.png
Accuracy:   0.5865
Precision:  0.3341
Recall:     0.7380
F1-score:   0.4599

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6347, Test Loss: 0.6358, F1: 0.4599, AUC: 0.6537
Epoch [10/30] Train Loss: 0.6154, Test Loss: 0.6032, F1: 0.4582, AUC: 0.6874
Epoch [20/30] Train Loss: 0.6126, Test Loss: 0.6253, F1: 0.4599, AUC: 0.6882
Mejores resultados en la época:  0
f1-score 0.459931155262999
AUC según el mejor F1-score 0.6537201416205859
Confusion Matrix:
 [[8874 7591]
 [1352 3808]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Release_Year/confusion_matrix_param_701441.png
Accuracy:   0.5865
Precision:  0.3341
Recall:     0.7380
F1-score:   0.4599
Tiempo total para red 6: 170.36 segundos
Saved on: outputs_ablation_one_feature/0/Release_Year

==============================
Model: Logistic Regression
Accuracy:  0.5568
Precision: 0.3231
Recall:    0.7833
F1-score:  0.4575
              precision    recall  f1-score   support

           0       0.88      0.49      0.63     16465
           1       0.32      0.78      0.46      5160

    accuracy                           0.56     21625
   macro avg       0.60      0.63      0.54     21625
weighted avg       0.75      0.56      0.59     21625

[[7998 8467]
 [1118 4042]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Release_Year/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Release_Year/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.5846
Precision: 0.1730
Recall:    0.1959
F1-score:  0.1838
              precision    recall  f1-score   support

           0       0.74      0.71      0.72     16465
           1       0.17      0.20      0.18      5160

    accuracy                           0.58     21625
   macro avg       0.46      0.45      0.45     21625
weighted avg       0.60      0.58      0.59     21625

[[11632  4833]
 [ 4149  1011]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Release_Year/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Release_Year/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.5865
Precision: 0.3356
Recall:    0.7479
F1-score:  0.4633
              precision    recall  f1-score   support

           0       0.87      0.54      0.66     16465
           1       0.34      0.75      0.46      5160

    accuracy                           0.59     21625
   macro avg       0.60      0.64      0.56     21625
weighted avg       0.74      0.59      0.62     21625

[[8824 7641]
 [1301 3859]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Release_Year/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Release_Year/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.5865
Precision: 0.3356
Recall:    0.7479
F1-score:  0.4633
              precision    recall  f1-score   support

           0       0.87      0.54      0.66     16465
           1       0.34      0.75      0.46      5160

    accuracy                           0.59     21625
   macro avg       0.60      0.64      0.56     21625
weighted avg       0.74      0.59      0.62     21625

[[8824 7641]
 [1301 3859]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Release_Year/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Release_Year/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.5865
Precision: 0.3356
Recall:    0.7479
F1-score:  0.4633
              precision    recall  f1-score   support

           0       0.87      0.54      0.66     16465
           1       0.34      0.75      0.46      5160

    accuracy                           0.59     21625
   macro avg       0.60      0.64      0.56     21625
weighted avg       0.74      0.59      0.62     21625

[[8824 7641]
 [1301 3859]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Release_Year/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Release_Year/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.4781
Precision: 0.2945
Recall:    0.8510
F1-score:  0.4376
              precision    recall  f1-score   support

           0       0.89      0.36      0.51     16465
           1       0.29      0.85      0.44      5160

    accuracy                           0.48     21625
   macro avg       0.59      0.61      0.48     21625
weighted avg       0.74      0.48      0.50     21625

[[ 5948 10517]
 [  769  4391]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Release_Year/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Release_Year/naive_bayes_model.pkl


Resumen de métricas:
Decision Tree: {'accuracy': 0.5865, 'precision': 0.3356, 'recall': 0.7479, 'f1_score': 0.4633}
Random Forest: {'accuracy': 0.5865, 'precision': 0.3356, 'recall': 0.7479, 'f1_score': 0.4633}
XGBoost: {'accuracy': 0.5865, 'precision': 0.3356, 'recall': 0.7479, 'f1_score': 0.4633}
Logistic Regression: {'accuracy': 0.5568, 'precision': 0.3231, 'recall': 0.7833, 'f1_score': 0.4575}
Naive Bayes: {'accuracy': 0.4781, 'precision': 0.2945, 'recall': 0.851, 'f1_score': 0.4376}
SVM: {'accuracy': 0.5846, 'precision': 0.173, 'recall': 0.1959, 'f1_score': 0.1838}

##################################################
Running experiment with RELEASE_MONTH feature
[Release_Month] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [17:59:31] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6922, Test Loss: 0.6865, F1: 0.3184, AUC: 0.5362
Epoch [10/30] Train Loss: 0.6894, Test Loss: 0.6873, F1: 0.3184, AUC: 0.5362
Epoch [20/30] Train Loss: 0.6883, Test Loss: 0.6854, F1: 0.3247, AUC: 0.5392
Mejores resultados en la época:  18
f1-score 0.3865290235234388
AUC según el mejor F1-score 0.5377115481041532
Confusion Matrix:
 [[ 2557 13908]
 [  592  4568]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Release_Month/confusion_matrix_param_97.png
Accuracy:   0.3295
Precision:  0.2472
Recall:     0.8853
F1-score:   0.3865

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6918, Test Loss: 0.6988, F1: 0.3570, AUC: 0.5362
Epoch [10/30] Train Loss: 0.6904, Test Loss: 0.6834, F1: 0.3184, AUC: 0.5362
Epoch [20/30] Train Loss: 0.6889, Test Loss: 0.6916, F1: 0.3769, AUC: 0.5362
Mejores resultados en la época:  15
f1-score 0.38295976115146385
AUC según el mejor F1-score 0.5362193412382856
Confusion Matrix:
 [[ 3481 12984]
 [  863  4297]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Release_Month/confusion_matrix_param_97.png
Accuracy:   0.3597
Precision:  0.2487
Recall:     0.8328
F1-score:   0.3830

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6921, Test Loss: 0.6907, F1: 0.3570, AUC: 0.5362
Epoch [10/30] Train Loss: 0.6888, Test Loss: 0.6926, F1: 0.3865, AUC: 0.5407
Epoch [20/30] Train Loss: 0.6882, Test Loss: 0.7022, F1: 0.3865, AUC: 0.5383
Mejores resultados en la época:  8
f1-score 0.3865290235234388
AUC según el mejor F1-score 0.5401699105690483
Confusion Matrix:
 [[ 2557 13908]
 [  592  4568]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Release_Month/confusion_matrix_param_97.png
Accuracy:   0.3295
Precision:  0.2472
Recall:     0.8853
F1-score:   0.3865
Tiempo total para red 1: 106.21 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6923, Test Loss: 0.7074, F1: 0.3570, AUC: 0.5362
Epoch [10/30] Train Loss: 0.6902, Test Loss: 0.6854, F1: 0.2144, AUC: 0.5229
Epoch [20/30] Train Loss: 0.6885, Test Loss: 0.6918, F1: 0.2144, AUC: 0.5388
Mejores resultados en la época:  18
f1-score 0.3865290235234388
AUC según el mejor F1-score 0.5341285308041253
Confusion Matrix:
 [[ 2557 13908]
 [  592  4568]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Release_Month/confusion_matrix_param_701441.png
Accuracy:   0.3295
Precision:  0.2472
Recall:     0.8853
F1-score:   0.3865

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6922, Test Loss: 0.6864, F1: 0.3570, AUC: 0.5362
Epoch [10/30] Train Loss: 0.6884, Test Loss: 0.6756, F1: 0.3418, AUC: 0.5365
Epoch [20/30] Train Loss: 0.6881, Test Loss: 0.6964, F1: 0.3713, AUC: 0.5392
Mejores resultados en la época:  3
f1-score 0.3865290235234388
AUC según el mejor F1-score 0.5362193412382856
Confusion Matrix:
 [[ 2557 13908]
 [  592  4568]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Release_Month/confusion_matrix_param_701441.png
Accuracy:   0.3295
Precision:  0.2472
Recall:     0.8853
F1-score:   0.3865

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6926, Test Loss: 0.7148, F1: 0.3830, AUC: 0.5362
Epoch [10/30] Train Loss: 0.6883, Test Loss: 0.6938, F1: 0.3713, AUC: 0.5383
Epoch [20/30] Train Loss: 0.6892, Test Loss: 0.6977, F1: 0.3677, AUC: 0.5305
Mejores resultados en la época:  1
f1-score 0.3865290235234388
AUC según el mejor F1-score 0.5362193412382856
Confusion Matrix:
 [[ 2557 13908]
 [  592  4568]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Release_Month/confusion_matrix_param_701441.png
Accuracy:   0.3295
Precision:  0.2472
Recall:     0.8853
F1-score:   0.3865
Tiempo total para red 6: 170.43 segundos
Saved on: outputs_ablation_one_feature/0/Release_Month

==============================
Model: Logistic Regression
Accuracy:  0.5109
Precision: 0.2527
Recall:    0.5364
F1-score:  0.3436
              precision    recall  f1-score   support

           0       0.78      0.50      0.61     16465
           1       0.25      0.54      0.34      5160

    accuracy                           0.51     21625
   macro avg       0.51      0.52      0.48     21625
weighted avg       0.65      0.51      0.55     21625

[[8281 8184]
 [2392 2768]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Release_Month/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Release_Month/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.5800
Precision: 0.2401
Recall:    0.3510
F1-score:  0.2851
              precision    recall  f1-score   support

           0       0.76      0.65      0.70     16465
           1       0.24      0.35      0.29      5160

    accuracy                           0.58     21625
   macro avg       0.50      0.50      0.49     21625
weighted avg       0.64      0.58      0.60     21625

[[10732  5733]
 [ 3349  1811]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Release_Month/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Release_Month/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.4781
Precision: 0.2545
Recall:    0.6153
F1-score:  0.3600
              precision    recall  f1-score   support

           0       0.78      0.44      0.56     16465
           1       0.25      0.62      0.36      5160

    accuracy                           0.48     21625
   macro avg       0.52      0.53      0.46     21625
weighted avg       0.66      0.48      0.51     21625

[[7163 9302]
 [1985 3175]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Release_Month/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Release_Month/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.4781
Precision: 0.2545
Recall:    0.6153
F1-score:  0.3600
              precision    recall  f1-score   support

           0       0.78      0.44      0.56     16465
           1       0.25      0.62      0.36      5160

    accuracy                           0.48     21625
   macro avg       0.52      0.53      0.46     21625
weighted avg       0.66      0.48      0.51     21625

[[7163 9302]
 [1985 3175]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Release_Month/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Release_Month/random_forest_model.pkl

==============================
Model: XGBoost
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__ablation_study_tfidf/ablation_study_only_one_feature.py:379: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Accuracy:  0.4781
Precision: 0.2545
Recall:    0.6153
F1-score:  0.3600
              precision    recall  f1-score   support

           0       0.78      0.44      0.56     16465
           1       0.25      0.62      0.36      5160

    accuracy                           0.48     21625
   macro avg       0.52      0.53      0.46     21625
weighted avg       0.66      0.48      0.51     21625

[[7163 9302]
 [1985 3175]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Release_Month/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Release_Month/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.5109
Precision: 0.2527
Recall:    0.5364
F1-score:  0.3436
              precision    recall  f1-score   support

           0       0.78      0.50      0.61     16465
           1       0.25      0.54      0.34      5160

    accuracy                           0.51     21625
   macro avg       0.51      0.52      0.48     21625
weighted avg       0.65      0.51      0.55     21625

[[8281 8184]
 [2392 2768]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Release_Month/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Release_Month/naive_bayes_model.pkl


Resumen de métricas:
Decision Tree: {'accuracy': 0.4781, 'precision': 0.2545, 'recall': 0.6153, 'f1_score': 0.36}
Random Forest: {'accuracy': 0.4781, 'precision': 0.2545, 'recall': 0.6153, 'f1_score': 0.36}
XGBoost: {'accuracy': 0.4781, 'precision': 0.2545, 'recall': 0.6153, 'f1_score': 0.36}
Logistic Regression: {'accuracy': 0.5109, 'precision': 0.2527, 'recall': 0.5364, 'f1_score': 0.3436}
Naive Bayes: {'accuracy': 0.5109, 'precision': 0.2527, 'recall': 0.5364, 'f1_score': 0.3436}
SVM: {'accuracy': 0.58, 'precision': 0.2401, 'recall': 0.351, 'f1_score': 0.2851}

##################################################
Running experiment with RELEASE_DAY feature
[Release_Day] in N_cols
Numeric features shape: (108125, 1)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 1)
X_train_Numeric:  (21625, 1)
==================================================
Data antes del undersampling ...
X: (86500, 1)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1, 32, 1]

--- Iteración 1 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6917, Test Loss: 0.6932, F1: 0.3732, AUC: 0.5247
Epoch [10/30] Train Loss: 0.6875, Test Loss: 0.6885, F1: 0.3893, AUC: 0.5611
Epoch [20/30] Train Loss: 0.6867, Test Loss: 0.6914, F1: 0.3892, AUC: 0.5597
Mejores resultados en la época:  17
f1-score 0.39259162645102386
AUC según el mejor F1-score 0.5586824706859983
Confusion Matrix:
 [[ 3139 13326]
 [  645  4515]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Release_Day/confusion_matrix_param_97.png
Accuracy:   0.3539
Precision:  0.2531
Recall:     0.8750
F1-score:   0.3926

--- Iteración 2 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6940, Test Loss: 0.7061, F1: 0.3757, AUC: 0.5247
Epoch [10/30] Train Loss: 0.6875, Test Loss: 0.6901, F1: 0.3878, AUC: 0.5567
Epoch [20/30] Train Loss: 0.6868, Test Loss: 0.6912, F1: 0.3888, AUC: 0.5592
Mejores resultados en la época:  6
f1-score 0.389334760121277
AUC según el mejor F1-score 0.5541762241729579
Confusion Matrix:
 [[ 3563 12902]
 [  794  4366]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Release_Day/confusion_matrix_param_97.png
Accuracy:   0.3667
Precision:  0.2528
Recall:     0.8461
F1-score:   0.3893

--- Iteración 3 de 3 para la red 1 ---
Epoch [0/30] Train Loss: 0.6926, Test Loss: 0.6853, F1: 0.3204, AUC: 0.5247
Epoch [10/30] Train Loss: 0.6874, Test Loss: 0.6859, F1: 0.3849, AUC: 0.5583
Epoch [20/30] Train Loss: 0.6866, Test Loss: 0.6920, F1: 0.3892, AUC: 0.5632
Mejores resultados en la época:  19
f1-score 0.3891733029958286
AUC según el mejor F1-score 0.5640713976322809
Confusion Matrix:
 [[ 4634 11831]
 [ 1055  4105]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Release_Day/confusion_matrix_param_97.png
Accuracy:   0.4041
Precision:  0.2576
Recall:     0.7955
F1-score:   0.3892
Tiempo total para red 1: 105.93 segundos

Entrenando red 6 con capas [1, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6927, Test Loss: 0.6674, F1: 0.2700, AUC: 0.5248
Epoch [10/30] Train Loss: 0.6870, Test Loss: 0.6727, F1: 0.3878, AUC: 0.5590
Epoch [20/30] Train Loss: 0.6858, Test Loss: 0.6948, F1: 0.3858, AUC: 0.5643
Mejores resultados en la época:  4
f1-score 0.39624724061810157
AUC según el mejor F1-score 0.5458770483313207
Confusion Matrix:
 [[ 2736 13729]
 [  493  4667]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Release_Day/confusion_matrix_param_701441.png
Accuracy:   0.3423
Precision:  0.2537
Recall:     0.9045
F1-score:   0.3962

--- Iteración 2 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6917, Test Loss: 0.7040, F1: 0.3853, AUC: 0.5247
Epoch [10/30] Train Loss: 0.6871, Test Loss: 0.6942, F1: 0.3962, AUC: 0.5579
Epoch [20/30] Train Loss: 0.6857, Test Loss: 0.6805, F1: 0.3779, AUC: 0.5621
Mejores resultados en la época:  3
f1-score 0.39624724061810157
AUC según el mejor F1-score 0.5535842590696262
Confusion Matrix:
 [[ 2736 13729]
 [  493  4667]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Release_Day/confusion_matrix_param_701441.png
Accuracy:   0.3423
Precision:  0.2537
Recall:     0.9045
F1-score:   0.3962

--- Iteración 3 de 3 para la red 6 ---
Epoch [0/30] Train Loss: 0.6926, Test Loss: 0.6977, F1: 0.3926, AUC: 0.5247
Epoch [10/30] Train Loss: 0.6870, Test Loss: 0.6819, F1: 0.3962, AUC: 0.5612
Epoch [20/30] Train Loss: 0.6860, Test Loss: 0.6855, F1: 0.3962, AUC: 0.5606
Mejores resultados en la época:  6
f1-score 0.39624724061810157
AUC según el mejor F1-score 0.5467157312787048
Confusion Matrix:
 [[ 2736 13729]
 [  493  4667]]
Matriz de confusión guardada en: outputs_ablation_one_feature/0/Release_Day/confusion_matrix_param_701441.png
Accuracy:   0.3423
Precision:  0.2537
Recall:     0.9045
F1-score:   0.3962
Tiempo total para red 6: 170.40 segundos
Saved on: outputs_ablation_one_feature/0/Release_Day

==============================
Model: Logistic Regression
Accuracy:  0.5089
Precision: 0.2392
Recall:    0.4851
F1-score:  0.3204
              precision    recall  f1-score   support

           0       0.76      0.52      0.62     16465
           1       0.24      0.49      0.32      5160

    accuracy                           0.51     21625
   macro avg       0.50      0.50      0.47     21625
weighted avg       0.64      0.51      0.55     21625

[[8502 7963]
 [2657 2503]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [18:04:14] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Confusion matrix saved as: outputs_ablation_one_feature/0/Release_Day/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_ablation_one_feature/0/Release_Day/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.4154
Precision: 0.2265
Recall:    0.6006
F1-score:  0.3290
              precision    recall  f1-score   support

           0       0.74      0.36      0.48     16465
           1       0.23      0.60      0.33      5160

    accuracy                           0.42     21625
   macro avg       0.48      0.48      0.41     21625
weighted avg       0.62      0.42      0.45     21625

[[ 5883 10582]
 [ 2061  3099]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Release_Day/conf_matrix_svm.png
Modelo guardado como: outputs_ablation_one_feature/0/Release_Day/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.5160
Precision: 0.2753
Recall:    0.6300
F1-score:  0.3832
              precision    recall  f1-score   support

           0       0.81      0.48      0.60     16465
           1       0.28      0.63      0.38      5160

    accuracy                           0.52     21625
   macro avg       0.54      0.56      0.49     21625
weighted avg       0.68      0.52      0.55     21625

[[7907 8558]
 [1909 3251]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_ablation_one_feature/0/Release_Day/conf_matrix_decision_tree.png
Modelo guardado como: outputs_ablation_one_feature/0/Release_Day/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.5160
Precision: 0.2753
Recall:    0.6300
F1-score:  0.3832
              precision    recall  f1-score   support

           0       0.81      0.48      0.60     16465
           1       0.28      0.63      0.38      5160

    accuracy                           0.52     21625
   macro avg       0.54      0.56      0.49     21625
weighted avg       0.68      0.52      0.55     21625

[[7907 8558]
 [1909 3251]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Release_Day/conf_matrix_random_forest.png
Modelo guardado como: outputs_ablation_one_feature/0/Release_Day/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.5160
Precision: 0.2753
Recall:    0.6300
F1-score:  0.3832
              precision    recall  f1-score   support

           0       0.81      0.48      0.60     16465
           1       0.28      0.63      0.38      5160

    accuracy                           0.52     21625
   macro avg       0.54      0.56      0.49     21625
weighted avg       0.68      0.52      0.55     21625

[[7907 8558]
 [1909 3251]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_ablation_one_feature/0/Release_Day/conf_matrix_xgboost.png
Modelo guardado como: outputs_ablation_one_feature/0/Release_Day/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.4606
Precision: 0.2400
Recall:    0.5818
F1-score:  0.3398
              precision    recall  f1-score   support

           0       0.76      0.42      0.54     16465
           1       0.24      0.58      0.34      5160

    accuracy                           0.46     21625
   macro avg       0.50      0.50      0.44     21625
weighted avg       0.64      0.46      0.50     21625

[[6958 9507]
 [2158 3002]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_ablation_one_feature/0/Release_Day/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_ablation_one_feature/0/Release_Day/naive_bayes_model.pkl


Resumen de métricas:
Decision Tree: {'accuracy': 0.516, 'precision': 0.2753, 'recall': 0.63, 'f1_score': 0.3832}
Random Forest: {'accuracy': 0.516, 'precision': 0.2753, 'recall': 0.63, 'f1_score': 0.3832}
XGBoost: {'accuracy': 0.516, 'precision': 0.2753, 'recall': 0.63, 'f1_score': 0.3832}
Naive Bayes: {'accuracy': 0.4606, 'precision': 0.24, 'recall': 0.5818, 'f1_score': 0.3398}
SVM: {'accuracy': 0.4154, 'precision': 0.2265, 'recall': 0.6006, 'f1_score': 0.329}
Logistic Regression: {'accuracy': 0.5089, 'precision': 0.2392, 'recall': 0.4851, 'f1_score': 0.3204}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: EMOTION
Naive Bayes: {'accuracy': 0.5197, 'precision': 0.2852, 'recall': 0.6727, 'f1_score': 0.4006}
Logistic Regression: {'accuracy': 0.7407, 'precision': 0.4421, 'recall': 0.3318, 'f1_score': 0.3791}
Decision Tree: {'accuracy': 0.7407, 'precision': 0.4421, 'recall': 0.3318, 'f1_score': 0.3791}
Random Forest: {'accuracy': 0.7406, 'precision': 0.442, 'recall': 0.3318, 'f1_score': 0.3791}
XGBoost: {'accuracy': 0.7407, 'precision': 0.4421, 'recall': 0.3318, 'f1_score': 0.3791}
MLP_289: {'accuracy': 0.7406705202312138, 'precision': 0.44214876033057854, 'recall': 0.33178294573643413, 'f1_score': 0.37909654561558903}
MLP_707585: {'accuracy': 0.7406705202312138, 'precision': 0.44214876033057854, 'recall': 0.33178294573643413, 'f1_score': 0.37909654561558903}
SVM: {'accuracy': 0.4716, 'precision': 0.2047, 'recall': 0.4211, 'f1_score': 0.2755}


EMBEDDINGS TYPE: KEY
SVM: {'accuracy': 0.2386, 'precision': 0.2386, 'recall': 1.0, 'f1_score': 0.3853}
Logistic Regression: {'accuracy': 0.6008, 'precision': 0.282, 'recall': 0.4351, 'f1_score': 0.3422}
Decision Tree: {'accuracy': 0.6008, 'precision': 0.282, 'recall': 0.4351, 'f1_score': 0.3422}
Random Forest: {'accuracy': 0.6008, 'precision': 0.282, 'recall': 0.4351, 'f1_score': 0.3422}
XGBoost: {'accuracy': 0.6008, 'precision': 0.282, 'recall': 0.4351, 'f1_score': 0.3422}
Naive Bayes: {'accuracy': 0.6008, 'precision': 0.282, 'recall': 0.4351, 'f1_score': 0.3422}
MLP_129: {'accuracy': 0.6008323699421966, 'precision': 0.2819643305702085, 'recall': 0.43507751937984496, 'f1_score': 0.34217344916933395}
MLP_702465: {'accuracy': 0.6008323699421966, 'precision': 0.2819643305702085, 'recall': 0.43507751937984496, 'f1_score': 0.34217344916933395}


EMBEDDINGS TYPE: TIME SIGNATURE
SVM: {'accuracy': 0.2386, 'precision': 0.2386, 'recall': 1.0, 'f1_score': 0.3853}
MLP_97: {'accuracy': 0.23861271676300577, 'precision': 0.23861271676300577, 'recall': 1.0, 'f1_score': 0.3852902744073175}
MLP_701441: {'accuracy': 0.23861271676300577, 'precision': 0.23861271676300577, 'recall': 1.0, 'f1_score': 0.3852902744073175}
Logistic Regression: {'accuracy': 0.7614, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}
Decision Tree: {'accuracy': 0.7614, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}
Random Forest: {'accuracy': 0.7614, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}
XGBoost: {'accuracy': 0.7614, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}
Naive Bayes: {'accuracy': 0.7614, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}


EMBEDDINGS TYPE: ARTIST(S)
MLP_5820417: {'accuracy': 0.8740809248554914, 'precision': 0.7319626879878165, 'recall': 0.7451550387596899, 'f1_score': 0.738499951983098}
MLP_160065: {'accuracy': 0.873942196531792, 'precision': 0.735944164404808, 'recall': 0.7356589147286822, 'f1_score': 0.7358015119209149}
Logistic Regression: {'accuracy': 0.871, 'precision': 0.7241, 'recall': 0.7424, 'f1_score': 0.7331}
XGBoost: {'accuracy': 0.8748, 'precision': 0.8838, 'recall': 0.5471, 'f1_score': 0.6758}
Naive Bayes: {'accuracy': 0.5963, 'precision': 0.3697, 'recall': 0.9816, 'f1_score': 0.5371}
Random Forest: {'accuracy': 0.8428, 'precision': 0.9356, 'recall': 0.3663, 'f1_score': 0.5265}
SVM: {'accuracy': 0.4839, 'precision': 0.2312, 'recall': 0.5002, 'f1_score': 0.3163}
Decision Tree: {'accuracy': 0.7941, 'precision': 0.9585, 'recall': 0.1432, 'f1_score': 0.2492}


EMBEDDINGS TYPE: SONG
Logistic Regression: {'accuracy': 0.6149, 'precision': 0.3423, 'recall': 0.6665, 'f1_score': 0.4523}
MLP_160065: {'accuracy': 0.6246473988439306, 'precision': 0.3469303240501087, 'recall': 0.6494186046511627, 'f1_score': 0.45225723733045414}
MLP_5820417: {'accuracy': 0.6118381502890173, 'precision': 0.33967876264128494, 'recall': 0.663953488372093, 'f1_score': 0.4494293585202676}
XGBoost: {'accuracy': 0.5016, 'precision': 0.2961, 'recall': 0.7907, 'f1_score': 0.4309}
Random Forest: {'accuracy': 0.4775, 'precision': 0.2889, 'recall': 0.8143, 'f1_score': 0.4265}
Naive Bayes: {'accuracy': 0.3456, 'precision': 0.2568, 'recall': 0.9203, 'f1_score': 0.4016}
SVM: {'accuracy': 0.709, 'precision': 0.3164, 'recall': 0.189, 'f1_score': 0.2366}
Decision Tree: {'accuracy': 0.7662, 'precision': 0.5907, 'recall': 0.0663, 'f1_score': 0.1192}


EMBEDDINGS TYPE: GENRE
MLP_1452033: {'accuracy': 0.7888554913294797, 'precision': 0.5379989764585466, 'recall': 0.814922480620155, 'f1_score': 0.6481196054254007}
XGBoost: {'accuracy': 0.7875, 'precision': 0.5363, 'recall': 0.8099, 'f1_score': 0.6453}
MLP_23553: {'accuracy': 0.7827514450867052, 'precision': 0.5292257085020243, 'recall': 0.8106589147286821, 'f1_score': 0.6403857930189835}
Decision Tree: {'accuracy': 0.7918, 'precision': 0.5453, 'recall': 0.7671, 'f1_score': 0.6375}
Logistic Regression: {'accuracy': 0.7791, 'precision': 0.5245, 'recall': 0.7967, 'f1_score': 0.6326}
Random Forest: {'accuracy': 0.7915, 'precision': 0.5462, 'recall': 0.7461, 'f1_score': 0.6307}
SVM: {'accuracy': 0.7406, 'precision': 0.4556, 'recall': 0.4463, 'f1_score': 0.4509}
Naive Bayes: {'accuracy': 0.3943, 'precision': 0.2804, 'recall': 0.9822, 'f1_score': 0.4362}


EMBEDDINGS TYPE: ALBUM
MLP_5820417: {'accuracy': 0.8285317919075145, 'precision': 0.6274131274131274, 'recall': 0.6928294573643411, 'f1_score': 0.6585006446859458}
MLP_160065: {'accuracy': 0.8181734104046243, 'precision': 0.6116363636363636, 'recall': 0.6519379844961241, 'f1_score': 0.6311444652908067}
Logistic Regression: {'accuracy': 0.8114, 'precision': 0.5942, 'recall': 0.6614, 'f1_score': 0.626}
XGBoost: {'accuracy': 0.824, 'precision': 0.6954, 'recall': 0.4669, 'f1_score': 0.5587}
Random Forest: {'accuracy': 0.8125, 'precision': 0.7036, 'recall': 0.3703, 'f1_score': 0.4853}
Naive Bayes: {'accuracy': 0.4695, 'precision': 0.3065, 'recall': 0.9684, 'f1_score': 0.4656}
Decision Tree: {'accuracy': 0.3127, 'precision': 0.2553, 'recall': 0.9812, 'f1_score': 0.4052}
SVM: {'accuracy': 0.6893, 'precision': 0.2476, 'recall': 0.1481, 'f1_score': 0.1853}


EMBEDDINGS TYPE: SIMILAR ARTIST 1
MLP_5820417: {'accuracy': 0.8856878612716763, 'precision': 0.755125284738041, 'recall': 0.7709302325581395, 'f1_score': 0.762945914844649}
Logistic Regression: {'accuracy': 0.8803, 'precision': 0.7424, 'recall': 0.763, 'f1_score': 0.7526}
MLP_160065: {'accuracy': 0.8811098265895954, 'precision': 0.7503384258363953, 'recall': 0.751937984496124, 'f1_score': 0.7511373535959733}
XGBoost: {'accuracy': 0.8734, 'precision': 0.8637, 'recall': 0.5574, 'f1_score': 0.6775}
Naive Bayes: {'accuracy': 0.6032, 'precision': 0.3733, 'recall': 0.9767, 'f1_score': 0.5402}
Random Forest: {'accuracy': 0.8425, 'precision': 0.9156, 'recall': 0.3742, 'f1_score': 0.5313}
SVM: {'accuracy': 0.4856, 'precision': 0.221, 'recall': 0.4578, 'f1_score': 0.2981}
Decision Tree: {'accuracy': 0.796, 'precision': 0.9539, 'recall': 0.1525, 'f1_score': 0.263}


EMBEDDINGS TYPE: SIMILAR SONG 1
MLP_5820417: {'accuracy': 0.6734335260115607, 'precision': 0.40087554721701063, 'recall': 0.7453488372093023, 'f1_score': 0.5213501423342822}
MLP_160065: {'accuracy': 0.6623815028901734, 'precision': 0.3868034260336259, 'recall': 0.7089147286821705, 'f1_score': 0.5005131011835534}
Logistic Regression: {'accuracy': 0.664, 'precision': 0.3876, 'recall': 0.7037, 'f1_score': 0.4998}
XGBoost: {'accuracy': 0.5247, 'precision': 0.3125, 'recall': 0.8264, 'f1_score': 0.4535}
Random Forest: {'accuracy': 0.5109, 'precision': 0.3043, 'recall': 0.8157, 'f1_score': 0.4432}
Naive Bayes: {'accuracy': 0.3762, 'precision': 0.271, 'recall': 0.9552, 'f1_score': 0.4222}
SVM: {'accuracy': 0.7056, 'precision': 0.3104, 'recall': 0.1915, 'f1_score': 0.2368}
Decision Tree: {'accuracy': 0.7703, 'precision': 0.6338, 'recall': 0.0886, 'f1_score': 0.1554}


EMBEDDINGS TYPE: SIMILAR ARTIST 2
MLP_5820417: {'accuracy': 0.8832369942196532, 'precision': 0.740991402963234, 'recall': 0.7850775193798449, 'f1_score': 0.7623976663216335}
Logistic Regression: {'accuracy': 0.8801, 'precision': 0.7401, 'recall': 0.7667, 'f1_score': 0.7532}
MLP_160065: {'accuracy': 0.8741271676300578, 'precision': 0.7169039145907473, 'recall': 0.7808139534883721, 'f1_score': 0.7474953617810761}
XGBoost: {'accuracy': 0.8748, 'precision': 0.8608, 'recall': 0.5671, 'f1_score': 0.6837}
Naive Bayes: {'accuracy': 0.6035, 'precision': 0.3736, 'recall': 0.9785, 'f1_score': 0.5408}
Random Forest: {'accuracy': 0.8427, 'precision': 0.9066, 'recall': 0.38, 'f1_score': 0.5356}
SVM: {'accuracy': 0.486, 'precision': 0.2185, 'recall': 0.4481, 'f1_score': 0.2938}
Decision Tree: {'accuracy': 0.7959, 'precision': 0.9388, 'recall': 0.1547, 'f1_score': 0.2656}


EMBEDDINGS TYPE: SIMILAR SONG 2
MLP_5820417: {'accuracy': 0.6857803468208092, 'precision': 0.41446060479229885, 'recall': 0.7676356589147287, 'f1_score': 0.5382890534755724}
MLP_160065: {'accuracy': 0.6785664739884393, 'precision': 0.40051105432729694, 'recall': 0.6986434108527132, 'f1_score': 0.50914483440435}
Logistic Regression: {'accuracy': 0.6668, 'precision': 0.3911, 'recall': 0.7124, 'f1_score': 0.505}
XGBoost: {'accuracy': 0.5342, 'precision': 0.3178, 'recall': 0.8304, 'f1_score': 0.4597}
Random Forest: {'accuracy': 0.5209, 'precision': 0.3116, 'recall': 0.8335, 'f1_score': 0.4536}
Naive Bayes: {'accuracy': 0.3772, 'precision': 0.2716, 'recall': 0.9574, 'f1_score': 0.4232}
SVM: {'accuracy': 0.7083, 'precision': 0.3145, 'recall': 0.1888, 'f1_score': 0.2359}
Decision Tree: {'accuracy': 0.7699, 'precision': 0.6439, 'recall': 0.0802, 'f1_score': 0.1427}


EMBEDDINGS TYPE: SIMILAR ARTIST 3
MLP_5820417: {'accuracy': 0.8867514450867052, 'precision': 0.7613263929053403, 'recall': 0.7653100775193798, 'f1_score': 0.7633130375954383}
Logistic Regression: {'accuracy': 0.8819, 'precision': 0.7513, 'recall': 0.755, 'f1_score': 0.7531}
MLP_160065: {'accuracy': 0.8793526011560694, 'precision': 0.7405242315670375, 'recall': 0.7610465116279069, 'f1_score': 0.7506451304597151}
XGBoost: {'accuracy': 0.877, 'precision': 0.8687, 'recall': 0.5707, 'f1_score': 0.6889}
Naive Bayes: {'accuracy': 0.6059, 'precision': 0.3749, 'recall': 0.9764, 'f1_score': 0.5418}
Random Forest: {'accuracy': 0.8407, 'precision': 0.9047, 'recall': 0.3715, 'f1_score': 0.5267}
SVM: {'accuracy': 0.4792, 'precision': 0.2143, 'recall': 0.4436, 'f1_score': 0.289}
Decision Tree: {'accuracy': 0.7974, 'precision': 0.9545, 'recall': 0.1585, 'f1_score': 0.2719}


EMBEDDINGS TYPE: SIMILAR SONG 3
MLP_5820417: {'accuracy': 0.6857341040462428, 'precision': 0.4131265930331351, 'recall': 0.7538759689922481, 'f1_score': 0.5337541163556532}
MLP_160065: {'accuracy': 0.6612254335260116, 'precision': 0.38737520798668884, 'recall': 0.7218992248062015, 'f1_score': 0.5041959935029778}
Logistic Regression: {'accuracy': 0.6651, 'precision': 0.3878, 'recall': 0.6979, 'f1_score': 0.4986}
XGBoost: {'accuracy': 0.5336, 'precision': 0.3152, 'recall': 0.8143, 'f1_score': 0.4545}
Random Forest: {'accuracy': 0.533, 'precision': 0.3133, 'recall': 0.8027, 'f1_score': 0.4507}
Naive Bayes: {'accuracy': 0.378, 'precision': 0.2723, 'recall': 0.9605, 'f1_score': 0.4243}
SVM: {'accuracy': 0.6757, 'precision': 0.2852, 'recall': 0.2382, 'f1_score': 0.2596}
Decision Tree: {'accuracy': 0.7671, 'precision': 0.5853, 'recall': 0.0818, 'f1_score': 0.1435}


EMBEDDINGS TYPE: SONG_NORMALIZED
MLP_160065: {'accuracy': 0.621364161849711, 'precision': 0.34557323541411666, 'recall': 0.6565891472868217, 'f1_score': 0.45282010157711844}
Logistic Regression: {'accuracy': 0.6154, 'precision': 0.3424, 'recall': 0.6651, 'f1_score': 0.4521}
MLP_5820417: {'accuracy': 0.6251098265895954, 'precision': 0.34601316752011707, 'recall': 0.6416666666666667, 'f1_score': 0.44958924570575054}
XGBoost: {'accuracy': 0.5072, 'precision': 0.298, 'recall': 0.786, 'f1_score': 0.4322}
Random Forest: {'accuracy': 0.4826, 'precision': 0.2909, 'recall': 0.813, 'f1_score': 0.4285}
Naive Bayes: {'accuracy': 0.3445, 'precision': 0.2568, 'recall': 0.9227, 'f1_score': 0.4018}
SVM: {'accuracy': 0.7092, 'precision': 0.3173, 'recall': 0.1899, 'f1_score': 0.2376}
Decision Tree: {'accuracy': 0.7668, 'precision': 0.6032, 'recall': 0.0663, 'f1_score': 0.1194}


EMBEDDINGS TYPE: ARTIST_NORMALIZED
MLP_5820417: {'accuracy': 0.8736184971098266, 'precision': 0.730835077040137, 'recall': 0.7445736434108527, 'f1_score': 0.7376403955073437}
Logistic Regression: {'accuracy': 0.871, 'precision': 0.7241, 'recall': 0.7424, 'f1_score': 0.7331}
MLP_160065: {'accuracy': 0.8681156069364162, 'precision': 0.7115102639296188, 'recall': 0.7523255813953489, 'f1_score': 0.7313489073097211}
XGBoost: {'accuracy': 0.8748, 'precision': 0.8838, 'recall': 0.5471, 'f1_score': 0.6758}
Naive Bayes: {'accuracy': 0.5963, 'precision': 0.3697, 'recall': 0.9816, 'f1_score': 0.5371}
Random Forest: {'accuracy': 0.8428, 'precision': 0.9356, 'recall': 0.3663, 'f1_score': 0.5265}
SVM: {'accuracy': 0.4839, 'precision': 0.2312, 'recall': 0.5002, 'f1_score': 0.3163}
Decision Tree: {'accuracy': 0.7941, 'precision': 0.9585, 'recall': 0.1432, 'f1_score': 0.2492}


EMBEDDINGS TYPE: TEMPO
XGBoost: {'accuracy': 0.5851, 'precision': 0.3063, 'recall': 0.5843, 'f1_score': 0.4019}
Random Forest: {'accuracy': 0.5932, 'precision': 0.3079, 'recall': 0.5647, 'f1_score': 0.3985}
Decision Tree: {'accuracy': 0.5961, 'precision': 0.3088, 'recall': 0.5595, 'f1_score': 0.398}
MLP_97: {'accuracy': 0.5136647398843931, 'precision': 0.28036080360803606, 'recall': 0.6625968992248062, 'f1_score': 0.3940074906367041}
MLP_701441: {'accuracy': 0.4807398843930636, 'precision': 0.2729517396184063, 'recall': 0.7069767441860465, 'f1_score': 0.39384615384615385}
Logistic Regression: {'accuracy': 0.5292, 'precision': 0.2583, 'recall': 0.52, 'f1_score': 0.3451}
Naive Bayes: {'accuracy': 0.6289, 'precision': 0.2855, 'recall': 0.3694, 'f1_score': 0.322}
SVM: {'accuracy': 0.5848, 'precision': 0.2518, 'recall': 0.3754, 'f1_score': 0.3014}


EMBEDDINGS TYPE: LENGTH
Random Forest: {'accuracy': 0.6295, 'precision': 0.3364, 'recall': 0.5682, 'f1_score': 0.4226}
XGBoost: {'accuracy': 0.6283, 'precision': 0.3344, 'recall': 0.5628, 'f1_score': 0.4195}
Decision Tree: {'accuracy': 0.647, 'precision': 0.3401, 'recall': 0.5097, 'f1_score': 0.408}
MLP_701441: {'accuracy': 0.62478612716763, 'precision': 0.3209262851600388, 'recall': 0.512984496124031, 'f1_score': 0.39483890214797135}
MLP_97: {'accuracy': 0.6204393063583815, 'precision': 0.31861461556772197, 'recall': 0.5187984496124031, 'f1_score': 0.3947795310426191}
Naive Bayes: {'accuracy': 0.4646, 'precision': 0.2688, 'recall': 0.7231, 'f1_score': 0.3919}
Logistic Regression: {'accuracy': 0.5532, 'precision': 0.2894, 'recall': 0.5994, 'f1_score': 0.3903}
SVM: {'accuracy': 0.6065, 'precision': 0.1801, 'recall': 0.1828, 'f1_score': 0.1814}


EMBEDDINGS TYPE: LOUDNESS (DB)
Random Forest: {'accuracy': 0.6053, 'precision': 0.3359, 'recall': 0.6694, 'f1_score': 0.4473}
Decision Tree: {'accuracy': 0.5778, 'precision': 0.3137, 'recall': 0.6475, 'f1_score': 0.4226}
XGBoost: {'accuracy': 0.5785, 'precision': 0.3127, 'recall': 0.6395, 'f1_score': 0.42}
MLP_701441: {'accuracy': 0.4552138728323699, 'precision': 0.2804853789536503, 'recall': 0.8197674418604651, 'f1_score': 0.4179635393508226}
MLP_97: {'accuracy': 0.4615028901734104, 'precision': 0.28145851587248094, 'recall': 0.8093023255813954, 'f1_score': 0.4176626493974096}
Naive Bayes: {'accuracy': 0.4188, 'precision': 0.2723, 'recall': 0.8585, 'f1_score': 0.4135}
Logistic Regression: {'accuracy': 0.515, 'precision': 0.2808, 'recall': 0.6614, 'f1_score': 0.3942}
SVM: {'accuracy': 0.4718, 'precision': 0.2283, 'recall': 0.5101, 'f1_score': 0.3155}


EMBEDDINGS TYPE: POPULARITY
MLP_701441: {'accuracy': 0.4516994219653179, 'precision': 0.25901403382511695, 'recall': 0.6974806201550388, 'f1_score': 0.37774862240881657}
Logistic Regression: {'accuracy': 0.6012, 'precision': 0.2907, 'recall': 0.4659, 'f1_score': 0.358}
MLP_97: {'accuracy': 0.6161849710982659, 'precision': 0.29752385865359815, 'recall': 0.44709302325581396, 'f1_score': 0.3572866656341954}
Decision Tree: {'accuracy': 0.6302, 'precision': 0.3023, 'recall': 0.4205, 'f1_score': 0.3518}
Random Forest: {'accuracy': 0.6302, 'precision': 0.3023, 'recall': 0.4205, 'f1_score': 0.3518}
XGBoost: {'accuracy': 0.6301, 'precision': 0.3022, 'recall': 0.4203, 'f1_score': 0.3516}
Naive Bayes: {'accuracy': 0.6466, 'precision': 0.3068, 'recall': 0.3818, 'f1_score': 0.3402}
SVM: {'accuracy': 0.4862, 'precision': 0.2324, 'recall': 0.5008, 'f1_score': 0.3175}


EMBEDDINGS TYPE: ENERGY
XGBoost: {'accuracy': 0.5751, 'precision': 0.3118, 'recall': 0.6467, 'f1_score': 0.4208}
Decision Tree: {'accuracy': 0.5676, 'precision': 0.3089, 'recall': 0.6562, 'f1_score': 0.42}
Random Forest: {'accuracy': 0.5676, 'precision': 0.3089, 'recall': 0.6562, 'f1_score': 0.42}
MLP_701441: {'accuracy': 0.4952601156069364, 'precision': 0.28834130194924606, 'recall': 0.7596899224806202, 'f1_score': 0.4180218608371101}
MLP_97: {'accuracy': 0.5076069364161849, 'precision': 0.2909811090798294, 'recall': 0.7403100775193798, 'f1_score': 0.41776027996500437}
Naive Bayes: {'accuracy': 0.4839, 'precision': 0.283, 'recall': 0.7583, 'f1_score': 0.4122}
Logistic Regression: {'accuracy': 0.4998, 'precision': 0.2541, 'recall': 0.5665, 'f1_score': 0.3509}
SVM: {'accuracy': 0.6189, 'precision': 0.179, 'recall': 0.1665, 'f1_score': 0.1725}


EMBEDDINGS TYPE: DANCEABILITY
Decision Tree: {'accuracy': 0.6401, 'precision': 0.353, 'recall': 0.6099, 'f1_score': 0.4471}
Random Forest: {'accuracy': 0.6401, 'precision': 0.353, 'recall': 0.6099, 'f1_score': 0.4471}
XGBoost: {'accuracy': 0.6401, 'precision': 0.353, 'recall': 0.6099, 'f1_score': 0.4471}
MLP_97: {'accuracy': 0.6228439306358382, 'precision': 0.3425478242589868, 'recall': 0.6315891472868217, 'f1_score': 0.4441869974103857}
MLP_701441: {'accuracy': 0.6228439306358382, 'precision': 0.3425478242589868, 'recall': 0.6315891472868217, 'f1_score': 0.4441869974103857}
Naive Bayes: {'accuracy': 0.6003, 'precision': 0.3317, 'recall': 0.6655, 'f1_score': 0.4428}
Logistic Regression: {'accuracy': 0.6112, 'precision': 0.3361, 'recall': 0.6455, 'f1_score': 0.4421}
SVM: {'accuracy': 0.4773, 'precision': 0.1559, 'recall': 0.2696, 'f1_score': 0.1975}


EMBEDDINGS TYPE: POSITIVENESS
MLP_701441: {'accuracy': 0.33077456647398845, 'precision': 0.2495697074010327, 'recall': 0.8992248062015504, 'f1_score': 0.3907039407207814}
Random Forest: {'accuracy': 0.5302, 'precision': 0.2812, 'recall': 0.6225, 'f1_score': 0.3874}
XGBoost: {'accuracy': 0.5274, 'precision': 0.2802, 'recall': 0.6254, 'f1_score': 0.387}
MLP_97: {'accuracy': 0.3758150289017341, 'precision': 0.25243467933491687, 'recall': 0.8238372093023256, 'f1_score': 0.38645454545454544}
Decision Tree: {'accuracy': 0.5388, 'precision': 0.2811, 'recall': 0.599, 'f1_score': 0.3827}
Naive Bayes: {'accuracy': 0.4378, 'precision': 0.2512, 'recall': 0.6847, 'f1_score': 0.3676}
Logistic Regression: {'accuracy': 0.5088, 'precision': 0.2445, 'recall': 0.5066, 'f1_score': 0.3298}
SVM: {'accuracy': 0.6183, 'precision': 0.2237, 'recall': 0.2426, 'f1_score': 0.2328}


EMBEDDINGS TYPE: SPEECHINESS
MLP_701441: {'accuracy': 0.8167398843930636, 'precision': 0.6249738985174358, 'recall': 0.5800387596899225, 'f1_score': 0.6016685093979295}
MLP_97: {'accuracy': 0.8232138728323699, 'precision': 0.6512785698121747, 'recall': 0.5577519379844961, 'f1_score': 0.6008977972648502}
Logistic Regression: {'accuracy': 0.808, 'precision': 0.5978, 'recall': 0.5979, 'f1_score': 0.5978}
Random Forest: {'accuracy': 0.7675, 'precision': 0.5094, 'recall': 0.6965, 'f1_score': 0.5884}
XGBoost: {'accuracy': 0.7675, 'precision': 0.5094, 'recall': 0.6965, 'f1_score': 0.5884}
Decision Tree: {'accuracy': 0.7674, 'precision': 0.5091, 'recall': 0.6961, 'f1_score': 0.5881}
Naive Bayes: {'accuracy': 0.8243, 'precision': 0.6773, 'recall': 0.5035, 'f1_score': 0.5776}
SVM: {'accuracy': 0.29, 'precision': 0.1772, 'recall': 0.5422, 'f1_score': 0.2671}


EMBEDDINGS TYPE: LIVENESS
MLP_701441: {'accuracy': 0.24938728323699422, 'precision': 0.23975178638585934, 'recall': 0.9883720930232558, 'f1_score': 0.3858958837772397}
MLP_97: {'accuracy': 0.4409248554913295, 'precision': 0.2554002541296061, 'recall': 0.7011627906976744, 'f1_score': 0.37441788264514125}
XGBoost: {'accuracy': 0.5725, 'precision': 0.274, 'recall': 0.4798, 'f1_score': 0.3488}
Random Forest: {'accuracy': 0.5943, 'precision': 0.2782, 'recall': 0.4391, 'f1_score': 0.3406}
Decision Tree: {'accuracy': 0.5963, 'precision': 0.2781, 'recall': 0.4333, 'f1_score': 0.3388}
Logistic Regression: {'accuracy': 0.6125, 'precision': 0.2667, 'recall': 0.3568, 'f1_score': 0.3053}
Naive Bayes: {'accuracy': 0.6271, 'precision': 0.2606, 'recall': 0.3064, 'f1_score': 0.2816}
SVM: {'accuracy': 0.6705, 'precision': 0.2715, 'recall': 0.2262, 'f1_score': 0.2467}


EMBEDDINGS TYPE: ACOUSTICNESS
MLP_701441: {'accuracy': 0.4064277456647399, 'precision': 0.2694894894894895, 'recall': 0.8695736434108527, 'f1_score': 0.41146263182026593}
MLP_97: {'accuracy': 0.4137803468208093, 'precision': 0.2701082635023549, 'recall': 0.8558139534883721, 'f1_score': 0.4106188107303919}
Naive Bayes: {'accuracy': 0.4277, 'precision': 0.2705, 'recall': 0.824, 'f1_score': 0.4073}
XGBoost: {'accuracy': 0.5436, 'precision': 0.2931, 'recall': 0.6465, 'f1_score': 0.4034}
Random Forest: {'accuracy': 0.5501, 'precision': 0.2938, 'recall': 0.6312, 'f1_score': 0.401}
Logistic Regression: {'accuracy': 0.4837, 'precision': 0.2766, 'recall': 0.7205, 'f1_score': 0.3997}
Decision Tree: {'accuracy': 0.5638, 'precision': 0.2957, 'recall': 0.599, 'f1_score': 0.3959}
SVM: {'accuracy': 0.6571, 'precision': 0.1678, 'recall': 0.1105, 'f1_score': 0.1332}


EMBEDDINGS TYPE: INSTRUMENTALNESS
Decision Tree: {'accuracy': 0.4204, 'precision': 0.2774, 'recall': 0.8903, 'f1_score': 0.423}
XGBoost: {'accuracy': 0.4204, 'precision': 0.2774, 'recall': 0.8903, 'f1_score': 0.423}
Random Forest: {'accuracy': 0.4197, 'precision': 0.2773, 'recall': 0.8911, 'f1_score': 0.4229}
MLP_97: {'accuracy': 0.42094797687861274, 'precision': 0.27701720377998545, 'recall': 0.8862403100775194, 'f1_score': 0.4220971017168174}
MLP_701441: {'accuracy': 0.42094797687861274, 'precision': 0.27701720377998545, 'recall': 0.8862403100775194, 'f1_score': 0.4220971017168174}
Logistic Regression: {'accuracy': 0.3608, 'precision': 0.2645, 'recall': 0.943, 'f1_score': 0.4132}
Naive Bayes: {'accuracy': 0.3286, 'precision': 0.2573, 'recall': 0.9614, 'f1_score': 0.406}
SVM: {'accuracy': 0.7026, 'precision': 0.1146, 'recall': 0.0366, 'f1_score': 0.0555}


EMBEDDINGS TYPE: GOOD FOR PARTY
MLP_701441: {'accuracy': 0.23861271676300577, 'precision': 0.23861271676300577, 'recall': 1.0, 'f1_score': 0.3852902744073175}
SVM: {'accuracy': 0.304, 'precision': 0.2171, 'recall': 0.7353, 'f1_score': 0.3352}
MLP_97: {'accuracy': 0.6959537572254335, 'precision': 0.3293947431878466, 'recall': 0.26472868217054263, 'f1_score': 0.2935424948963146}
Logistic Regression: {'accuracy': 0.696, 'precision': 0.3294, 'recall': 0.2647, 'f1_score': 0.2935}
Decision Tree: {'accuracy': 0.696, 'precision': 0.3294, 'recall': 0.2647, 'f1_score': 0.2935}
Random Forest: {'accuracy': 0.696, 'precision': 0.3294, 'recall': 0.2647, 'f1_score': 0.2935}
XGBoost: {'accuracy': 0.696, 'precision': 0.3294, 'recall': 0.2647, 'f1_score': 0.2935}
Naive Bayes: {'accuracy': 0.696, 'precision': 0.3294, 'recall': 0.2647, 'f1_score': 0.2935}


EMBEDDINGS TYPE: GOOD FOR WORK/STUDY
MLP_97: {'accuracy': 0.30497109826589597, 'precision': 0.2533486605357857, 'recall': 0.9823643410852713, 'f1_score': 0.40281309599491416}
MLP_701441: {'accuracy': 0.30497109826589597, 'precision': 0.2533486605357857, 'recall': 0.9823643410852713, 'f1_score': 0.40281309599491416}
Logistic Regression: {'accuracy': 0.305, 'precision': 0.2533, 'recall': 0.9824, 'f1_score': 0.4028}
Decision Tree: {'accuracy': 0.305, 'precision': 0.2533, 'recall': 0.9824, 'f1_score': 0.4028}
Random Forest: {'accuracy': 0.305, 'precision': 0.2533, 'recall': 0.9824, 'f1_score': 0.4028}
XGBoost: {'accuracy': 0.305, 'precision': 0.2533, 'recall': 0.9824, 'f1_score': 0.4028}
Naive Bayes: {'accuracy': 0.305, 'precision': 0.2533, 'recall': 0.9824, 'f1_score': 0.4028}
SVM: {'accuracy': 0.695, 'precision': 0.0563, 'recall': 0.0176, 'f1_score': 0.0269}


EMBEDDINGS TYPE: GOOD FOR RELAXATION/MEDITATION
Logistic Regression: {'accuracy': 0.2662, 'precision': 0.2449, 'recall': 0.9957, 'f1_score': 0.3931}
Decision Tree: {'accuracy': 0.2662, 'precision': 0.2449, 'recall': 0.9957, 'f1_score': 0.3931}
Random Forest: {'accuracy': 0.2662, 'precision': 0.2449, 'recall': 0.9957, 'f1_score': 0.3931}
XGBoost: {'accuracy': 0.2662, 'precision': 0.2449, 'recall': 0.9957, 'f1_score': 0.3931}
Naive Bayes: {'accuracy': 0.2662, 'precision': 0.2449, 'recall': 0.9957, 'f1_score': 0.3931}
MLP_97: {'accuracy': 0.2662196531791908, 'precision': 0.24485322150209685, 'recall': 0.9957364341085271, 'f1_score': 0.39305385556915545}
MLP_701441: {'accuracy': 0.2662196531791908, 'precision': 0.24485322150209685, 'recall': 0.9957364341085271, 'f1_score': 0.39305385556915545}
SVM: {'accuracy': 0.2386, 'precision': 0.2386, 'recall': 1.0, 'f1_score': 0.3853}


EMBEDDINGS TYPE: GOOD FOR EXERCISE
Logistic Regression: {'accuracy': 0.6929, 'precision': 0.3322, 'recall': 0.2839, 'f1_score': 0.3062}
Decision Tree: {'accuracy': 0.6929, 'precision': 0.3322, 'recall': 0.2839, 'f1_score': 0.3062}
Random Forest: {'accuracy': 0.6929, 'precision': 0.3322, 'recall': 0.2839, 'f1_score': 0.3062}
XGBoost: {'accuracy': 0.6929, 'precision': 0.3322, 'recall': 0.2839, 'f1_score': 0.3062}
Naive Bayes: {'accuracy': 0.6929, 'precision': 0.3322, 'recall': 0.2839, 'f1_score': 0.3062}
MLP_97: {'accuracy': 0.6929479768786128, 'precision': 0.3321995464852608, 'recall': 0.28391472868217055, 'f1_score': 0.30616509926854757}
MLP_701441: {'accuracy': 0.6929479768786128, 'precision': 0.3321995464852608, 'recall': 0.28391472868217055, 'f1_score': 0.30616509926854757}
SVM: {'accuracy': 0.7614, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}


EMBEDDINGS TYPE: GOOD FOR RUNNING
SVM: {'accuracy': 0.2496, 'precision': 0.2264, 'recall': 0.8876, 'f1_score': 0.3608}
Logistic Regression: {'accuracy': 0.7504, 'precision': 0.4149, 'recall': 0.1124, 'f1_score': 0.1769}
Decision Tree: {'accuracy': 0.7504, 'precision': 0.4149, 'recall': 0.1124, 'f1_score': 0.1769}
Random Forest: {'accuracy': 0.7504, 'precision': 0.4149, 'recall': 0.1124, 'f1_score': 0.1769}
XGBoost: {'accuracy': 0.7504, 'precision': 0.4149, 'recall': 0.1124, 'f1_score': 0.1769}
Naive Bayes: {'accuracy': 0.7504, 'precision': 0.4149, 'recall': 0.1124, 'f1_score': 0.1769}
MLP_97: {'accuracy': 0.7503815028901734, 'precision': 0.41487839771101576, 'recall': 0.1124031007751938, 'f1_score': 0.17688319609637085}
MLP_701441: {'accuracy': 0.7503815028901734, 'precision': 0.41487839771101576, 'recall': 0.1124031007751938, 'f1_score': 0.17688319609637085}


EMBEDDINGS TYPE: GOOD FOR YOGA/STRETCHING
Logistic Regression: {'accuracy': 0.2595, 'precision': 0.2434, 'recall': 0.9975, 'f1_score': 0.3913}
Decision Tree: {'accuracy': 0.2595, 'precision': 0.2434, 'recall': 0.9975, 'f1_score': 0.3913}
Random Forest: {'accuracy': 0.2595, 'precision': 0.2434, 'recall': 0.9975, 'f1_score': 0.3913}
XGBoost: {'accuracy': 0.2595, 'precision': 0.2434, 'recall': 0.9975, 'f1_score': 0.3913}
Naive Bayes: {'accuracy': 0.2595, 'precision': 0.2434, 'recall': 0.9975, 'f1_score': 0.3913}
MLP_97: {'accuracy': 0.25946820809248555, 'precision': 0.2433799886514091, 'recall': 0.9974806201550388, 'f1_score': 0.3912878211950737}
MLP_701441: {'accuracy': 0.25946820809248555, 'precision': 0.2433799886514091, 'recall': 0.9974806201550388, 'f1_score': 0.3912878211950737}
SVM: {'accuracy': 0.2386, 'precision': 0.2386, 'recall': 1.0, 'f1_score': 0.3853}


EMBEDDINGS TYPE: GOOD FOR DRIVING
MLP_97: {'accuracy': 0.23861271676300577, 'precision': 0.23861271676300577, 'recall': 1.0, 'f1_score': 0.3852902744073175}
MLP_701441: {'accuracy': 0.23861271676300577, 'precision': 0.23861271676300577, 'recall': 1.0, 'f1_score': 0.3852902744073175}
Logistic Regression: {'accuracy': 0.7403, 'precision': 0.3254, 'recall': 0.0826, 'f1_score': 0.1317}
SVM: {'accuracy': 0.7403, 'precision': 0.3254, 'recall': 0.0826, 'f1_score': 0.1317}
Decision Tree: {'accuracy': 0.7403, 'precision': 0.3254, 'recall': 0.0826, 'f1_score': 0.1317}
Random Forest: {'accuracy': 0.7403, 'precision': 0.3254, 'recall': 0.0826, 'f1_score': 0.1317}
XGBoost: {'accuracy': 0.7403, 'precision': 0.3254, 'recall': 0.0826, 'f1_score': 0.1317}
Naive Bayes: {'accuracy': 0.7403, 'precision': 0.3254, 'recall': 0.0826, 'f1_score': 0.1317}


EMBEDDINGS TYPE: GOOD FOR SOCIAL GATHERINGS
MLP_97: {'accuracy': 0.23861271676300577, 'precision': 0.23861271676300577, 'recall': 1.0, 'f1_score': 0.3852902744073175}
MLP_701441: {'accuracy': 0.23861271676300577, 'precision': 0.23861271676300577, 'recall': 1.0, 'f1_score': 0.3852902744073175}
Logistic Regression: {'accuracy': 0.7466, 'precision': 0.2978, 'recall': 0.0455, 'f1_score': 0.079}
Decision Tree: {'accuracy': 0.7466, 'precision': 0.2978, 'recall': 0.0455, 'f1_score': 0.079}
Random Forest: {'accuracy': 0.7466, 'precision': 0.2978, 'recall': 0.0455, 'f1_score': 0.079}
XGBoost: {'accuracy': 0.7466, 'precision': 0.2978, 'recall': 0.0455, 'f1_score': 0.079}
Naive Bayes: {'accuracy': 0.7466, 'precision': 0.2978, 'recall': 0.0455, 'f1_score': 0.079}
SVM: {'accuracy': 0.7614, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}


EMBEDDINGS TYPE: GOOD FOR MORNING ROUTINE
MLP_97: {'accuracy': 0.23861271676300577, 'precision': 0.23861271676300577, 'recall': 1.0, 'f1_score': 0.3852902744073175}
MLP_701441: {'accuracy': 0.23861271676300577, 'precision': 0.23861271676300577, 'recall': 1.0, 'f1_score': 0.3852902744073175}
Logistic Regression: {'accuracy': 0.7249, 'precision': 0.2468, 'recall': 0.0744, 'f1_score': 0.1144}
Decision Tree: {'accuracy': 0.7249, 'precision': 0.2468, 'recall': 0.0744, 'f1_score': 0.1144}
Random Forest: {'accuracy': 0.7249, 'precision': 0.2468, 'recall': 0.0744, 'f1_score': 0.1144}
XGBoost: {'accuracy': 0.7249, 'precision': 0.2468, 'recall': 0.0744, 'f1_score': 0.1144}
Naive Bayes: {'accuracy': 0.7249, 'precision': 0.2468, 'recall': 0.0744, 'f1_score': 0.1144}
SVM: {'accuracy': 0.7614, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}


EMBEDDINGS TYPE: RELEASE_YEAR
Decision Tree: {'accuracy': 0.5865, 'precision': 0.3356, 'recall': 0.7479, 'f1_score': 0.4633}
Random Forest: {'accuracy': 0.5865, 'precision': 0.3356, 'recall': 0.7479, 'f1_score': 0.4633}
XGBoost: {'accuracy': 0.5865, 'precision': 0.3356, 'recall': 0.7479, 'f1_score': 0.4633}
MLP_701441: {'accuracy': 0.5864508670520231, 'precision': 0.3340643916132994, 'recall': 0.737984496124031, 'f1_score': 0.459931155262999}
MLP_97: {'accuracy': 0.5708208092485549, 'precision': 0.32810544756819887, 'recall': 0.7622093023255814, 'f1_score': 0.45873913804163996}
Logistic Regression: {'accuracy': 0.5568, 'precision': 0.3231, 'recall': 0.7833, 'f1_score': 0.4575}
Naive Bayes: {'accuracy': 0.4781, 'precision': 0.2945, 'recall': 0.851, 'f1_score': 0.4376}
SVM: {'accuracy': 0.5846, 'precision': 0.173, 'recall': 0.1959, 'f1_score': 0.1838}


EMBEDDINGS TYPE: RELEASE_MONTH
MLP_97: {'accuracy': 0.32947976878612717, 'precision': 0.24723966226455943, 'recall': 0.8852713178294573, 'f1_score': 0.3865290235234388}
MLP_701441: {'accuracy': 0.32947976878612717, 'precision': 0.24723966226455943, 'recall': 0.8852713178294573, 'f1_score': 0.3865290235234388}
Decision Tree: {'accuracy': 0.4781, 'precision': 0.2545, 'recall': 0.6153, 'f1_score': 0.36}
Random Forest: {'accuracy': 0.4781, 'precision': 0.2545, 'recall': 0.6153, 'f1_score': 0.36}
XGBoost: {'accuracy': 0.4781, 'precision': 0.2545, 'recall': 0.6153, 'f1_score': 0.36}
Logistic Regression: {'accuracy': 0.5109, 'precision': 0.2527, 'recall': 0.5364, 'f1_score': 0.3436}
Naive Bayes: {'accuracy': 0.5109, 'precision': 0.2527, 'recall': 0.5364, 'f1_score': 0.3436}
SVM: {'accuracy': 0.58, 'precision': 0.2401, 'recall': 0.351, 'f1_score': 0.2851}


EMBEDDINGS TYPE: RELEASE_DAY
MLP_701441: {'accuracy': 0.34233526011560694, 'precision': 0.25369645575125027, 'recall': 0.9044573643410853, 'f1_score': 0.39624724061810157}
MLP_97: {'accuracy': 0.4041156069364162, 'precision': 0.2575928714859438, 'recall': 0.7955426356589147, 'f1_score': 0.3891733029958286}
Decision Tree: {'accuracy': 0.516, 'precision': 0.2753, 'recall': 0.63, 'f1_score': 0.3832}
Random Forest: {'accuracy': 0.516, 'precision': 0.2753, 'recall': 0.63, 'f1_score': 0.3832}
XGBoost: {'accuracy': 0.516, 'precision': 0.2753, 'recall': 0.63, 'f1_score': 0.3832}
Naive Bayes: {'accuracy': 0.4606, 'precision': 0.24, 'recall': 0.5818, 'f1_score': 0.3398}
SVM: {'accuracy': 0.4154, 'precision': 0.2265, 'recall': 0.6006, 'f1_score': 0.329}
Logistic Regression: {'accuracy': 0.5089, 'precision': 0.2392, 'recall': 0.4851, 'f1_score': 0.3204}
Feature:  emotion
Feature:  Key
Feature:  Time signature
Feature:  Artist(s)
Feature:  song
Feature:  Genre
Feature:  Album
Feature:  Similar Artist 1
Feature:  Similar Song 1
Feature:  Similar Artist 2
Feature:  Similar Song 2
Feature:  Similar Artist 3
Feature:  Similar Song 3
Feature:  song_normalized
Feature:  artist_normalized
Feature:  Tempo
Feature:  Length
Feature:  Loudness (db)
Feature:  Popularity
Feature:  Energy
Feature:  Danceability
Feature:  Positiveness
Feature:  Speechiness
Feature:  Liveness
Feature:  Acousticness
Feature:  Instrumentalness
Feature:  Good for Party
Feature:  Good for Work/Study
Feature:  Good for Relaxation/Meditation
Feature:  Good for Exercise
Feature:  Good for Running
Feature:  Good for Yoga/Stretching
Feature:  Good for Driving
Feature:  Good for Social Gatherings
Feature:  Good for Morning Routine
Feature:  Release_Year
Feature:  Release_Month
Feature:  Release_Day

 Mejores modelos por embedding:
                         embedding           best_model  ...    recall  f1_score
0                          emotion          Naive Bayes  ...  0.672700  0.400600
1                              Key                  SVM  ...  1.000000  0.385300
2                   Time signature                  SVM  ...  1.000000  0.385300
3                        Artist(s)          MLP_5820417  ...  0.745155  0.738500
4                             song  Logistic Regression  ...  0.666500  0.452300
5                            Genre          MLP_1452033  ...  0.814922  0.648120
6                            Album          MLP_5820417  ...  0.692829  0.658501
7                 Similar Artist 1          MLP_5820417  ...  0.770930  0.762946
8                   Similar Song 1          MLP_5820417  ...  0.745349  0.521350
9                 Similar Artist 2          MLP_5820417  ...  0.785078  0.762398
10                  Similar Song 2          MLP_5820417  ...  0.767636  0.538289
11                Similar Artist 3          MLP_5820417  ...  0.765310  0.763313
12                  Similar Song 3          MLP_5820417  ...  0.753876  0.533754
13                 song_normalized           MLP_160065  ...  0.656589  0.452820
14               artist_normalized          MLP_5820417  ...  0.744574  0.737640
15                           Tempo              XGBoost  ...  0.584300  0.401900
16                          Length        Random Forest  ...  0.568200  0.422600
17                   Loudness (db)        Random Forest  ...  0.669400  0.447300
18                      Popularity           MLP_701441  ...  0.697481  0.377749
19                          Energy              XGBoost  ...  0.646700  0.420800
20                    Danceability        Random Forest  ...  0.609900  0.447100
21                    Positiveness           MLP_701441  ...  0.899225  0.390704
22                     Speechiness           MLP_701441  ...  0.580039  0.601669
23                        Liveness           MLP_701441  ...  0.988372  0.385896
24                    Acousticness           MLP_701441  ...  0.869574  0.411463
25                Instrumentalness              XGBoost  ...  0.890300  0.423000
26                  Good for Party           MLP_701441  ...  1.000000  0.385290
27             Good for Work/Study               MLP_97  ...  0.982364  0.402813
28  Good for Relaxation/Meditation  Logistic Regression  ...  0.995700  0.393100
29               Good for Exercise  Logistic Regression  ...  0.283900  0.306200
30                Good for Running                  SVM  ...  0.887600  0.360800
31        Good for Yoga/Stretching  Logistic Regression  ...  0.997500  0.391300
32                Good for Driving               MLP_97  ...  1.000000  0.385290
33      Good for Social Gatherings               MLP_97  ...  1.000000  0.385290
34        Good for Morning Routine               MLP_97  ...  1.000000  0.385290
35                    Release_Year        Random Forest  ...  0.747900  0.463300
36                   Release_Month               MLP_97  ...  0.885271  0.386529
37                     Release_Day           MLP_701441  ...  0.904457  0.396247

[38 rows x 8 columns]

 Mejor global: Similar Artist 3 con MLP_5820417 (f1=0.7633)

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
====================================

