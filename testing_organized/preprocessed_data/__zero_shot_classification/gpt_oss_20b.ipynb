{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: triton in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install triton --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eseuTtnA3WeD",
        "outputId": "57ea6d49-dd7e-4495-90f8-88a07f9e942f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (2.9.0)\n",
            "Requirement already satisfied: torchvision in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (0.24.0+cu128)\n",
            "Requirement already satisfied: torchaudio in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (2.9.0+cu128)\n",
            "Requirement already satisfied: filelock in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (2025.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: transformers in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (5.0.0.dev0)\n",
            "Requirement already satisfied: accelerate in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (1.11.0)\n",
            "Requirement already satisfied: huggingface-hub in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (0.36.0)\n",
            "Requirement already satisfied: filelock in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from transformers) (3.20.0)\n",
            "Collecting huggingface-hub\n",
            "  Using cached huggingface_hub-1.0.0rc6-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from transformers) (2025.10.23)\n",
            "Requirement already satisfied: requests in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: typer-slim in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from transformers) (0.20.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from huggingface-hub) (2025.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from huggingface-hub) (0.28.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from huggingface-hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from huggingface-hub) (1.2.0)\n",
            "Requirement already satisfied: anyio in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub) (4.11.0)\n",
            "Requirement already satisfied: certifi in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub) (1.0.9)\n",
            "Requirement already satisfied: idna in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub) (0.16.0)\n",
            "Requirement already satisfied: psutil in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from accelerate) (7.1.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from accelerate) (2.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub) (1.2.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from typer-slim->transformers) (8.3.0)\n",
            "Using cached huggingface_hub-1.0.0rc6-py3-none-any.whl (502 kB)\n",
            "Installing collected packages: huggingface-hub\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.36.0\n",
            "    Uninstalling huggingface-hub-0.36.0:\n",
            "      Successfully uninstalled huggingface-hub-0.36.0\n",
            "Successfully installed huggingface-hub-1.0.0rc6\n",
            "Requirement already satisfied: bitsandbytes in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (0.48.1)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from bitsandbytes) (2.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from bitsandbytes) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.9 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (2025.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# 1. Instalar o actualizar PyTorch con soporte CUDA (ejemplo para CUDA 12.1)\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# 2. Instalar y actualizar los paquetes de Hugging Face\n",
        "!pip install transformers accelerate huggingface-hub --upgrade\n",
        "\n",
        "# 3. Instalar bitsandbytes (para la cuantificación de 8 bits)\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.5.0\n"
          ]
        }
      ],
      "source": [
        "import triton\n",
        "print(triton.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (1.11.0)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from bitsandbytes) (2.9.0+cu128)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from bitsandbytes) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.9 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (2025.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: psutil in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from accelerate) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from accelerate) (1.0.0rc6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from accelerate) (0.6.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (0.28.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typer-slim in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (0.20.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: anyio in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: certifi in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (1.0.9)\n",
            "Requirement already satisfied: idna in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (0.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (from typer-slim->huggingface_hub>=0.21.0->accelerate) (8.3.0)\n",
            "Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "RANDOM_STATE = 42\n",
        "path_DATA = \"../../../data\"\n",
        "csv_path = f\"{path_DATA}/spotify_dataset_sin_duplicados_4.csv\"\n",
        "\n",
        "TESTING = True\n",
        "\n",
        "if TESTING:\n",
        "    NROWS = 50\n",
        "else:\n",
        "    NROWS = None\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_array(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        array_ = json.load(f)  \n",
        "    return array_\n",
        "\n",
        "def get_song_and_target(csv_path, faltantes_path = \"faltantes_according_token.json\", sample_size=None):\n",
        "    df = pd.read_csv(csv_path, nrows=sample_size)\n",
        "    indices_to_remove = get_array(faltantes_path)\n",
        "    if indices_to_remove[-1]>df.shape[0]:\n",
        "        print(\"You are executing in testing way\")\n",
        "    else:\n",
        "        df = df.drop(indices_to_remove).reset_index(drop=True)\n",
        "    X = df['text']\n",
        "    df['Explicit_binary'] = (df['Explicit'].str.lower() == 'yes').astype(int)\n",
        "    y = df['Explicit_binary']\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are executing in testing way\n",
            "(50,)\n",
            "(50,)\n"
          ]
        }
      ],
      "source": [
        "X,y = get_song_and_target(csv_path,\"faltantes_according_token.json\", sample_size=NROWS)\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N-2vq2m_xbw"
      },
      "source": [
        "gpt_oss_20b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: triton in /home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages (3.5.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "pip install triton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Unrecognized keys in `rope_parameters` for 'rope_type'='yarn': {'truncate'}\n",
            "Unrecognized keys in `rope_parameters` for 'rope_type'='yarn': {'truncate'}\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Unrecognized keys in `rope_parameters` for 'rope_type'='yarn': {'truncate'}\n",
            "Unrecognized keys in `rope_parameters` for 'rope_type'='yarn': {'truncate'}\n",
            "MXFP4 quantization requires Triton and kernels installed: CUDA requires Triton >= 3.4.0, XPU requires Triton >= 3.5.0, we will default to dequantizing the model to bf16\n",
            "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.24s/it]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "expected scalar type Half but found BFloat16",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m      6\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m      9\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     10\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain quantum mechanics clearly and concisely.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     15\u001b[0m ]\n\u001b[0;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:325\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 325\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m         chats \u001b[38;5;241m=\u001b[39m (Chat(chat) \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m text_inputs)  \u001b[38;5;66;03m# 🐈 🐈 🐈\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/pipelines/base.py:1261\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1255\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[1;32m   1259\u001b[0m     )\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/pipelines/base.py:1268\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1267\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1268\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1269\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/pipelines/base.py:1172\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1171\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1172\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1173\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:436\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    434\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 436\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    439\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/generation/utils.py:2661\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2658\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m   2660\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[0;32m-> 2661\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2662\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2663\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2667\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2668\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2669\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/generation/utils.py:2874\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2871\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 2874\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2875\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2876\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/utils/generic.py:757\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    756\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 757\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    759\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/models/gpt_oss/modeling_gpt_oss.py:698\u001b[0m, in \u001b[0;36mGptOssForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_router_logits, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m output_router_logits \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    694\u001b[0m     output_router_logits \u001b[38;5;28;01mif\u001b[39;00m output_router_logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_router_logits\n\u001b[1;32m    695\u001b[0m )\n\u001b[1;32m    697\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 698\u001b[0m outputs: MoeModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_router_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_router_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    710\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/utils/generic.py:927\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 927\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[1;32m    932\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/models/gpt_oss/modeling_gpt_oss.py:537\u001b[0m, in \u001b[0;36mGptOssModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 537\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m MoeModelOutputWithPast(\n\u001b[1;32m    549\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    550\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    551\u001b[0m )\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/models/gpt_oss/modeling_gpt_oss.py:416\u001b[0m, in \u001b[0;36mGptOssDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    415\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 416\u001b[0m hidden_states, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# diff with llama: router scores\u001b[39;00m\n\u001b[1;32m    417\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/models/gpt_oss/modeling_gpt_oss.py:170\u001b[0m, in \u001b[0;36mGptOssMLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    169\u001b[0m     router_scores, router_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrouter(hidden_states)\n\u001b[0;32m--> 170\u001b[0m     routed_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouter_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouter_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m routed_out, router_scores\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/models/gpt_oss/modeling_gpt_oss.py:130\u001b[0m, in \u001b[0;36mGptOssExperts.forward\u001b[0;34m(self, hidden_states, router_indices, routing_weights)\u001b[0m\n\u001b[1;32m    128\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mrepeat(num_experts, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    129\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mview(num_experts, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[0;32m--> 130\u001b[0m gate_up \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_up_proj\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_up_proj_bias[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[1;32m    131\u001b[0m gate, up \u001b[38;5;241m=\u001b[39m gate_up[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, ::\u001b[38;5;241m2\u001b[39m], gate_up[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    132\u001b[0m gate \u001b[38;5;241m=\u001b[39m gate\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlimit)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Half but found BFloat16"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"openai/gpt-oss-20b\"\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n",
        "]\n",
        "\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        ")\n",
        "print(outputs[0][\"generated_text\"][-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Unrecognized keys in `rope_parameters` for 'rope_type'='yarn': {'truncate'}\n",
            "Unrecognized keys in `rope_parameters` for 'rope_type'='yarn': {'truncate'}\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Unrecognized keys in `rope_parameters` for 'rope_type'='yarn': {'truncate'}\n",
            "Unrecognized keys in `rope_parameters` for 'rope_type'='yarn': {'truncate'}\n",
            "MXFP4 quantization requires Triton and kernels installed: CUDA requires Triton >= 3.4.0, XPU requires Triton >= 3.5.0, we will default to dequantizing the model to bf16\n",
            "Loading checkpoint shards: 100%|██████████| 3/3 [00:44<00:00, 14.74s/it]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The following `model_kwargs` are not used by the model: ['quantization_config'] (note: typos in the generate arguments will also show up in this list)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# ... El resto de tu código de inferencia ...\u001b[39;00m\n\u001b[1;32m     27\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     28\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain quantum mechanics clearly and concisely.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     29\u001b[0m ]\n\u001b[0;32m---> 31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:325\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 325\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m         chats \u001b[38;5;241m=\u001b[39m (Chat(chat) \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m text_inputs)  \u001b[38;5;66;03m# 🐈 🐈 🐈\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/pipelines/base.py:1261\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1255\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[1;32m   1259\u001b[0m     )\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/pipelines/base.py:1268\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1267\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1268\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1269\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/pipelines/base.py:1172\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1171\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1172\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1173\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:436\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    434\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 436\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    439\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/generation/utils.py:2485\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2481\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2482\u001b[0m     \u001b[38;5;66;03m# type() required to access the unbound class-level method\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m     decoding_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), GENERATION_MODES_MAPPING[generation_mode])\n\u001b[0;32m-> 2485\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_generation_mode(generation_mode, generation_config, generation_mode_kwargs)\n\u001b[1;32m   2488\u001b[0m \u001b[38;5;66;03m# Deprecation-related step: set Hub repo for deprecated strategies.\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m \u001b[38;5;66;03m# NOTE: This must come after initializing generation_config, since we need it to determine if this is a deprecated mode.\u001b[39;00m\n\u001b[1;32m   2490\u001b[0m \u001b[38;5;66;03m# It must also be before any preparation steps, since Hub repos expect to be loaded before preparation steps.\u001b[39;00m\n\u001b[1;32m   2491\u001b[0m \u001b[38;5;66;03m# TODO joao, manuel: remove this in v4.62.0\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/generation/utils.py:1642\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1639\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1642\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1643\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1644\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1645\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['quantization_config'] (note: typos in the generate arguments will also show up in this list)"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "import bitsandbytes\n",
        "model_id = \"openai/gpt-oss-20b\"\n",
        "\n",
        "# --- Configuración para forzar la carga en 8 bits (INT8) ---\n",
        "# Esto ofrece un excelente equilibrio entre ahorro de VRAM y calidad del modelo.\n",
        "int8_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    # El resto de parámetros se pueden omitir, ya que la librería\n",
        "    # usará automáticamente los kernels de 8 bits.\n",
        ")\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    quantization_config=int8_config, \n",
        "    # Usar torch.float16 aquí es común para los cálculos intermedios, \n",
        "    # pero a menudo es inferido por la config de 8 bits si no se especifica.\n",
        "    # Podemos añadirlo explícitamente para mayor control.\n",
        "    torch_dtype=torch.float16, \n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# ... El resto de tu código de inferencia ...\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n",
        "]\n",
        "\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        ")\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m zeroshot_classifier \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero-shot-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMoritzLaurer/deberta-v3-large-zeroshot-v2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m train_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lyric \u001b[38;5;129;01min\u001b[39;00m \u001b[43mX_train\u001b[49m:\n\u001b[1;32m     26\u001b[0m     output \u001b[38;5;241m=\u001b[39m zeroshot_classifier(\n\u001b[1;32m     27\u001b[0m         lyric,\n\u001b[1;32m     28\u001b[0m         classes_verbalized,\n\u001b[1;32m     29\u001b[0m         hypothesis_template\u001b[38;5;241m=\u001b[39mhypothesis_template,\n\u001b[1;32m     30\u001b[0m         multi_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     32\u001b[0m     score_explicit \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m][output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplicit\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     X,\n",
        "#     y,\n",
        "#     test_size=0.2,\n",
        "#     stratify=y,\n",
        "#     random_state=RANDOM_STATE\n",
        "# )\n",
        "\n",
        "# print(\"X_train.shape:\", X_train.shape)\n",
        "# print(\"y_test.shape:\", y_train.shape)\n",
        "\n",
        "\n",
        "hypothesis_template = \"This lyric contains {} content\"\n",
        "classes_verbalized = [\"explicit\", \"not explicit\"]\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "zeroshot_classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\",\n",
        "    device=device\n",
        ")\n",
        "\n",
        "train_scores = []\n",
        "for lyric in X_train:\n",
        "    output = zeroshot_classifier(\n",
        "        lyric,\n",
        "        classes_verbalized,\n",
        "        hypothesis_template=hypothesis_template,\n",
        "        multi_label=False\n",
        "    )\n",
        "    score_explicit = output['scores'][output['labels'].index('explicit')]\n",
        "    # print(output['labels'])\n",
        "    # print(output['scores'])\n",
        "    # print(score_explicit)\n",
        "    train_scores.append(score_explicit)\n",
        "    \n",
        "\n",
        "train_scores = np.array(train_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor threshold: 0.8000000000000002 con F1: 0.6363636363636364\n"
          ]
        }
      ],
      "source": [
        "thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "best_threshold = 0.5\n",
        "best_f1 = 0\n",
        "\n",
        "for t in thresholds:\n",
        "    preds_binary = (train_scores >= t).astype(int)\n",
        "    f1 = f1_score(y_train, preds_binary)\n",
        "    # print(f\"Threshold {t:.2f} -> F1: {f1:.4f}\")\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_threshold = t\n",
        "\n",
        "print(\"Mejor threshold:\", best_threshold, \"con F1:\", best_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy en test: 0.80000000\n",
            "Precision en test: 0.50000000\n",
            "Recall en test: 1.00000000\n",
            "F1 en test: 0.66666667\n"
          ]
        }
      ],
      "source": [
        "test_scores = []\n",
        "for lyric in X_test:\n",
        "    output = zeroshot_classifier(\n",
        "        lyric,\n",
        "        classes_verbalized,\n",
        "        hypothesis_template=hypothesis_template,\n",
        "        multi_label=False\n",
        "    )\n",
        "    score_explicit = output['scores'][output['labels'].index('explicit')]\n",
        "    test_scores.append(score_explicit)\n",
        "\n",
        "test_scores = np.array(test_scores)\n",
        "test_preds = (test_scores >= best_threshold).astype(int)\n",
        "\n",
        "# Métricas\n",
        "f1_test = f1_score(y_test, test_preds)\n",
        "accuracy_test = accuracy_score(y_test, test_preds)\n",
        "precision_test = precision_score(y_test, test_preds)\n",
        "recall_test = recall_score(y_test, test_preds)\n",
        "\n",
        "print(f\"Accuracy en test: {accuracy_test:.8f}\")\n",
        "print(f\"Precision en test: {precision_test:.8f}\")\n",
        "print(f\"Recall en test: {recall_test:.8f}\")\n",
        "print(f\"F1 en test: {f1_test:.8f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prueba con treshold 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probando con:  0.5\n",
            "Accuracy en test: 0.60000000\n",
            "Precision en test: 0.33333333\n",
            "Recall en test: 1.00000000\n",
            "F1 en test: 0.50000000\n"
          ]
        }
      ],
      "source": [
        "best_threshold = 0.5\n",
        "print(\"Probando con: \",best_threshold )\n",
        "test_scores = []\n",
        "for lyric in X_test:\n",
        "    output = zeroshot_classifier(\n",
        "        lyric,\n",
        "        classes_verbalized,\n",
        "        hypothesis_template=hypothesis_template,\n",
        "        multi_label=False\n",
        "    )\n",
        "    score_explicit = output['scores'][output['labels'].index('explicit')]\n",
        "    test_scores.append(score_explicit)\n",
        "\n",
        "test_scores = np.array(test_scores)\n",
        "test_preds = (test_scores >= best_threshold).astype(int)\n",
        "\n",
        "# Métricas\n",
        "f1_test = f1_score(y_test, test_preds)\n",
        "accuracy_test = accuracy_score(y_test, test_preds)\n",
        "precision_test = precision_score(y_test, test_preds)\n",
        "recall_test = recall_score(y_test, test_preds)\n",
        "\n",
        "print(f\"Accuracy en test: {accuracy_test:.8f}\")\n",
        "print(f\"Precision en test: {precision_test:.8f}\")\n",
        "print(f\"Recall en test: {recall_test:.8f}\")\n",
        "print(f\"F1 en test: {f1_test:.8f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "057b4c8a62784c5f90127aac33906a14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61d15c86ae7542a8b9f81de23e9b6095",
              "IPY_MODEL_91affd1f5cc4427cb6421e69d39ef920",
              "IPY_MODEL_369900896591403b8a5fcdeb741e8e5e"
            ],
            "layout": "IPY_MODEL_f564639481b340a5bc992e9066b8b1f7"
          }
        },
        "0bc384d2ac1a449083721058606ff010": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e9bb4b3de344fa690de87ffa54665a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "136a3cced0dd43ed9e6498343201e473": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16ea4ea01eb0420094c98fad93109af8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a80846d7a3d4e63aaa9a959cab46a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d71582957fa9443e8b6797f05907d3b5",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e76945c914c48ebaf2bd7f0c1c177bc",
            "value": 0
          }
        },
        "227378646d414834bca6393bca44404d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c86bac908d0e4754b97f60c14b3fe995",
            "placeholder": "​",
            "style": "IPY_MODEL_22eb42ada91843ffb9dc25a2a0114894",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "22eb42ada91843ffb9dc25a2a0114894": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23fa4720a38b467d88ee494e5127c79e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25096717b950436f8cb736e60e2f4d32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25f68a05821f484a8dec63df745ba234": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50dec452666142348b22db0c9db41d43",
              "IPY_MODEL_4af5ab7a39c24ceab5abedeb9d394517",
              "IPY_MODEL_5dd9f31352874dd59788ac0265e4e50a"
            ],
            "layout": "IPY_MODEL_e4b40b6fdae5407898f724dc82222b67"
          }
        },
        "2f5cc5fd6e204f758474a73c7dc94405": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "369900896591403b8a5fcdeb741e8e5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23fa4720a38b467d88ee494e5127c79e",
            "placeholder": "​",
            "style": "IPY_MODEL_a07a849319964133b874cfcd1a223735",
            "value": " 0/3 [00:02&lt;?, ?it/s]"
          }
        },
        "4af5ab7a39c24ceab5abedeb9d394517": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f91fab6fc354d06872280c4ec9cb723",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_136a3cced0dd43ed9e6498343201e473",
            "value": 3
          }
        },
        "4e4b14122c0144f9afe00022668c6211": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de6f616cb42d4b4e99804e748d6ed3dc",
              "IPY_MODEL_7e789462e1af4cffb90ed4493d44dbcd",
              "IPY_MODEL_f781e0a6d6534eddbe23994a7a1974b0"
            ],
            "layout": "IPY_MODEL_0e9bb4b3de344fa690de87ffa54665a9"
          }
        },
        "4ea4ab8f5eb94bd4a32a2e5e1f83c4cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50dec452666142348b22db0c9db41d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86a41c7f4d5042bb88b8508a9c57162f",
            "placeholder": "​",
            "style": "IPY_MODEL_dceaa9cd06d84d40a0fc9ce3d45a5ebe",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5814ed40c8d04b8b9c4a0e75877db3ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "589efee1d49c4e7397c0311d1cf0ae97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d60a984aa574faeace8417730eb023d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dd9f31352874dd59788ac0265e4e50a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f5cc5fd6e204f758474a73c7dc94405",
            "placeholder": "​",
            "style": "IPY_MODEL_d3bebdb82ddb429ca7442975395157a3",
            "value": " 3/3 [00:00&lt;00:00, 329.63it/s]"
          }
        },
        "61d15c86ae7542a8b9f81de23e9b6095": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d60a984aa574faeace8417730eb023d",
            "placeholder": "​",
            "style": "IPY_MODEL_5814ed40c8d04b8b9c4a0e75877db3ac",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "64762298c70844e183f7821d341a7e78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64c2c9f850e64e7c8a88b39c9ee0c9f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6570956ee8f5454bb2de9cb8310b8c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f91fab6fc354d06872280c4ec9cb723": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "702d4057268d42afb879db1b9228aefc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_227378646d414834bca6393bca44404d",
              "IPY_MODEL_1a80846d7a3d4e63aaa9a959cab46a8e",
              "IPY_MODEL_9c14ab5a478445e0b859650fbc4c9860"
            ],
            "layout": "IPY_MODEL_a64c885449d3458982e17c8676a4f979"
          }
        },
        "7e76945c914c48ebaf2bd7f0c1c177bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e789462e1af4cffb90ed4493d44dbcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25096717b950436f8cb736e60e2f4d32",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ea4ab8f5eb94bd4a32a2e5e1f83c4cf",
            "value": 3
          }
        },
        "86a41c7f4d5042bb88b8508a9c57162f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91affd1f5cc4427cb6421e69d39ef920": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16ea4ea01eb0420094c98fad93109af8",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_589efee1d49c4e7397c0311d1cf0ae97",
            "value": 0
          }
        },
        "9c14ab5a478445e0b859650fbc4c9860": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bc384d2ac1a449083721058606ff010",
            "placeholder": "​",
            "style": "IPY_MODEL_64c2c9f850e64e7c8a88b39c9ee0c9f4",
            "value": " 0/3 [00:00&lt;?, ?it/s]"
          }
        },
        "a07a849319964133b874cfcd1a223735": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a64c885449d3458982e17c8676a4f979": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdcee46a1dad44e09d8728df43a11a06": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c86bac908d0e4754b97f60c14b3fe995": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3bebdb82ddb429ca7442975395157a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d71582957fa9443e8b6797f05907d3b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dceaa9cd06d84d40a0fc9ce3d45a5ebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de6f616cb42d4b4e99804e748d6ed3dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdcee46a1dad44e09d8728df43a11a06",
            "placeholder": "​",
            "style": "IPY_MODEL_6570956ee8f5454bb2de9cb8310b8c8a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e4b40b6fdae5407898f724dc82222b67": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f564639481b340a5bc992e9066b8b1f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f781e0a6d6534eddbe23994a7a1974b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64762298c70844e183f7821d341a7e78",
            "placeholder": "​",
            "style": "IPY_MODEL_fa159e2e1f364b9dbe18c225bf9c6105",
            "value": " 3/3 [00:00&lt;00:00, 338.34it/s]"
          }
        },
        "fa159e2e1f364b9dbe18c225bf9c6105": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
