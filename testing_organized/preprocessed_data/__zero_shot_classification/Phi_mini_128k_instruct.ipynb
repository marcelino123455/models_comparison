{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eseuTtnA3WeD",
        "outputId": "57ea6d49-dd7e-4495-90f8-88a07f9e942f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.3.1 (from versions: 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1, 2.8.0, 2.9.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.3.1\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting flash-attn==2.5.8\n",
            "  Downloading flash_attn-2.5.8.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[40 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m fatal: not a git repository (or any of the parent directories): .git\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m torch.__version__  = 2.9.0+cu128\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m /tmp/pip-install-hzdbp7md/flash-attn_782f853b95924b65a0650000d92535dc/setup.py:78: UserWarning: flash_attn was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.\n",
            "  \u001b[31m   \u001b[0m   warnings.warn(\n",
            "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
            "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m2\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     \u001b[31mexec\u001b[0m\u001b[1;31m(compile('''\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     \u001b[1;31m# This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     ...<32 lines>...\n",
            "  \u001b[31m   \u001b[0m     \u001b[1;31mexec(compile(setup_py_code, filename, \"exec\"))\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     \u001b[1;31m''' % ('/tmp/pip-install-hzdbp7md/flash-attn_782f853b95924b65a0650000d92535dc/setup.py',), \"<pip-setuptools-caller>\", \"exec\"))\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "  \u001b[31m   \u001b[0m   File \u001b[35m\"<pip-setuptools-caller>\"\u001b[0m, line \u001b[35m35\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-install-hzdbp7md/flash-attn_782f853b95924b65a0650000d92535dc/setup.py\"\u001b[0m, line \u001b[35m134\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     \u001b[31mCUDAExtension\u001b[0m\u001b[1;31m(\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
            "  \u001b[31m   \u001b[0m         \u001b[1;31mname=\"flash_attn_2_cuda\",\u001b[0m\n",
            "  \u001b[31m   \u001b[0m         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     ...<81 lines>...\n",
            "  \u001b[31m   \u001b[0m         \u001b[1;31m],\u001b[0m\n",
            "  \u001b[31m   \u001b[0m         \u001b[1;31m^^\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     \u001b[1;31m)\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     \u001b[1;31m^\u001b[0m\n",
            "  \u001b[31m   \u001b[0m   File \u001b[35m\"/home/marcelino.maita/.venv/lib/python3.13/site-packages/torch/utils/cpp_extension.py\"\u001b[0m, line \u001b[35m1347\u001b[0m, in \u001b[35mCUDAExtension\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     library_dirs += \u001b[31mlibrary_paths\u001b[0m\u001b[1;31m(device_type=\"cuda\")\u001b[0m\n",
            "  \u001b[31m   \u001b[0m                     \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "  \u001b[31m   \u001b[0m   File \u001b[35m\"/home/marcelino.maita/.venv/lib/python3.13/site-packages/torch/utils/cpp_extension.py\"\u001b[0m, line \u001b[35m1559\u001b[0m, in \u001b[35mlibrary_paths\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     if (not os.path.exists(\u001b[31m_join_cuda_home\u001b[0m\u001b[1;31m(lib_dir)\u001b[0m) and\n",
            "  \u001b[31m   \u001b[0m                            \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^\u001b[0m\n",
            "  \u001b[31m   \u001b[0m   File \u001b[35m\"/home/marcelino.maita/.venv/lib/python3.13/site-packages/torch/utils/cpp_extension.py\"\u001b[0m, line \u001b[35m2986\u001b[0m, in \u001b[35m_join_cuda_home\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     raise OSError('CUDA_HOME environment variable is not set. '\n",
            "  \u001b[31m   \u001b[0m                   'Please set it to your CUDA install root.')\n",
            "  \u001b[31m   \u001b[0m \u001b[1;35mOSError\u001b[0m: \u001b[35mCUDA_HOME environment variable is not set. Please set it to your CUDA install root.\u001b[0m\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting accelerate==0.31.0\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from accelerate==0.31.0) (2.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from accelerate==0.31.0) (25.0)\n",
            "Requirement already satisfied: psutil in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from accelerate==0.31.0) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from accelerate==0.31.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from accelerate==0.31.0) (2.9.0)\n",
            "Requirement already satisfied: huggingface-hub in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from accelerate==0.31.0) (0.34.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from accelerate==0.31.0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (4.12.2)\n",
            "Requirement already satisfied: setuptools in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from torch>=1.10.0->accelerate==0.31.0) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==0.31.0) (1.3.0)\n",
            "Requirement already satisfied: requests in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from huggingface-hub->accelerate==0.31.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from huggingface-hub->accelerate==0.31.0) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from huggingface-hub->accelerate==0.31.0) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from jinja2->torch>=1.10.0->accelerate==0.31.0) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from requests->huggingface-hub->accelerate==0.31.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from requests->huggingface-hub->accelerate==0.31.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from requests->huggingface-hub->accelerate==0.31.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from requests->huggingface-hub->accelerate==0.31.0) (2025.7.14)\n",
            "Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.31.0\n",
            "Collecting transformers==4.41.2\n",
            "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: filelock in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from transformers==4.41.2) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from transformers==4.41.2) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from transformers==4.41.2) (2.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from transformers==4.41.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from transformers==4.41.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from transformers==4.41.2) (2025.7.34)\n",
            "Requirement already satisfied: requests in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from transformers==4.41.2) (2.32.4)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.2)\n",
            "  Downloading tokenizers-0.19.1.tar.gz (321 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.4.1 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from transformers==4.41.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from transformers==4.41.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (4.12.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (1.1.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from requests->transformers==4.41.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from requests->transformers==4.41.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from requests->transformers==4.41.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/marcelino.maita/.venv/lib/python3.13/site-packages (from requests->transformers==4.41.2) (2025.7.14)\n",
            "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tokenizers\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[143 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m Running `maturin pep517 build-wheel -i /home/marcelino.maita/.venv/bin/python --compatibility off`\n",
            "  \u001b[31m   \u001b[0m Rust not found, installing into a temporary directory\n",
            "  \u001b[31m   \u001b[0m Python reports SOABI: cpython-313-x86_64-linux-gnu\n",
            "  \u001b[31m   \u001b[0m Computed rustc target triple: x86_64-unknown-linux-gnu\n",
            "  \u001b[31m   \u001b[0m Installation directory: /home/marcelino.maita/.cache/puccinialin\n",
            "  \u001b[31m   \u001b[0m Rustup already downloaded\n",
            "  \u001b[31m   \u001b[0m Installing rust to /home/marcelino.maita/.cache/puccinialin/rustup\n",
            "  \u001b[31m   \u001b[0m warn: It looks like you have an existing rustup settings file at:\n",
            "  \u001b[31m   \u001b[0m warn: /home/marcelino.maita/.rustup/settings.toml\n",
            "  \u001b[31m   \u001b[0m warn: Rustup will install the default toolchain as specified in the settings file,\n",
            "  \u001b[31m   \u001b[0m warn: instead of the one inferred from the default host triple.\n",
            "  \u001b[31m   \u001b[0m info: profile set to 'minimal'\n",
            "  \u001b[31m   \u001b[0m info: default host triple is x86_64-unknown-linux-gnu\n",
            "  \u001b[31m   \u001b[0m warn: Updating existing toolchain, profile choice will be ignored\n",
            "  \u001b[31m   \u001b[0m info: syncing channel updates for 'stable-x86_64-unknown-linux-gnu'\n",
            "  \u001b[31m   \u001b[0m info: default toolchain set to 'stable-x86_64-unknown-linux-gnu'\n",
            "  \u001b[31m   \u001b[0m Checking if cargo is installed\n",
            "  \u001b[31m   \u001b[0m cargo 1.90.0 (840b83a10 2025-07-30)\n",
            "  \u001b[31m   \u001b[0m ⚠️  Warning: `project.version` field is required in pyproject.toml unless it is present in the `project.dynamic` list\n",
            "  \u001b[31m   \u001b[0m 🍹 Building a mixed python/rust project\n",
            "  \u001b[31m   \u001b[0m 🔗 Found pyo3 bindings\n",
            "  \u001b[31m   \u001b[0m 🐍 Found CPython 3.13 at /home/marcelino.maita/.venv/bin/python\n",
            "  \u001b[31m   \u001b[0m 📡 Using build options features, bindings from pyproject.toml\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m proc-macro2 v1.0.81\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m unicode-ident v1.0.12\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m autocfg v1.2.0\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m target-lexicon v0.12.14\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m libc v0.2.153\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m once_cell v1.19.0\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m cfg-if v1.0.0\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m memchr v2.7.2\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m cc v1.0.94\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m crossbeam-utils v0.8.19\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m strsim v0.10.0\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m fnv v1.0.7\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m ident_case v1.0.1\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m num-traits v0.2.18\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m serde v1.0.198\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m portable-atomic v1.6.0\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m smallvec v1.13.2\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m pyo3-build-config v0.21.2\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m aho-corasick v1.1.3\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m quote v1.0.36\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m crossbeam-epoch v0.9.18\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m lock_api v0.4.11\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m regex-syntax v0.8.3\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m parking_lot_core v0.9.9\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m syn v2.0.60\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m rayon-core v1.12.1\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m pkg-config v0.3.30\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m either v1.11.0\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m crossbeam-deque v0.8.5\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m getrandom v0.2.14\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m memoffset v0.9.1\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m onig_sys v69.8.1\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m matrixmultiply v0.3.8\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m heck v0.4.1\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m paste v1.0.14\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m scopeguard v1.2.0\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m pyo3-ffi v0.21.2\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m pyo3 v0.21.2\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[31merror\u001b[0m\u001b[1m:\u001b[0m failed to run custom build command for `pyo3-ffi v0.21.2`\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m Caused by:\n",
            "  \u001b[31m   \u001b[0m   process didn't exit successfully: `/tmp/pip-install-flur30x5/tokenizers_d3455625ec684a4abc05673e09ff9449/bindings/python/target/release/build/pyo3-ffi-6b926e6b725f39bd/build-script-build` (exit status: 1)\n",
            "  \u001b[31m   \u001b[0m   --- stdout\n",
            "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=PYO3_CROSS\n",
            "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=PYO3_CROSS_LIB_DIR\n",
            "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=PYO3_CROSS_PYTHON_VERSION\n",
            "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=PYO3_CROSS_PYTHON_IMPLEMENTATION\n",
            "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=PYO3_PRINT_CONFIG\n",
            "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=PYO3_USE_ABI3_FORWARD_COMPATIBILITY\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m   --- stderr\n",
            "  \u001b[31m   \u001b[0m   error: the configured Python interpreter version (3.13) is newer than PyO3's maximum supported version (3.12)\n",
            "  \u001b[31m   \u001b[0m   = help: please check if an updated version of PyO3 is available. Current version: 0.21.2\n",
            "  \u001b[31m   \u001b[0m   = help: set PYO3_USE_ABI3_FORWARD_COMPATIBILITY=1 to suppress this check and build anyway using the stable ABI\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[33mwarning\u001b[0m\u001b[1m:\u001b[0m build failed, waiting for other jobs to finish...\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[33mwarning\u001b[0m\u001b[1m:\u001b[0m onig_sys@69.8.1: In file included from /opt/ohpc/pub/compiler/gcc/12.4.0/lib/gcc/x86_64-pc-linux-gnu/12.4.0/include-fixed/syslimits.h:7,\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[33mwarning\u001b[0m\u001b[1m:\u001b[0m onig_sys@69.8.1:                  from /opt/ohpc/pub/compiler/gcc/12.4.0/lib/gcc/x86_64-pc-linux-gnu/12.4.0/include-fixed/limits.h:34,\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[33mwarning\u001b[0m\u001b[1m:\u001b[0m onig_sys@69.8.1:                  from oniguruma/src/regint.h:109,\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[33mwarning\u001b[0m\u001b[1m:\u001b[0m onig_sys@69.8.1:                  from oniguruma/src/regexec.c:36:\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[33mwarning\u001b[0m\u001b[1m:\u001b[0m onig_sys@69.8.1: /opt/ohpc/pub/compiler/gcc/12.4.0/lib/gcc/x86_64-pc-linux-gnu/12.4.0/include-fixed/limits.h:203:75: error: no include path in which to search for limits.h\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[33mwarning\u001b[0m\u001b[1m:\u001b[0m onig_sys@69.8.1:   203 | #include_next <limits.h>                /* recurse down to the real one */\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[33mwarning\u001b[0m\u001b[1m:\u001b[0m onig_sys@69.8.1:       |                                                                           ^\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[33mwarning\u001b[0m\u001b[1m:\u001b[0m onig_sys@69.8.1: oniguruma/src/regint.h:110:10: fatal error: stdlib.h: No such file or directory\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[33mwarning\u001b[0m\u001b[1m:\u001b[0m onig_sys@69.8.1:   110 | #include <stdlib.h>\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[33mwarning\u001b[0m\u001b[1m:\u001b[0m onig_sys@69.8.1:       |          ^~~~~~~~~~\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[33mwarning\u001b[0m\u001b[1m:\u001b[0m onig_sys@69.8.1: compilation terminated.\n",
            "  \u001b[31m   \u001b[0m \u001b[1m\u001b[31merror\u001b[0m\u001b[1m:\u001b[0m failed to run custom build command for `onig_sys v69.8.1`\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m Caused by:\n",
            "  \u001b[31m   \u001b[0m   process didn't exit successfully: `/tmp/pip-install-flur30x5/tokenizers_d3455625ec684a4abc05673e09ff9449/bindings/python/target/release/build/onig_sys-69e531e23122b013/build-script-build` (exit status: 1)\n",
            "  \u001b[31m   \u001b[0m   --- stdout\n",
            "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=RUSTONIG_DYNAMIC_LIBONIG\n",
            "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=RUSTONIG_STATIC_LIBONIG\n",
            "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=RUSTONIG_SYSTEM_LIBONIG\n",
            "  \u001b[31m   \u001b[0m   TARGET = Some(\"x86_64-unknown-linux-gnu\")\n",
            "  \u001b[31m   \u001b[0m   OPT_LEVEL = Some(\"3\")\n",
            "  \u001b[31m   \u001b[0m   HOST = Some(\"x86_64-unknown-linux-gnu\")\n",
            "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=CC_x86_64-unknown-linux-gnu\n",
            "  \u001b[31m   \u001b[0m   CC_x86_64-unknown-linux-gnu = None\n",
            "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=CC_x86_64_unknown_linux_gnu\n",
            "  \u001b[31m   \u001b[0m   CC_x86_64_unknown_linux_gnu = None\n",
            "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=HOST_CC\n",
            "  \u001b[31m   \u001b[0m   HOST_CC = None\n",
            "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=CC\n",
            "  \u001b[31m   \u001b[0m   CC = None\n",
            "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=CC_ENABLE_DEBUG_OUTPUT\n",
            "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=CRATE_CC_NO_DEFAULTS\n",
            "  \u001b[31m   \u001b[0m   CRATE_CC_NO_DEFAULTS = None\n",
            "  \u001b[31m   \u001b[0m   DEBUG = Some(\"false\")\n",
            "  \u001b[31m   \u001b[0m   CARGO_CFG_TARGET_FEATURE = Some(\"fxsr,sse,sse2\")\n",
            "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=CFLAGS_x86_64-unknown-linux-gnu\n",
            "  \u001b[31m   \u001b[0m   CFLAGS_x86_64-unknown-linux-gnu = None\n",
            "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=CFLAGS_x86_64_unknown_linux_gnu\n",
            "  \u001b[31m   \u001b[0m   CFLAGS_x86_64_unknown_linux_gnu = None\n",
            "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=HOST_CFLAGS\n",
            "  \u001b[31m   \u001b[0m   HOST_CFLAGS = None\n",
            "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=CFLAGS\n",
            "  \u001b[31m   \u001b[0m   CFLAGS = None\n",
            "  \u001b[31m   \u001b[0m   cargo:warning=In file included from /opt/ohpc/pub/compiler/gcc/12.4.0/lib/gcc/x86_64-pc-linux-gnu/12.4.0/include-fixed/syslimits.h:7,\n",
            "  \u001b[31m   \u001b[0m   cargo:warning=                 from /opt/ohpc/pub/compiler/gcc/12.4.0/lib/gcc/x86_64-pc-linux-gnu/12.4.0/include-fixed/limits.h:34,\n",
            "  \u001b[31m   \u001b[0m   cargo:warning=                 from oniguruma/src/regint.h:109,\n",
            "  \u001b[31m   \u001b[0m   cargo:warning=                 from oniguruma/src/regexec.c:36:\n",
            "  \u001b[31m   \u001b[0m   cargo:warning=/opt/ohpc/pub/compiler/gcc/12.4.0/lib/gcc/x86_64-pc-linux-gnu/12.4.0/include-fixed/limits.h:203:75: error: no include path in which to search for limits.h\n",
            "  \u001b[31m   \u001b[0m   cargo:warning=  203 | #include_next <limits.h>                /* recurse down to the real one */\n",
            "  \u001b[31m   \u001b[0m   cargo:warning=      |                                                                           ^\n",
            "  \u001b[31m   \u001b[0m   cargo:warning=oniguruma/src/regint.h:110:10: fatal error: stdlib.h: No such file or directory\n",
            "  \u001b[31m   \u001b[0m   cargo:warning=  110 | #include <stdlib.h>\n",
            "  \u001b[31m   \u001b[0m   cargo:warning=      |          ^~~~~~~~~~\n",
            "  \u001b[31m   \u001b[0m   cargo:warning=compilation terminated.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m   --- stderr\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m   error occurred: Command \"cc\" \"-O3\" \"-ffunction-sections\" \"-fdata-sections\" \"-fPIC\" \"-m64\" \"-I\" \"/tmp/pip-install-flur30x5/tokenizers_d3455625ec684a4abc05673e09ff9449/bindings/python/target/release/build/onig_sys-c31f50857f5172bf/out\" \"-I\" \"oniguruma/src\" \"-DHAVE_UNISTD_H=1\" \"-DHAVE_SYS_TYPES_H=1\" \"-DHAVE_SYS_TIME_H=1\" \"-o\" \"/tmp/pip-install-flur30x5/tokenizers_d3455625ec684a4abc05673e09ff9449/bindings/python/target/release/build/onig_sys-c31f50857f5172bf/out/c77b18e714869709-regexec.o\" \"-c\" \"oniguruma/src/regexec.c\" with args cc did not execute successfully (status code exit status: 1).\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m 💥 maturin failed\n",
            "  \u001b[31m   \u001b[0m   Caused by: Failed to build a native library through cargo\n",
            "  \u001b[31m   \u001b[0m   Caused by: Cargo build finished with \"exit status: 101\": `env -u CARGO PYO3_BUILD_EXTENSION_MODULE=\"1\" PYO3_ENVIRONMENT_SIGNATURE=\"cpython-3.13-64bit\" PYO3_PYTHON=\"/home/marcelino.maita/.venv/bin/python\" PYTHON_SYS_EXECUTABLE=\"/home/marcelino.maita/.venv/bin/python\" \"cargo\" \"rustc\" \"--features\" \"pyo3/extension-module\" \"--message-format\" \"json-render-diagnostics\" \"--manifest-path\" \"/tmp/pip-install-flur30x5/tokenizers_d3455625ec684a4abc05673e09ff9449/bindings/python/Cargo.toml\" \"--release\" \"--lib\"`\n",
            "  \u001b[31m   \u001b[0m Error: command ['maturin', 'pep517', 'build-wheel', '-i', '/home/marcelino.maita/.venv/bin/python', '--compatibility', 'off'] returned non-zero exit status 1\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build tokenizers\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mfailed-wheel-build-for-install\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Failed to build installable wheels for some pyproject.toml based projects\n",
            "\u001b[31m╰─>\u001b[0m tokenizers\n"
          ]
        }
      ],
      "source": [
        "# !pip install torch==2.3.1\n",
        "!pip install flash-attn==2.5.8 --no-build-isolation\n",
        "!pip install accelerate==0.31.0\n",
        "!pip install transformers==4.41.2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/marcelino.maita/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2025-10-25 01:17:57.132017: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-10-25 01:17:57.200924: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-10-25 01:18:01.588309: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "RANDOM_STATE = 42\n",
        "path_DATA = \"../../../data\"\n",
        "csv_path = f\"{path_DATA}/spotify_dataset_sin_duplicados_4.csv\"\n",
        "\n",
        "TESTING = True\n",
        "\n",
        "if TESTING:\n",
        "    NROWS = 50\n",
        "else:\n",
        "    NROWS = None\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_array(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        array_ = json.load(f)  \n",
        "    return array_\n",
        "\n",
        "def get_song_and_target(csv_path, faltantes_path = \"faltantes_according_token.json\", sample_size=None):\n",
        "    df = pd.read_csv(csv_path, nrows=sample_size)\n",
        "    indices_to_remove = get_array(faltantes_path)\n",
        "    if indices_to_remove[-1]>df.shape[0]:\n",
        "        print(\"You are executing in testing way\")\n",
        "    else:\n",
        "        df = df.drop(indices_to_remove).reset_index(drop=True)\n",
        "    X = df['text']\n",
        "    df['Explicit_binary'] = (df['Explicit'].str.lower() == 'yes').astype(int)\n",
        "    y = df['Explicit_binary']\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are executing in testing way\n",
            "(50,)\n",
            "(50,)\n"
          ]
        }
      ],
      "source": [
        "X,y = get_song_and_target(csv_path,\"faltantes_according_token.json\", sample_size=NROWS)\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N-2vq2m_xbw"
      },
      "source": [
        "microsoft/Phi-3-mini-128k-instruct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/marcelino.maita/.conda/envs/hf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
            "- configuration_phi3.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
            "- modeling_phi3.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Cancellation requested; stopping current tasks.\n",
            "Fetching 2 files:   0%|          | 0/2 [01:03<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, pipeline \n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m) \n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/Phi-3-mini-4k-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m \n\u001b[1;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/Phi-3-mini-4k-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m     14\u001b[0m messages \u001b[38;5;241m=\u001b[39m [ \n\u001b[1;32m     15\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful AI assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m}, \n\u001b[1;32m     16\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan you provide ways to eat combinations of bananas and dragonfruits?\u001b[39m\u001b[38;5;124m\"\u001b[39m}, \n\u001b[1;32m     17\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\u001b[39m\u001b[38;5;124m\"\u001b[39m}, \n\u001b[1;32m     18\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat about solving an 2x + 3 = 7 equation?\u001b[39m\u001b[38;5;124m\"\u001b[39m}, \n\u001b[1;32m     19\u001b[0m ] \n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:365\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m         model_class\u001b[38;5;241m.\u001b[39mregister_for_auto_class(auto_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    364\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping:\n\u001b[1;32m    369\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/modeling_utils.py:270\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/modeling_utils.py:4417\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4412\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   4413\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA kernel_config was provided but use_kernels is False; setting use_kernels=True automatically. To suppress this warning, explicitly set use_kernels to True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4414\u001b[0m     )\n\u001b[1;32m   4415\u001b[0m     use_kernels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 4417\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4421\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_kwargs_with_commit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4423\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformers_weights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4426\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4428\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gguf_file:\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/modeling_utils.py:1078\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, variant, gguf_file, use_safetensors, download_kwargs, user_agent, is_remote_code, transformers_explicit_filename)\u001b[0m\n\u001b[1;32m   1076\u001b[0m sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[0;32m-> 1078\u001b[0m     checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1092\u001b[0m     checkpoint_files \u001b[38;5;241m=\u001b[39m [resolved_archive_file] \u001b[38;5;28;01mif\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/utils/hub.py:1026\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m shard_filenames, sharded_metadata\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# At this stage pretrained_model_name_or_path is a model identifier on the Hub. Try to get everything from cache,\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;66;03m# or download the files\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m cached_filenames \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshard_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached_filenames, sharded_metadata\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/transformers/utils/hub.py:483\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m         hf_hub_download(\n\u001b[1;32m    470\u001b[0m             path_or_repo_id,\n\u001b[1;32m    471\u001b[0m             filenames[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    481\u001b[0m         )\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m         \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:89\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         validate_repo_id(arg_value)\n\u001b[1;32m     87\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_legacy_arguments(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/huggingface_hub/_snapshot_download.py:412\u001b[0m, in \u001b[0;36msnapshot_download\u001b[0;34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, dry_run)\u001b[0m\n\u001b[1;32m    410\u001b[0m         _inner_hf_hub_download(file)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_inner_hf_hub_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001b[39;49;00m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dry_run:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(r, DryRunFileInfo) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results)\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/tqdm/contrib/concurrent.py:69\u001b[0m, in \u001b[0;36mthread_map\u001b[0;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/tqdm/contrib/concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name\u001b[38;5;241m=\u001b[39mlock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                       initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
            "File \u001b[0;32m~/.conda/envs/hf/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch \n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
        "\n",
        "torch.random.manual_seed(0) \n",
        "model = AutoModelForCausalLM.from_pretrained( \n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",  \n",
        "    device_map=\"cuda\",  \n",
        "    torch_dtype=\"auto\",  \n",
        "    trust_remote_code=True,  \n",
        ") \n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\") \n",
        "\n",
        "messages = [ \n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
        "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}, \n",
        "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"}, \n",
        "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"}, \n",
        "] \n",
        "\n",
        "pipe = pipeline( \n",
        "    \"text-generation\", \n",
        "    model=model, \n",
        "    tokenizer=tokenizer, \n",
        ") \n",
        "\n",
        "generation_args = { \n",
        "    \"max_new_tokens\": 500, \n",
        "    \"return_full_text\": False, \n",
        "    \"temperature\": 0.0, \n",
        "    \"do_sample\": False, \n",
        "} \n",
        "\n",
        "output = pipe(messages, **generation_args) \n",
        "print(output[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train.shape: (40,)\n",
            "y_test.shape: (40,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(\"X_train.shape:\", X_train.shape)\n",
        "print(\"y_test.shape:\", y_train.shape)\n",
        "\n",
        "\n",
        "hypothesis_template = \"This lyric contains {} content\"\n",
        "classes_verbalized = [\"explicit\", \"not explicit\"]\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "zeroshot_classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\",\n",
        "    device=device\n",
        ")\n",
        "\n",
        "train_scores = []\n",
        "for lyric in X_train:\n",
        "    output = zeroshot_classifier(\n",
        "        lyric,\n",
        "        classes_verbalized,\n",
        "        hypothesis_template=hypothesis_template,\n",
        "        multi_label=False\n",
        "    )\n",
        "    score_explicit = output['scores'][output['labels'].index('explicit')]\n",
        "    # print(output['labels'])\n",
        "    # print(output['scores'])\n",
        "    # print(score_explicit)\n",
        "    train_scores.append(score_explicit)\n",
        "    \n",
        "\n",
        "train_scores = np.array(train_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor threshold: 0.8000000000000002 con F1: 0.6363636363636364\n"
          ]
        }
      ],
      "source": [
        "thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "best_threshold = 0.5\n",
        "best_f1 = 0\n",
        "\n",
        "for t in thresholds:\n",
        "    preds_binary = (train_scores >= t).astype(int)\n",
        "    f1 = f1_score(y_train, preds_binary)\n",
        "    # print(f\"Threshold {t:.2f} -> F1: {f1:.4f}\")\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_threshold = t\n",
        "\n",
        "print(\"Mejor threshold:\", best_threshold, \"con F1:\", best_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy en test: 0.80000000\n",
            "Precision en test: 0.50000000\n",
            "Recall en test: 1.00000000\n",
            "F1 en test: 0.66666667\n"
          ]
        }
      ],
      "source": [
        "test_scores = []\n",
        "for lyric in X_test:\n",
        "    output = zeroshot_classifier(\n",
        "        lyric,\n",
        "        classes_verbalized,\n",
        "        hypothesis_template=hypothesis_template,\n",
        "        multi_label=False\n",
        "    )\n",
        "    score_explicit = output['scores'][output['labels'].index('explicit')]\n",
        "    test_scores.append(score_explicit)\n",
        "\n",
        "test_scores = np.array(test_scores)\n",
        "test_preds = (test_scores >= best_threshold).astype(int)\n",
        "\n",
        "# Métricas\n",
        "f1_test = f1_score(y_test, test_preds)\n",
        "accuracy_test = accuracy_score(y_test, test_preds)\n",
        "precision_test = precision_score(y_test, test_preds)\n",
        "recall_test = recall_score(y_test, test_preds)\n",
        "\n",
        "print(f\"Accuracy en test: {accuracy_test:.8f}\")\n",
        "print(f\"Precision en test: {precision_test:.8f}\")\n",
        "print(f\"Recall en test: {recall_test:.8f}\")\n",
        "print(f\"F1 en test: {f1_test:.8f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prueba con treshold 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probando con:  0.5\n",
            "Accuracy en test: 0.60000000\n",
            "Precision en test: 0.33333333\n",
            "Recall en test: 1.00000000\n",
            "F1 en test: 0.50000000\n"
          ]
        }
      ],
      "source": [
        "best_threshold = 0.5\n",
        "print(\"Probando con: \",best_threshold )\n",
        "test_scores = []\n",
        "for lyric in X_test:\n",
        "    output = zeroshot_classifier(\n",
        "        lyric,\n",
        "        classes_verbalized,\n",
        "        hypothesis_template=hypothesis_template,\n",
        "        multi_label=False\n",
        "    )\n",
        "    score_explicit = output['scores'][output['labels'].index('explicit')]\n",
        "    test_scores.append(score_explicit)\n",
        "\n",
        "test_scores = np.array(test_scores)\n",
        "test_preds = (test_scores >= best_threshold).astype(int)\n",
        "\n",
        "# Métricas\n",
        "f1_test = f1_score(y_test, test_preds)\n",
        "accuracy_test = accuracy_score(y_test, test_preds)\n",
        "precision_test = precision_score(y_test, test_preds)\n",
        "recall_test = recall_score(y_test, test_preds)\n",
        "\n",
        "print(f\"Accuracy en test: {accuracy_test:.8f}\")\n",
        "print(f\"Precision en test: {precision_test:.8f}\")\n",
        "print(f\"Recall en test: {recall_test:.8f}\")\n",
        "print(f\"F1 en test: {f1_test:.8f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "057b4c8a62784c5f90127aac33906a14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61d15c86ae7542a8b9f81de23e9b6095",
              "IPY_MODEL_91affd1f5cc4427cb6421e69d39ef920",
              "IPY_MODEL_369900896591403b8a5fcdeb741e8e5e"
            ],
            "layout": "IPY_MODEL_f564639481b340a5bc992e9066b8b1f7"
          }
        },
        "0bc384d2ac1a449083721058606ff010": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e9bb4b3de344fa690de87ffa54665a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "136a3cced0dd43ed9e6498343201e473": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16ea4ea01eb0420094c98fad93109af8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a80846d7a3d4e63aaa9a959cab46a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d71582957fa9443e8b6797f05907d3b5",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e76945c914c48ebaf2bd7f0c1c177bc",
            "value": 0
          }
        },
        "227378646d414834bca6393bca44404d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c86bac908d0e4754b97f60c14b3fe995",
            "placeholder": "​",
            "style": "IPY_MODEL_22eb42ada91843ffb9dc25a2a0114894",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "22eb42ada91843ffb9dc25a2a0114894": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23fa4720a38b467d88ee494e5127c79e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25096717b950436f8cb736e60e2f4d32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25f68a05821f484a8dec63df745ba234": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50dec452666142348b22db0c9db41d43",
              "IPY_MODEL_4af5ab7a39c24ceab5abedeb9d394517",
              "IPY_MODEL_5dd9f31352874dd59788ac0265e4e50a"
            ],
            "layout": "IPY_MODEL_e4b40b6fdae5407898f724dc82222b67"
          }
        },
        "2f5cc5fd6e204f758474a73c7dc94405": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "369900896591403b8a5fcdeb741e8e5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23fa4720a38b467d88ee494e5127c79e",
            "placeholder": "​",
            "style": "IPY_MODEL_a07a849319964133b874cfcd1a223735",
            "value": " 0/3 [00:02&lt;?, ?it/s]"
          }
        },
        "4af5ab7a39c24ceab5abedeb9d394517": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f91fab6fc354d06872280c4ec9cb723",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_136a3cced0dd43ed9e6498343201e473",
            "value": 3
          }
        },
        "4e4b14122c0144f9afe00022668c6211": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de6f616cb42d4b4e99804e748d6ed3dc",
              "IPY_MODEL_7e789462e1af4cffb90ed4493d44dbcd",
              "IPY_MODEL_f781e0a6d6534eddbe23994a7a1974b0"
            ],
            "layout": "IPY_MODEL_0e9bb4b3de344fa690de87ffa54665a9"
          }
        },
        "4ea4ab8f5eb94bd4a32a2e5e1f83c4cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50dec452666142348b22db0c9db41d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86a41c7f4d5042bb88b8508a9c57162f",
            "placeholder": "​",
            "style": "IPY_MODEL_dceaa9cd06d84d40a0fc9ce3d45a5ebe",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5814ed40c8d04b8b9c4a0e75877db3ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "589efee1d49c4e7397c0311d1cf0ae97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d60a984aa574faeace8417730eb023d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dd9f31352874dd59788ac0265e4e50a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f5cc5fd6e204f758474a73c7dc94405",
            "placeholder": "​",
            "style": "IPY_MODEL_d3bebdb82ddb429ca7442975395157a3",
            "value": " 3/3 [00:00&lt;00:00, 329.63it/s]"
          }
        },
        "61d15c86ae7542a8b9f81de23e9b6095": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d60a984aa574faeace8417730eb023d",
            "placeholder": "​",
            "style": "IPY_MODEL_5814ed40c8d04b8b9c4a0e75877db3ac",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "64762298c70844e183f7821d341a7e78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64c2c9f850e64e7c8a88b39c9ee0c9f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6570956ee8f5454bb2de9cb8310b8c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f91fab6fc354d06872280c4ec9cb723": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "702d4057268d42afb879db1b9228aefc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_227378646d414834bca6393bca44404d",
              "IPY_MODEL_1a80846d7a3d4e63aaa9a959cab46a8e",
              "IPY_MODEL_9c14ab5a478445e0b859650fbc4c9860"
            ],
            "layout": "IPY_MODEL_a64c885449d3458982e17c8676a4f979"
          }
        },
        "7e76945c914c48ebaf2bd7f0c1c177bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e789462e1af4cffb90ed4493d44dbcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25096717b950436f8cb736e60e2f4d32",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ea4ab8f5eb94bd4a32a2e5e1f83c4cf",
            "value": 3
          }
        },
        "86a41c7f4d5042bb88b8508a9c57162f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91affd1f5cc4427cb6421e69d39ef920": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16ea4ea01eb0420094c98fad93109af8",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_589efee1d49c4e7397c0311d1cf0ae97",
            "value": 0
          }
        },
        "9c14ab5a478445e0b859650fbc4c9860": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bc384d2ac1a449083721058606ff010",
            "placeholder": "​",
            "style": "IPY_MODEL_64c2c9f850e64e7c8a88b39c9ee0c9f4",
            "value": " 0/3 [00:00&lt;?, ?it/s]"
          }
        },
        "a07a849319964133b874cfcd1a223735": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a64c885449d3458982e17c8676a4f979": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdcee46a1dad44e09d8728df43a11a06": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c86bac908d0e4754b97f60c14b3fe995": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3bebdb82ddb429ca7442975395157a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d71582957fa9443e8b6797f05907d3b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dceaa9cd06d84d40a0fc9ce3d45a5ebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de6f616cb42d4b4e99804e748d6ed3dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdcee46a1dad44e09d8728df43a11a06",
            "placeholder": "​",
            "style": "IPY_MODEL_6570956ee8f5454bb2de9cb8310b8c8a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e4b40b6fdae5407898f724dc82222b67": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f564639481b340a5bc992e9066b8b1f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f781e0a6d6534eddbe23994a7a1974b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64762298c70844e183f7821d341a7e78",
            "placeholder": "​",
            "style": "IPY_MODEL_fa159e2e1f364b9dbe18c225bf9c6105",
            "value": " 3/3 [00:00&lt;00:00, 338.34it/s]"
          }
        },
        "fa159e2e1f364b9dbe18c225bf9c6105": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
