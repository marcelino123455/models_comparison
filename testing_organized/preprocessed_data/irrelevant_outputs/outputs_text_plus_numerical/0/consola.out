2025-09-29 00:38:54.694834: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text_plus_numerical.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
For both embbedings your are adding this columns: 
--> ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
--> PaTH:  ../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy

##################################################
Running experiment with TFIDF embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5020)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5020)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5020, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4467, Test Loss: 0.4240, F1: 0.6970, AUC: 0.9036
Epoch [10/30] Train Loss: 0.1973, Test Loss: 0.4269, F1: 0.7096, AUC: 0.9106
Epoch [20/30] Train Loss: 0.0590, Test Loss: 0.5819, F1: 0.7100, AUC: 0.9056
Mejores resultados en la época:  12
f1-score 0.7222643416729846
AUC según el mejor F1-score 0.9107965569436696
Confusion Matrix:
 [[14041  2424]
 [  873  4287]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_160705.png
Accuracy:   0.8475
Precision:  0.6388
Recall:     0.8308
F1-score:   0.7223

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4483, Test Loss: 0.3834, F1: 0.7105, AUC: 0.9037
Epoch [10/30] Train Loss: 0.1987, Test Loss: 0.4162, F1: 0.7141, AUC: 0.9109
Epoch [20/30] Train Loss: 0.0576, Test Loss: 0.6247, F1: 0.6971, AUC: 0.9041
Mejores resultados en la época:  2
f1-score 0.7219630205633316
AUC según el mejor F1-score 0.9094170215420543
Confusion Matrix:
 [[14229  2236]
 [  982  4178]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_160705.png
Accuracy:   0.8512
Precision:  0.6514
Recall:     0.8097
F1-score:   0.7220

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4485, Test Loss: 0.3842, F1: 0.7131, AUC: 0.9040
Epoch [10/30] Train Loss: 0.2042, Test Loss: 0.4174, F1: 0.7124, AUC: 0.9112
Epoch [20/30] Train Loss: 0.0716, Test Loss: 0.5816, F1: 0.6993, AUC: 0.9025
Mejores resultados en la época:  4
f1-score 0.7252256123764503
AUC según el mejor F1-score 0.9107213210074458
Confusion Matrix:
 [[14209  2256]
 [  941  4219]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_160705.png
Accuracy:   0.8522
Precision:  0.6516
Recall:     0.8176
F1-score:   0.7252

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4500, Test Loss: 0.3710, F1: 0.7145, AUC: 0.9033
Epoch [10/30] Train Loss: 0.2085, Test Loss: 0.4325, F1: 0.7085, AUC: 0.9112
Epoch [20/30] Train Loss: 0.0660, Test Loss: 0.6004, F1: 0.7000, AUC: 0.9035
Mejores resultados en la época:  8
f1-score 0.7257325611548102
AUC según el mejor F1-score 0.9112247085078284
Confusion Matrix:
 [[14254  2211]
 [  962  4198]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_160705.png
Accuracy:   0.8533
Precision:  0.6550
Recall:     0.8136
F1-score:   0.7257
Tiempo total para red 1: 382.68 segundos

Entrenando red 2 con capas [5020, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4310, Test Loss: 0.3642, F1: 0.7184, AUC: 0.9076
Epoch [10/30] Train Loss: 0.0202, Test Loss: 1.0810, F1: 0.6763, AUC: 0.8949
Epoch [20/30] Train Loss: 0.0006, Test Loss: 1.5651, F1: 0.6839, AUC: 0.8925
Mejores resultados en la época:  4
f1-score 0.727771115298955
AUC según el mejor F1-score 0.9130228320821475
Confusion Matrix:
 [[14199  2266]
 [  912  4248]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_323457.png
Accuracy:   0.8530
Precision:  0.6521
Recall:     0.8233
F1-score:   0.7278

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4290, Test Loss: 0.3662, F1: 0.7202, AUC: 0.9070
Epoch [10/30] Train Loss: 0.0131, Test Loss: 1.0086, F1: 0.6987, AUC: 0.8987
Epoch [20/30] Train Loss: 0.0008, Test Loss: 1.5385, F1: 0.6975, AUC: 0.8978
Mejores resultados en la época:  2
f1-score 0.7258362750042023
AUC según el mejor F1-score 0.9150442152369249
Confusion Matrix:
 [[14045  2420]
 [  842  4318]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_323457.png
Accuracy:   0.8492
Precision:  0.6408
Recall:     0.8368
F1-score:   0.7258

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4280, Test Loss: 0.3465, F1: 0.7250, AUC: 0.9079
Epoch [10/30] Train Loss: 0.0056, Test Loss: 0.9924, F1: 0.7057, AUC: 0.9008
Epoch [20/30] Train Loss: 0.0028, Test Loss: 1.5031, F1: 0.6818, AUC: 0.8954
Mejores resultados en la época:  2
f1-score 0.7308975785060323
AUC según el mejor F1-score 0.9165571614206316
Confusion Matrix:
 [[14209  2256]
 [  889  4271]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_323457.png
Accuracy:   0.8546
Precision:  0.6544
Recall:     0.8277
F1-score:   0.7309

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4289, Test Loss: 0.3857, F1: 0.7091, AUC: 0.9079
Epoch [10/30] Train Loss: 0.0130, Test Loss: 1.0048, F1: 0.6947, AUC: 0.8985
Epoch [20/30] Train Loss: 0.0010, Test Loss: 1.7289, F1: 0.6823, AUC: 0.8980
Mejores resultados en la época:  3
f1-score 0.7319825596306745
AUC según el mejor F1-score 0.9152865839448019
Confusion Matrix:
 [[14209  2256]
 [  879  4281]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_323457.png
Accuracy:   0.8550
Precision:  0.6549
Recall:     0.8297
F1-score:   0.7320
Tiempo total para red 2: 399.58 segundos

Entrenando red 3 con capas [5020, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4232, Test Loss: 0.3898, F1: 0.7133, AUC: 0.9088
Epoch [10/30] Train Loss: 0.0107, Test Loss: 1.4118, F1: 0.6805, AUC: 0.8924
Epoch [20/30] Train Loss: 0.0041, Test Loss: 1.6384, F1: 0.6824, AUC: 0.8962
Mejores resultados en la época:  1
f1-score 0.7215232343566516
AUC según el mejor F1-score 0.9155085546743503
Confusion Matrix:
 [[14004  2461]
 [  859  4301]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_653057.png
Accuracy:   0.8465
Precision:  0.6361
Recall:     0.8335
F1-score:   0.7215

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4217, Test Loss: 0.3619, F1: 0.7212, AUC: 0.9081
Epoch [10/30] Train Loss: 0.0052, Test Loss: 1.4628, F1: 0.6786, AUC: 0.8917
Epoch [20/30] Train Loss: 0.0029, Test Loss: 1.8523, F1: 0.6717, AUC: 0.8958
Mejores resultados en la época:  3
f1-score 0.7269941261598706
AUC según el mejor F1-score 0.9134112705598203
Confusion Matrix:
 [[14148  2317]
 [  890  4270]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_653057.png
Accuracy:   0.8517
Precision:  0.6482
Recall:     0.8275
F1-score:   0.7270

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4226, Test Loss: 0.3533, F1: 0.7269, AUC: 0.9101
Epoch [10/30] Train Loss: 0.0145, Test Loss: 1.2978, F1: 0.6818, AUC: 0.8944
Epoch [20/30] Train Loss: 0.0064, Test Loss: 1.3684, F1: 0.6954, AUC: 0.8980
Mejores resultados en la época:  2
f1-score 0.7395916911830258
AUC según el mejor F1-score 0.9161030092020426
Confusion Matrix:
 [[14556  1909]
 [ 1012  4148]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_653057.png
Accuracy:   0.8649
Precision:  0.6848
Recall:     0.8039
F1-score:   0.7396

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4259, Test Loss: 0.3746, F1: 0.7141, AUC: 0.9073
Epoch [10/30] Train Loss: 0.0080, Test Loss: 1.2107, F1: 0.7064, AUC: 0.8946
Epoch [20/30] Train Loss: 0.0070, Test Loss: 1.3878, F1: 0.6916, AUC: 0.8980
Mejores resultados en la época:  1
f1-score 0.7305816788575409
AUC según el mejor F1-score 0.9155951960583526
Confusion Matrix:
 [[14336  2129]
 [  965  4195]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_653057.png
Accuracy:   0.8569
Precision:  0.6633
Recall:     0.8130
F1-score:   0.7306
Tiempo total para red 3: 414.38 segundos

Entrenando red 4 con capas [5020, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4224, Test Loss: 0.3570, F1: 0.7322, AUC: 0.9117
Epoch [10/30] Train Loss: 0.0072, Test Loss: 1.2408, F1: 0.6864, AUC: 0.9007
Epoch [20/30] Train Loss: 0.0024, Test Loss: 1.5061, F1: 0.6759, AUC: 0.9033
Mejores resultados en la época:  0
f1-score 0.7322236294554008
AUC según el mejor F1-score 0.911706491571268
Confusion Matrix:
 [[14618  1847]
 [ 1113  4047]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_1328641.png
Accuracy:   0.8631
Precision:  0.6866
Recall:     0.7843
F1-score:   0.7322

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4197, Test Loss: 0.3237, F1: 0.7367, AUC: 0.9135
Epoch [10/30] Train Loss: 0.0058, Test Loss: 1.4488, F1: 0.6971, AUC: 0.9007
Epoch [20/30] Train Loss: 0.0022, Test Loss: 1.7871, F1: 0.6874, AUC: 0.8970
Mejores resultados en la época:  0
f1-score 0.7366744730679157
AUC según el mejor F1-score 0.9134869714239978
Confusion Matrix:
 [[14882  1583]
 [ 1228  3932]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_1328641.png
Accuracy:   0.8700
Precision:  0.7130
Recall:     0.7620
F1-score:   0.7367

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4196, Test Loss: 0.3720, F1: 0.7176, AUC: 0.9129
Epoch [10/30] Train Loss: 0.0052, Test Loss: 1.3209, F1: 0.6951, AUC: 0.8988
Epoch [20/30] Train Loss: 0.0043, Test Loss: 1.2633, F1: 0.6883, AUC: 0.9008
Mejores resultados en la época:  0
f1-score 0.7176499958392277
AUC según el mejor F1-score 0.9129113965023293
Confusion Matrix:
 [[13920  2545]
 [  848  4312]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_1328641.png
Accuracy:   0.8431
Precision:  0.6288
Recall:     0.8357
F1-score:   0.7176

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4217, Test Loss: 0.3524, F1: 0.7249, AUC: 0.9142
Epoch [10/30] Train Loss: 0.0079, Test Loss: 1.2623, F1: 0.6858, AUC: 0.8979
Epoch [20/30] Train Loss: 0.0024, Test Loss: 1.7742, F1: 0.6716, AUC: 0.9004
Mejores resultados en la época:  0
f1-score 0.724881822088526
AUC según el mejor F1-score 0.9142118058743354
Confusion Matrix:
 [[14207  2258]
 [  943  4217]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_1328641.png
Accuracy:   0.8520
Precision:  0.6513
Recall:     0.8172
F1-score:   0.7249
Tiempo total para red 4: 434.42 segundos

Entrenando red 5 con capas [5020, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4147, Test Loss: 0.3305, F1: 0.7314, AUC: 0.9158
Epoch [10/30] Train Loss: 0.0058, Test Loss: 1.6484, F1: 0.6988, AUC: 0.9041
Epoch [20/30] Train Loss: 0.0041, Test Loss: 1.3822, F1: 0.6983, AUC: 0.9014
Mejores resultados en la época:  1
f1-score 0.7352889820150166
AUC según el mejor F1-score 0.9160161029856615
Confusion Matrix:
 [[14382  2083]
 [  949  4211]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_2743297.png
Accuracy:   0.8598
Precision:  0.6690
Recall:     0.8161
F1-score:   0.7353

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4160, Test Loss: 0.3850, F1: 0.7194, AUC: 0.9148
Epoch [10/30] Train Loss: 0.0088, Test Loss: 1.1035, F1: 0.7148, AUC: 0.9028
Epoch [20/30] Train Loss: 0.0037, Test Loss: 1.9121, F1: 0.6605, AUC: 0.8993
Mejores resultados en la época:  1
f1-score 0.7282746926663841
AUC según el mejor F1-score 0.9171318300270482
Confusion Matrix:
 [[14125  2340]
 [  865  4295]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_2743297.png
Accuracy:   0.8518
Precision:  0.6473
Recall:     0.8324
F1-score:   0.7283

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4145, Test Loss: 0.3899, F1: 0.7196, AUC: 0.9143
Epoch [10/30] Train Loss: 0.0051, Test Loss: 1.3538, F1: 0.6751, AUC: 0.8987
Epoch [20/30] Train Loss: 0.0042, Test Loss: 1.6059, F1: 0.6729, AUC: 0.8991
Mejores resultados en la época:  0
f1-score 0.7195633921074727
AUC según el mejor F1-score 0.9142753774155655
Confusion Matrix:
 [[14000  2465]
 [  875  4285]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_2743297.png
Accuracy:   0.8455
Precision:  0.6348
Recall:     0.8304
F1-score:   0.7196

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4135, Test Loss: 0.3538, F1: 0.7340, AUC: 0.9137
Epoch [10/30] Train Loss: 0.0062, Test Loss: 1.1700, F1: 0.6989, AUC: 0.9038
Epoch [20/30] Train Loss: 0.0018, Test Loss: 1.9248, F1: 0.6782, AUC: 0.9021
Mejores resultados en la época:  1
f1-score 0.740497146725989
AUC según el mejor F1-score 0.9160451344995375
Confusion Matrix:
 [[15114  1351]
 [ 1332  3828]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_2743297.png
Accuracy:   0.8759
Precision:  0.7391
Recall:     0.7419
F1-score:   0.7405
Tiempo total para red 5: 440.87 segundos

Entrenando red 6 con capas [5020, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4145, Test Loss: 0.3377, F1: 0.7315, AUC: 0.9147
Epoch [10/30] Train Loss: 0.0088, Test Loss: 1.0009, F1: 0.7009, AUC: 0.9047
Epoch [20/30] Train Loss: 0.0038, Test Loss: 1.7602, F1: 0.6874, AUC: 0.9045
Mejores resultados en la época:  1
f1-score 0.7406458695453298
AUC según el mejor F1-score 0.9158914022462494
Confusion Matrix:
 [[14742  1723]
 [ 1112  4048]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_5840897.png
Accuracy:   0.8689
Precision:  0.7014
Recall:     0.7845
F1-score:   0.7406

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4165, Test Loss: 0.3939, F1: 0.7189, AUC: 0.9151
Epoch [10/30] Train Loss: 0.0052, Test Loss: 1.3954, F1: 0.6872, AUC: 0.9059
Epoch [20/30] Train Loss: 0.0042, Test Loss: 1.4839, F1: 0.7062, AUC: 0.9054
Mejores resultados en la época:  27
f1-score 0.7197094179711148
AUC según el mejor F1-score 0.9032966393359652
Confusion Matrix:
 [[14223  2242]
 [  999  4161]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_5840897.png
Accuracy:   0.8501
Precision:  0.6499
Recall:     0.8064
F1-score:   0.7197

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4183, Test Loss: 0.4122, F1: 0.7143, AUC: 0.9130
Epoch [10/30] Train Loss: 0.0067, Test Loss: 1.9107, F1: 0.6697, AUC: 0.8992
Epoch [20/30] Train Loss: 0.0048, Test Loss: 1.5247, F1: 0.6876, AUC: 0.8975
Mejores resultados en la época:  1
f1-score 0.7233935742971888
AUC según el mejor F1-score 0.9147324604458129
Confusion Matrix:
 [[13996  2469]
 [  837  4323]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_5840897.png
Accuracy:   0.8471
Precision:  0.6365
Recall:     0.8378
F1-score:   0.7234

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4163, Test Loss: 0.3951, F1: 0.7349, AUC: 0.9143
Epoch [10/30] Train Loss: 0.0069, Test Loss: 1.7789, F1: 0.6859, AUC: 0.9038
Epoch [20/30] Train Loss: 0.0017, Test Loss: 1.6013, F1: 0.6861, AUC: 0.9022
Mejores resultados en la época:  1
f1-score 0.7386802386802387
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [01:34:34] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text_plus_numerical.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
AUC según el mejor F1-score 0.9165099388649165
Confusion Matrix:
 [[14438  2027]
 [  951  4209]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/tfidf/confusion_matrix_param_5840897.png
Accuracy:   0.8623
Precision:  0.6750
Recall:     0.8157
F1-score:   0.7387
Tiempo total para red 6: 483.20 segundos
Saved on: outputs_text_plus_numerical/0/tfidf

==============================
Model: Logistic Regression
Accuracy:  0.8422
Precision: 0.6336
Recall:    0.8027
F1-score:  0.7082
              precision    recall  f1-score   support

           0       0.93      0.85      0.89     16465
           1       0.63      0.80      0.71      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.83      0.80     21625
weighted avg       0.86      0.84      0.85     21625

[[14070  2395]
 [ 1018  4142]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical/0/tfidf/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_text_plus_numerical/0/tfidf/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.6128
Precision: 0.3436
Recall:    0.6843
F1-score:  0.4575
              precision    recall  f1-score   support

           0       0.86      0.59      0.70     16465
           1       0.34      0.68      0.46      5160

    accuracy                           0.61     21625
   macro avg       0.60      0.64      0.58     21625
weighted avg       0.73      0.61      0.64     21625

[[9720 6745]
 [1629 3531]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_text_plus_numerical/0/tfidf/conf_matrix_svm.png
Modelo guardado como: outputs_text_plus_numerical/0/tfidf/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8807
Precision: 0.7079
Recall:    0.8512
F1-score:  0.7730
              precision    recall  f1-score   support

           0       0.95      0.89      0.92     16465
           1       0.71      0.85      0.77      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.87      0.85     21625
weighted avg       0.89      0.88      0.88     21625

[[14653  1812]
 [  768  4392]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_text_plus_numerical/0/tfidf/conf_matrix_decision_tree.png
Modelo guardado como: outputs_text_plus_numerical/0/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8621
Precision: 0.6898
Recall:    0.7667
F1-score:  0.7262
              precision    recall  f1-score   support

           0       0.92      0.89      0.91     16465
           1       0.69      0.77      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.83      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14686  1779]
 [ 1204  3956]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical/0/tfidf/conf_matrix_random_forest.png
Modelo guardado como: outputs_text_plus_numerical/0/tfidf/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9029
Precision: 0.7514
Recall:    0.8864
F1-score:  0.8134
              precision    recall  f1-score   support

           0       0.96      0.91      0.93     16465
           1       0.75      0.89      0.81      5160

    accuracy                           0.90     21625
   macro avg       0.86      0.90      0.87     21625
weighted avg       0.91      0.90      0.91     21625

[[14952  1513]
 [  586  4574]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_text_plus_numerical/0/tfidf/conf_matrix_xgboost.png
Modelo guardado como: outputs_text_plus_numerical/0/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8272
Precision: 0.6180
Recall:    0.7227
F1-score:  0.6662
              precision    recall  f1-score   support

           0       0.91      0.86      0.88     16465
           1       0.62      0.72      0.67      5160

    accuracy                           0.83     21625
   macro avg       0.76      0.79      0.77     21625
weighted avg       0.84      0.83      0.83     21625

[[14160  2305]
 [ 1431  3729]]
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}
Confusion matrix saved as: outputs_text_plus_numerical/0/tfidf/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_text_plus_numerical/0/tfidf/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.9029, 'precision': 0.7514, 'recall': 0.8864, 'f1_score': 0.8134}
Decision Tree: {'accuracy': 0.8807, 'precision': 0.7079, 'recall': 0.8512, 'f1_score': 0.773}
Random Forest: {'accuracy': 0.8621, 'precision': 0.6898, 'recall': 0.7667, 'f1_score': 0.7262}
Logistic Regression: {'accuracy': 0.8422, 'precision': 0.6336, 'recall': 0.8027, 'f1_score': 0.7082}
Naive Bayes: {'accuracy': 0.8272, 'precision': 0.618, 'recall': 0.7227, 'f1_score': 0.6662}
SVM: {'accuracy': 0.6128, 'precision': 0.3436, 'recall': 0.6843, 'f1_score': 0.4575}

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 320)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 320)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [320, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5827, Test Loss: 0.4410, F1: 0.6745, AUC: 0.8725
Epoch [10/30] Train Loss: 0.4105, Test Loss: 0.4215, F1: 0.6866, AUC: 0.8982
Epoch [20/30] Train Loss: 0.4025, Test Loss: 0.3301, F1: 0.7154, AUC: 0.9005
Mejores resultados en la época:  20
f1-score 0.7154375614552606
AUC según el mejor F1-score 0.9004652575230051
Confusion Matrix:
 [[15093  1372]
 [ 1522  3638]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_10305.png
Accuracy:   0.8662
Precision:  0.7261
Recall:     0.7050
F1-score:   0.7154

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5772, Test Loss: 0.4947, F1: 0.6405, AUC: 0.8755
Epoch [10/30] Train Loss: 0.4093, Test Loss: 0.4842, F1: 0.6577, AUC: 0.8981
Epoch [20/30] Train Loss: 0.4079, Test Loss: 0.3714, F1: 0.7065, AUC: 0.9003
Mejores resultados en la época:  14
f1-score 0.7113335186729682
AUC según el mejor F1-score 0.8991439028524213
Confusion Matrix:
 [[14672  1793]
 [ 1322  3838]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_10305.png
Accuracy:   0.8560
Precision:  0.6816
Recall:     0.7438
F1-score:   0.7113

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5746, Test Loss: 0.4873, F1: 0.6433, AUC: 0.8748
Epoch [10/30] Train Loss: 0.4098, Test Loss: 0.3732, F1: 0.7047, AUC: 0.8978
Epoch [20/30] Train Loss: 0.4044, Test Loss: 0.4060, F1: 0.6964, AUC: 0.9005
Mejores resultados en la época:  29
f1-score 0.7178741150227912
AUC según el mejor F1-score 0.9013618445987143
Confusion Matrix:
 [[15015  1450]
 [ 1459  3701]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_10305.png
Accuracy:   0.8655
Precision:  0.7185
Recall:     0.7172
F1-score:   0.7179

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5949, Test Loss: 0.4767, F1: 0.6564, AUC: 0.8677
Epoch [10/30] Train Loss: 0.4099, Test Loss: 0.4942, F1: 0.6560, AUC: 0.8983
Epoch [20/30] Train Loss: 0.4031, Test Loss: 0.3582, F1: 0.7148, AUC: 0.9015
Mejores resultados en la época:  25
f1-score 0.7175033403321245
AUC según el mejor F1-score 0.9021073418597588
Confusion Matrix:
 [[14906  1559]
 [ 1401  3759]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_10305.png
Accuracy:   0.8631
Precision:  0.7068
Recall:     0.7285
F1-score:   0.7175
Tiempo total para red 1: 166.61 segundos

Entrenando red 2 con capas [320, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.5493, Test Loss: 0.4025, F1: 0.6840, AUC: 0.8833
Epoch [10/30] Train Loss: 0.4123, Test Loss: 0.3515, F1: 0.7137, AUC: 0.8981
Epoch [20/30] Train Loss: 0.3994, Test Loss: 0.3458, F1: 0.7158, AUC: 0.9026
Mejores resultados en la época:  25
f1-score 0.7187441248354954
AUC según el mejor F1-score 0.9038654404339015
Confusion Matrix:
 [[14810  1655]
 [ 1337  3823]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_22657.png
Accuracy:   0.8616
Precision:  0.6979
Recall:     0.7409
F1-score:   0.7187

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.5250, Test Loss: 0.3863, F1: 0.6969, AUC: 0.8864
Epoch [10/30] Train Loss: 0.4093, Test Loss: 0.3566, F1: 0.7128, AUC: 0.9006
Epoch [20/30] Train Loss: 0.4018, Test Loss: 0.4684, F1: 0.6657, AUC: 0.9041
Mejores resultados en la época:  24
f1-score 0.7212098440728912
AUC según el mejor F1-score 0.9056206729331893
Confusion Matrix:
 [[14818  1647]
 [ 1321  3839]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_22657.png
Accuracy:   0.8628
Precision:  0.6998
Recall:     0.7440
F1-score:   0.7212

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.5375, Test Loss: 0.4086, F1: 0.6860, AUC: 0.8857
Epoch [10/30] Train Loss: 0.4063, Test Loss: 0.4640, F1: 0.6695, AUC: 0.9009
Epoch [20/30] Train Loss: 0.4005, Test Loss: 0.3471, F1: 0.7205, AUC: 0.9032
Mejores resultados en la época:  20
f1-score 0.7205041202132816
AUC según el mejor F1-score 0.9032140940260878
Confusion Matrix:
 [[15026  1439]
 [ 1444  3716]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_22657.png
Accuracy:   0.8667
Precision:  0.7209
Recall:     0.7202
F1-score:   0.7205

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.5379, Test Loss: 0.4747, F1: 0.6531, AUC: 0.8872
Epoch [10/30] Train Loss: 0.4165, Test Loss: 0.3648, F1: 0.7143, AUC: 0.8992
Epoch [20/30] Train Loss: 0.4042, Test Loss: 0.3972, F1: 0.7003, AUC: 0.9022
Mejores resultados en la época:  24
f1-score 0.7184337463350042
AUC según el mejor F1-score 0.9029367497887227
Confusion Matrix:
 [[14850  1615]
 [ 1362  3798]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_22657.png
Accuracy:   0.8623
Precision:  0.7016
Recall:     0.7360
F1-score:   0.7184
Tiempo total para red 2: 179.36 segundos

Entrenando red 3 con capas [320, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.5072, Test Loss: 0.6356, F1: 0.5780, AUC: 0.8893
Epoch [10/30] Train Loss: 0.4138, Test Loss: 0.5620, F1: 0.6122, AUC: 0.8984
Epoch [20/30] Train Loss: 0.4003, Test Loss: 0.5018, F1: 0.6640, AUC: 0.9026
Mejores resultados en la época:  29
f1-score 0.723574906903466
AUC según el mejor F1-score 0.9059099405127624
Confusion Matrix:
 [[14941  1524]
 [ 1371  3789]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_51457.png
Accuracy:   0.8661
Precision:  0.7132
Recall:     0.7343
F1-score:   0.7236

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.5360, Test Loss: 0.3646, F1: 0.7020, AUC: 0.8866
Epoch [10/30] Train Loss: 0.4109, Test Loss: 0.3442, F1: 0.7180, AUC: 0.9009
Epoch [20/30] Train Loss: 0.4035, Test Loss: 0.3589, F1: 0.7155, AUC: 0.9033
Mejores resultados en la época:  28
f1-score 0.7200304936153993
AUC según el mejor F1-score 0.9036209354115026
Confusion Matrix:
 [[14909  1556]
 [ 1382  3778]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_51457.png
Accuracy:   0.8641
Precision:  0.7083
Recall:     0.7322
F1-score:   0.7200

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.5191, Test Loss: 0.3848, F1: 0.7011, AUC: 0.8884
Epoch [10/30] Train Loss: 0.4132, Test Loss: 0.3785, F1: 0.7099, AUC: 0.9010
Epoch [20/30] Train Loss: 0.3999, Test Loss: 0.3866, F1: 0.7155, AUC: 0.9037
Mejores resultados en la época:  27
f1-score 0.7229916897506925
AUC según el mejor F1-score 0.9052165799193497
Confusion Matrix:
 [[15171  1294]
 [ 1506  3654]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_51457.png
Accuracy:   0.8705
Precision:  0.7385
Recall:     0.7081
F1-score:   0.7230

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.5290, Test Loss: 0.4089, F1: 0.6898, AUC: 0.8882
Epoch [10/30] Train Loss: 0.4133, Test Loss: 0.4935, F1: 0.6544, AUC: 0.8988
Epoch [20/30] Train Loss: 0.4028, Test Loss: 0.3845, F1: 0.7080, AUC: 0.9035
Mejores resultados en la época:  21
f1-score 0.7190924076408417
AUC según el mejor F1-score 0.9032214092849056
Confusion Matrix:
 [[15020  1445]
 [ 1452  3708]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_51457.png
Accuracy:   0.8660
Precision:  0.7196
Recall:     0.7186
F1-score:   0.7191
Tiempo total para red 3: 197.31 segundos

Entrenando red 4 con capas [320, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.5244, Test Loss: 0.3509, F1: 0.6622, AUC: 0.8876
Epoch [10/30] Train Loss: 0.4131, Test Loss: 0.4165, F1: 0.6894, AUC: 0.9009
Epoch [20/30] Train Loss: 0.3989, Test Loss: 0.4154, F1: 0.6910, AUC: 0.9043
Mejores resultados en la época:  21
f1-score 0.7218016291327264
AUC según el mejor F1-score 0.9045958599048487
Confusion Matrix:
 [[14956  1509]
 [ 1394  3766]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_125441.png
Accuracy:   0.8658
Precision:  0.7139
Recall:     0.7298
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
F1-score:   0.7218

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.5093, Test Loss: 0.4662, F1: 0.6632, AUC: 0.8887
Epoch [10/30] Train Loss: 0.4115, Test Loss: 0.3420, F1: 0.7175, AUC: 0.9008
Epoch [20/30] Train Loss: 0.4054, Test Loss: 0.4702, F1: 0.6648, AUC: 0.9034
Mejores resultados en la época:  27
f1-score 0.7232486167119947
AUC según el mejor F1-score 0.9056538005211902
Confusion Matrix:
 [[14818  1647]
 [ 1304  3856]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_125441.png
Accuracy:   0.8635
Precision:  0.7007
Recall:     0.7473
F1-score:   0.7232

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.5160, Test Loss: 0.4361, F1: 0.6867, AUC: 0.8882
Epoch [10/30] Train Loss: 0.4080, Test Loss: 0.3473, F1: 0.7187, AUC: 0.9012
Epoch [20/30] Train Loss: 0.3972, Test Loss: 0.3762, F1: 0.7086, AUC: 0.9053
Mejores resultados en la época:  24
f1-score 0.7230501257983356
AUC según el mejor F1-score 0.9052757022766168
Confusion Matrix:
 [[15027  1438]
 [ 1424  3736]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_125441.png
Accuracy:   0.8677
Precision:  0.7221
Recall:     0.7240
F1-score:   0.7231

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.5150, Test Loss: 0.4980, F1: 0.6633, AUC: 0.8890
Epoch [10/30] Train Loss: 0.4146, Test Loss: 0.3718, F1: 0.7107, AUC: 0.9002
Epoch [20/30] Train Loss: 0.3991, Test Loss: 0.3213, F1: 0.7128, AUC: 0.9026
Mejores resultados en la época:  27
f1-score 0.7256603046076731
AUC según el mejor F1-score 0.9064106973448495
Confusion Matrix:
 [[15015  1450]
 [ 1396  3764]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_125441.png
Accuracy:   0.8684
Precision:  0.7219
Recall:     0.7295
F1-score:   0.7257
Tiempo total para red 4: 216.55 segundos

Entrenando red 5 con capas [320, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.5068, Test Loss: 0.3731, F1: 0.7071, AUC: 0.8896
Epoch [10/30] Train Loss: 0.4125, Test Loss: 0.4107, F1: 0.6879, AUC: 0.9013
Epoch [20/30] Train Loss: 0.3999, Test Loss: 0.4641, F1: 0.6664, AUC: 0.9036
Mejores resultados en la época:  25
f1-score 0.721679125934445
AUC según el mejor F1-score 0.9048070372436716
Confusion Matrix:
 [[14956  1509]
 [ 1395  3765]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_336897.png
Accuracy:   0.8657
Precision:  0.7139
Recall:     0.7297
F1-score:   0.7217

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.5044, Test Loss: 0.5416, F1: 0.6324, AUC: 0.8902
Epoch [10/30] Train Loss: 0.4089, Test Loss: 0.3461, F1: 0.7182, AUC: 0.9012
Epoch [20/30] Train Loss: 0.3983, Test Loss: 0.3591, F1: 0.7133, AUC: 0.9049
Mejores resultados en la época:  28
f1-score 0.7233502538071066
AUC según el mejor F1-score 0.906848941965221
Confusion Matrix:
 [[15086  1379]
 [ 1455  3705]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_336897.png
Accuracy:   0.8689
Precision:  0.7288
Recall:     0.7180
F1-score:   0.7234

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.5031, Test Loss: 0.4904, F1: 0.6562, AUC: 0.8899
Epoch [10/30] Train Loss: 0.4128, Test Loss: 0.5838, F1: 0.6130, AUC: 0.8996
Epoch [20/30] Train Loss: 0.4017, Test Loss: 0.4115, F1: 0.6967, AUC: 0.9051
Mejores resultados en la época:  28
f1-score 0.7243358956894035
AUC según el mejor F1-score 0.9059369475302322
Confusion Matrix:
 [[15070  1395]
 [ 1438  3722]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_336897.png
Accuracy:   0.8690
Precision:  0.7274
Recall:     0.7213
F1-score:   0.7243

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.5090, Test Loss: 0.6777, F1: 0.5500, AUC: 0.8890
Epoch [10/30] Train Loss: 0.4130, Test Loss: 0.3215, F1: 0.7023, AUC: 0.9006
Epoch [20/30] Train Loss: 0.3995, Test Loss: 0.3884, F1: 0.7098, AUC: 0.9046
Mejores resultados en la época:  21
f1-score 0.7222383580211056
AUC según el mejor F1-score 0.9047815074023593
Confusion Matrix:
 [[15026  1439]
 [ 1430  3730]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_336897.png
Accuracy:   0.8673
Precision:  0.7216
Recall:     0.7229
F1-score:   0.7222
Tiempo total para red 5: 224.38 segundos

Entrenando red 6 con capas [320, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4967, Test Loss: 0.5147, F1: 0.6435, AUC: 0.8914
Epoch [10/30] Train Loss: 0.4126, Test Loss: 0.3343, F1: 0.7175, AUC: 0.9011
Epoch [20/30] Train Loss: 0.3980, Test Loss: 0.3331, F1: 0.7181, AUC: 0.9030
Mejores resultados en la época:  29
f1-score 0.7235366198510699
AUC según el mejor F1-score 0.9066913961256788
Confusion Matrix:
 [[14854  1611]
 [ 1322  3838]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_1028097.png
Accuracy:   0.8644
Precision:  0.7043
Recall:     0.7438
F1-score:   0.7235

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6496, Test Loss: 0.5005, F1: 0.6734, AUC: 0.8770
Epoch [10/30] Train Loss: 0.4133, Test Loss: 0.3430, F1: 0.7153, AUC: 0.9001
Epoch [20/30] Train Loss: 0.4021, Test Loss: 0.4041, F1: 0.7002, AUC: 0.9049
Mejores resultados en la época:  27
f1-score 0.7237866986219293
AUC según el mejor F1-score 0.9059186328999499
Confusion Matrix:
 [[15235  1230]
 [ 1536  3624]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_1028097.png
Accuracy:   0.8721
Precision:  0.7466
Recall:     0.7023
F1-score:   0.7238

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.5151, Test Loss: 0.3861, F1: 0.6063, AUC: 0.8887
Epoch [10/30] Train Loss: 0.4153, Test Loss: 0.4138, F1: 0.6986, AUC: 0.9014
Epoch [20/30] Train Loss: 0.4018, Test Loss: 0.3858, F1: 0.7112, AUC: 0.9045
Mejores resultados en la época:  24
f1-score 0.7214974475326149
AUC según el mejor F1-score 0.9048768588290407
Confusion Matrix:
 [[14863  1602]
 [ 1344  3816]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_1028097.png
Accuracy:   0.8638
Precision:  0.7043
Recall:     0.7395
F1-score:   0.7215

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.5169, Test Loss: 0.3982, F1: 0.7061, AUC: 0.8893
Epoch [10/30] Train Loss: 0.4154, Test Loss: 0.4677, F1: 0.6719, AUC: 0.9008
Epoch [20/30] Train Loss: 0.3998, Test Loss: 0.3442, F1: 0.7216, AUC: 0.9042
Mejores resultados en la época:  29
f1-score 0.7258557758705731
AUC según el mejor F1-score 0.9055287466719397
Confusion Matrix:
 [[15167  1298]
 [ 1481  3679]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/lyrics_bert/confusion_matrix_param_1028097.png
Accuracy:   0.8715
Precision:  0.7392
Recall:     0.7130
F1-score:   0.7259
Tiempo total para red 6: 254.36 segundos
Saved on: outputs_text_plus_numerical/0/lyrics_bert

==============================
Model: Logistic Regression
Accuracy:  0.8394
Precision: 0.6294
Recall:    0.7957
F1-score:  0.7028
              precision    recall  f1-score   support

           0       0.93      0.85      0.89     16465
           1       0.63      0.80      0.70      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.82      0.80     21625
weighted avg       0.86      0.84      0.85     21625

[[14047  2418]
 [ 1054  4106]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical/0/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_text_plus_numerical/0/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.3252
Precision: 0.2535
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [02:06:02] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text_plus_numerical.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Recall:    0.9401
F1-score:  0.3994
              precision    recall  f1-score   support

           0       0.88      0.13      0.23     16465
           1       0.25      0.94      0.40      5160

    accuracy                           0.33     21625
   macro avg       0.56      0.54      0.31     21625
weighted avg       0.73      0.33      0.27     21625

[[ 2182 14283]
 [  309  4851]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_text_plus_numerical/0/lyrics_bert/conf_matrix_svm.png
Modelo guardado como: outputs_text_plus_numerical/0/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8035
Precision: 0.5666
Recall:    0.7500
F1-score:  0.6455
              precision    recall  f1-score   support

           0       0.91      0.82      0.86     16465
           1       0.57      0.75      0.65      5160

    accuracy                           0.80     21625
   macro avg       0.74      0.79      0.75     21625
weighted avg       0.83      0.80      0.81     21625

[[13505  2960]
 [ 1290  3870]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_text_plus_numerical/0/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs_text_plus_numerical/0/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8416
Precision: 0.6432
Recall:    0.7552
F1-score:  0.6947
              precision    recall  f1-score   support

           0       0.92      0.87      0.89     16465
           1       0.64      0.76      0.69      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.81      0.79     21625
weighted avg       0.85      0.84      0.85     21625

[[14303  2162]
 [ 1263  3897]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical/0/lyrics_bert/conf_matrix_random_forest.png
Modelo guardado como: outputs_text_plus_numerical/0/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8688
Precision: 0.6893
Recall:    0.8194
F1-score:  0.7487
              precision    recall  f1-score   support

           0       0.94      0.88      0.91     16465
           1       0.69      0.82      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.85      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14559  1906]
 [  932  4228]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_text_plus_numerical/0/lyrics_bert/conf_matrix_xgboost.png
Modelo guardado como: outputs_text_plus_numerical/0/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6613
Precision: 0.3958
Recall:    0.7965
F1-score:  0.5288
              precision    recall  f1-score   support

           0       0.91      0.62      0.74     16465
           1       0.40      0.80      0.53      5160

    accuracy                           0.66     21625
   macro avg       0.65      0.71      0.63     21625
weighted avg       0.78      0.66      0.69     21625

[[10191  6274]
 [ 1050  4110]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_text_plus_numerical/0/lyrics_bert/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_text_plus_numerical/0/lyrics_bert/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.8688, 'precision': 0.6893, 'recall': 0.8194, 'f1_score': 0.7487}
Logistic Regression: {'accuracy': 0.8394, 'precision': 0.6294, 'recall': 0.7957, 'f1_score': 0.7028}
Random Forest: {'accuracy': 0.8416, 'precision': 0.6432, 'recall': 0.7552, 'f1_score': 0.6947}
Decision Tree: {'accuracy': 0.8035, 'precision': 0.5666, 'recall': 0.75, 'f1_score': 0.6455}
Naive Bayes: {'accuracy': 0.6613, 'precision': 0.3958, 'recall': 0.7965, 'f1_score': 0.5288}
SVM: {'accuracy': 0.3252, 'precision': 0.2535, 'recall': 0.9401, 'f1_score': 0.3994}

##################################################
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1556)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1556)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1556, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4647, Test Loss: 0.4703, F1: 0.6856, AUC: 0.8984
Epoch [10/30] Train Loss: 0.3590, Test Loss: 0.4155, F1: 0.7174, AUC: 0.9255
Epoch [20/30] Train Loss: 0.3389, Test Loss: 0.3032, F1: 0.7580, AUC: 0.9308
Mejores resultados en la época:  21
f1-score 0.7623762376237624
AUC según el mejor F1-score 0.9311584592169908
Confusion Matrix:
 [[14875  1590]
 [ 1002  4158]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_49857.png
Accuracy:   0.8801
Precision:  0.7234
Recall:     0.8058
F1-score:   0.7624

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4580, Test Loss: 0.3941, F1: 0.7164, AUC: 0.9010
Epoch [10/30] Train Loss: 0.3438, Test Loss: 0.3608, F1: 0.7378, AUC: 0.9310
Epoch [20/30] Train Loss: 0.3346, Test Loss: 0.3230, F1: 0.7545, AUC: 0.9335
Mejores resultados en la época:  28
f1-score 0.770996640537514
AUC según el mejor F1-score 0.9346257683081566
Confusion Matrix:
 [[15040  1425]
 [ 1029  4131]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_49857.png
Accuracy:   0.8865
Precision:  0.7435
Recall:     0.8006
F1-score:   0.7710

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5729, Test Loss: 0.4670, F1: 0.6745, AUC: 0.8718
Epoch [10/30] Train Loss: 0.3696, Test Loss: 0.3575, F1: 0.7427, AUC: 0.9154
Epoch [20/30] Train Loss: 0.3552, Test Loss: 0.3680, F1: 0.7411, AUC: 0.9218
Mejores resultados en la época:  24
f1-score 0.7517149390243902
AUC según el mejor F1-score 0.9251066980228202
Confusion Matrix:
 [[15074  1391]
 [ 1215  3945]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_49857.png
Accuracy:   0.8795
Precision:  0.7393
Recall:     0.7645
F1-score:   0.7517

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4528, Test Loss: 0.3508, F1: 0.7154, AUC: 0.9004
Epoch [10/30] Train Loss: 0.3431, Test Loss: 0.3603, F1: 0.7381, AUC: 0.9308
Epoch [20/30] Train Loss: 0.3325, Test Loss: 0.3259, F1: 0.7514, AUC: 0.9330
Mejores resultados en la época:  24
f1-score 0.7647446148057567
AUC según el mejor F1-score 0.9328367843934867
Confusion Matrix:
 [[15059  1406]
 [ 1095  4065]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_49857.png
Accuracy:   0.8843
Precision:  0.7430
Recall:     0.7878
F1-score:   0.7647
Tiempo total para red 1: 225.67 segundos

Entrenando red 2 con capas [1556, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4499, Test Loss: 0.3437, F1: 0.7233, AUC: 0.9039
Epoch [10/30] Train Loss: 0.3454, Test Loss: 0.3122, F1: 0.7564, AUC: 0.9315
Epoch [20/30] Train Loss: 0.3336, Test Loss: 0.3412, F1: 0.7497, AUC: 0.9343
Mejores resultados en la época:  25
f1-score 0.7713961836387682
AUC según el mejor F1-score 0.9340682961508675
Confusion Matrix:
 [[15122  1343]
 [ 1077  4083]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_101761.png
Accuracy:   0.8881
Precision:  0.7525
Recall:     0.7913
F1-score:   0.7714

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4526, Test Loss: 0.3990, F1: 0.7139, AUC: 0.9033
Epoch [10/30] Train Loss: 0.3476, Test Loss: 0.3135, F1: 0.7543, AUC: 0.9302
Epoch [20/30] Train Loss: 0.3311, Test Loss: 0.3925, F1: 0.7359, AUC: 0.9344
Mejores resultados en la época:  22
f1-score 0.7727439471753484
AUC según el mejor F1-score 0.9355549886180929
Confusion Matrix:
 [[14934  1531]
 [  947  4213]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_101761.png
Accuracy:   0.8854
Precision:  0.7335
Recall:     0.8165
F1-score:   0.7727

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4446, Test Loss: 0.3523, F1: 0.7251, AUC: 0.9051
Epoch [10/30] Train Loss: 0.3462, Test Loss: 0.3053, F1: 0.7622, AUC: 0.9309
Epoch [20/30] Train Loss: 0.3322, Test Loss: 0.3095, F1: 0.7591, AUC: 0.9334
Mejores resultados en la época:  28
f1-score 0.7732995199710171
AUC según el mejor F1-score 0.9361339416238815
Confusion Matrix:
 [[14853  1612]
 [  891  4269]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_101761.png
Accuracy:   0.8843
Precision:  0.7259
Recall:     0.8273
F1-score:   0.7733

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4562, Test Loss: 0.3441, F1: 0.7164, AUC: 0.9006
Epoch [10/30] Train Loss: 0.3495, Test Loss: 0.3648, F1: 0.7346, AUC: 0.9305
Epoch [20/30] Train Loss: 0.3329, Test Loss: 0.3090, F1: 0.7633, AUC: 0.9340
Mejores resultados en la época:  29
f1-score 0.7681978798586573
AUC según el mejor F1-score 0.9363664997634165
Confusion Matrix:
 [[14653  1812]
 [  812  4348]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_101761.png
Accuracy:   0.8787
Precision:  0.7058
Recall:     0.8426
F1-score:   0.7682
Tiempo total para red 2: 243.16 segundos

Entrenando red 3 con capas [1556, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4547, Test Loss: 0.4840, F1: 0.6763, AUC: 0.9032
Epoch [10/30] Train Loss: 0.3478, Test Loss: 0.3897, F1: 0.7330, AUC: 0.9280
Epoch [20/30] Train Loss: 0.3325, Test Loss: 0.3288, F1: 0.7594, AUC: 0.9352
Mejores resultados en la época:  19
f1-score 0.7714778191055066
AUC según el mejor F1-score 0.9352450346871565
Confusion Matrix:
 [[14854  1611]
 [  908  4252]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_209665.png
Accuracy:   0.8835
Precision:  0.7252
Recall:     0.8240
F1-score:   0.7715

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4506, Test Loss: 0.4045, F1: 0.7152, AUC: 0.9043
Epoch [10/30] Train Loss: 0.3515, Test Loss: 0.4787, F1: 0.6805, AUC: 0.9293
Epoch [20/30] Train Loss: 0.3318, Test Loss: 0.4734, F1: 0.6882, AUC: 0.9352
Mejores resultados en la época:  27
f1-score 0.7751473922902494
AUC según el mejor F1-score 0.9371778225834926
Confusion Matrix:
 [[14873  1592]
 [  887  4273]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_209665.png
Accuracy:   0.8854
Precision:  0.7286
Recall:     0.8281
F1-score:   0.7751

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4569, Test Loss: 0.4262, F1: 0.7001, AUC: 0.9023
Epoch [10/30] Train Loss: 0.3439, Test Loss: 0.3192, F1: 0.7630, AUC: 0.9320
Epoch [20/30] Train Loss: 0.3312, Test Loss: 0.2817, F1: 0.7725, AUC: 0.9348
Mejores resultados en la época:  20
f1-score 0.7725329864337483
AUC según el mejor F1-score 0.9348275529252796
Confusion Matrix:
 [[15020  1445]
 [ 1003  4157]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_209665.png
Accuracy:   0.8868
Precision:  0.7421
Recall:     0.8056
F1-score:   0.7725

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4520, Test Loss: 0.3521, F1: 0.7229, AUC: 0.9030
Epoch [10/30] Train Loss: 0.3464, Test Loss: 0.3765, F1: 0.7335, AUC: 0.9318
Epoch [20/30] Train Loss: 0.3310, Test Loss: 0.3068, F1: 0.7576, AUC: 0.9334
Mejores resultados en la época:  19
f1-score 0.7664330701676573
AUC según el mejor F1-score 0.9354074946386155
Confusion Matrix:
 [[14672  1793]
 [  840  4320]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_209665.png
Accuracy:   0.8782
Precision:  0.7067
Recall:     0.8372
F1-score:   0.7664
Tiempo total para red 3: 255.61 segundos

Entrenando red 4 con capas [1556, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4582, Test Loss: 0.4014, F1: 0.7143, AUC: 0.9034
Epoch [10/30] Train Loss: 0.3502, Test Loss: 0.2900, F1: 0.7429, AUC: 0.9278
Epoch [20/30] Train Loss: 0.3347, Test Loss: 0.3196, F1: 0.7494, AUC: 0.9312
Mejores resultados en la época:  28
f1-score 0.7723321620122064
AUC según el mejor F1-score 0.9358006353623025
Confusion Matrix:
 [[14987  1478]
 [  984  4176]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_441857.png
Accuracy:   0.8862
Precision:  0.7386
Recall:     0.8093
F1-score:   0.7723

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4487, Test Loss: 0.3244, F1: 0.6974, AUC: 0.9061
Epoch [10/30] Train Loss: 0.3514, Test Loss: 0.3150, F1: 0.7504, AUC: 0.9287
Epoch [20/30] Train Loss: 0.3307, Test Loss: 0.4649, F1: 0.6915, AUC: 0.9344
Mejores resultados en la época:  27
f1-score 0.7702520685010583
AUC según el mejor F1-score 0.9358122467908201
Confusion Matrix:
 [[15234  1231]
 [ 1157  4003]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_441857.png
Accuracy:   0.8896
Precision:  0.7648
Recall:     0.7758
F1-score:   0.7703

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4593, Test Loss: 0.3706, F1: 0.7220, AUC: 0.9018
Epoch [10/30] Train Loss: 0.3486, Test Loss: 0.3374, F1: 0.7468, AUC: 0.9303
Epoch [20/30] Train Loss: 0.3287, Test Loss: 0.2699, F1: 0.7652, AUC: 0.9331
Mejores resultados en la época:  28
f1-score 0.7715861544313427
AUC según el mejor F1-score 0.9353139381869459
Confusion Matrix:
 [[15166  1299]
 [ 1103  4057]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_441857.png
Accuracy:   0.8889
Precision:  0.7575
Recall:     0.7862
F1-score:   0.7716

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4593, Test Loss: 0.4295, F1: 0.7015, AUC: 0.9032
Epoch [10/30] Train Loss: 0.3484, Test Loss: 0.2827, F1: 0.7385, AUC: 0.9273
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Epoch [20/30] Train Loss: 0.3356, Test Loss: 0.3840, F1: 0.7367, AUC: 0.9328
Mejores resultados en la época:  28
f1-score 0.7719298245614035
AUC según el mejor F1-score 0.9361470066879004
Confusion Matrix:
 [[15255  1210]
 [ 1156  4004]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_441857.png
Accuracy:   0.8906
Precision:  0.7679
Recall:     0.7760
F1-score:   0.7719
Tiempo total para red 4: 269.76 segundos

Entrenando red 5 con capas [1556, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4556, Test Loss: 0.3776, F1: 0.7237, AUC: 0.9050
Epoch [10/30] Train Loss: 0.3473, Test Loss: 0.3285, F1: 0.7509, AUC: 0.9303
Epoch [20/30] Train Loss: 0.3341, Test Loss: 0.3786, F1: 0.7422, AUC: 0.9329
Mejores resultados en la época:  24
f1-score 0.7701200533570476
AUC según el mejor F1-score 0.9357414541533957
Confusion Matrix:
 [[14710  1755]
 [  830  4330]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_969729.png
Accuracy:   0.8805
Precision:  0.7116
Recall:     0.8391
F1-score:   0.7701

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4528, Test Loss: 0.4939, F1: 0.6617, AUC: 0.9061
Epoch [10/30] Train Loss: 0.3533, Test Loss: 0.3842, F1: 0.7211, AUC: 0.9302
Epoch [20/30] Train Loss: 0.3352, Test Loss: 0.3533, F1: 0.7447, AUC: 0.9344
Mejores resultados en la época:  23
f1-score 0.7686670627847099
AUC según el mejor F1-score 0.9347110443341173
Confusion Matrix:
 [[15408  1057]
 [ 1279  3881]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_969729.png
Accuracy:   0.8920
Precision:  0.7859
Recall:     0.7521
F1-score:   0.7687

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4558, Test Loss: 0.4207, F1: 0.7069, AUC: 0.9062
Epoch [10/30] Train Loss: 0.3534, Test Loss: 0.3017, F1: 0.7612, AUC: 0.9311
Epoch [20/30] Train Loss: 0.3326, Test Loss: 0.3587, F1: 0.7440, AUC: 0.9346
Mejores resultados en la época:  19
f1-score 0.7662082514734774
AUC según el mejor F1-score 0.9345123906242275
Confusion Matrix:
 [[14717  1748]
 [  870  4290]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_969729.png
Accuracy:   0.8789
Precision:  0.7105
Recall:     0.8314
F1-score:   0.7662

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4483, Test Loss: 0.6314, F1: 0.5723, AUC: 0.9075
Epoch [10/30] Train Loss: 0.3542, Test Loss: 0.4760, F1: 0.6914, AUC: 0.9301
Epoch [20/30] Train Loss: 0.3313, Test Loss: 0.2981, F1: 0.7477, AUC: 0.9291
Mejores resultados en la época:  29
f1-score 0.7711678832116788
AUC según el mejor F1-score 0.9364820490728512
Confusion Matrix:
 [[14891  1574]
 [  934  4226]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_969729.png
Accuracy:   0.8840
Precision:  0.7286
Recall:     0.8190
F1-score:   0.7712
Tiempo total para red 5: 273.70 segundos

Entrenando red 6 con capas [1556, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4618, Test Loss: 0.3439, F1: 0.6814, AUC: 0.9048
Epoch [10/30] Train Loss: 0.3495, Test Loss: 0.3149, F1: 0.7505, AUC: 0.9298
Epoch [20/30] Train Loss: 0.3386, Test Loss: 0.3565, F1: 0.7488, AUC: 0.9329
Mejores resultados en la época:  18
f1-score 0.7672690578845978
AUC según el mejor F1-score 0.9336643031848154
Confusion Matrix:
 [[14906  1559]
 [  978  4182]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_2293761.png
Accuracy:   0.8827
Precision:  0.7284
Recall:     0.8105
F1-score:   0.7673

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4623, Test Loss: 0.3567, F1: 0.7237, AUC: 0.9048
Epoch [10/30] Train Loss: 0.3533, Test Loss: 0.3503, F1: 0.7482, AUC: 0.9297
Epoch [20/30] Train Loss: 0.3397, Test Loss: 0.3008, F1: 0.7646, AUC: 0.9336
Mejores resultados en la época:  18
f1-score 0.7680993314231137
AUC según el mejor F1-score 0.9332015703971545
Confusion Matrix:
 [[15176  1289]
 [ 1139  4021]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_2293761.png
Accuracy:   0.8877
Precision:  0.7573
Recall:     0.7793
F1-score:   0.7681

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4689, Test Loss: 0.3412, F1: 0.7181, AUC: 0.9038
Epoch [10/30] Train Loss: 0.3495, Test Loss: 0.4100, F1: 0.7027, AUC: 0.9313
Epoch [20/30] Train Loss: 0.3296, Test Loss: 0.2892, F1: 0.7700, AUC: 0.9345
Mejores resultados en la época:  29
f1-score 0.7726918582002338
AUC según el mejor F1-score 0.9355780937718488
Confusion Matrix:
 [[15324  1141]
 [ 1193  3967]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_2293761.png
Accuracy:   0.8921
Precision:  0.7766
Recall:     0.7688
F1-score:   0.7727

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4611, Test Loss: 0.5843, F1: 0.6423, AUC: 0.9051
Epoch [10/30] Train Loss: 0.3464, Test Loss: 0.3087, F1: 0.7591, AUC: 0.9311
Epoch [20/30] Train Loss: 0.3329, Test Loss: 0.2852, F1: 0.7705, AUC: 0.9335
Mejores resultados en la época:  25
f1-score 0.7747237177670728
AUC según el mejor F1-score 0.935968727415683
Confusion Matrix:
 [[15139  1326]
 [ 1059  4101]]
Matriz de confusión guardada en: outputs_text_plus_numerical/0/gpt/confusion_matrix_param_2293761.png
Accuracy:   0.8897
Precision:  0.7557
Recall:     0.7948
F1-score:   0.7747
Tiempo total para red 6: 306.20 segundos
Saved on: outputs_text_plus_numerical/0/gpt

==============================
Model: Logistic Regression
Accuracy:  0.8604
Precision: 0.6624
Recall:    0.8465
F1-score:  0.7432
              precision    recall  f1-score   support

           0       0.95      0.86      0.90     16465
           1       0.66      0.85      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.86      0.82     21625
weighted avg       0.88      0.86      0.87     21625

[[14239  2226]
 [  792  4368]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical/0/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_text_plus_numerical/0/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.6310
Precision: 0.3758
Recall:    0.8267
F1-score:  0.5167
              precision    recall  f1-score   support

           0       0.91      0.57      0.70     16465
           1       0.38      0.83      0.52      5160

    accuracy                           0.63     21625
   macro avg       0.64      0.70      0.61     21625
weighted avg       0.78      0.63      0.66     21625

[[9380 7085]
 [ 894 4266]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_text_plus_numerical/0/gpt/conf_matrix_svm.png
Modelo guardado como: outputs_text_plus_numerical/0/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7932
Precision: 0.5476
Recall:    0.7678
F1-score:  0.6393
              precision    recall  f1-score   support

           0       0.92      0.80      0.86     16465
           1       0.55      0.77      0.64      5160

    accuracy                           0.79     21625
   macro avg       0.73      0.78      0.75     21625
weighted avg       0.83      0.79      0.80     21625

[[13192  3273]
 [ 1198  3962]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [02:44:36] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_text_plus_numerical/0/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputs_text_plus_numerical/0/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8456
Precision: 0.6528
Recall:    0.7537
F1-score:  0.6996
              precision    recall  f1-score   support

           0       0.92      0.87      0.90     16465
           1       0.65      0.75      0.70      5160

    accuracy                           0.85     21625
   macro avg       0.79      0.81      0.80     21625
weighted avg       0.86      0.85      0.85     21625

[[14397  2068]
 [ 1271  3889]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical/0/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputs_text_plus_numerical/0/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8826
Precision: 0.7161
Recall:    0.8415
F1-score:  0.7738
              precision    recall  f1-score   support

           0       0.95      0.90      0.92     16465
           1       0.72      0.84      0.77      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.87      0.85     21625
weighted avg       0.89      0.88      0.89     21625

[[14744  1721]
 [  818  4342]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_text_plus_numerical/0/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputs_text_plus_numerical/0/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8331
Precision: 0.6299
Recall:    0.7281
F1-score:  0.6755
              precision    recall  f1-score   support

           0       0.91      0.87      0.89     16465
           1       0.63      0.73      0.68      5160

    accuracy                           0.83     21625
   macro avg       0.77      0.80      0.78     21625
weighted avg       0.84      0.83      0.84     21625

[[14258  2207]
 [ 1403  3757]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_text_plus_numerical/0/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_text_plus_numerical/0/gpt/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.8826, 'precision': 0.7161, 'recall': 0.8415, 'f1_score': 0.7738}
Logistic Regression: {'accuracy': 0.8604, 'precision': 0.6624, 'recall': 0.8465, 'f1_score': 0.7432}
Random Forest: {'accuracy': 0.8456, 'precision': 0.6528, 'recall': 0.7537, 'f1_score': 0.6996}
Naive Bayes: {'accuracy': 0.8331, 'precision': 0.6299, 'recall': 0.7281, 'f1_score': 0.6755}
Decision Tree: {'accuracy': 0.7932, 'precision': 0.5476, 'recall': 0.7678, 'f1_score': 0.6393}
SVM: {'accuracy': 0.631, 'precision': 0.3758, 'recall': 0.8267, 'f1_score': 0.5167}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
XGBoost: {'accuracy': 0.9029, 'precision': 0.7514, 'recall': 0.8864, 'f1_score': 0.8134}
Decision Tree: {'accuracy': 0.8807, 'precision': 0.7079, 'recall': 0.8512, 'f1_score': 0.773}
MLP_5840897: {'accuracy': 0.8622890173410405, 'precision': 0.6749518922386145, 'recall': 0.8156976744186046, 'f1_score': 0.7406458695453298, 'f1_score_avg': 0.730607275123468}
MLP_2743297: {'accuracy': 0.8759306358381503, 'precision': 0.7391388298899402, 'recall': 0.7418604651162791, 'f1_score': 0.740497146725989, 'f1_score_avg': 0.7309060533787156}
MLP_653057: {'accuracy': 0.8569248554913295, 'precision': 0.6633459835547122, 'recall': 0.812984496124031, 'f1_score': 0.7395916911830258, 'f1_score_avg': 0.7296726826392722}
MLP_1328641: {'accuracy': 0.8519768786127168, 'precision': 0.6512741312741313, 'recall': 0.8172480620155039, 'f1_score': 0.7395916911830258, 'f1_score_avg': 0.7278574801127675}
MLP_323457: {'accuracy': 0.8550289017341041, 'precision': 0.6548875631023405, 'recall': 0.8296511627906977, 'f1_score': 0.7319825596306745, 'f1_score_avg': 0.7291218821099661}
Random Forest: {'accuracy': 0.8621, 'precision': 0.6898, 'recall': 0.7667, 'f1_score': 0.7262}
MLP_160705: {'accuracy': 0.853271676300578, 'precision': 0.6550163832111093, 'recall': 0.8135658914728682, 'f1_score': 0.7257325611548102, 'f1_score_avg': 0.7237963839418942}
Logistic Regression: {'accuracy': 0.8422, 'precision': 0.6336, 'recall': 0.8027, 'f1_score': 0.7082}
Naive Bayes: {'accuracy': 0.8272, 'precision': 0.618, 'recall': 0.7227, 'f1_score': 0.6662}
SVM: {'accuracy': 0.6128, 'precision': 0.3436, 'recall': 0.6843, 'f1_score': 0.4575}


EMBEDDINGS TYPE: LYRICS_BERT
XGBoost: {'accuracy': 0.8688, 'precision': 0.6893, 'recall': 0.8194, 'f1_score': 0.7487}
MLP_1028097: {'accuracy': 0.8714913294797688, 'precision': 0.7392003214788025, 'recall': 0.712984496124031, 'f1_score': 0.7258557758705731, 'f1_score_avg': 0.7236691354690469}
MLP_125441: {'accuracy': 0.8683930635838151, 'precision': 0.7219025700038358, 'recall': 0.7294573643410853, 'f1_score': 0.7256603046076731, 'f1_score_avg': 0.7234401690626824}
MLP_336897: {'accuracy': 0.8673294797687862, 'precision': 0.7216095956664732, 'recall': 0.7228682170542635, 'f1_score': 0.7256603046076731, 'f1_score_avg': 0.7229009083630152}
MLP_51457: {'accuracy': 0.8660346820809248, 'precision': 0.7195808267028915, 'recall': 0.7186046511627907, 'f1_score': 0.723574906903466, 'f1_score_avg': 0.7214223744775999}
MLP_22657: {'accuracy': 0.8623352601156069, 'precision': 0.701644189913172, 'recall': 0.736046511627907, 'f1_score': 0.7212098440728912, 'f1_score_avg': 0.7197229588641681}
MLP_10305: {'accuracy': 0.863121387283237, 'precision': 0.7068446784505453, 'recall': 0.7284883720930233, 'f1_score': 0.7178741150227912, 'f1_score_avg': 0.7155371338707862}
Logistic Regression: {'accuracy': 0.8394, 'precision': 0.6294, 'recall': 0.7957, 'f1_score': 0.7028}
Random Forest: {'accuracy': 0.8416, 'precision': 0.6432, 'recall': 0.7552, 'f1_score': 0.6947}
Decision Tree: {'accuracy': 0.8035, 'precision': 0.5666, 'recall': 0.75, 'f1_score': 0.6455}
Naive Bayes: {'accuracy': 0.6613, 'precision': 0.3958, 'recall': 0.7965, 'f1_score': 0.5288}
SVM: {'accuracy': 0.3252, 'precision': 0.2535, 'recall': 0.9401, 'f1_score': 0.3994}


EMBEDDINGS TYPE: GPT
MLP_209665: {'accuracy': 0.878242774566474, 'precision': 0.706690659250777, 'recall': 0.8372093023255814, 'f1_score': 0.7751473922902494, 'f1_score_avg': 0.7713978169992904}
MLP_441857: {'accuracy': 0.8905895953757226, 'precision': 0.7679324894514767, 'recall': 0.775968992248062, 'f1_score': 0.7751473922902494, 'f1_score_avg': 0.7715250523765027}
MLP_969729: {'accuracy': 0.8840231213872832, 'precision': 0.7286206896551725, 'recall': 0.8189922480620155, 'f1_score': 0.7751473922902494, 'f1_score_avg': 0.7690408127067285}
MLP_2293761: {'accuracy': 0.8897109826589595, 'precision': 0.7556661138750691, 'recall': 0.7947674418604651, 'f1_score': 0.7751473922902494, 'f1_score_avg': 0.7706959913187544}
XGBoost: {'accuracy': 0.8826, 'precision': 0.7161, 'recall': 0.8415, 'f1_score': 0.7738}
MLP_101761: {'accuracy': 0.8786589595375722, 'precision': 0.7058441558441558, 'recall': 0.8426356589147287, 'f1_score': 0.7732995199710171, 'f1_score_avg': 0.7714093826609477}
MLP_49857: {'accuracy': 0.8843468208092485, 'precision': 0.7430085907512338, 'recall': 0.7877906976744186, 'f1_score': 0.770996640537514, 'f1_score_avg': 0.7624581079978558}
Logistic Regression: {'accuracy': 0.8604, 'precision': 0.6624, 'recall': 0.8465, 'f1_score': 0.7432}
Random Forest: {'accuracy': 0.8456, 'precision': 0.6528, 'recall': 0.7537, 'f1_score': 0.6996}
Naive Bayes: {'accuracy': 0.8331, 'precision': 0.6299, 'recall': 0.7281, 'f1_score': 0.6755}
Decision Tree: {'accuracy': 0.7932, 'precision': 0.5476, 'recall': 0.7678, 'f1_score': 0.6393}
SVM: {'accuracy': 0.631, 'precision': 0.3758, 'recall': 0.8267, 'f1_score': 0.5167}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
====================================

