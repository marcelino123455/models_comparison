2025-10-29 01:02:34.158683: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-29 01:02:34.158683: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__pseudo_labeling/experimentation/only_lyrics/main_only_text_gpt.py:279: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__pseudo_labeling/experimentation/only_lyrics/main_only_text_gpt.py:279: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
Your are not adding cols numercis
You are executing with [ALL] dataset
--> PaTH:  ../../../../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy

##################################################
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../../../../data/spotify_dataset_pseudo_labeling.csv
Label distribution: {0: 70845, 1: 37280}
X shape: (108138, 1536)
y shape: (108125,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 1536)
Shape filtrado  X: (108125, 1536)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 56676, 1: 29824}
Label distribution en TEST: {0: 14169, 1: 7456}


==================================================
Data antes del undersampling ...
X: (86500, 1536)
y: (86500,)
Apliying UNDERSAMPLE
29824
Label distribution: {0: 29824, 1: 29824}
X shape: (59648, 1536)
y shape: (59648,)
Resultados con MLP

Entrenando red 1 con capas [1536, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.3811, Test Loss: 0.3457, F1: 0.8084, AUC: 0.9375
Epoch [10/30] Train Loss: 0.3025, Test Loss: 0.3119, F1: 0.8322, AUC: 0.9446
Epoch [20/30] Train Loss: 0.2961, Test Loss: 0.2774, F1: 0.8438, AUC: 0.9454
Mejores resultados en la época:  20
f1-score 0.8437879401052205
AUC según el mejor F1-score 0.945387636734611
Confusion Matrix:
 [[13054  1115]
 [ 1201  6255]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8929
Precision:  0.8487
Recall:     0.8389
F1-score:   0.8438

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.3831, Test Loss: 0.3138, F1: 0.8250, AUC: 0.9365
Epoch [10/30] Train Loss: 0.2994, Test Loss: 0.2738, F1: 0.8404, AUC: 0.9448
Epoch [20/30] Train Loss: 0.2924, Test Loss: 0.2647, F1: 0.8431, AUC: 0.9461
Mejores resultados en la época:  16
f1-score 0.8445378151260504
AUC según el mejor F1-score 0.9458785161843072
Confusion Matrix:
 [[13100  1069]
 [ 1225  6231]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8939
Precision:  0.8536
Recall:     0.8357
F1-score:   0.8445

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.3825, Test Loss: 0.3055, F1: 0.8288, AUC: 0.9375
Epoch [10/30] Train Loss: 0.3011, Test Loss: 0.2876, F1: 0.8409, AUC: 0.9448
Epoch [20/30] Train Loss: 0.2912, Test Loss: 0.2718, F1: 0.8442, AUC: 0.9464
Mejores resultados en la época:  28
f1-score 0.8452261976147645
AUC según el mejor F1-score 0.9466098066806669
Confusion Matrix:
 [[12959  1210]
 [ 1113  6343]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8926
Precision:  0.8398
Recall:     0.8507
F1-score:   0.8452

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6933, Test Loss: 0.6933, F1: 0.5128, AUC: 0.5000
Epoch [10/30] Train Loss: 0.6932, Test Loss: 0.6937, F1: 0.5128, AUC: 0.5000
Epoch [20/30] Train Loss: 0.6932, Test Loss: 0.6936, F1: 0.5128, AUC: 0.5000
Mejores resultados en la época:  0
f1-score 0.5127746638698807
AUC según el mejor F1-score 0.5000352883054555
Confusion Matrix:
 [[    0 14169]
 [    0  7456]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.3448
Precision:  0.3448
Recall:     1.0000
F1-score:   0.5128
Tiempo total para red 1: 907.58 segundos

Entrenando red 2 con capas [1536, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3721, Test Loss: 0.3062, F1: 0.8287, AUC: 0.9389
Epoch [10/30] Train Loss: 0.3001, Test Loss: 0.3541, F1: 0.8087, AUC: 0.9451
Epoch [20/30] Train Loss: 0.2953, Test Loss: 0.2667, F1: 0.8404, AUC: 0.9460
Mejores resultados en la época:  23
f1-score 0.8441506410256411
AUC según el mejor F1-score 0.9464241265841496
Confusion Matrix:
 [[12970  1199]
 [ 1135  6321]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8921
Precision:  0.8406
Recall:     0.8478
F1-score:   0.8442

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3713, Test Loss: 0.3248, F1: 0.8216, AUC: 0.9386
Epoch [10/30] Train Loss: 0.3020, Test Loss: 0.2780, F1: 0.8417, AUC: 0.9453
Epoch [20/30] Train Loss: 0.2914, Test Loss: 0.2805, F1: 0.8442, AUC: 0.9464
Mejores resultados en la época:  24
f1-score 0.8457234212629896
AUC según el mejor F1-score 0.9465360827088213
Confusion Matrix:
 [[12961  1208]
 [ 1108  6348]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8929
Precision:  0.8401
Recall:     0.8514
F1-score:   0.8457

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3760, Test Loss: 0.3207, F1: 0.7593, AUC: 0.9387
Epoch [10/30] Train Loss: 0.3025, Test Loss: 0.3166, F1: 0.8313, AUC: 0.9453
Epoch [20/30] Train Loss: 0.2938, Test Loss: 0.2711, F1: 0.8275, AUC: 0.9455
Mejores resultados en la época:  19
f1-score 0.845064752063343
AUC según el mejor F1-score 0.9462352139349731
Confusion Matrix:
 [[13019  1150]
 [ 1159  6297]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8932
Precision:  0.8456
Recall:     0.8446
F1-score:   0.8451

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3751, Test Loss: 0.2947, F1: 0.8323, AUC: 0.9384
Epoch [10/30] Train Loss: 0.3029, Test Loss: 0.2762, F1: 0.8421, AUC: 0.9450
Epoch [20/30] Train Loss: 0.2922, Test Loss: 0.3085, F1: 0.8345, AUC: 0.9462
Mejores resultados en la época:  26
f1-score 0.8448311017467546
AUC según el mejor F1-score 0.9465307014315542
Confusion Matrix:
 [[13127  1042]
 [ 1241  6215]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8944
Precision:  0.8564
Recall:     0.8336
F1-score:   0.8448
Tiempo total para red 2: 1091.30 segundos

Entrenando red 3 con capas [1536, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3696, Test Loss: 0.2968, F1: 0.8335, AUC: 0.9396
Epoch [10/30] Train Loss: 0.3032, Test Loss: 0.2766, F1: 0.8420, AUC: 0.9453
Epoch [20/30] Train Loss: 0.2960, Test Loss: 0.2680, F1: 0.8362, AUC: 0.9458
Mejores resultados en la época:  25
f1-score 0.8451330414280903
AUC según el mejor F1-score 0.9462599148022173
Confusion Matrix:
 [[13053  1116]
 [ 1183  6273]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8937
Precision:  0.8490
Recall:     0.8413
F1-score:   0.8451

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3741, Test Loss: 0.3024, F1: 0.8231, AUC: 0.9387
Epoch [10/30] Train Loss: 0.3027, Test Loss: 0.2697, F1: 0.8416, AUC: 0.9452
Epoch [20/30] Train Loss: 0.2939, Test Loss: 0.2743, F1: 0.8392, AUC: 0.9455
Mejores resultados en la época:  25
f1-score 0.8445917944467468
AUC según el mejor F1-score 0.9463601097360284
Confusion Matrix:
 [[13261   908]
 [ 1342  6114]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8960
Precision:  0.8707
Recall:     0.8200
F1-score:   0.8446

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3720, Test Loss: 0.4053, F1: 0.7778, AUC: 0.9389
Epoch [10/30] Train Loss: 0.3012, Test Loss: 0.2748, F1: 0.8423, AUC: 0.9452
Epoch [20/30] Train Loss: 0.2940, Test Loss: 0.2664, F1: 0.8422, AUC: 0.9461
Mejores resultados en la época:  29
f1-score 0.844453403211718
AUC según el mejor F1-score 0.9465366317221572
Confusion Matrix:
 [[13026  1143]
 [ 1172  6284]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8929
Precision:  0.8461
Recall:     0.8428
F1-score:   0.8445

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3772, Test Loss: 0.4899, F1: 0.7368, AUC: 0.9388
Epoch [10/30] Train Loss: 0.3023, Test Loss: 0.2722, F1: 0.8428, AUC: 0.9452
Epoch [20/30] Train Loss: 0.2939, Test Loss: 0.2849, F1: 0.8424, AUC: 0.9462
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
Your are not adding cols numercis
You are executing with [ALL] dataset
--> PaTH:  ../../../../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy

##################################################
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../../../../data/spotify_dataset_pseudo_labeling.csv
Label distribution: {0: 70845, 1: 37280}
X shape: (108138, 1536)
y shape: (108125,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 1536)
Shape filtrado  X: (108125, 1536)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 56676, 1: 29824}
Label distribution en TEST: {0: 14169, 1: 7456}


==================================================
Data antes del undersampling ...
X: (86500, 1536)
y: (86500,)
Apliying UNDERSAMPLE
29824
Label distribution: {0: 29824, 1: 29824}
X shape: (59648, 1536)
y shape: (59648,)
Resultados con MLP

Entrenando red 1 con capas [1536, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6934, Test Loss: 0.6917, F1: 0.0000, AUC: 0.5000
Epoch [10/30] Train Loss: 0.6932, Test Loss: 0.6948, F1: 0.5128, AUC: 0.5000
Epoch [20/30] Train Loss: 0.6932, Test Loss: 0.6935, F1: 0.5128, AUC: 0.5000
Mejores resultados en la época:  1
f1-score 0.5127746638698807
AUC según el mejor F1-score 0.5
Confusion Matrix:
 [[    0 14169]
 [    0  7456]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.3448
Precision:  0.3448
Recall:     1.0000
F1-score:   0.5128

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.3995, Test Loss: 0.3209, F1: 0.8201, AUC: 0.9350
Epoch [10/30] Train Loss: 0.2997, Test Loss: 0.3338, F1: 0.8291, AUC: 0.9447
Epoch [20/30] Train Loss: 0.2950, Test Loss: 0.3405, F1: 0.8232, AUC: 0.9453
Mejores resultados en la época:  22
f1-score 0.843808756379264
AUC según el mejor F1-score 0.9457669292237755
Confusion Matrix:
 [[13016  1153]
 [ 1173  6283]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8924
Precision:  0.8449
Recall:     0.8427
F1-score:   0.8438

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.3908, Test Loss: 0.3332, F1: 0.8184, AUC: 0.9357
Epoch [10/30] Train Loss: 0.2995, Test Loss: 0.2953, F1: 0.8426, AUC: 0.9449
Epoch [20/30] Train Loss: 0.2962, Test Loss: 0.2745, F1: 0.8392, AUC: 0.9454
Mejores resultados en la época:  23
f1-score 0.8443966675624832
AUC según el mejor F1-score 0.9459816313011207
Confusion Matrix:
 [[13025  1144]
 [ 1172  6284]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8929
Precision:  0.8460
Recall:     0.8428
F1-score:   0.8444

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.3899, Test Loss: 0.3479, F1: 0.8072, AUC: 0.9363
Epoch [10/30] Train Loss: 0.3024, Test Loss: 0.2682, F1: 0.8399, AUC: 0.9447
Epoch [20/30] Train Loss: 0.2919, Test Loss: 0.2722, F1: 0.8437, AUC: 0.9461
Mejores resultados en la época:  21
f1-score 0.8440904139433552
AUC según el mejor F1-score 0.9461458714802943
Confusion Matrix:
 [[13136  1033]
 [ 1257  6199]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8941
Precision:  0.8572
Recall:     0.8314
F1-score:   0.8441
Tiempo total para red 1: 905.36 segundos

Entrenando red 2 con capas [1536, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3777, Test Loss: 0.3195, F1: 0.8220, AUC: 0.9383
Epoch [10/30] Train Loss: 0.3001, Test Loss: 0.2712, F1: 0.8425, AUC: 0.9452
Epoch [20/30] Train Loss: 0.2918, Test Loss: 0.2699, F1: 0.8293, AUC: 0.9452
Mejores resultados en la época:  24
f1-score 0.843998033293531
AUC según el mejor F1-score 0.946401669098985
Confusion Matrix:
 [[13396   773]
 [ 1448  6008]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8973
Precision:  0.8860
Recall:     0.8058
F1-score:   0.8440

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3786, Test Loss: 0.3576, F1: 0.8014, AUC: 0.9382
Epoch [10/30] Train Loss: 0.3013, Test Loss: 0.2688, F1: 0.8419, AUC: 0.9449
Epoch [20/30] Train Loss: 0.2961, Test Loss: 0.2709, F1: 0.8437, AUC: 0.9458
Mejores resultados en la época:  27
f1-score 0.8443778519376149
AUC según el mejor F1-score 0.9461771936376853
Confusion Matrix:
 [[13141  1028]
 [ 1257  6199]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8943
Precision:  0.8578
Recall:     0.8314
F1-score:   0.8444

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3706, Test Loss: 0.2843, F1: 0.8281, AUC: 0.9386
Epoch [10/30] Train Loss: 0.3019, Test Loss: 0.3531, F1: 0.8068, AUC: 0.9451
Epoch [20/30] Train Loss: 0.2931, Test Loss: 0.3454, F1: 0.8122, AUC: 0.9461
Mejores resultados en la época:  23
f1-score 0.8445184938353882
AUC según el mejor F1-score 0.9465379048651518
Confusion Matrix:
 [[12956  1213]
 [ 1120  6336]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8921
Precision:  0.8393
Recall:     0.8498
F1-score:   0.8445

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3731, Test Loss: 0.2930, F1: 0.8074, AUC: 0.9386
Epoch [10/30] Train Loss: 0.3005, Test Loss: 0.2882, F1: 0.8405, AUC: 0.9452
Epoch [20/30] Train Loss: 0.2932, Test Loss: 0.2709, F1: 0.8450, AUC: 0.9463
Mejores resultados en la época:  27
f1-score 0.845397191506682
AUC según el mejor F1-score 0.946151143901469
Confusion Matrix:
 [[13115  1054]
 [ 1225  6231]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8946
Precision:  0.8553
Recall:     0.8357
F1-score:   0.8454
Tiempo total para red 2: 1098.57 segundos

Entrenando red 3 con capas [1536, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3695, Test Loss: 0.3477, F1: 0.8077, AUC: 0.9393
Epoch [10/30] Train Loss: 0.2998, Test Loss: 0.2691, F1: 0.8416, AUC: 0.9453
Epoch [20/30] Train Loss: 0.2956, Test Loss: 0.3083, F1: 0.8314, AUC: 0.9463
Mejores resultados en la época:  27
f1-score 0.8448493715501763
AUC según el mejor F1-score 0.9466499083185592
Confusion Matrix:
 [[12940  1229]
 [ 1104  6352]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8921
Precision:  0.8379
Recall:     0.8519
F1-score:   0.8448

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3785, Test Loss: 0.3001, F1: 0.7949, AUC: 0.9384
Epoch [10/30] Train Loss: 0.3022, Test Loss: 0.3039, F1: 0.8330, AUC: 0.9451
Epoch [20/30] Train Loss: 0.2945, Test Loss: 0.2745, F1: 0.8436, AUC: 0.9463
Mejores resultados en la época:  27
f1-score 0.8446490218642118
AUC según el mejor F1-score 0.9465371381396307
Confusion Matrix:
 [[13091  1078]
 [ 1217  6239]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8939
Precision:  0.8527
Recall:     0.8368
F1-score:   0.8446

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3669, Test Loss: 0.3218, F1: 0.8228, AUC: 0.9396
Epoch [10/30] Train Loss: 0.3032, Test Loss: 0.2689, F1: 0.8422, AUC: 0.9453
Epoch [20/30] Train Loss: 0.2939, Test Loss: 0.3269, F1: 0.8203, AUC: 0.9463
Mejores resultados en la época:  25
f1-score 0.8445959494686184
AUC según el mejor F1-score 0.9467723666897178
Confusion Matrix:
 [[12982  1187]
 [ 1138  6318]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8925
Precision:  0.8418
Recall:     0.8474
F1-score:   0.8446

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3733, Test Loss: 0.4569, F1: 0.7520, AUC: 0.9390
Epoch [10/30] Train Loss: 0.3015, Test Loss: 0.2847, F1: 0.8438, AUC: 0.9453
Epoch [20/30] Train Loss: 0.2962, Test Loss: 0.2665, F1: 0.8442, AUC: 0.9463
Mejores resultados en la época:  19
f1-score 0.8444871091607241
AUC según el mejor F1-score 0.9462922403288082
Confusion Matrix:
 [[13199   970]
 [ 1298  6158]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8951
Precision:  0.8639
Recall:     0.8259
F1-score:   0.8445
Tiempo total para red 3: 1208.86 segundos

Entrenando red 4 con capas [1536, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3706, Test Loss: 0.3200, F1: 0.7707, AUC: 0.9396
Epoch [10/30] Train Loss: 0.3029, Test Loss: 0.2693, F1: 0.8360, AUC: 0.9450
Epoch [20/30] Train Loss: 0.2940, Test Loss: 0.3008, F1: 0.8364, AUC: 0.9463
Mejores resultados en la época:  27
f1-score 0.844675740592474
AUC según el mejor F1-score 0.946508821356967
Confusion Matrix:
 [[12967  1202]
 [ 1126  6330]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8923
Precision:  0.8404
Recall:     0.8490
F1-score:   0.8447

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3735, Test Loss: 0.2833, F1: 0.8225, AUC: 0.9392
Epoch [10/30] Train Loss: 0.2998, Test Loss: 0.2763, F1: 0.8228, AUC: 0.9448
Epoch [20/30] Train Loss: 0.2946, Test Loss: 0.3431, F1: 0.8164, AUC: 0.9461
Mejores resultados en la época:  29
f1-score 0.8448181569592562
AUC según el mejor F1-score 0.9465547349636229
Confusion Matrix:
 [[13176   993]
 [ 1277  6179]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8950
Precision:  0.8615
Recall:     0.8287
F1-score:   0.8448

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3725, Test Loss: 0.2962, F1: 0.8335, AUC: 0.9392
Epoch [10/30] Train Loss: 0.3041, Test Loss: 0.2698, F1: 0.8400, AUC: 0.9451
Epoch [20/30] Train Loss: 0.2946, Test Loss: 0.2831, F1: 0.8398, AUC: 0.9463
Mejores resultados en la época:  21
f1-score 0.8445731994879742
AUC según el mejor F1-score 0.9461934747228203
Confusion Matrix:
 [[13050  1119]
 [ 1188  6268]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8933
Precision:  0.8485
Recall:     0.8407
F1-score:   0.8446

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3813, Test Loss: 0.2869, F1: 0.8205, AUC: 0.9387
Epoch [10/30] Train Loss: 0.3053, Test Loss: 0.4175, F1: 0.7712, AUC: 0.9450
Epoch [20/30] Train Loss: 0.2937, Test Loss: 0.2998, F1: 0.8351, AUC: 0.9463
Mejores resultados en la época:  23
f1-score 0.84472385587778
AUC según el mejor F1-score 0.94646543983768
Confusion Matrix:
 [[13080  1089]
 [ 1208  6248]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8938
Precision:  0.8516
Recall:     0.8380
F1-score:   0.8447
Tiempo total para red 4: 1383.81 segundos

Entrenando red 5 con capas [1536, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3807, Test Loss: 0.2954, F1: 0.8298, AUC: 0.9385
Epoch [10/30] Train Loss: 0.3028, Test Loss: 0.2810, F1: 0.8427, AUC: 0.9455
Epoch [20/30] Train Loss: 0.2934, Test Loss: 0.2640, F1: 0.8441, AUC: 0.9463
Mejores resultados en la época:  24
f1-score 0.8447084830063554
AUC según el mejor F1-score 0.9464810772520075
Confusion Matrix:
 [[13263   906]
 [ 1342  6114]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8960
Precision:  0.8709
Recall:     0.8200
F1-score:   0.8447

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3737, Test Loss: 0.2914, F1: 0.8352, AUC: 0.9395
Epoch [10/30] Train Loss: 0.3036, Test Loss: 0.2875, F1: 0.8410, AUC: 0.9454
Epoch [20/30] Train Loss: 0.2932, Test Loss: 0.2690, F1: 0.8289, AUC: 0.9460
Mejores resultados en la época:  25
f1-score 0.8452276854779041
AUC según el mejor F1-score 0.9465401908430937
Confusion Matrix:
 [[13041  1128]
 [ 1173  6283]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8936
Precision:  0.8478
Recall:     0.8427
F1-score:   0.8452

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3786, Test Loss: 0.3030, F1: 0.8325, AUC: 0.9385
Epoch [10/30] Train Loss: 0.3029, Test Loss: 0.2781, F1: 0.8425, AUC: 0.9451
Epoch [20/30] Train Loss: 0.2947, Test Loss: 0.2720, F1: 0.8436, AUC: 0.9461
Mejores resultados en la época:  22
f1-score 0.8441988950276244
AUC según el mejor F1-score 0.9461981602676702
Confusion Matrix:
 [[13257   912]
 [ 1344  6112]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8957
Precision:  0.8702
Recall:     0.8197
F1-score:   0.8442

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3758, Test Loss: 0.3358, F1: 0.8216, AUC: 0.9393
Epoch [10/30] Train Loss: 0.2999, Test Loss: 0.3182, F1: 0.8265, AUC: 0.9453
Epoch [20/30] Train Loss: 0.2928, Test Loss: 0.2690, F1: 0.8438, AUC: 0.9464
Mejores resultados en la época:  27
f1-score 0.8450913011252611
AUC según el mejor F1-score 0.9465723885820978
Confusion Matrix:
 [[13055  1114]
 [ 1185  6271]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8937
Precision:  0.8492
Recall:     0.8411
F1-score:   0.8451
Tiempo total para red 5: 1284.47 segundos

Entrenando red 6 con capas [1536, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3783, Test Loss: 0.2969, F1: 0.8303, AUC: 0.9393
Epoch [10/30] Train Loss: 0.3062, Test Loss: 0.2751, F1: 0.8430, AUC: 0.9450
Epoch [20/30] Train Loss: 0.2969, Test Loss: 0.3585, F1: 0.8041, AUC: 0.9462
Mejores resultados en la época:  29
f1-score 0.8438520493168944
AUC según el mejor F1-score 0.9465957263817493
Confusion Matrix:
 [[12951  1218]
 [ 1125  6331]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8917
Precision:  0.8387
Recall:     0.8491
F1-score:   0.8439

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3800, Test Loss: 0.2861, F1: 0.8073, AUC: 0.9389
Epoch [10/30] Train Loss: 0.3034, Test Loss: 0.3102, F1: 0.8312, AUC: 0.9450
Epoch [20/30] Train Loss: 0.2951, Test Loss: 0.3142, F1: 0.8294, AUC: 0.9463
Mejores resultados en la época:  28
f1-score 0.8456285982059178
AUC según el mejor F1-score 0.9466801939766347
Confusion Matrix:
 [[13003  1166]
 [ 1140  6316]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8934
Precision:  0.8442
Recall:     0.8471
F1-score:   0.8456

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3899, Test Loss: 0.2935, F1: 0.8331, AUC: 0.9388
Epoch [10/30] Train Loss: 0.3042, Test Loss: 0.2919, F1: 0.8425, AUC: 0.9453
Epoch [20/30] Train Loss: 0.2941, Test Loss: 0.2969, F1: 0.8369, AUC: 0.9463
Mejores resultados en la época:  29
f1-score 0.8450020234722785
AUC según el mejor F1-score 0.9466089831606629
Confusion Matrix:
 [[13063  1106]
 [ 1192  6264]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8937
Precision:  0.8499
Recall:     0.8401
F1-score:   0.8450

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3845, Test Loss: 0.3229, F1: 0.8347, AUC: 0.9390
Epoch [10/30] Train Loss: 0.3046, Test Loss: 0.2796, F1: 0.8429, AUC: 0.9455
Epoch [20/30] Train Loss: 0.2968, Test Loss: 0.2813, F1: 0.8437, AUC: 0.9460
Mejores resultados en la época:  26
f1-score 0.843977028990521
AUC según el mejor F1-score 0.9465295087474104
Confusion Matrix:
 [[13271   898]
 [ 1357  6099]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8957
Precision:  0.8717
Recall:     0.8180
F1-score:   0.8440
Tiempo total para red 6: 1318.74 segundos
Saved on: outputs_only_text/2/gpt

==============================
Model: Logistic Regression
Accuracy:  0.8899
Precision: 0.8363
Recall:    0.8464
F1-score:  0.8414
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     14169
           1       0.84      0.85      0.84      7456

    accuracy                           0.89     21625
   macro avg       0.88      0.88      0.88     21625
weighted avg       0.89      0.89      0.89     21625
Mejores resultados en la época:  24
f1-score 0.8454033255462415
AUC según el mejor F1-score 0.9463891790455923
Confusion Matrix:
 [[13223   946]
 [ 1304  6152]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8960
Precision:  0.8667
Recall:     0.8251
F1-score:   0.8454
Tiempo total para red 3: 1205.66 segundos

Entrenando red 4 con capas [1536, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3751, Test Loss: 0.3202, F1: 0.8228, AUC: 0.9392
Epoch [10/30] Train Loss: 0.3003, Test Loss: 0.3303, F1: 0.8286, AUC: 0.9455
Epoch [20/30] Train Loss: 0.2946, Test Loss: 0.2892, F1: 0.8418, AUC: 0.9463
Mejores resultados en la época:  28
f1-score 0.8445472481121165
AUC según el mejor F1-score 0.94657693214074
Confusion Matrix:
 [[13133  1036]
 [ 1249  6207]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8943
Precision:  0.8570
Recall:     0.8325
F1-score:   0.8445

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3709, Test Loss: 0.3548, F1: 0.8083, AUC: 0.9395
Epoch [10/30] Train Loss: 0.3011, Test Loss: 0.2668, F1: 0.8370, AUC: 0.9453
Epoch [20/30] Train Loss: 0.2934, Test Loss: 0.2643, F1: 0.8388, AUC: 0.9464
Mejores resultados en la época:  26
f1-score 0.8442447008317682
AUC según el mejor F1-score 0.9463798126887659
Confusion Matrix:
 [[13010  1159]
 [ 1163  6293]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8926
Precision:  0.8445
Recall:     0.8440
F1-score:   0.8442

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3774, Test Loss: 0.4088, F1: 0.7810, AUC: 0.9391
Epoch [10/30] Train Loss: 0.3041, Test Loss: 0.3364, F1: 0.8186, AUC: 0.9451
Epoch [20/30] Train Loss: 0.2932, Test Loss: 0.2818, F1: 0.8404, AUC: 0.9463
Mejores resultados en la época:  28
f1-score 0.8451022033448368
AUC según el mejor F1-score 0.9466672259030096
Confusion Matrix:
 [[12924  1245]
 [ 1089  6367]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8921
Precision:  0.8364
Recall:     0.8539
F1-score:   0.8451

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3744, Test Loss: 0.2865, F1: 0.8175, AUC: 0.9390
Epoch [10/30] Train Loss: 0.3036, Test Loss: 0.2694, F1: 0.8425, AUC: 0.9452
Epoch [20/30] Train Loss: 0.2953, Test Loss: 0.3301, F1: 0.8229, AUC: 0.9461
Mejores resultados en la época:  24
f1-score 0.8447624190064795
AUC según el mejor F1-score 0.9462489865971078
Confusion Matrix:
 [[13067  1102]
 [ 1198  6258]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8936
Precision:  0.8503
Recall:     0.8393
F1-score:   0.8448
Tiempo total para red 4: 1374.84 segundos

Entrenando red 5 con capas [1536, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3782, Test Loss: 0.3314, F1: 0.8174, AUC: 0.9389
Epoch [10/30] Train Loss: 0.3026, Test Loss: 0.3383, F1: 0.8085, AUC: 0.9449
Epoch [20/30] Train Loss: 0.2943, Test Loss: 0.2806, F1: 0.8380, AUC: 0.9453
Mejores resultados en la época:  15
f1-score 0.8445652173913043
AUC según el mejor F1-score 0.9460787640657218
Confusion Matrix:
 [[13121  1048]
 [ 1240  6216]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8942
Precision:  0.8557
Recall:     0.8337
F1-score:   0.8446

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3742, Test Loss: 0.3063, F1: 0.8213, AUC: 0.9393
Epoch [10/30] Train Loss: 0.3051, Test Loss: 0.3051, F1: 0.8279, AUC: 0.9450
Epoch [20/30] Train Loss: 0.2953, Test Loss: 0.2655, F1: 0.8426, AUC: 0.9462
Mejores resultados en la época:  26
f1-score 0.8443690637720489
AUC según el mejor F1-score 0.9465099903767428
Confusion Matrix:
 [[13108  1061]
 [ 1233  6223]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8939
Precision:  0.8543
Recall:     0.8346
F1-score:   0.8444

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3793, Test Loss: 0.2883, F1: 0.8340, AUC: 0.9388
Epoch [10/30] Train Loss: 0.3020, Test Loss: 0.2671, F1: 0.8407, AUC: 0.9453
Epoch [20/30] Train Loss: 0.2964, Test Loss: 0.2910, F1: 0.8402, AUC: 0.9461
Mejores resultados en la época:  15
f1-score 0.8443275327128018
AUC según el mejor F1-score 0.9460601638725296
Confusion Matrix:
 [[13058  1111]
 [ 1197  6259]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8933
Precision:  0.8493
Recall:     0.8395
F1-score:   0.8443

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3789, Test Loss: 0.3218, F1: 0.8231, AUC: 0.9392
Epoch [10/30] Train Loss: 0.3006, Test Loss: 0.2919, F1: 0.8400, AUC: 0.9452
Epoch [20/30] Train Loss: 0.2938, Test Loss: 0.2651, F1: 0.8437, AUC: 0.9464
Mejores resultados en la época:  26
f1-score 0.84499831593129
AUC según el mejor F1-score 0.9463917963246852
Confusion Matrix:
 [[13052  1117]
 [ 1184  6272]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8936
Precision:  0.8488
Recall:     0.8412
F1-score:   0.8450
Tiempo total para red 5: 1289.63 segundos

Entrenando red 6 con capas [1536, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3788, Test Loss: 0.3165, F1: 0.7588, AUC: 0.9393
Epoch [10/30] Train Loss: 0.3048, Test Loss: 0.2738, F1: 0.8416, AUC: 0.9450
Epoch [20/30] Train Loss: 0.2942, Test Loss: 0.2935, F1: 0.8395, AUC: 0.9462
Mejores resultados en la época:  24
f1-score 0.8444178150589176
AUC según el mejor F1-score 0.946406548691652
Confusion Matrix:
 [[12946  1223]
 [ 1114  6342]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8919
Precision:  0.8383
Recall:     0.8506
F1-score:   0.8444

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3792, Test Loss: 0.2912, F1: 0.8344, AUC: 0.9390
Epoch [10/30] Train Loss: 0.3073, Test Loss: 0.3735, F1: 0.8052, AUC: 0.9450
Epoch [20/30] Train Loss: 0.2937, Test Loss: 0.2691, F1: 0.8426, AUC: 0.9456
Mejores resultados en la época:  28
f1-score 0.8454275694631778
AUC según el mejor F1-score 0.9467313516072233
Confusion Matrix:
 [[13065  1104]
 [ 1188  6268]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8940
Precision:  0.8502
Recall:     0.8407
F1-score:   0.8454

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3757, Test Loss: 0.2979, F1: 0.8010, AUC: 0.9395
Epoch [10/30] Train Loss: 0.3030, Test Loss: 0.3175, F1: 0.8306, AUC: 0.9450
Epoch [20/30] Train Loss: 0.2956, Test Loss: 0.2745, F1: 0.8440, AUC: 0.9462
Mejores resultados en la época:  21
f1-score 0.8442298665593777
AUC según el mejor F1-score 0.9462798733301285
Confusion Matrix:
 [[13007  1162]
 [ 1161  6295]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8926
Precision:  0.8442
Recall:     0.8443
F1-score:   0.8442

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3787, Test Loss: 0.2933, F1: 0.8353, AUC: 0.9397
Epoch [10/30] Train Loss: 0.3054, Test Loss: 0.3270, F1: 0.8171, AUC: 0.9450
Epoch [20/30] Train Loss: 0.2932, Test Loss: 0.4002, F1: 0.7886, AUC: 0.9460
Mejores resultados en la época:  22
f1-score 0.8450704225352113
AUC según el mejor F1-score 0.9464505786146206
Confusion Matrix:
 [[13015  1154]
 [ 1156  6300]]
Matriz de confusión guardada en: outputs_only_text/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8932
Precision:  0.8452
Recall:     0.8450
F1-score:   0.8451
Tiempo total para red 6: 1360.46 segundos
Saved on: outputs_only_text/2/gpt

==============================
Model: Logistic Regression
Accuracy:  0.8899
Precision: 0.8363
Recall:    0.8464
F1-score:  0.8414
              precision    recall  f1-score   support

           0       0.92      0.91      0.92     14169
           1       0.84      0.85      0.84      7456

    accuracy                           0.89     21625
   macro avg       0.88      0.88      0.88     21625
weighted avg       0.89      0.89      0.89     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:32:22] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:32:41] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

[[12934  1235]
 [ 1145  6311]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/2/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text/2/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.6996
Precision: 0.5441
Recall:    0.7936
F1-score:  0.6456
              precision    recall  f1-score   support

           0       0.86      0.65      0.74     14169
           1       0.54      0.79      0.65      7456

    accuracy                           0.70     21625
   macro avg       0.70      0.72      0.69     21625
weighted avg       0.75      0.70      0.71     21625

[[9211 4958]
 [1539 5917]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text/2/gpt/conf_matrix_svm.png
Modelo guardado como: outputs_only_text/2/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7840
Precision: 0.6665
Recall:    0.7480
F1-score:  0.7049
              precision    recall  f1-score   support

           0       0.86      0.80      0.83     14169
           1       0.67      0.75      0.70      7456

    accuracy                           0.78     21625
   macro avg       0.76      0.78      0.77     21625
weighted avg       0.79      0.78      0.79     21625

[[11378  2791]
 [ 1879  5577]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text/2/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text/2/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8718
Precision: 0.8462
Recall:    0.7677
F1-score:  0.8051
              precision    recall  f1-score   support

           0       0.88      0.93      0.90     14169
           1       0.85      0.77      0.81      7456

    accuracy                           0.87     21625
   macro avg       0.86      0.85      0.85     21625
weighted avg       0.87      0.87      0.87     21625

[[13129  1040]
 [ 1732  5724]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/2/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text/2/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8827
Precision: 0.8313
Recall:    0.8279
F1-score:  0.8296
              precision    recall  f1-score   support

           0       0.91      0.91      0.91     14169
           1       0.83      0.83      0.83      7456

    accuracy                           0.88     21625
   macro avg       0.87      0.87      0.87     21625
weighted avg       0.88      0.88      0.88     21625

[[12916  1253]
 [ 1283  6173]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text/2/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text/2/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8543
Precision: 0.8248
Recall:    0.7332
F1-score:  0.7763
              precision    recall  f1-score   support

           0       0.87      0.92      0.89     14169
           1       0.82      0.73      0.78      7456

    accuracy                           0.85     21625
   macro avg       0.85      0.83      0.83     21625
weighted avg       0.85      0.85      0.85     21625

[[13008  1161]
 [ 1989  5467]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_only_text/2/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text/2/gpt/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.8899, 'precision': 0.8363, 'recall': 0.8464, 'f1_score': 0.8414}
XGBoost: {'accuracy': 0.8827, 'precision': 0.8313, 'recall': 0.8279, 'f1_score': 0.8296}
Random Forest: {'accuracy': 0.8718, 'precision': 0.8462, 'recall': 0.7677, 'f1_score': 0.8051}
Naive Bayes: {'accuracy': 0.8543, 'precision': 0.8248, 'recall': 0.7332, 'f1_score': 0.7763}
Decision Tree: {'accuracy': 0.784, 'precision': 0.6665, 'recall': 0.748, 'f1_score': 0.7049}
SVM: {'accuracy': 0.6996, 'precision': 0.5441, 'recall': 0.7936, 'f1_score': 0.6456}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: GPT
MLP_2273281: {'accuracy': 0.8957225433526012, 'precision': 0.8716592825496642, 'recall': 0.8179989270386266, 'f1_score': 0.8456285982059178, 'f1_score_avg': 0.8446149249964029}
MLP_100481: {'accuracy': 0.8946127167630058, 'precision': 0.8553191489361702, 'recall': 0.8357027896995708, 'f1_score': 0.845397191506682, 'f1_score_avg': 0.844572892643304}
MLP_207105: {'accuracy': 0.895121387283237, 'precision': 0.8639169472502806, 'recall': 0.8259120171673819, 'f1_score': 0.845397191506682, 'f1_score_avg': 0.8446453630109326}
MLP_436737: {'accuracy': 0.8937803468208092, 'precision': 0.8515742128935532, 'recall': 0.8379828326180258, 'f1_score': 0.845397191506682, 'f1_score_avg': 0.8446977382293711}
MLP_959489: {'accuracy': 0.8936878612716763, 'precision': 0.8491536899119837, 'recall': 0.8410675965665236, 'f1_score': 0.845397191506682, 'f1_score_avg': 0.8448065911592862}
MLP_49217: {'accuracy': 0.8941040462427746, 'precision': 0.8571626106194691, 'recall': 0.8314109442060086, 'f1_score': 0.8443966675624832, 'f1_score_avg': 0.7612676254387457}
Logistic Regression: {'accuracy': 0.8899, 'precision': 0.8363, 'recall': 0.8464, 'f1_score': 0.8414}
XGBoost: {'accuracy': 0.8827, 'precision': 0.8313, 'recall': 0.8279, 'f1_score': 0.8296}
Random Forest: {'accuracy': 0.8718, 'precision': 0.8462, 'recall': 0.7677, 'f1_score': 0.8051}
Naive Bayes: {'accuracy': 0.8543, 'precision': 0.8248, 'recall': 0.7332, 'f1_score': 0.7763}
Decision Tree: {'accuracy': 0.784, 'precision': 0.6665, 'recall': 0.748, 'f1_score': 0.7049}
SVM: {'accuracy': 0.6996, 'precision': 0.5441, 'recall': 0.7936, 'f1_score': 0.6456}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: False
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: Not used
====================================


[[12934  1235]
 [ 1145  6311]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/2/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text/2/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.6996
Precision: 0.5441
Recall:    0.7936
F1-score:  0.6456
              precision    recall  f1-score   support

           0       0.86      0.65      0.74     14169
           1       0.54      0.79      0.65      7456

    accuracy                           0.70     21625
   macro avg       0.70      0.72      0.69     21625
weighted avg       0.75      0.70      0.71     21625

[[9211 4958]
 [1539 5917]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text/2/gpt/conf_matrix_svm.png
Modelo guardado como: outputs_only_text/2/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7840
Precision: 0.6665
Recall:    0.7480
F1-score:  0.7049
              precision    recall  f1-score   support

           0       0.86      0.80      0.83     14169
           1       0.67      0.75      0.70      7456

    accuracy                           0.78     21625
   macro avg       0.76      0.78      0.77     21625
weighted avg       0.79      0.78      0.79     21625

[[11378  2791]
 [ 1879  5577]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text/2/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text/2/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8718
Precision: 0.8462
Recall:    0.7677
F1-score:  0.8051
              precision    recall  f1-score   support

           0       0.88      0.93      0.90     14169
           1       0.85      0.77      0.81      7456

    accuracy                           0.87     21625
   macro avg       0.86      0.85      0.85     21625
weighted avg       0.87      0.87      0.87     21625

[[13129  1040]
 [ 1732  5724]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/2/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text/2/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8827
Precision: 0.8313
Recall:    0.8279
F1-score:  0.8296
              precision    recall  f1-score   support

           0       0.91      0.91      0.91     14169
           1       0.83      0.83      0.83      7456

    accuracy                           0.88     21625
   macro avg       0.87      0.87      0.87     21625
weighted avg       0.88      0.88      0.88     21625

[[12916  1253]
 [ 1283  6173]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text/2/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text/2/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8543
Precision: 0.8248
Recall:    0.7332
F1-score:  0.7763
              precision    recall  f1-score   support

           0       0.87      0.92      0.89     14169
           1       0.82      0.73      0.78      7456

    accuracy                           0.85     21625
   macro avg       0.85      0.83      0.83     21625
weighted avg       0.85      0.85      0.85     21625

[[13008  1161]
 [ 1989  5467]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_only_text/2/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text/2/gpt/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.8899, 'precision': 0.8363, 'recall': 0.8464, 'f1_score': 0.8414}
XGBoost: {'accuracy': 0.8827, 'precision': 0.8313, 'recall': 0.8279, 'f1_score': 0.8296}
Random Forest: {'accuracy': 0.8718, 'precision': 0.8462, 'recall': 0.7677, 'f1_score': 0.8051}
Naive Bayes: {'accuracy': 0.8543, 'precision': 0.8248, 'recall': 0.7332, 'f1_score': 0.7763}
Decision Tree: {'accuracy': 0.784, 'precision': 0.6665, 'recall': 0.748, 'f1_score': 0.7049}
SVM: {'accuracy': 0.6996, 'precision': 0.5441, 'recall': 0.7936, 'f1_score': 0.6456}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: GPT
MLP_100481: {'accuracy': 0.8944277456647399, 'precision': 0.8564144963483533, 'recall': 0.8335568669527897, 'f1_score': 0.8457234212629896, 'f1_score_avg': 0.8449424790246821}
MLP_207105: {'accuracy': 0.8959537572254336, 'precision': 0.8667230205691744, 'recall': 0.825107296137339, 'f1_score': 0.8457234212629896, 'f1_score_avg': 0.8448953911581992}
MLP_436737: {'accuracy': 0.8936416184971099, 'precision': 0.8502717391304347, 'recall': 0.839324034334764, 'f1_score': 0.8457234212629896, 'f1_score_avg': 0.8446641428238002}
MLP_959489: {'accuracy': 0.8935953757225433, 'precision': 0.8488293409121668, 'recall': 0.8412017167381974, 'f1_score': 0.8457234212629896, 'f1_score_avg': 0.8445650324518612}
MLP_2273281: {'accuracy': 0.8931791907514451, 'precision': 0.8451837939361416, 'recall': 0.8449570815450643, 'f1_score': 0.8457234212629896, 'f1_score_avg': 0.8447864184041711}
MLP_49217: {'accuracy': 0.3447861271676301, 'precision': 0.3447861271676301, 'recall': 1.0, 'f1_score': 0.8452261976147645, 'f1_score_avg': 0.761581654178979}
Logistic Regression: {'accuracy': 0.8899, 'precision': 0.8363, 'recall': 0.8464, 'f1_score': 0.8414}
XGBoost: {'accuracy': 0.8827, 'precision': 0.8313, 'recall': 0.8279, 'f1_score': 0.8296}
Random Forest: {'accuracy': 0.8718, 'precision': 0.8462, 'recall': 0.7677, 'f1_score': 0.8051}
Naive Bayes: {'accuracy': 0.8543, 'precision': 0.8248, 'recall': 0.7332, 'f1_score': 0.7763}
Decision Tree: {'accuracy': 0.784, 'precision': 0.6665, 'recall': 0.748, 'f1_score': 0.7049}
SVM: {'accuracy': 0.6996, 'precision': 0.5441, 'recall': 0.7936, 'f1_score': 0.6456}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: False
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: Not used
====================================

