{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62a83fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfd71808",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_first_number(filename):\n",
    "    match = re.search(r\"(\\d+)\", filename)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "def get_second_number(filename: str) -> int:\n",
    "    matches = re.findall(r\"(\\d+)\", filename)\n",
    "    if len(matches) >= 2:\n",
    "        return int(matches[1])  # el segundo número\n",
    "    return float('inf')  # si no lo encuentra\n",
    "\n",
    "\n",
    "def load_embeddings_and_check_dim(file_path):\n",
    "    embeddings = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            j = json.loads(line)\n",
    "            embeddings.append([get_first_number(j[\"custom_id\"]),j[\"embedding\"]])\n",
    "    \n",
    "    if embeddings:\n",
    "        print(f\"Número de embeddings: {len(embeddings)}\")\n",
    "        # print(f\"Dimensión del primer embedding: {len(embeddings[0][1])}\")\n",
    "    else:\n",
    "        print(\"No se encontraron embeddings en el archivo.\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def sort_according_id(embeddings):\n",
    "    return sorted(embeddings, key=lambda x: x[0])\n",
    "\n",
    "def get_faltantes(start, end, embeddings):\n",
    "    faltantes = []\n",
    "    n = len(embeddings)\n",
    "    index = 0   \n",
    "    for i in range(start, end + 1):\n",
    "        if index >= n:  \n",
    "            faltantes.append(i)\n",
    "        elif embeddings[index][0] == i:  \n",
    "            index += 1\n",
    "        else:\n",
    "            faltantes.append(i) \n",
    "    return faltantes\n",
    "\n",
    "def save_embb(embeddings, output_path):\n",
    "    only_embb =[]\n",
    "    for embb in embeddings:\n",
    "        only_embb.append(embb[1])\n",
    "    np.save(output_path, np.array(only_embb))\n",
    "    print(f\"Embeddings saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eca2ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de embeddings: 7319\n",
      "Embeddings saved to ../../../../../../data/spanish/gpt_T/only_embb/embeddings_0_7318.npy\n",
      "Se tienen un total de 7319 embbedings\n"
     ]
    }
   ],
   "source": [
    "# data_dir = \"../../../../data\"\n",
    "data_dir = \"../../../../../../data/spanish\"\n",
    "\n",
    "# path_embb = \"../../../../data/embeddings_only\"\n",
    "path_embb = f\"{data_dir}/gpt_T/embeddings_with_id\"\n",
    "path_final_embb = f\"{data_dir}/gpt_T/only_embb\"\n",
    "os.makedirs(path_final_embb, exist_ok=True)\n",
    "\n",
    "file_embb = os.listdir(path_embb)\n",
    "files_sorted = sorted(file_embb, key=get_first_number)\n",
    "faltantes = []\n",
    "\n",
    "# index = 96814\n",
    "total = 0\n",
    "for relative_path in files_sorted:\n",
    "    start = get_first_number(relative_path)\n",
    "\n",
    "    # if start != index:\n",
    "    #     continue\n",
    "    path_file_embb = os.path.join(path_embb, relative_path)\n",
    "    embbdings = load_embeddings_and_check_dim(path_file_embb)\n",
    "    total+=len(embbdings)\n",
    "    embbdings_sorted = sort_according_id(embbdings)\n",
    "    end = get_second_number(relative_path)\n",
    "    faltantes.append(get_faltantes(start, end, embbdings_sorted))\n",
    "    # directory = os.path.join(data_dir, \"gpt_embd\")\n",
    "    save_embb(embbdings_sorted, f\"{path_final_embb}/embeddings_{start}_{end}.npy\")\n",
    "\n",
    "print(f\"Se tienen un total de {total} embbedings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ffb7f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(faltantes)\n",
    "faltantes_flaten = [ elment for array_ in faltantes for elment in array_]\n",
    "print(faltantes_flaten)\n",
    "with open(\"T_faltantes.json\", \"w\") as f:\n",
    "    json.dump(faltantes_flaten, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe5f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        array_ = json.load(f)  \n",
    "    return array_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d06744",
   "metadata": {},
   "source": [
    "Fusionar los embbedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1277fbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando: ../../../../../../data/spanish/gpt_T/only_embb/embeddings_0_7318.npy\n",
      "../../../../../../data/spanish/gpt_T/only_embb/embeddings_0_7318.npy\n",
      "(7319, 1536)\n",
      "Embeddings fusionados guardados en: ../../../../../../data/spanish/gpt_T/gpt_fussioned/gpt_T.npy\n",
      "Shape final: (7319, 1536)\n",
      "Tamaño del archivo: 89,936,000 bytes\n",
      "Tamaño aproximado: 85.77 MB (0.08 GB)\n"
     ]
    }
   ],
   "source": [
    "# A partir de 105512 es que tenemos los embbedings\n",
    "# data_dir = \"../../../../data\"\n",
    "# directory = os.path.join(data_dir, \"new_gpt\")\n",
    "# directory = \"../../../../data/new_gpt/embeddings_with_id\"\n",
    "directory = path_final_embb\n",
    "dir_fussioned = f\"{data_dir}/gpt_T/gpt_fussioned\"\n",
    "path_fused = os.path.join(dir_fussioned, \"gpt_T.npy\")\n",
    "\n",
    "\n",
    "files_ = os.listdir(directory)\n",
    "sorted_files = sorted(files_, key=get_first_number)\n",
    "\n",
    "all_embeddings = []\n",
    "# dir_fussioned = os.path.join(directory, \"gpt_fussioned\")\n",
    "os.makedirs(dir_fussioned, exist_ok=True)\n",
    "for relatvie_embb_path in sorted_files:\n",
    "    path_embb = os.path.join(directory, relatvie_embb_path)\n",
    "    print(\"Cargando:\", path_embb)\n",
    "    print(path_embb)\n",
    "    arr = np.load(path_embb)\n",
    "    all_embeddings.append(arr)\n",
    "\n",
    "# Fusionar todos\n",
    "fused_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "print(fused_embeddings.shape)\n",
    "\n",
    "np.save(path_fused, fused_embeddings)\n",
    "\n",
    "print(\"Embeddings fusionados guardados en:\", path_fused)\n",
    "print(\"Shape final:\", fused_embeddings.shape)\n",
    "\n",
    "#Tamaño del archivo en disco\n",
    "size_bytes = os.path.getsize(path_fused)\n",
    "size_mb = size_bytes / (1024**2)\n",
    "size_gb = size_bytes / (1024**3)\n",
    "\n",
    "print(f\"Tamaño del archivo: {size_bytes:,} bytes\")\n",
    "print(f\"Tamaño aproximado: {size_mb:.2f} MB ({size_gb:.2f} GB)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
