{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "997792f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T04:29:02.927708Z",
     "iopub.status.busy": "2025-09-27T04:29:02.926783Z",
     "iopub.status.idle": "2025-09-27T04:29:03.964171Z",
     "shell.execute_reply": "2025-09-27T04:29:03.962770Z"
    },
    "papermill": {
     "duration": 1.050342,
     "end_time": "2025-09-27T04:29:03.967595",
     "exception": false,
     "start_time": "2025-09-27T04:29:02.917253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b3dda3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T04:29:03.981554Z",
     "iopub.status.busy": "2025-09-27T04:29:03.981013Z",
     "iopub.status.idle": "2025-09-27T04:29:03.987280Z",
     "shell.execute_reply": "2025-09-27T04:29:03.986395Z"
    },
    "papermill": {
     "duration": 0.014178,
     "end_time": "2025-09-27T04:29:03.989119",
     "exception": false,
     "start_time": "2025-09-27T04:29:03.974941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "# DATA_PATH =\"../../data\"\n",
    "DATA_PATH = \"../../../../../../data/spanish\"\n",
    "\n",
    "\n",
    "# path_lb_embb = os.path.join(DATA_PATH, \"lb_npy.npy\")\n",
    "path_dataset = os.path.join(DATA_PATH, \"dataset/oficialDatasetEAIM2026.csv\")\n",
    "# df = pd.read_csv(path_dataset)\n",
    "\n",
    "# display(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5d4920",
   "metadata": {
    "papermill": {
     "duration": 0.003922,
     "end_time": "2025-09-27T04:29:03.997641",
     "exception": false,
     "start_time": "2025-09-27T04:29:03.993719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Extracci√≥n de embbedings con Lyrics Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b21a506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T04:29:04.006144Z",
     "iopub.status.busy": "2025-09-27T04:29:04.005804Z",
     "iopub.status.idle": "2025-09-27T04:29:30.956567Z",
     "shell.execute_reply": "2025-09-27T04:29:30.955767Z"
    },
    "papermill": {
     "duration": 26.956497,
     "end_time": "2025-09-27T04:29:30.957825",
     "exception": false,
     "start_time": "2025-09-27T04:29:04.001328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 03:39:34.027935: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-29 03:39:34.097015: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-29 03:39:36.607936: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor([[ 8.5982e-02, -1.9938e-01,  3.1403e-01, -2.1909e-01, -3.9347e-04,\n",
      "          2.8418e-01,  1.1639e+00, -2.5347e-01, -5.9031e-01,  2.8978e-01,\n",
      "         -4.3361e-01,  2.6265e-01,  1.2186e-02, -2.1822e-01,  1.0540e-01,\n",
      "          1.8403e-01,  5.8598e-01, -1.6739e-01, -2.6611e-01,  3.8995e-01,\n",
      "         -1.9267e-01,  3.5165e-01,  5.3251e-01,  4.8379e-01,  5.9596e-01,\n",
      "         -1.0685e+00,  3.0516e-01, -4.1388e-01,  3.8347e-01, -3.3050e-01,\n",
      "         -3.3174e-01,  4.0777e-01,  1.8344e-01, -2.9582e-01, -1.4029e-01,\n",
      "         -4.2408e-01,  2.6159e-01, -2.7347e-01, -7.7970e-02, -3.2933e-01,\n",
      "          3.9029e-01,  8.3547e-02,  1.7815e-01, -4.2287e-01,  2.5291e-02,\n",
      "          5.4537e-01, -3.9749e-01,  1.0715e-01, -1.1429e+00,  4.7099e-01,\n",
      "          2.0548e-01,  1.8556e-01, -3.1428e-01,  6.5785e-01,  2.4795e-01,\n",
      "          5.3135e-01, -1.9034e-01,  3.3224e-01,  4.9253e-01, -1.7496e-02,\n",
      "          4.9282e-01,  5.9564e-01,  2.2089e-01,  1.9639e-01, -1.7313e-01,\n",
      "         -2.4567e-01, -2.8542e-01, -1.4741e-01, -1.7384e-02,  3.5515e-01,\n",
      "         -8.3135e-02, -6.3132e-02,  2.9226e-01,  6.3939e-01, -2.9777e-01,\n",
      "         -9.7521e-02, -6.0739e-02,  2.5673e-01, -3.0768e-01, -2.2224e-01,\n",
      "         -2.5321e-01,  3.4734e-01,  1.0203e+00,  1.9196e-01, -9.1606e-02,\n",
      "         -4.5693e-01,  3.3026e-01, -2.2074e-01, -6.0526e-02,  3.9479e-01,\n",
      "         -2.7525e-01,  6.0574e-01, -9.6178e-03, -2.3490e-01,  3.3226e-01,\n",
      "         -2.9552e-01, -1.8357e-01,  3.3977e-01, -3.9793e-01,  2.8528e-01,\n",
      "         -9.7769e-02,  1.8406e-01,  2.3932e-01, -2.5663e-01,  1.2102e-01,\n",
      "         -1.1334e-01, -4.7447e-01,  5.8402e-01, -5.0222e-01,  5.5676e-02,\n",
      "         -2.3052e-01, -1.7998e-01, -2.4742e-01,  4.9252e-01, -3.1728e-01,\n",
      "         -3.8005e-01, -3.6834e-02,  6.1935e-01,  4.5879e-01, -1.1285e-02,\n",
      "         -8.1743e-01, -1.5657e-01, -2.2445e-01,  2.6995e-02,  6.0124e-01,\n",
      "          4.5618e-01,  5.0829e-01, -4.4134e-02, -6.2951e-01,  5.7058e-02,\n",
      "         -3.2004e-01,  8.4487e-01, -4.1202e-01, -9.3632e-02, -3.3231e-01,\n",
      "          8.0646e-02, -9.0269e-02, -7.9957e-02,  1.5627e-01, -2.4454e-01,\n",
      "         -7.8198e-02, -7.3007e-01,  3.4790e-01, -7.0381e-02,  2.4043e-01,\n",
      "         -3.9198e-01, -3.3993e-01, -2.2476e-01,  1.6756e-01,  6.1990e-01,\n",
      "          1.6506e-01, -1.7112e-02, -2.6266e-01, -4.4491e-01,  8.5385e-02,\n",
      "         -1.0234e+00,  1.4751e-02,  6.6274e-01,  7.9915e-02, -3.5930e-01,\n",
      "          1.5881e-01, -3.4966e-01, -2.2996e-03, -4.9989e-01, -2.3276e-01,\n",
      "          2.5108e-01, -1.6013e-01,  3.0339e-02,  1.2239e-01,  2.2526e-01,\n",
      "         -5.7139e-01,  1.5721e-01,  4.0606e-01,  5.2835e-01,  4.1934e-01,\n",
      "         -9.2094e-02, -1.8845e-01,  4.0464e-01, -3.8726e-01,  2.7759e-02,\n",
      "          3.4773e-01, -7.7780e-02,  3.8785e-01,  8.1375e-02,  1.4306e-01,\n",
      "          3.7101e-01, -3.4955e-01,  9.9817e-02, -4.5038e-02,  4.2395e-02,\n",
      "         -5.4337e-01, -5.7880e-03,  3.9858e-02, -2.4195e-01, -1.8601e-01,\n",
      "         -6.1422e-02, -2.0549e-01,  9.6923e-01, -4.1731e-01,  2.2264e-01,\n",
      "          9.8880e-02, -5.0703e-01,  4.4393e-02, -8.6652e-01,  2.2599e-01,\n",
      "          1.9653e-01, -7.9508e-02,  1.0202e-01, -3.3823e-01, -6.1861e-01,\n",
      "          2.5656e-01, -6.1935e-01,  1.5115e-01,  7.1349e-02, -1.6747e-02,\n",
      "          2.9823e-01, -4.8073e-01,  3.7778e-01, -1.0376e+00, -6.7299e-01,\n",
      "         -1.9395e-01, -4.9116e-01, -1.5958e-01, -1.7036e-01, -3.5001e-03,\n",
      "         -1.2437e-01, -2.7077e-01,  9.6722e-02,  1.9883e-01,  1.9982e-01,\n",
      "         -2.2514e-01,  1.4580e-01,  7.6273e-02,  2.7559e-01, -3.4379e-01,\n",
      "          3.3641e-01, -3.7492e-01,  6.8472e-02,  7.1614e-01, -8.7140e-01,\n",
      "         -1.3807e-01,  3.7063e-01, -7.8194e-02,  3.6888e-01, -2.4814e-01,\n",
      "          2.4197e-01,  2.3086e-01,  6.3857e-01,  2.2130e-02, -4.1341e-01,\n",
      "         -2.2392e-01, -4.3592e-01,  7.5251e-02, -2.5254e-01, -2.5270e-01,\n",
      "          5.2911e-01, -4.9042e-01,  9.3977e-02,  7.0798e-01,  1.1315e-01,\n",
      "         -1.2777e-01, -5.3069e-01,  2.5840e-01, -5.4220e-01,  3.6502e-01,\n",
      "         -1.7641e-01, -5.5217e-01, -3.2474e-01,  2.6816e-01,  6.9207e-02,\n",
      "         -1.3044e-01, -1.6432e-01, -7.7984e-02, -4.3597e-01,  5.2860e-01,\n",
      "         -4.4020e-01,  6.3608e-01, -1.8013e-01,  1.0452e-01,  9.2711e-02,\n",
      "          2.8882e-01, -3.7297e-01,  1.1613e-01,  1.4239e-01,  1.2496e-01,\n",
      "          4.6682e-01, -2.6213e-01, -1.8380e-01,  3.8388e-02,  2.7842e-01,\n",
      "          3.8825e-01,  9.9131e-02,  3.6541e-01,  2.2169e-01, -6.4273e-02,\n",
      "         -1.4846e-01, -3.8930e-01,  4.1290e-02, -1.4719e-01,  1.7134e-01],\n",
      "        [-1.8556e-01, -2.3414e-01,  6.9767e-01, -5.9317e-01,  4.1742e-02,\n",
      "          6.4290e-01,  7.2030e-01, -2.7653e-02, -2.8739e-01,  1.5241e-01,\n",
      "         -4.2843e-01,  9.2531e-02,  4.7558e-02, -3.9465e-01,  5.5514e-02,\n",
      "         -5.1168e-02,  4.3439e-01,  2.2112e-01,  3.6437e-02,  8.2028e-03,\n",
      "         -2.4122e-01,  8.6939e-02, -1.5564e-01,  5.8968e-01,  8.5847e-02,\n",
      "         -7.4826e-01, -1.6808e-01, -5.1483e-01,  2.0089e-01, -8.9687e-01,\n",
      "         -8.7315e-02, -3.0170e-02,  4.4630e-01, -9.3042e-02,  1.7543e-01,\n",
      "          6.9105e-02, -2.1615e-01,  2.7740e-01, -2.8637e-01, -5.3099e-01,\n",
      "         -3.4070e-01,  2.9618e-02,  1.1835e-02, -4.3073e-01, -3.9812e-01,\n",
      "          2.5667e-01,  5.8097e-02,  6.5925e-02, -5.1549e-01,  3.6404e-01,\n",
      "         -5.0626e-02,  5.8025e-01, -2.9077e-02,  1.2362e-01,  3.2774e-01,\n",
      "          4.7578e-01, -2.3779e-01,  1.0257e-01,  3.0880e-01, -7.6611e-02,\n",
      "          6.5769e-02,  3.0900e-01,  6.0790e-01,  2.2069e-01,  1.1033e-01,\n",
      "         -3.1599e-01, -1.4517e-01, -1.7289e-01, -5.1651e-01, -2.2595e-01,\n",
      "          3.5468e-01, -3.5386e-01,  2.2534e-01,  3.2495e-01, -4.6008e-01,\n",
      "         -1.3179e-01, -4.1913e-01,  3.8582e-01, -3.3780e-01,  4.6620e-02,\n",
      "         -2.2575e-01, -3.1442e-02,  8.1278e-01,  3.8404e-01,  3.6150e-01,\n",
      "         -1.9062e-01,  8.8800e-02, -1.4353e-01,  2.9068e-01, -2.3676e-03,\n",
      "         -2.8496e-01,  3.9493e-01,  1.0401e-01,  2.2580e-01,  1.0948e-02,\n",
      "         -2.6930e-01, -2.8779e-01,  1.9514e-02, -2.8700e-01, -4.0525e-01,\n",
      "          2.3328e-02,  9.6536e-02, -2.7494e-01,  1.4102e-01,  6.4354e-03,\n",
      "         -2.3677e-02, -4.6965e-01,  4.8585e-01, -1.6564e-01, -8.3012e-02,\n",
      "         -4.7720e-01, -1.9912e-01, -5.9300e-01,  1.9116e-01,  8.5618e-03,\n",
      "          9.5205e-02, -2.0540e-01,  2.7894e-01,  1.4698e-01, -9.5836e-02,\n",
      "         -4.2669e-01, -2.4652e-01, -5.3677e-02, -3.1679e-01,  5.4954e-01,\n",
      "          4.1546e-01,  1.4417e-01, -2.5318e-01, -1.1009e-01, -6.7664e-02,\n",
      "          4.5918e-02,  4.1470e-01, -1.2796e-01,  1.5244e-01, -1.6676e-01,\n",
      "         -9.3694e-02,  2.7664e-01, -5.0372e-02,  2.8108e-01, -1.3317e-02,\n",
      "          4.6536e-02, -9.3524e-01,  7.5057e-01, -9.7263e-02,  1.2152e-01,\n",
      "         -1.9416e-01,  3.8080e-02, -2.7422e-01,  4.6427e-01,  7.7246e-01,\n",
      "          1.7658e-01,  2.1876e-01, -3.8301e-02, -1.0317e+00,  1.7213e-01,\n",
      "         -3.1510e-01, -2.4518e-02,  9.2497e-01,  7.5101e-02, -3.5740e-02,\n",
      "          1.4949e-01, -1.5926e-01,  5.6959e-02, -3.8221e-01, -3.0832e-01,\n",
      "          1.2010e-01,  8.1529e-02, -4.9017e-01,  1.6145e-01, -4.7504e-02,\n",
      "         -1.3135e-01,  3.6154e-01,  3.8953e-01,  2.8907e-01,  3.0981e-01,\n",
      "          5.6216e-01,  3.0213e-01,  7.4494e-02, -1.9247e-01,  4.7317e-02,\n",
      "         -2.0462e-01,  1.1924e-01,  4.0228e-01, -4.4200e-01, -2.8812e-01,\n",
      "          3.7329e-01, -1.8125e-01,  4.3832e-01,  6.0617e-02,  7.1732e-02,\n",
      "         -3.1154e-01, -7.6242e-01, -1.6706e-01, -1.8356e-01,  2.2914e-01,\n",
      "         -5.5761e-02, -2.0641e-01,  6.7781e-01, -3.2683e-01, -3.0420e-01,\n",
      "          5.0281e-02, -4.1753e-01,  1.5901e-01, -6.1253e-01,  1.8099e-01,\n",
      "          3.7711e-01, -6.6657e-02, -1.9884e-01, -5.9226e-01, -7.5436e-02,\n",
      "          5.8123e-01, -5.4815e-01,  7.6522e-01, -2.5977e-01,  2.3490e-01,\n",
      "          2.5078e-01, -1.1813e-01,  2.2829e-01, -8.1220e-01, -7.3786e-01,\n",
      "         -2.7705e-01, -1.0638e-01, -2.7551e-01, -2.1730e-01,  6.9398e-02,\n",
      "          2.9412e-01,  2.9370e-01,  2.1655e-01,  4.2540e-01, -6.4420e-03,\n",
      "         -2.5713e-01, -5.3957e-02,  1.5795e-01, -5.6318e-01, -8.5588e-02,\n",
      "          7.2471e-01,  4.0842e-01, -2.0714e-01,  4.5418e-01, -3.4961e-01,\n",
      "          2.9817e-01,  3.5192e-01,  1.1789e-03,  4.2360e-01,  1.2787e-01,\n",
      "          1.0356e-01, -5.1026e-01,  6.7822e-01,  3.0800e-01, -4.8840e-01,\n",
      "         -3.6930e-01, -6.3272e-01, -3.1579e-03, -1.9691e-01, -3.7475e-01,\n",
      "          2.9680e-01, -8.5276e-01, -1.3666e-01,  4.4409e-01, -4.2051e-02,\n",
      "          3.3312e-02, -3.6514e-02, -1.7959e-01, -3.1418e-01,  3.7233e-01,\n",
      "         -4.4793e-03, -1.2049e-01, -2.3109e-01,  7.4618e-02, -6.2734e-01,\n",
      "          3.8622e-01,  6.3858e-02, -2.8429e-02, -4.7838e-01,  8.1511e-01,\n",
      "         -4.1438e-01,  2.2357e-01,  2.1366e-01, -1.7401e-01, -1.7124e-01,\n",
      "          3.2326e-01,  5.0160e-02, -2.6319e-01, -3.1630e-03, -5.4008e-01,\n",
      "          2.1738e-01,  2.8207e-01, -3.4864e-01,  6.6520e-02,  3.9670e-01,\n",
      "          8.0074e-01, -3.7280e-02,  3.5032e-01,  4.8999e-01,  1.9968e-01,\n",
      "          1.8769e-01, -4.4721e-01,  9.2358e-02,  6.3430e-02,  3.7120e-01]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "MODEL_NAME = 'brunokreiner/lyrics-bert'\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling. In this case, mean pooling.\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93fb50f",
   "metadata": {
    "papermill": {
     "duration": 0.003999,
     "end_time": "2025-09-27T04:29:30.967054",
     "exception": false,
     "start_time": "2025-09-27T04:29:30.963055",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Clase que se encarga de la limpieza de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c006931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T04:29:30.975365Z",
     "iopub.status.busy": "2025-09-27T04:29:30.974684Z",
     "iopub.status.idle": "2025-09-27T04:29:31.843627Z",
     "shell.execute_reply": "2025-09-27T04:29:31.842219Z"
    },
    "papermill": {
     "duration": 0.876153,
     "end_time": "2025-09-27T04:29:31.846460",
     "exception": false,
     "start_time": "2025-09-27T04:29:30.970307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Text preprocessing class for lyrics data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, language: str = 'spanish'):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor\n",
    "\n",
    "        Args:\n",
    "            language: Language for stopwords (default: spanish)\n",
    "        \"\"\"\n",
    "        self.language = language\n",
    "        self.stemmer = SnowballStemmer(language)\n",
    "        self.stop_words = set(stopwords.words(language))\n",
    "        import re\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize text\n",
    "\n",
    "        Args:\n",
    "            text: Input text to clean\n",
    "\n",
    "        Returns:\n",
    "            Cleaned text\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return ''\n",
    "\n",
    "        # Convert to lowercase\n",
    "        text = str(text).lower()\n",
    "\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "        # Remove special characters but keep spaces\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "\n",
    "        return text\n",
    "    \n",
    "\n",
    "\n",
    "    def tokenize_and_process(self, text, remove_stopwords = True,\n",
    "                           apply_stemming = True):\n",
    "        \"\"\"\n",
    "        Tokenize and process text\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "            remove_stopwords: Whether to remove stopwords\n",
    "            apply_stemming: Whether to apply stemming\n",
    "\n",
    "        Returns:\n",
    "            List of processed tokens\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text, language=self.language)\n",
    "\n",
    "        # Remove stopwords if requested\n",
    "        if remove_stopwords:\n",
    "            tokens = [token for token in tokens if token not in self.stop_words]\n",
    "\n",
    "        # Apply stemming if requested\n",
    "        if apply_stemming:\n",
    "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "\n",
    "        # Remove very short tokens\n",
    "        tokens = [token for token in tokens if len(token) > 2]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def preprocess(self, text: str, remove_stopwords: bool = True,\n",
    "                   apply_stemming: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Complete preprocessing pipeline\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "            remove_stopwords: Whether to remove stopwords\n",
    "            apply_stemming: Whether to apply stemming\n",
    "\n",
    "        Returns:\n",
    "            Preprocessed text as string\n",
    "        \"\"\"\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        tokens = self.tokenize_and_process(cleaned_text, remove_stopwords, apply_stemming)\n",
    "        return ' '.join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ea1717d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T04:29:31.862286Z",
     "iopub.status.busy": "2025-09-27T04:29:31.861811Z",
     "iopub.status.idle": "2025-09-27T05:33:16.141271Z",
     "shell.execute_reply": "2025-09-27T05:33:16.140833Z"
    },
    "papermill": {
     "duration": 3824.287356,
     "end_time": "2025-09-27T05:33:16.142105",
     "exception": false,
     "start_time": "2025-09-27T04:29:31.854749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embbedings generated with this cols: \n",
      "['lyrics']\n",
      "\n",
      "Procesando filas 0 a 499...\n",
      "Preprocessing text...\n",
      "shape de sentences:  500\n",
      "\n",
      "Tiempo total: 7.87 segundos\n",
      "Iniciando guardado de los vectores: \n",
      "Sentence embeddings:\n",
      "torch.Size([500, 300])\n",
      "Guardado batch 0-499\n",
      "\n",
      "Procesando filas 500 a 999...\n",
      "Preprocessing text...\n",
      "shape de sentences:  500\n",
      "\n",
      "Tiempo total: 6.95 segundos\n",
      "Iniciando guardado de los vectores: \n",
      "Sentence embeddings:\n",
      "torch.Size([500, 300])\n",
      "Guardado batch 500-999\n",
      "\n",
      "Procesando filas 1000 a 1499...\n",
      "Preprocessing text...\n",
      "shape de sentences:  500\n",
      "\n",
      "Tiempo total: 6.84 segundos\n",
      "Iniciando guardado de los vectores: \n",
      "Sentence embeddings:\n",
      "torch.Size([500, 300])\n",
      "Guardado batch 1000-1499\n",
      "\n",
      "Procesando filas 1500 a 1999...\n",
      "Preprocessing text...\n",
      "shape de sentences:  500\n",
      "\n",
      "Tiempo total: 7.03 segundos\n",
      "Iniciando guardado de los vectores: \n",
      "Sentence embeddings:\n",
      "torch.Size([500, 300])\n",
      "Guardado batch 1500-1999\n",
      "\n",
      "Procesando filas 2000 a 2499...\n",
      "Preprocessing text...\n",
      "shape de sentences:  500\n",
      "\n",
      "Tiempo total: 7.09 segundos\n",
      "Iniciando guardado de los vectores: \n",
      "Sentence embeddings:\n",
      "torch.Size([500, 300])\n",
      "Guardado batch 2000-2499\n",
      "\n",
      "Procesando filas 2500 a 2999...\n",
      "Preprocessing text...\n",
      "shape de sentences:  500\n",
      "\n",
      "Tiempo total: 7.10 segundos\n",
      "Iniciando guardado de los vectores: \n",
      "Sentence embeddings:\n",
      "torch.Size([500, 300])\n",
      "Guardado batch 2500-2999\n",
      "\n",
      "Procesando filas 3000 a 3499...\n",
      "Preprocessing text...\n",
      "shape de sentences:  500\n",
      "\n",
      "Tiempo total: 7.22 segundos\n",
      "Iniciando guardado de los vectores: \n",
      "Sentence embeddings:\n",
      "torch.Size([500, 300])\n",
      "Guardado batch 3000-3499\n",
      "\n",
      "Procesando filas 3500 a 3999...\n",
      "Preprocessing text...\n",
      "shape de sentences:  500\n",
      "\n",
      "Tiempo total: 7.02 segundos\n",
      "Iniciando guardado de los vectores: \n",
      "Sentence embeddings:\n",
      "torch.Size([500, 300])\n",
      "Guardado batch 3500-3999\n",
      "\n",
      "Procesando filas 4000 a 4499...\n",
      "Preprocessing text...\n",
      "shape de sentences:  500\n",
      "\n",
      "Tiempo total: 7.23 segundos\n",
      "Iniciando guardado de los vectores: \n",
      "Sentence embeddings:\n",
      "torch.Size([500, 300])\n",
      "Guardado batch 4000-4499\n",
      "\n",
      "Procesando filas 4500 a 4999...\n",
      "Preprocessing text...\n",
      "shape de sentences:  500\n",
      "\n",
      "Tiempo total: 7.03 segundos\n",
      "Iniciando guardado de los vectores: \n",
      "Sentence embeddings:\n",
      "torch.Size([500, 300])\n",
      "Guardado batch 4500-4999\n",
      "\n",
      "Procesando filas 5000 a 5499...\n",
      "Preprocessing text...\n",
      "shape de sentences:  500\n",
      "\n",
      "Tiempo total: 7.27 segundos\n",
      "Iniciando guardado de los vectores: \n",
      "Sentence embeddings:\n",
      "torch.Size([500, 300])\n",
      "Guardado batch 5000-5499\n",
      "\n",
      "Procesando filas 5500 a 5999...\n",
      "Preprocessing text...\n",
      "shape de sentences:  500\n",
      "\n",
      "Tiempo total: 6.80 segundos\n",
      "Iniciando guardado de los vectores: \n",
      "Sentence embeddings:\n",
      "torch.Size([500, 300])\n",
      "Guardado batch 5500-5999\n",
      "\n",
      "Procesando filas 6000 a 6499...\n",
      "Preprocessing text...\n",
      "shape de sentences:  500\n",
      "\n",
      "Tiempo total: 6.92 segundos\n",
      "Iniciando guardado de los vectores: \n",
      "Sentence embeddings:\n",
      "torch.Size([500, 300])\n",
      "Guardado batch 6000-6499\n",
      "\n",
      "Procesando filas 6500 a 6999...\n",
      "Preprocessing text...\n",
      "shape de sentences:  500\n",
      "\n",
      "Tiempo total: 7.02 segundos\n",
      "Iniciando guardado de los vectores: \n",
      "Sentence embeddings:\n",
      "torch.Size([500, 300])\n",
      "Guardado batch 6500-6999\n",
      "\n",
      "Procesando filas 7000 a 7318...\n",
      "Preprocessing text...\n",
      "shape de sentences:  319\n",
      "\n",
      "Tiempo total: 4.89 segundos\n",
      "Iniciando guardado de los vectores: \n",
      "Sentence embeddings:\n",
      "torch.Size([319, 300])\n",
      "Guardado batch 7000-7318\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 500 # Porque nos quedamos sin RAM :(\n",
    "INIT = 0\n",
    "TOTAL_ROWS = 7319\n",
    "# TOTAL_ROWS = 1000\n",
    "\n",
    "# To save vectors\n",
    "# save_dir = \"/content/drive/MyDrive/embeddings_lyricsbert_to_fusion/\"\n",
    "save_dir = os.path.join(DATA_PATH, \"LB_T\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "# start = 0\n",
    "# end = 5000\n",
    "# ROWS = end - start\n",
    "A = ['text', 'song', 'Artist(s)', 'Album', 'Similar Artist 1', 'Genre']\n",
    "B = ['Artist(s)', 'song', 'emotion', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']\n",
    "C = ['text', 'Artist(s)', 'song', 'emotion', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']\n",
    "\n",
    "D = ['emotion', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Release Date', 'Key', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']\n",
    "\n",
    "T = ['lyrics']\n",
    "COL_LB = T\n",
    "\n",
    "print(\"Embbedings generated with this cols: \")\n",
    "print(COL_LB)\n",
    "# ------ Configurations--------\n",
    "steaming = True \n",
    "\n",
    "# Incluiremos limpieza de datos \n",
    "\n",
    "for start in range(INIT, TOTAL_ROWS, BATCH_SIZE):\n",
    "  end = min(start + BATCH_SIZE, TOTAL_ROWS)\n",
    "  print(f\"\\nProcesando filas {start} a {end-1}...\")\n",
    "  df = pd.read_csv(path_dataset, skiprows=range(1, start + 1), nrows=end - start)\n",
    "  \n",
    "  df['combined_text'] = df[COL_LB].fillna('').agg(' '.join, axis=1)\n",
    "  # Start preprocesing \n",
    "  preprocessor = TextPreprocessor()\n",
    "  if steaming: \n",
    "    print(\"Preprocessing text...\")\n",
    "    # tqdm.pandas(desc=\"Text preprocessing\")\n",
    "    # df['processed_text'] = df['combined_text'].progress_apply(\n",
    "    #     lambda x: preprocessor.preprocess(x, remove_stopwords=True, apply_stemming=steaming)\n",
    "    # )\n",
    "    df['processed_text'] = df['combined_text'].apply(\n",
    "        lambda x: preprocessor.preprocess(x, remove_stopwords=True, apply_stemming=steaming)\n",
    "    )\n",
    "  else:\n",
    "    df['processed_text'] = df['combined_text']\n",
    "\n",
    "  # To se how is working the cleaning of the data\n",
    "  # if start == 1: \n",
    "  #   print(\"Data witouth cleaning\") \n",
    "  #   print(df['combined_text'])\n",
    "  #   print(\"\\nData cleaned\")\n",
    "  #   print(df['processed_text'])\n",
    "\n",
    "\n",
    "  df_sentences = df['processed_text'].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "  print(\"shape de sentences: \", len(df_sentences))\n",
    "\n",
    "  # Tokenize input\n",
    "  start_time = time.time()\n",
    "\n",
    "  encoded_df_input = tokenizer(df_sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "  # Compute embeddings\n",
    "  with torch.no_grad():\n",
    "    model_output = model(**encoded_df_input)\n",
    "\n",
    "  # Pooling (mean)\n",
    "  sentence_df_embeddings = mean_pooling(model_output, encoded_df_input['attention_mask'])\n",
    "\n",
    "\n",
    "\n",
    "  # Print results\n",
    "  end_time = time.time()\n",
    "  print(f\"\\nTiempo total: {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "  print(\"Iniciando guardado de los vectores: \")\n",
    "  embeddings_np = sentence_df_embeddings.numpy()\n",
    "\n",
    "  # Guardar en formato binario .npy\n",
    "  npy_filename = f\"embeddings_lyricsbert_{start}_{end-1}.npy\"\n",
    "\n",
    "  np.save(os.path.join(save_dir, npy_filename), embeddings_np)\n",
    "\n",
    "  # # Guardar en CSV\n",
    "  # csv_filename = f\"embeddings_lyricsbert_{start}_{end-1}.csv\"\n",
    "  # np.savetxt(os.path.join(save_dir, csv_filename), embeddings_np, delimiter=\",\")\n",
    "  print(\"Sentence embeddings:\")\n",
    "  # print(sentence_df_embeddings)\n",
    "  print(sentence_df_embeddings.shape)\n",
    "  print(f\"Guardado batch {start}-{end-1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8484f9f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T05:33:16.183332Z",
     "iopub.status.busy": "2025-09-27T05:33:16.183171Z",
     "iopub.status.idle": "2025-09-27T05:33:17.802302Z",
     "shell.execute_reply": "2025-09-27T05:33:17.800823Z"
    },
    "papermill": {
     "duration": 1.642326,
     "end_time": "2025-09-27T05:33:17.804973",
     "exception": false,
     "start_time": "2025-09-27T05:33:16.162647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le√≠do: embeddings_lyricsbert_0_499.npy - Shape: (500, 300)\n",
      "Total: 1\n",
      "Le√≠do: embeddings_lyricsbert_500_999.npy - Shape: (500, 300)\n",
      "Total: 2\n",
      "Le√≠do: embeddings_lyricsbert_1000_1499.npy - Shape: (500, 300)\n",
      "Total: 3\n",
      "Le√≠do: embeddings_lyricsbert_1500_1999.npy - Shape: (500, 300)\n",
      "Total: 4\n",
      "Le√≠do: embeddings_lyricsbert_2000_2499.npy - Shape: (500, 300)\n",
      "Total: 5\n",
      "Le√≠do: embeddings_lyricsbert_2500_2999.npy - Shape: (500, 300)\n",
      "Total: 6\n",
      "Le√≠do: embeddings_lyricsbert_3000_3499.npy - Shape: (500, 300)\n",
      "Total: 7\n",
      "Le√≠do: embeddings_lyricsbert_3500_3999.npy - Shape: (500, 300)\n",
      "Total: 8\n",
      "Le√≠do: embeddings_lyricsbert_4000_4499.npy - Shape: (500, 300)\n",
      "Total: 9\n",
      "Le√≠do: embeddings_lyricsbert_4500_4999.npy - Shape: (500, 300)\n",
      "Total: 10\n",
      "Le√≠do: embeddings_lyricsbert_5000_5499.npy - Shape: (500, 300)\n",
      "Total: 11\n",
      "Le√≠do: embeddings_lyricsbert_5500_5999.npy - Shape: (500, 300)\n",
      "Total: 12\n",
      "Le√≠do: embeddings_lyricsbert_6000_6499.npy - Shape: (500, 300)\n",
      "Total: 13\n",
      "Le√≠do: embeddings_lyricsbert_6500_6999.npy - Shape: (500, 300)\n",
      "Total: 14\n",
      "Le√≠do: embeddings_lyricsbert_7000_7318.npy - Shape: (319, 300)\n",
      "Total: 15\n",
      "Shape final: (7319, 300)\n",
      "Embeddings guardados en: ../../../../../../data/spanish/LB_T/LB_fuss/lb_khipu_T.npy\n"
     ]
    }
   ],
   "source": [
    "## Make fusion of the embeddings of example\n",
    "save_fussion = os.path.join(save_dir, \"LB_fuss\")\n",
    "os.makedirs(save_fussion, exist_ok=True)\n",
    "\n",
    "save_dir_df_npy = os.path.join(save_fussion, \"lb_khipu_T.npy\")\n",
    "# print(os.listdir(save_dir))\n",
    "\n",
    "embbedings_df_npy =os.listdir(save_dir)\n",
    "\n",
    "def get_start_number(filename):\n",
    "    match = re.search(r'embeddings_lyricsbert_(\\d+)_\\d+\\.npy', filename)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "embbedings_df_npy = sorted(embbedings_df_npy, key=get_start_number)\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "for embb in embbedings_df_npy:\n",
    "  if embb.endswith('.npy'):\n",
    "    # print(embb)\n",
    "    file_path = os.path.join(save_dir, embb)\n",
    "    embeddings = np.load(file_path)\n",
    "    all_embeddings.append(embeddings)\n",
    "    print(f\"Le√≠do: {embb} - Shape: {embeddings.shape}\")\n",
    "    print(f\"Total: {len(all_embeddings)}\")\n",
    "\n",
    "final_embeddings = np.vstack(all_embeddings)\n",
    "print(\"Shape final:\", final_embeddings.shape)\n",
    "\n",
    "# Guardar en .npy\n",
    "np.save(save_dir_df_npy, final_embeddings)\n",
    "print(f\"Embeddings guardados en: {save_dir_df_npy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c43fee93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T05:33:17.870254Z",
     "iopub.status.busy": "2025-09-27T05:33:17.870093Z",
     "iopub.status.idle": "2025-09-27T05:33:18.083978Z",
     "shell.execute_reply": "2025-09-27T05:33:18.082458Z"
    },
    "papermill": {
     "duration": 0.239851,
     "end_time": "2025-09-27T05:33:18.086802",
     "exception": false,
     "start_time": "2025-09-27T05:33:17.846951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../../../../data/spanish/LB_T\n",
      "../../../../../../data/spanish/LB_T/embeddings_lyricsbert_1000_1499.npy\n",
      "../../../../../../data/spanish/LB_T/embeddings_lyricsbert_500_999.npy\n",
      "../../../../../../data/spanish/LB_T/embeddings_lyricsbert_6500_6999.npy\n",
      "../../../../../../data/spanish/LB_T/embeddings_lyricsbert_4500_4999.npy\n",
      "../../../../../../data/spanish/LB_T/embeddings_lyricsbert_2500_2999.npy\n",
      "../../../../../../data/spanish/LB_T/embeddings_lyricsbert_0_499.npy\n",
      "../../../../../../data/spanish/LB_T/embeddings_lyricsbert_7000_7318.npy\n",
      "../../../../../../data/spanish/LB_T/embeddings_lyricsbert_5500_5999.npy\n",
      "../../../../../../data/spanish/LB_T/embeddings_lyricsbert_3500_3999.npy\n",
      "../../../../../../data/spanish/LB_T/embeddings_lyricsbert_1500_1999.npy\n",
      "../../../../../../data/spanish/LB_T/embeddings_lyricsbert_2000_2499.npy\n",
      "../../../../../../data/spanish/LB_T/embeddings_lyricsbert_4000_4499.npy\n",
      "../../../../../../data/spanish/LB_T/embeddings_lyricsbert_3000_3499.npy\n",
      "../../../../../../data/spanish/LB_T/embeddings_lyricsbert_6000_6499.npy\n",
      "../../../../../../data/spanish/LB_T/embeddings_lyricsbert_5000_5499.npy\n"
     ]
    }
   ],
   "source": [
    "#Delete the butches files\n",
    "\n",
    "dir_clean = save_dir\n",
    "print(dir_clean)\n",
    "\n",
    "butches_embb = [\n",
    "    f for f in os.listdir(dir_clean)\n",
    "    if os.path.isfile(os.path.join(dir_clean, f))\n",
    "]\n",
    "\n",
    "for butch in butches_embb:\n",
    "    # if butch != \"lb_khipu_A.npy\" and butch != \"lb_khipu.npy\" and butch !=\"lb_khipu_B.npy\" and butch !=\"lb_khipu_C.npy\" and butch !=\"lb_khipu_D.npy\"  :\n",
    "        # Delete \n",
    "    file_path = os.path.join(dir_clean, butch)\n",
    "    print(file_path)\n",
    "    os.remove(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3859.585608,
   "end_time": "2025-09-27T05:33:19.648074",
   "environment_variables": {},
   "exception": null,
   "input_path": "estracting_LB_emBB.ipynb",
   "output_path": "estracting_LB_emBB_X_T_NP20250926_232858.ipynb",
   "parameters": {},
   "start_time": "2025-09-27T04:29:00.062466",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
