2025-10-29 20:19:11.831453: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-29 20:19:11.831466: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
For TF-IDF embbedings you are selecteing this columns:
--> ['lyrics']
Your are not adding cols numercis
You are executing with [ALL] dataset
--> PaTH:  ../../../../../data/spanish/LB_T/LB_fuss/lb_khipu_T.npy

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../../../../data/spanish/dataset/oficialDatasetEAIM2026.csv
Label distribution: {False: 6765, True: 554}
X shape: (7319, 300)
y shape: (7319,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 5412, 1: 443}
Label distribution en TEST: {0: 1353, 1: 111}




Aplicando SMOTE oversampling...
Nueva distribución de clases: {0: 5412, 1: 5412}
Resultados con MLP

Entrenando red 1 con capas [300, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6610, Test Loss: 0.5827, F1: 0.2686, AUC: 0.7717
Epoch [10/30] Train Loss: 0.3979, Test Loss: 0.4450, F1: 0.3172, AUC: 0.8224
Epoch [20/30] Train Loss: 0.3678, Test Loss: 0.4466, F1: 0.3172, AUC: 0.8242
Mejores resultados en la época:  26
f1-score 0.3311827956989247
AUC según el mejor F1-score 0.8243942390283854
Confusion Matrix:
 [[1076  277]
 [  34   77]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.7876
Precision:  0.2175
Recall:     0.6937
F1-score:   0.3312

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6675, Test Loss: 0.5904, F1: 0.2787, AUC: 0.7585
Epoch [10/30] Train Loss: 0.4101, Test Loss: 0.5284, F1: 0.3013, AUC: 0.8173
Epoch [20/30] Train Loss: 0.3794, Test Loss: 0.5492, F1: 0.3045, AUC: 0.8235
Mejores resultados en la época:  29
f1-score 0.33403805496828753
AUC según el mejor F1-score 0.8235219698634333
Confusion Matrix:
 [[1070  283]
 [  32   79]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.7848
Precision:  0.2182
Recall:     0.7117
F1-score:   0.3340

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6753, Test Loss: 0.5846, F1: 0.2949, AUC: 0.7724
Epoch [10/30] Train Loss: 0.4097, Test Loss: 0.3904, F1: 0.3056, AUC: 0.8186
Epoch [20/30] Train Loss: 0.3755, Test Loss: 0.3818, F1: 0.3211, AUC: 0.8238
Mejores resultados en la época:  29
f1-score 0.3333333333333333
AUC según el mejor F1-score 0.8228161642795788
Confusion Matrix:
 [[1174  179]
 [  53   58]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8415
Precision:  0.2447
Recall:     0.5225
F1-score:   0.3333

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6728, Test Loss: 0.6182, F1: 0.2683, AUC: 0.7536
Epoch [10/30] Train Loss: 0.4128, Test Loss: 0.4318, F1: 0.3002, AUC: 0.8163
Epoch [20/30] Train Loss: 0.3834, Test Loss: 0.6599, F1: 0.2920, AUC: 0.8226
Mejores resultados en la época:  24
f1-score 0.33249370277078083
AUC según el mejor F1-score 0.8228427984525546
Confusion Matrix:
 [[1133  220]
 [  45   66]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8190
Precision:  0.2308
Recall:     0.5946
F1-score:   0.3325
Tiempo total para red 1: 68.38 segundos

Entrenando red 2 con capas [300, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6847, Test Loss: 0.7985, F1: 0.1476, AUC: 0.7547
Epoch [10/30] Train Loss: 0.3907, Test Loss: 0.4348, F1: 0.3200, AUC: 0.8201
Epoch [20/30] Train Loss: 0.3663, Test Loss: 0.3278, F1: 0.3292, AUC: 0.8213
Mejores resultados en la época:  28
f1-score 0.3342776203966006
AUC según el mejor F1-score 0.8215110898037727
Confusion Matrix:
 [[1170  183]
 [  52   59]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8395
Precision:  0.2438
Recall:     0.5315
F1-score:   0.3343

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6604, Test Loss: 0.4639, F1: 0.2845, AUC: 0.7684
Epoch [10/30] Train Loss: 0.3814, Test Loss: 0.4392, F1: 0.3130, AUC: 0.8249
Epoch [20/30] Train Loss: 0.3703, Test Loss: 0.3843, F1: 0.3223, AUC: 0.8225
Mejores resultados en la época:  29
f1-score 0.3358208955223881
AUC según el mejor F1-score 0.8218440169659682
Confusion Matrix:
 [[1241  112]
 [  66   45]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8784
Precision:  0.2866
Recall:     0.4054
F1-score:   0.3358

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6766, Test Loss: 0.4284, F1: 0.0000, AUC: 0.7609
Epoch [10/30] Train Loss: 0.3789, Test Loss: 0.5003, F1: 0.3211, AUC: 0.8239
Epoch [20/30] Train Loss: 0.3722, Test Loss: 0.4336, F1: 0.3259, AUC: 0.8251
Mejores resultados en la época:  29
f1-score 0.3403141361256545
AUC según el mejor F1-score 0.8221037001524806
Confusion Matrix:
 [[1147  206]
 [  46   65]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8279
Precision:  0.2399
Recall:     0.5856
F1-score:   0.3403

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6762, Test Loss: 0.5470, F1: 0.2724, AUC: 0.7605
Epoch [10/30] Train Loss: 0.3952, Test Loss: 0.3350, F1: 0.3049, AUC: 0.8218
Epoch [20/30] Train Loss: 0.3657, Test Loss: 0.3756, F1: 0.3147, AUC: 0.8222
Mejores resultados en la época:  16
f1-score 0.32489451476793246
AUC según el mejor F1-score 0.8224099931417005
Confusion Matrix:
 [[1067  286]
 [  34   77]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.7814
Precision:  0.2121
Recall:     0.6937
F1-score:   0.3249
Tiempo total para red 2: 78.33 segundos

Entrenando red 3 con capas [300, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6648, Test Loss: 0.5925, F1: 0.2512, AUC: 0.7669
Epoch [10/30] Train Loss: 0.3976, Test Loss: 0.2974, F1: 0.3245, AUC: 0.8232
Epoch [20/30] Train Loss: 0.3482, Test Loss: 0.3485, F1: 0.3295, AUC: 0.8219
Mejores resultados en la época:  6
f1-score 0.3333333333333333
AUC según el mejor F1-score 0.8204390643415034
Confusion Matrix:
 [[1234  119]
 [  65   46]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8743
Precision:  0.2788
Recall:     0.4144
F1-score:   0.3333

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6196, Test Loss: 0.6528, F1: 0.2370, AUC: 0.7724
Epoch [10/30] Train Loss: 0.3965, Test Loss: 0.2634, F1: 0.3417, AUC: 0.8233
Epoch [20/30] Train Loss: 0.3494, Test Loss: 0.5812, F1: 0.3053, AUC: 0.8203
Mejores resultados en la época:  24
f1-score 0.3492063492063492
AUC según el mejor F1-score 0.8216642362983826
Confusion Matrix:
 [[1152  201]
 [  45   66]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8320
Precision:  0.2472
Recall:     0.5946
F1-score:   0.3492

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6890, Test Loss: 0.6378, F1: 0.2649, AUC: 0.7569
Epoch [10/30] Train Loss: 0.3851, Test Loss: 0.3616, F1: 0.3288, AUC: 0.8224
Epoch [20/30] Train Loss: 0.3466, Test Loss: 0.6218, F1: 0.3010, AUC: 0.8223
Mejores resultados en la época:  28
f1-score 0.33714285714285713
AUC según el mejor F1-score 0.8212114553577968
Confusion Matrix:
 [[1173  180]
 [  52   59]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8415
Precision:  0.2469
Recall:     0.5315
F1-score:   0.3371

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6655, Test Loss: 0.3712, F1: 0.2933, AUC: 0.7664
Epoch [10/30] Train Loss: 0.3935, Test Loss: 0.6097, F1: 0.2948, AUC: 0.8226
Epoch [20/30] Train Loss: 0.3493, Test Loss: 0.2982, F1: 0.3249, AUC: 0.8209
Mejores resultados en la época:  23
f1-score 0.3404255319148936
AUC según el mejor F1-score 0.8215776752362117
Confusion Matrix:
 [[1152  201]
 [  47   64]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8306
For TF-IDF embbedings you are selecteing this columns:
--> ['lyrics']
Your are not adding cols numercis
You are executing with [ALL] dataset
--> PaTH:  ../../../../../data/spanish/LB_T/LB_fuss/lb_khipu_T.npy

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../../../../data/spanish/dataset/oficialDatasetEAIM2026.csv
Label distribution: {False: 6765, True: 554}
X shape: (7319, 300)
y shape: (7319,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 5412, 1: 443}
Label distribution en TEST: {0: 1353, 1: 111}




Aplicando SMOTE oversampling...
Nueva distribución de clases: {0: 5412, 1: 5412}
Resultados con MLP

Entrenando red 1 con capas [300, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6720, Test Loss: 0.6590, F1: 0.2293, AUC: 0.7507
Epoch [10/30] Train Loss: 0.4174, Test Loss: 0.3458, F1: 0.3114, AUC: 0.8178
Epoch [20/30] Train Loss: 0.3732, Test Loss: 0.5166, F1: 0.3188, AUC: 0.8231
Mejores resultados en la época:  27
f1-score 0.3368421052631579
AUC según el mejor F1-score 0.8226497006984812
Confusion Matrix:
 [[1069  284]
 [  31   80]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.7848
Precision:  0.2198
Recall:     0.7207
F1-score:   0.3368

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6810, Test Loss: 0.6583, F1: 0.2534, AUC: 0.7641
Epoch [10/30] Train Loss: 0.4237, Test Loss: 0.4335, F1: 0.2954, AUC: 0.8174
Epoch [20/30] Train Loss: 0.3664, Test Loss: 0.5188, F1: 0.3069, AUC: 0.8231
Mejores resultados en la época:  17
f1-score 0.33773087071240104
AUC según el mejor F1-score 0.8219172609416512
Confusion Matrix:
 [[1149  204]
 [  47   64]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8286
Precision:  0.2388
Recall:     0.5766
F1-score:   0.3377

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6717, Test Loss: 0.6264, F1: 0.2717, AUC: 0.7721
Epoch [10/30] Train Loss: 0.4114, Test Loss: 0.4629, F1: 0.3026, AUC: 0.8180
Epoch [20/30] Train Loss: 0.3770, Test Loss: 0.3394, F1: 0.3110, AUC: 0.8235
Mejores resultados en la época:  22
f1-score 0.33047210300429186
AUC según el mejor F1-score 0.8241944827310681
Confusion Matrix:
 [[1075  278]
 [  34   77]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.7869
Precision:  0.2169
Recall:     0.6937
F1-score:   0.3305

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6588, Test Loss: 0.5804, F1: 0.2701, AUC: 0.7641
Epoch [10/30] Train Loss: 0.4114, Test Loss: 0.5155, F1: 0.2986, AUC: 0.8172
Epoch [20/30] Train Loss: 0.3840, Test Loss: 0.3328, F1: 0.3195, AUC: 0.8230
Mejores resultados en la época:  17
f1-score 0.32514177693761814
AUC según el mejor F1-score 0.8228294813660667
Confusion Matrix:
 [[1021  332]
 [  25   86]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.7561
Precision:  0.2057
Recall:     0.7748
F1-score:   0.3251
Tiempo total para red 1: 68.60 segundos

Entrenando red 2 con capas [300, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6687, Test Loss: 0.6724, F1: 0.2309, AUC: 0.7663
Epoch [10/30] Train Loss: 0.3868, Test Loss: 0.2809, F1: 0.3146, AUC: 0.8209
Epoch [20/30] Train Loss: 0.3697, Test Loss: 0.4586, F1: 0.3362, AUC: 0.8230
Mejores resultados en la época:  20
f1-score 0.33617021276595743
AUC según el mejor F1-score 0.8229693107741888
Confusion Matrix:
 [[1073  280]
 [  32   79]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.7869
Precision:  0.2201
Recall:     0.7117
F1-score:   0.3362

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6555, Test Loss: 0.4748, F1: 0.2887, AUC: 0.7712
Epoch [10/30] Train Loss: 0.3905, Test Loss: 0.2931, F1: 0.3251, AUC: 0.8226
Epoch [20/30] Train Loss: 0.3645, Test Loss: 0.5221, F1: 0.3199, AUC: 0.8223
Mejores resultados en la época:  21
f1-score 0.3356164383561644
AUC según el mejor F1-score 0.8221569684984319
Confusion Matrix:
 [[1221  132]
 [  62   49]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8675
Precision:  0.2707
Recall:     0.4414
F1-score:   0.3356

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6540, Test Loss: 0.5299, F1: 0.2700, AUC: 0.7661
Epoch [10/30] Train Loss: 0.3808, Test Loss: 0.3603, F1: 0.3193, AUC: 0.8223
Epoch [20/30] Train Loss: 0.3688, Test Loss: 0.4661, F1: 0.3214, AUC: 0.8212
Mejores resultados en la época:  23
f1-score 0.3431952662721893
AUC según el mejor F1-score 0.8198331369063075
Confusion Matrix:
 [[1184  169]
 [  53   58]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8484
Precision:  0.2555
Recall:     0.5225
F1-score:   0.3432

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6458, Test Loss: 0.5958, F1: 0.2504, AUC: 0.7659
Epoch [10/30] Train Loss: 0.3959, Test Loss: 0.2760, F1: 0.3370, AUC: 0.8248
Epoch [20/30] Train Loss: 0.3609, Test Loss: 0.4670, F1: 0.3299, AUC: 0.8234
Mejores resultados en la época:  10
f1-score 0.336996336996337
AUC según el mejor F1-score 0.8248137272527518
Confusion Matrix:
 [[1237  116]
 [  65   46]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8764
Precision:  0.2840
Recall:     0.4144
F1-score:   0.3370
Tiempo total para red 2: 78.25 segundos

Entrenando red 3 con capas [300, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6774, Test Loss: 0.8908, F1: 0.1720, AUC: 0.7648
Epoch [10/30] Train Loss: 0.3895, Test Loss: 0.5195, F1: 0.3040, AUC: 0.8229
Epoch [20/30] Train Loss: 0.3572, Test Loss: 0.4644, F1: 0.3230, AUC: 0.8241
Mejores resultados en la época:  14
f1-score 0.33876221498371334
AUC según el mejor F1-score 0.8242344339905315
Confusion Matrix:
 [[1209  144]
 [  59   52]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8613
Precision:  0.2653
Recall:     0.4685
F1-score:   0.3388

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6529, Test Loss: 0.3644, F1: 0.2927, AUC: 0.7700
Epoch [10/30] Train Loss: 0.3820, Test Loss: 0.5314, F1: 0.3158, AUC: 0.8233
Epoch [20/30] Train Loss: 0.3476, Test Loss: 0.5189, F1: 0.3247, AUC: 0.8230
Mejores resultados en la época:  27
f1-score 0.3435897435897436
AUC según el mejor F1-score 0.8211315528388698
Confusion Matrix:
 [[1141  212]
 [  44   67]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8251
Precision:  0.2401
Recall:     0.6036
F1-score:   0.3436

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6532, Test Loss: 0.5189, F1: 0.2681, AUC: 0.7660
Epoch [10/30] Train Loss: 0.3673, Test Loss: 0.4875, F1: 0.3224, AUC: 0.8227
Epoch [20/30] Train Loss: 0.3470, Test Loss: 0.5621, F1: 0.2950, AUC: 0.8241
Mejores resultados en la época:  21
f1-score 0.34545454545454546
AUC según el mejor F1-score 0.8219838463740903
Confusion Matrix:
 [[1191  162]
 [  54   57]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8525
Precision:  0.2603
Recall:     0.5135
F1-score:   0.3455

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6414, Test Loss: 0.5576, F1: 0.2539, AUC: 0.7737
Epoch [10/30] Train Loss: 0.3854, Test Loss: 0.4500, F1: 0.3191, AUC: 0.8232
Epoch [20/30] Train Loss: 0.3529, Test Loss: 0.4921, F1: 0.3176, AUC: 0.8224
Mejores resultados en la época:  16
f1-score 0.3416370106761566
AUC según el mejor F1-score 0.8229826278606768
Confusion Matrix:
 [[1231  122]
 [  63   48]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8736
Precision:  0.2415
Recall:     0.5766
F1-score:   0.3404
Tiempo total para red 3: 86.72 segundos

Entrenando red 4 con capas [300, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6592, Test Loss: 0.4115, F1: 0.2748, AUC: 0.7713
Epoch [10/30] Train Loss: 0.3766, Test Loss: 0.3705, F1: 0.3117, AUC: 0.8244
Epoch [20/30] Train Loss: 0.3534, Test Loss: 0.4560, F1: 0.3256, AUC: 0.8246
Mejores resultados en la época:  5
f1-score 0.3445692883895131
AUC según el mejor F1-score 0.8218773096821876
Confusion Matrix:
 [[1243  110]
 [  65   46]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8805
Precision:  0.2949
Recall:     0.4144
F1-score:   0.3446

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6368, Test Loss: 0.6758, F1: 0.2428, AUC: 0.7744
Epoch [10/30] Train Loss: 0.3849, Test Loss: 0.3563, F1: 0.3130, AUC: 0.8211
Epoch [20/30] Train Loss: 0.3632, Test Loss: 0.6613, F1: 0.2633, AUC: 0.8211
Mejores resultados en la época:  29
f1-score 0.3379501385041551
AUC según el mejor F1-score 0.8181818181818181
Confusion Matrix:
 [[1164  189]
 [  50   61]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8367
Precision:  0.2440
Recall:     0.5495
F1-score:   0.3380

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6017, Test Loss: 0.3752, F1: 0.2865, AUC: 0.7811
Epoch [10/30] Train Loss: 0.3780, Test Loss: 0.6166, F1: 0.2941, AUC: 0.8223
Epoch [20/30] Train Loss: 0.3577, Test Loss: 0.2834, F1: 0.3128, AUC: 0.8217
Mejores resultados en la época:  28
f1-score 0.345
AUC según el mejor F1-score 0.8221503099551881
Confusion Matrix:
 [[1133  220]
 [  42   69]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8210
Precision:  0.2388
Recall:     0.6216
F1-score:   0.3450

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6220, Test Loss: 0.3572, F1: 0.2848, AUC: 0.7728
Epoch [10/30] Train Loss: 0.3843, Test Loss: 0.4464, F1: 0.3280, AUC: 0.8247
Epoch [20/30] Train Loss: 0.3663, Test Loss: 0.4392, F1: 0.3311, AUC: 0.8243
Mejores resultados en la época:  25
f1-score 0.3415977961432507
AUC según el mejor F1-score 0.8218706511389438
Confusion Matrix:
 [[1163  190]
 [  49   62]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8367
Precision:  0.2460
Recall:     0.5586
F1-score:   0.3416
Tiempo total para red 4: 100.34 segundos

Entrenando red 5 con capas [300, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6172, Test Loss: 0.5843, F1: 0.2606, AUC: 0.7761
Epoch [10/30] Train Loss: 0.3884, Test Loss: 0.4382, F1: 0.3285, AUC: 0.8228
Epoch [20/30] Train Loss: 0.3527, Test Loss: 0.4532, F1: 0.3230, AUC: 0.8216
Mejores resultados en la época:  3
f1-score 0.34306569343065696
AUC según el mejor F1-score 0.8158646451329379
Confusion Matrix:
 [[1237  116]
 [  64   47]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8770
Precision:  0.2883
Recall:     0.4234
F1-score:   0.3431

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6331, Test Loss: 0.3732, F1: 0.2849, AUC: 0.7778
Epoch [10/30] Train Loss: 0.4020, Test Loss: 0.6409, F1: 0.2870, AUC: 0.8256
Epoch [20/30] Train Loss: 0.3504, Test Loss: 0.3695, F1: 0.3427, AUC: 0.8178
Mejores resultados en la época:  17
f1-score 0.3485714285714286
AUC según el mejor F1-score 0.8241878241878241
Confusion Matrix:
 [[1175  178]
 [  50   61]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8443
Precision:  0.2552
Recall:     0.5495
F1-score:   0.3486

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.5984, Test Loss: 0.3261, F1: 0.2953, AUC: 0.7887
Epoch [10/30] Train Loss: 0.3874, Test Loss: 0.3514, F1: 0.3233, AUC: 0.8234
Epoch [20/30] Train Loss: 0.3408, Test Loss: 0.3344, F1: 0.3393, AUC: 0.8211
Mejores resultados en la época:  28
f1-score 0.36860068259385664
AUC según el mejor F1-score 0.8169300120519633
Confusion Matrix:
 [[1225  128]
 [  57   54]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8736
Precision:  0.2967
Recall:     0.4865
F1-score:   0.3686

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.5956, Test Loss: 0.2489, F1: 0.2811, AUC: 0.7836
Epoch [10/30] Train Loss: 0.3890, Test Loss: 0.4997, F1: 0.3094, AUC: 0.8202
Epoch [20/30] Train Loss: 0.3571, Test Loss: 0.3434, F1: 0.3402, AUC: 0.8249
Mejores resultados en la época:  18
f1-score 0.340632603406326
AUC según el mejor F1-score 0.8274571689205834
Confusion Matrix:
 [[1123  230]
 [  41   70]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8149
Precision:  0.2333
Recall:     0.6306
F1-score:   0.3406
Tiempo total para red 5: 122.81 segundos

Entrenando red 6 con capas [300, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6921, Test Loss: 0.7060, F1: 0.1410, AUC: 0.7545
Epoch [10/30] Train Loss: 0.3784, Test Loss: 0.3878, F1: 0.3092, AUC: 0.8221
Epoch [20/30] Train Loss: 0.3600, Test Loss: 0.3142, F1: 0.3154, AUC: 0.8201
Mejores resultados en la época:  26
f1-score 0.3469387755102041
AUC según el mejor F1-score 0.8169633047681828
Confusion Matrix:
 [[1140  213]
 [  43   68]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8251
Precision:  0.2420
Recall:     0.6126
F1-score:   0.3469

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6595, Test Loss: 0.3686, F1: 0.0787, AUC: 0.7707
Epoch [10/30] Train Loss: 0.4812, Test Loss: 0.6527, F1: 0.2654, AUC: 0.8181
Epoch [20/30] Train Loss: 0.3559, Test Loss: 0.4283, F1: 0.3043, AUC: 0.8187
Mejores resultados en la época:  19
f1-score 0.3333333333333333
AUC según el mejor F1-score 0.8169233535087194
Confusion Matrix:
 [[1059  294]
 [  30   81]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.7787
Precision:  0.2160
Recall:     0.7297
F1-score:   0.3333

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6484, Test Loss: 0.2599, F1: 0.2577, AUC: 0.7756
Epoch [10/30] Train Loss: 0.3998, Test Loss: 0.4128, F1: 0.3181, AUC: 0.8221
Epoch [20/30] Train Loss: 0.3490, Test Loss: 0.3689, F1: 0.3448, AUC: 0.8188
Mejores resultados en la época:  20
f1-score 0.3448275862068966
AUC según el mejor F1-score 0.8188210383332334
Confusion Matrix:
 [[1176  177]
 [  51   60]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8443
Precision:  0.2532
Recall:     0.5405
F1-score:   0.3448

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6712, Test Loss: 0.7016, F1: 0.2373, AUC: 0.7764
Epoch [10/30] Train Loss: 0.4175, Test Loss: 0.2799, F1: 0.3394, AUC: 0.8241
Epoch [20/30] Train Loss: 0.3734, Test Loss: 0.3414, F1: 0.3235, AUC: 0.8196
Mejores resultados en la época:  18
f1-score 0.3463687150837989
AUC según el mejor F1-score 0.8241212387553851
Confusion Matrix:
 [[1168  185]
 [  49   62]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8402
Precision:  0.2510
Recall:     0.5586
F1-score:   0.3464
Tiempo total para red 6: 181.50 segundos
Saved on: outputs_only_text_os/2/lyrics_bert

==============================
Model: Logistic Regression
Accuracy:  0.8067
Precision: 0.2278
Recall:    0.6486
F1-score:  0.3372
              precision    recall  f1-score   support

           0       0.97      0.82      0.89      1353
           1       0.23      0.65      0.34       111

    accuracy                           0.81      1464
   macro avg       0.60      0.73      0.61      1464
weighted avg       0.91      0.81      0.85      1464

[[1109  244]
Precision:  0.2824
Recall:     0.4324
F1-score:   0.3416
Tiempo total para red 3: 87.12 segundos

Entrenando red 4 con capas [300, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6146, Test Loss: 0.6905, F1: 0.2566, AUC: 0.7863
Epoch [10/30] Train Loss: 0.3899, Test Loss: 0.5910, F1: 0.3085, AUC: 0.8246
Epoch [20/30] Train Loss: 0.3621, Test Loss: 0.3040, F1: 0.3421, AUC: 0.8224
Mejores resultados en la época:  20
f1-score 0.34210526315789475
AUC según el mejor F1-score 0.8223500662525052
Confusion Matrix:
 [[1212  141]
 [  59   52]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8634
Precision:  0.2694
Recall:     0.4685
F1-score:   0.3421

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6289, Test Loss: 0.3918, F1: 0.2894, AUC: 0.7810
Epoch [10/30] Train Loss: 0.3848, Test Loss: 0.4943, F1: 0.3261, AUC: 0.8314
Epoch [20/30] Train Loss: 0.3464, Test Loss: 0.3180, F1: 0.3407, AUC: 0.8278
Mejores resultados en la época:  25
f1-score 0.34560906515580736
AUC según el mejor F1-score 0.8284692674936577
Confusion Matrix:
 [[1172  181]
 [  50   61]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8422
Precision:  0.2521
Recall:     0.5495
F1-score:   0.3456

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6549, Test Loss: 0.9018, F1: 0.1970, AUC: 0.7687
Epoch [10/30] Train Loss: 0.3884, Test Loss: 0.6308, F1: 0.2870, AUC: 0.8231
Epoch [20/30] Train Loss: 0.3576, Test Loss: 0.4738, F1: 0.3266, AUC: 0.8225
Mejores resultados en la época:  23
f1-score 0.3383838383838384
AUC según el mejor F1-score 0.8215110898037727
Confusion Matrix:
 [[1135  218]
 [  44   67]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8210
Precision:  0.2351
Recall:     0.6036
F1-score:   0.3384

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6365, Test Loss: 1.0108, F1: 0.1915, AUC: 0.7692
Epoch [10/30] Train Loss: 0.3698, Test Loss: 0.2738, F1: 0.3035, AUC: 0.8243
Epoch [20/30] Train Loss: 0.3385, Test Loss: 0.4114, F1: 0.3387, AUC: 0.8244
Mejores resultados en la época:  15
f1-score 0.3470031545741325
AUC según el mejor F1-score 0.8246206294986783
Confusion Matrix:
 [[1202  151]
 [  56   55]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8586
Precision:  0.2670
Recall:     0.4955
F1-score:   0.3470
Tiempo total para red 4: 100.44 segundos

Entrenando red 5 con capas [300, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6474, Test Loss: 0.4927, F1: 0.2644, AUC: 0.7695
Epoch [10/30] Train Loss: 0.3815, Test Loss: 0.3479, F1: 0.3209, AUC: 0.8228
Epoch [20/30] Train Loss: 0.3694, Test Loss: 0.2557, F1: 0.2870, AUC: 0.8224
Mejores resultados en la época:  29
f1-score 0.34877384196185285
AUC según el mejor F1-score 0.8231357743552865
Confusion Matrix:
 [[1161  192]
 [  47   64]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8367
Precision:  0.2500
Recall:     0.5766
F1-score:   0.3488

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.5676, Test Loss: 0.7048, F1: 0.2566, AUC: 0.7935
Epoch [10/30] Train Loss: 0.3765, Test Loss: 0.5223, F1: 0.3194, AUC: 0.8239
Epoch [20/30] Train Loss: 0.3439, Test Loss: 0.4192, F1: 0.3224, AUC: 0.8213
Mejores resultados en la época:  26
f1-score 0.34806629834254144
AUC según el mejor F1-score 0.8205322839469181
Confusion Matrix:
 [[1165  188]
 [  48   63]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8388
Precision:  0.2510
Recall:     0.5676
F1-score:   0.3481

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6297, Test Loss: 0.8375, F1: 0.2174, AUC: 0.7790
Epoch [10/30] Train Loss: 0.3760, Test Loss: 0.2749, F1: 0.3309, AUC: 0.8266
Epoch [20/30] Train Loss: 0.3534, Test Loss: 0.5777, F1: 0.3087, AUC: 0.8251
Mejores resultados en la época:  25
f1-score 0.35777126099706746
AUC según el mejor F1-score 0.8293149024856341
Confusion Matrix:
 [[1184  169]
 [  50   61]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8504
Precision:  0.2652
Recall:     0.5495
F1-score:   0.3578

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6321, Test Loss: 0.3641, F1: 0.3048, AUC: 0.7754
Epoch [10/30] Train Loss: 0.3952, Test Loss: 0.4188, F1: 0.3267, AUC: 0.8240
Epoch [20/30] Train Loss: 0.3503, Test Loss: 0.5350, F1: 0.3194, AUC: 0.8243
Mejores resultados en la época:  23
f1-score 0.3425925925925926
AUC según el mejor F1-score 0.8242810437932389
Confusion Matrix:
 [[1106  247]
 [  37   74]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8060
Precision:  0.2305
Recall:     0.6667
F1-score:   0.3426
Tiempo total para red 5: 123.38 segundos

Entrenando red 6 con capas [300, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6401, Test Loss: 0.5866, F1: 0.2555, AUC: 0.7692
Epoch [10/30] Train Loss: 0.4005, Test Loss: 0.5190, F1: 0.3038, AUC: 0.8246
Epoch [20/30] Train Loss: 0.3511, Test Loss: 0.5323, F1: 0.3122, AUC: 0.8218
Mejores resultados en la época:  15
f1-score 0.3389830508474576
AUC según el mejor F1-score 0.8204923326874548
Confusion Matrix:
 [[1219  134]
 [  61   50]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8668
Precision:  0.2717
Recall:     0.4505
F1-score:   0.3390

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.5993, Test Loss: 0.4940, F1: 0.2797, AUC: 0.7887
Epoch [10/30] Train Loss: 0.4139, Test Loss: 0.4048, F1: 0.3251, AUC: 0.8245
Epoch [20/30] Train Loss: 0.3428, Test Loss: 0.6101, F1: 0.2938, AUC: 0.8207
Mejores resultados en la época:  29
f1-score 0.3402985074626866
AUC según el mejor F1-score 0.8246472636716539
Confusion Matrix:
 [[1186  167]
 [  54   57]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8490
Precision:  0.2545
Recall:     0.5135
F1-score:   0.3403

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.5991, Test Loss: 0.5853, F1: 0.2648, AUC: 0.7939
Epoch [10/30] Train Loss: 0.3831, Test Loss: 0.8708, F1: 0.2373, AUC: 0.8237
Epoch [20/30] Train Loss: 0.3409, Test Loss: 0.6580, F1: 0.2830, AUC: 0.8238
Mejores resultados en la época:  28
f1-score 0.35451505016722407
AUC según el mejor F1-score 0.8216708948416266
Confusion Matrix:
 [[1218  135]
 [  58   53]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8682
Precision:  0.2819
Recall:     0.4775
F1-score:   0.3545

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6538, Test Loss: 0.8978, F1: 0.2086, AUC: 0.7679
Epoch [10/30] Train Loss: 0.3891, Test Loss: 0.5745, F1: 0.3046, AUC: 0.8203
Epoch [20/30] Train Loss: 0.3539, Test Loss: 0.3482, F1: 0.3371, AUC: 0.8204
Mejores resultados en la época:  16
f1-score 0.34951456310679613
AUC según el mejor F1-score 0.8213712603956507
Confusion Matrix:
 [[1209  144]
 [  57   54]]
Matriz de confusión guardada en: outputs_only_text_os/2/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8627
Precision:  0.2727
Recall:     0.4865
F1-score:   0.3495
Tiempo total para red 6: 181.99 segundos
Saved on: outputs_only_text_os/2/lyrics_bert

==============================
Model: Logistic Regression
Accuracy:  0.8067
Precision: 0.2278
Recall:    0.6486
F1-score:  0.3372
              precision    recall  f1-score   support

           0       0.97      0.82      0.89      1353
           1       0.23      0.65      0.34       111

    accuracy                           0.81      1464
   macro avg       0.60      0.73      0.61      1464
weighted avg       0.91      0.81      0.85      1464
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [20:31:10] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [20:31:11] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
 [  39   72]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text_os/2/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text_os/2/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.9037
Precision: 0.3828
Recall:    0.4414
F1-score:  0.4100
              precision    recall  f1-score   support

           0       0.95      0.94      0.95      1353
           1       0.38      0.44      0.41       111

    accuracy                           0.90      1464
   macro avg       0.67      0.69      0.68      1464
weighted avg       0.91      0.90      0.91      1464

[[1274   79]
 [  62   49]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text_os/2/lyrics_bert/conf_matrix_svm.png
Modelo guardado como: outputs_only_text_os/2/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7473
Precision: 0.1583
Recall:    0.5405
F1-score:  0.2449
              precision    recall  f1-score   support

           0       0.95      0.76      0.85      1353
           1       0.16      0.54      0.24       111

    accuracy                           0.75      1464
   macro avg       0.56      0.65      0.55      1464
weighted avg       0.89      0.75      0.80      1464

[[1034  319]
 [  51   60]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text_os/2/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text_os/2/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.9030
Precision: 0.3465
Recall:    0.3153
F1-score:  0.3302
              precision    recall  f1-score   support

           0       0.94      0.95      0.95      1353
           1       0.35      0.32      0.33       111

    accuracy                           0.90      1464
   macro avg       0.65      0.63      0.64      1464
weighted avg       0.90      0.90      0.90      1464

[[1287   66]
 [  76   35]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text_os/2/lyrics_bert/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text_os/2/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9133
Precision: 0.4000
Recall:    0.2883
F1-score:  0.3351
              precision    recall  f1-score   support

           0       0.94      0.96      0.95      1353
           1       0.40      0.29      0.34       111

    accuracy                           0.91      1464
   macro avg       0.67      0.63      0.64      1464
weighted avg       0.90      0.91      0.91      1464

[[1305   48]
 [  79   32]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text_os/2/lyrics_bert/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text_os/2/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7698
Precision: 0.1771
Recall:    0.5586
F1-score:  0.2690
              precision    recall  f1-score   support

           0       0.96      0.79      0.86      1353
           1       0.18      0.56      0.27       111

    accuracy                           0.77      1464
   macro avg       0.57      0.67      0.57      1464
weighted avg       0.90      0.77      0.82      1464

[[1065  288]
 [  49   62]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_only_text_os/2/lyrics_bert/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text_os/2/lyrics_bert/naive_bayes_model.pkl


Resumen de métricas:
SVM: {'accuracy': 0.9037, 'precision': 0.3828, 'recall': 0.4414, 'f1_score': 0.41}
Logistic Regression: {'accuracy': 0.8067, 'precision': 0.2278, 'recall': 0.6486, 'f1_score': 0.3372}
XGBoost: {'accuracy': 0.9133, 'precision': 0.4, 'recall': 0.2883, 'f1_score': 0.3351}
Random Forest: {'accuracy': 0.903, 'precision': 0.3465, 'recall': 0.3153, 'f1_score': 0.3302}
Naive Bayes: {'accuracy': 0.7698, 'precision': 0.1771, 'recall': 0.5586, 'f1_score': 0.269}
Decision Tree: {'accuracy': 0.7473, 'precision': 0.1583, 'recall': 0.5405, 'f1_score': 0.2449}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: LYRICS_BERT
SVM: {'accuracy': 0.9037, 'precision': 0.3828, 'recall': 0.4414, 'f1_score': 0.41}
MLP_326657: {'accuracy': 0.8148907103825137, 'precision': 0.23333333333333334, 'recall': 0.6306306306306306, 'f1_score': 0.36860068259385664, 'f1_score_avg': 0.35021760200056706}
MLP_1007617: {'accuracy': 0.8401639344262295, 'precision': 0.25101214574898784, 'recall': 0.5585585585585585, 'f1_score': 0.36860068259385664, 'f1_score_avg': 0.34286710253355823}
MLP_48897: {'accuracy': 0.8306010928961749, 'precision': 0.24150943396226415, 'recall': 0.5765765765765766, 'f1_score': 0.3492063492063492, 'f1_score_avg': 0.34002701789935835}
MLP_120321: {'accuracy': 0.8367486338797814, 'precision': 0.24603174603174602, 'recall': 0.5585585585585585, 'f1_score': 0.3492063492063492, 'f1_score_avg': 0.3422793057592297}
MLP_21377: {'accuracy': 0.7814207650273224, 'precision': 0.21212121212121213, 'recall': 0.6936936936936937, 'f1_score': 0.3403141361256545, 'f1_score_avg': 0.33382679170314394}
Logistic Regression: {'accuracy': 0.8067, 'precision': 0.2278, 'recall': 0.6486, 'f1_score': 0.3372}
XGBoost: {'accuracy': 0.9133, 'precision': 0.4, 'recall': 0.2883, 'f1_score': 0.3351}
MLP_9665: {'accuracy': 0.8189890710382514, 'precision': 0.23076923076923078, 'recall': 0.5945945945945946, 'f1_score': 0.33403805496828753, 'f1_score_avg': 0.3327619716928316}
Random Forest: {'accuracy': 0.903, 'precision': 0.3465, 'recall': 0.3153, 'f1_score': 0.3302}
Naive Bayes: {'accuracy': 0.7698, 'precision': 0.1771, 'recall': 0.5586, 'f1_score': 0.269}
Decision Tree: {'accuracy': 0.7473, 'precision': 0.1583, 'recall': 0.5405, 'f1_score': 0.2449}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: False
USE_SMOTE: True
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: False
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['lyrics']
Numeric Columns: Not used
====================================


[[1109  244]
 [  39   72]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text_os/2/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text_os/2/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.9037
Precision: 0.3828
Recall:    0.4414
F1-score:  0.4100
              precision    recall  f1-score   support

           0       0.95      0.94      0.95      1353
           1       0.38      0.44      0.41       111

    accuracy                           0.90      1464
   macro avg       0.67      0.69      0.68      1464
weighted avg       0.91      0.90      0.91      1464

[[1274   79]
 [  62   49]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text_os/2/lyrics_bert/conf_matrix_svm.png
Modelo guardado como: outputs_only_text_os/2/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7473
Precision: 0.1583
Recall:    0.5405
F1-score:  0.2449
              precision    recall  f1-score   support

           0       0.95      0.76      0.85      1353
           1       0.16      0.54      0.24       111

    accuracy                           0.75      1464
   macro avg       0.56      0.65      0.55      1464
weighted avg       0.89      0.75      0.80      1464

[[1034  319]
 [  51   60]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text_os/2/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text_os/2/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.9030
Precision: 0.3465
Recall:    0.3153
F1-score:  0.3302
              precision    recall  f1-score   support

           0       0.94      0.95      0.95      1353
           1       0.35      0.32      0.33       111

    accuracy                           0.90      1464
   macro avg       0.65      0.63      0.64      1464
weighted avg       0.90      0.90      0.90      1464

[[1287   66]
 [  76   35]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text_os/2/lyrics_bert/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text_os/2/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9133
Precision: 0.4000
Recall:    0.2883
F1-score:  0.3351
              precision    recall  f1-score   support

           0       0.94      0.96      0.95      1353
           1       0.40      0.29      0.34       111

    accuracy                           0.91      1464
   macro avg       0.67      0.63      0.64      1464
weighted avg       0.90      0.91      0.91      1464

[[1305   48]
 [  79   32]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text_os/2/lyrics_bert/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text_os/2/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7698
Precision: 0.1771
Recall:    0.5586
F1-score:  0.2690
              precision    recall  f1-score   support

           0       0.96      0.79      0.86      1353
           1       0.18      0.56      0.27       111

    accuracy                           0.77      1464
   macro avg       0.57      0.67      0.57      1464
weighted avg       0.90      0.77      0.82      1464

[[1065  288]
 [  49   62]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_only_text_os/2/lyrics_bert/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text_os/2/lyrics_bert/naive_bayes_model.pkl


Resumen de métricas:
SVM: {'accuracy': 0.9037, 'precision': 0.3828, 'recall': 0.4414, 'f1_score': 0.41}
Logistic Regression: {'accuracy': 0.8067, 'precision': 0.2278, 'recall': 0.6486, 'f1_score': 0.3372}
XGBoost: {'accuracy': 0.9133, 'precision': 0.4, 'recall': 0.2883, 'f1_score': 0.3351}
Random Forest: {'accuracy': 0.903, 'precision': 0.3465, 'recall': 0.3153, 'f1_score': 0.3302}
Naive Bayes: {'accuracy': 0.7698, 'precision': 0.1771, 'recall': 0.5586, 'f1_score': 0.269}
Decision Tree: {'accuracy': 0.7473, 'precision': 0.1583, 'recall': 0.5405, 'f1_score': 0.2449}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: LYRICS_BERT
SVM: {'accuracy': 0.9037, 'precision': 0.3828, 'recall': 0.4414, 'f1_score': 0.41}
MLP_326657: {'accuracy': 0.8060109289617486, 'precision': 0.23052959501557632, 'recall': 0.6666666666666666, 'f1_score': 0.35777126099706746, 'f1_score_avg': 0.3493009984735136}
MLP_1007617: {'accuracy': 0.8627049180327869, 'precision': 0.2727272727272727, 'recall': 0.4864864864864865, 'f1_score': 0.35777126099706746, 'f1_score_avg': 0.3458277928960411}
MLP_120321: {'accuracy': 0.8586065573770492, 'precision': 0.2669902912621359, 'recall': 0.4954954954954955, 'f1_score': 0.3470031545741325, 'f1_score_avg': 0.3432753303179183}
MLP_48897: {'accuracy': 0.8736338797814208, 'precision': 0.2823529411764706, 'recall': 0.43243243243243246, 'f1_score': 0.34545454545454546, 'f1_score_avg': 0.34236087867603976}
MLP_21377: {'accuracy': 0.8763661202185792, 'precision': 0.2839506172839506, 'recall': 0.4144144144144144, 'f1_score': 0.3431952662721893, 'f1_score_avg': 0.337994563597662}
MLP_9665: {'accuracy': 0.7561475409836066, 'precision': 0.20574162679425836, 'recall': 0.7747747747747747, 'f1_score': 0.33773087071240104, 'f1_score_avg': 0.33254671397936725}
Logistic Regression: {'accuracy': 0.8067, 'precision': 0.2278, 'recall': 0.6486, 'f1_score': 0.3372}
XGBoost: {'accuracy': 0.9133, 'precision': 0.4, 'recall': 0.2883, 'f1_score': 0.3351}
Random Forest: {'accuracy': 0.903, 'precision': 0.3465, 'recall': 0.3153, 'f1_score': 0.3302}
Naive Bayes: {'accuracy': 0.7698, 'precision': 0.1771, 'recall': 0.5586, 'f1_score': 0.269}
Decision Tree: {'accuracy': 0.7473, 'precision': 0.1583, 'recall': 0.5405, 'f1_score': 0.2449}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: False
USE_SMOTE: True
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: False
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['lyrics']
Numeric Columns: Not used
====================================

