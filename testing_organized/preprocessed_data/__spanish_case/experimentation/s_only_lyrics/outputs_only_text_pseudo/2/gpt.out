2025-10-30 04:07:30.574699: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-30 04:07:30.574699: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__spanish_case/experimentation/s_only_lyrics/main_only_text_gpt.py:273: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__spanish_case/experimentation/s_only_lyrics/main_only_text_gpt.py:273: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
For TF-IDF embbedings you are selecteing this columns:
--> ['lyrics']
Your are not adding cols numercis
You are executing with [ALL] dataset
--> PaTH:  ../../../../../data/spanish/LB_T/LB_fuss/lb_khipu_T.npy

##################################################
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../../../../data/spanish/dataset/oficialDatasetEAIM2026_pseudolabeling.csv
Label distribution: {False: 6765, True: 554}
X shape: (7319, 1536)
y shape: (7319,)

Data con el spliting...
Label distribution en TRAIN: {0: 5412, 1: 443}
Label distribution en TEST: {0: 1353, 1: 111}


==================================================
Data antes del undersampling ...
X: (5855, 1536)
y: (5855,)
Apliying UNDERSAMPLE
443
Label distribution: {0: 443, 1: 443}
X shape: (886, 1536)
y shape: (886,)
Resultados con MLP

Entrenando red 1 con capas [1536, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6496, Test Loss: 0.7169, F1: 0.2272, AUC: 0.8498
Epoch [10/30] Train Loss: 0.3669, Test Loss: 0.4838, F1: 0.3644, AUC: 0.8783
Epoch [20/30] Train Loss: 0.3126, Test Loss: 0.4734, F1: 0.3860, AUC: 0.8911
Mejores resultados en la época:  28
f1-score 0.4489795918367347
AUC según el mejor F1-score 0.895034724303017
Confusion Matrix:
 [[1160  193]
 [  23   88]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8525
Precision:  0.3132
Recall:     0.7928
F1-score:   0.4490

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6999, Test Loss: 0.6743, F1: 0.2184, AUC: 0.8157
Epoch [10/30] Train Loss: 0.5765, Test Loss: 0.7366, F1: 0.2552, AUC: 0.8609
Epoch [20/30] Train Loss: 0.5116, Test Loss: 0.7216, F1: 0.2933, AUC: 0.8694
Mejores resultados en la época:  24
f1-score 0.371900826446281
AUC según el mejor F1-score 0.8596945060359694
Confusion Matrix:
 [[1070  283]
 [  21   90]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.7923
Precision:  0.2413
Recall:     0.8108
F1-score:   0.3719

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6964, Test Loss: 0.7045, F1: 0.1681, AUC: 0.8127
Epoch [10/30] Train Loss: 0.4503, Test Loss: 0.5450, F1: 0.3262, AUC: 0.8603
Epoch [20/30] Train Loss: 0.3826, Test Loss: 0.5765, F1: 0.3218, AUC: 0.8745
Mejores resultados en la época:  16
f1-score 0.4253164556962025
AUC según el mejor F1-score 0.8692661619490888
Confusion Matrix:
 [[1153  200]
 [  27   84]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8449
Precision:  0.2958
Recall:     0.7568
F1-score:   0.4253

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6776, Test Loss: 0.5629, F1: 0.1944, AUC: 0.8456
Epoch [10/30] Train Loss: 0.4076, Test Loss: 0.4966, F1: 0.3543, AUC: 0.8685
Epoch [20/30] Train Loss: 0.3415, Test Loss: 0.4929, F1: 0.3593, AUC: 0.8838
Mejores resultados en la época:  21
f1-score 0.47752808988764045
AUC según el mejor F1-score 0.8851068363263485
Confusion Matrix:
 [[1193  160]
 [  26   85]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8730
Precision:  0.3469
Recall:     0.7658
F1-score:   0.4775
Tiempo total para red 1: 16.39 segundos

Entrenando red 2 con capas [1536, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6816, Test Loss: 0.5285, F1: 0.0000, AUC: 0.8561
Epoch [10/30] Train Loss: 0.3796, Test Loss: 0.3923, F1: 0.4163, AUC: 0.8743
Epoch [20/30] Train Loss: 0.3094, Test Loss: 0.5287, F1: 0.3561, AUC: 0.8910
Mejores resultados en la época:  17
f1-score 0.4732394366197183
AUC según el mejor F1-score 0.8878368390563514
Confusion Matrix:
 [[1193  160]
 [  27   84]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8723
Precision:  0.3443
Recall:     0.7568
F1-score:   0.4732

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6951, Test Loss: 0.5873, F1: 0.0000, AUC: 0.8407
Epoch [10/30] Train Loss: 0.3586, Test Loss: 0.3867, F1: 0.4203, AUC: 0.8819
Epoch [20/30] Train Loss: 0.3052, Test Loss: 0.5435, F1: 0.3499, AUC: 0.8928
Mejores resultados en la época:  26
f1-score 0.5044510385756676
AUC según el mejor F1-score 0.8961799937409695
Confusion Matrix:
 [[1212  141]
 [  26   85]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8859
Precision:  0.3761
Recall:     0.7658
F1-score:   0.5045

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6927, Test Loss: 0.7869, F1: 0.1410, AUC: 0.8497
Epoch [10/30] Train Loss: 0.4104, Test Loss: 0.9791, F1: 0.2339, AUC: 0.8753
Epoch [20/30] Train Loss: 0.3165, Test Loss: 0.6537, F1: 0.3188, AUC: 0.8889
Mejores resultados en la época:  18
f1-score 0.47293447293447294
AUC según el mejor F1-score 0.8863186911967401
Confusion Matrix:
 [[1196  157]
 [  28   83]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8736
Precision:  0.3458
Recall:     0.7477
F1-score:   0.4729

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6779, Test Loss: 0.6265, F1: 0.3893, AUC: 0.8515
Epoch [10/30] Train Loss: 0.4007, Test Loss: 0.5471, F1: 0.3327, AUC: 0.8809
Epoch [20/30] Train Loss: 0.2923, Test Loss: 0.5106, F1: 0.3672, AUC: 0.8933
Mejores resultados en la época:  24
f1-score 0.5044510385756676
AUC según el mejor F1-score 0.8953010660327734
Confusion Matrix:
 [[1212  141]
 [  26   85]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8859
Precision:  0.3761
Recall:     0.7658
F1-score:   0.5045
Tiempo total para red 2: 14.17 segundos

Entrenando red 3 con capas [1536, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6915, Test Loss: 0.6712, F1: 0.4028, AUC: 0.8572
Epoch [10/30] Train Loss: 0.3550, Test Loss: 0.4285, F1: 0.4018, AUC: 0.8830
Epoch [20/30] Train Loss: 0.2956, Test Loss: 0.3400, F1: 0.4718, AUC: 0.8952
Mejores resultados en la época:  21
f1-score 0.4887640449438202
AUC según el mejor F1-score 0.8956473102814566
Confusion Matrix:
 [[1195  158]
 [  24   87]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8757
Precision:  0.3551
Recall:     0.7838
F1-score:   0.4888

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6923, Test Loss: 0.7645, F1: 0.1410, AUC: 0.8480
Epoch [10/30] Train Loss: 0.3563, Test Loss: 0.6446, F1: 0.3102, AUC: 0.8813
Epoch [20/30] Train Loss: 0.2760, Test Loss: 0.5150, F1: 0.3665, AUC: 0.8951
Mejores resultados en la época:  23
f1-score 0.5151515151515151
AUC según el mejor F1-score 0.8963264816923354
Confusion Matrix:
 [[1219  134]
 [  26   85]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8907
Precision:  0.3881
Recall:     0.7658
F1-score:   0.5152

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6958, Test Loss: 0.7924, F1: 0.1410, AUC: 0.8499
Epoch [10/30] Train Loss: 0.3629, Test Loss: 0.4682, F1: 0.3689, AUC: 0.8805
Epoch [20/30] Train Loss: 0.2745, Test Loss: 0.4864, F1: 0.3752, AUC: 0.8940
Mejores resultados en la época:  13
f1-score 0.46900269541778977
AUC según el mejor F1-score 0.8868646917427406
Confusion Matrix:
 [[1180  173]
 [  24   87]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8654
Precision:  0.3346
Recall:     0.7838
F1-score:   0.4690

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6900, Test Loss: 0.6482, F1: 0.1463, AUC: 0.8503
Epoch [10/30] Train Loss: 0.3656, Test Loss: 0.3579, F1: 0.4485, AUC: 0.8828
Epoch [20/30] Train Loss: 0.3065, Test Loss: 0.5781, F1: 0.3458, AUC: 0.8942
Mejores resultados en la época:  28
f1-score 0.4727272727272727
AUC según el mejor F1-score 0.8961733351977254
Confusion Matrix:
 [[1170  183]
 [  20   91]]
For TF-IDF embbedings you are selecteing this columns:
--> ['lyrics']
Your are not adding cols numercis
You are executing with [ALL] dataset
--> PaTH:  ../../../../../data/spanish/LB_T/LB_fuss/lb_khipu_T.npy

##################################################
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../../../../data/spanish/dataset/oficialDatasetEAIM2026_pseudolabeling.csv
Label distribution: {False: 6765, True: 554}
X shape: (7319, 1536)
y shape: (7319,)

Data con el spliting...
Label distribution en TRAIN: {0: 5412, 1: 443}
Label distribution en TEST: {0: 1353, 1: 111}


==================================================
Data antes del undersampling ...
X: (5855, 1536)
y: (5855,)
Apliying UNDERSAMPLE
443
Label distribution: {0: 443, 1: 443}
X shape: (886, 1536)
y shape: (886,)
Resultados con MLP

Entrenando red 1 con capas [1536, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6304, Test Loss: 0.6468, F1: 0.2819, AUC: 0.8475
Epoch [10/30] Train Loss: 0.3674, Test Loss: 0.7368, F1: 0.2849, AUC: 0.8788
Epoch [20/30] Train Loss: 0.3092, Test Loss: 0.2955, F1: 0.4956, AUC: 0.8914
Mejores resultados en la época:  29
f1-score 0.5209003215434084
AUC según el mejor F1-score 0.894814992375968
Confusion Matrix:
 [[1234  119]
 [  30   81]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8982
Precision:  0.4050
Recall:     0.7297
F1-score:   0.5209

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6774, Test Loss: 0.7409, F1: 0.1716, AUC: 0.8514
Epoch [10/30] Train Loss: 0.4077, Test Loss: 0.6456, F1: 0.2915, AUC: 0.8724
Epoch [20/30] Train Loss: 0.3421, Test Loss: 0.2793, F1: 0.4969, AUC: 0.8881
Mejores resultados en la época:  29
f1-score 0.49851632047477745
AUC según el mejor F1-score 0.8937229912839669
Confusion Matrix:
 [[1211  142]
 [  27   84]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8846
Precision:  0.3717
Recall:     0.7568
F1-score:   0.4985

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6351, Test Loss: 0.6139, F1: 0.3156, AUC: 0.8535
Epoch [10/30] Train Loss: 0.3744, Test Loss: 0.4964, F1: 0.3569, AUC: 0.8809
Epoch [20/30] Train Loss: 0.3132, Test Loss: 0.4705, F1: 0.3891, AUC: 0.8919
Mejores resultados en la época:  15
f1-score 0.5082508250825083
AUC según el mejor F1-score 0.8882829614536932
Confusion Matrix:
 [[1238  115]
 [  34   77]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8982
Precision:  0.4010
Recall:     0.6937
F1-score:   0.5083

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6526, Test Loss: 0.5289, F1: 0.4054, AUC: 0.8437
Epoch [10/30] Train Loss: 0.4210, Test Loss: 0.3058, F1: 0.4615, AUC: 0.8751
Epoch [20/30] Train Loss: 0.3235, Test Loss: 0.5173, F1: 0.3654, AUC: 0.8887
Mejores resultados en la época:  28
f1-score 0.5015673981191222
AUC según el mejor F1-score 0.8933634299487958
Confusion Matrix:
 [[1225  128]
 [  31   80]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8914
Precision:  0.3846
Recall:     0.7207
F1-score:   0.5016
Tiempo total para red 1: 16.40 segundos

Entrenando red 2 con capas [1536, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6947, Test Loss: 0.6922, F1: 0.2332, AUC: 0.8338
Epoch [10/30] Train Loss: 0.3651, Test Loss: 0.3425, F1: 0.4427, AUC: 0.8764
Epoch [20/30] Train Loss: 0.3123, Test Loss: 0.3567, F1: 0.4548, AUC: 0.8913
Mejores resultados en la época:  29
f1-score 0.5245901639344263
AUC según el mejor F1-score 0.8956606273679444
Confusion Matrix:
 [[1239  114]
 [  31   80]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.9010
Precision:  0.4124
Recall:     0.7207
F1-score:   0.5246

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6963, Test Loss: 0.6590, F1: 0.0000, AUC: 0.8276
Epoch [10/30] Train Loss: 0.3958, Test Loss: 0.4576, F1: 0.3750, AUC: 0.8675
Epoch [20/30] Train Loss: 0.3271, Test Loss: 0.5252, F1: 0.3602, AUC: 0.8863
Mejores resultados en la época:  24
f1-score 0.45478036175710596
AUC según el mejor F1-score 0.8903670854890366
Confusion Matrix:
 [[1165  188]
 [  23   88]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8559
Precision:  0.3188
Recall:     0.7928
F1-score:   0.4548

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6901, Test Loss: 0.6979, F1: 0.2211, AUC: 0.8455
Epoch [10/30] Train Loss: 0.3621, Test Loss: 0.3887, F1: 0.4289, AUC: 0.8794
Epoch [20/30] Train Loss: 0.3099, Test Loss: 0.3551, F1: 0.4595, AUC: 0.8925
Mejores resultados en la época:  22
f1-score 0.4835164835164835
AUC según el mejor F1-score 0.8931570151082346
Confusion Matrix:
 [[1188  165]
 [  23   88]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8716
Precision:  0.3478
Recall:     0.7928
F1-score:   0.4835

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6863, Test Loss: 0.6947, F1: 0.2296, AUC: 0.8496
Epoch [10/30] Train Loss: 0.3453, Test Loss: 0.6292, F1: 0.3122, AUC: 0.8812
Epoch [20/30] Train Loss: 0.2841, Test Loss: 0.4552, F1: 0.3917, AUC: 0.8929
Mejores resultados en la época:  24
f1-score 0.48863636363636365
AUC según el mejor F1-score 0.8948149923759681
Confusion Matrix:
 [[1198  155]
 [  25   86]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8770
Precision:  0.3568
Recall:     0.7748
F1-score:   0.4886
Tiempo total para red 2: 14.15 segundos

Entrenando red 3 con capas [1536, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6849, Test Loss: 0.6816, F1: 0.2745, AUC: 0.8473
Epoch [10/30] Train Loss: 0.3458, Test Loss: 0.3771, F1: 0.4279, AUC: 0.8844
Epoch [20/30] Train Loss: 0.2916, Test Loss: 0.6433, F1: 0.3328, AUC: 0.8930
Mejores resultados en la época:  8
f1-score 0.4797297297297297
AUC según el mejor F1-score 0.8803726120799291
Confusion Matrix:
 [[1239  114]
 [  40   71]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8948
Precision:  0.3838
Recall:     0.6396
F1-score:   0.4797

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6814, Test Loss: 0.6817, F1: 0.2683, AUC: 0.8436
Epoch [10/30] Train Loss: 0.3449, Test Loss: 0.4487, F1: 0.3828, AUC: 0.8839
Epoch [20/30] Train Loss: 0.2828, Test Loss: 0.4201, F1: 0.4199, AUC: 0.8949
Mejores resultados en la época:  25
f1-score 0.509090909090909
AUC según el mejor F1-score 0.8960667985058229
Confusion Matrix:
 [[1218  135]
 [  27   84]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8893
Precision:  0.3836
Recall:     0.7568
F1-score:   0.5091

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6807, Test Loss: 0.6913, F1: 0.2420, AUC: 0.8498
Epoch [10/30] Train Loss: 0.3817, Test Loss: 0.6319, F1: 0.3144, AUC: 0.8876
Epoch [20/30] Train Loss: 0.2539, Test Loss: 0.4089, F1: 0.4326, AUC: 0.8959
Mejores resultados en la época:  22
f1-score 0.5222929936305732
AUC según el mejor F1-score 0.8963264816923353
Confusion Matrix:
 [[1232  121]
 [  29   82]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8975
Precision:  0.4039
Recall:     0.7387
F1-score:   0.5223

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6909, Test Loss: 0.7422, F1: 0.1410, AUC: 0.8513
Epoch [10/30] Train Loss: 0.4314, Test Loss: 0.2605, F1: 0.5035, AUC: 0.8850
Epoch [20/30] Train Loss: 0.3005, Test Loss: 0.5999, F1: 0.3475, AUC: 0.8945
Mejores resultados en la época:  17
f1-score 0.50920245398773
AUC según el mejor F1-score 0.8933634299487958
Confusion Matrix:
 [[1221  132]
 [  28   83]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8907
Precision:  0.3860
Recall:     0.7477
F1-score:   0.5092
Tiempo total para red 3: 14.86 segundos

Entrenando red 4 con capas [1536, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6916, Test Loss: 0.7055, F1: 0.1612, AUC: 0.8585
Epoch [10/30] Train Loss: 0.3455, Test Loss: 0.4934, F1: 0.3694, AUC: 0.8880
Epoch [20/30] Train Loss: 0.3405, Test Loss: 0.4486, F1: 0.3835, AUC: 0.8954
Mejores resultados en la época:  26
f1-score 0.5043988269794721
AUC según el mejor F1-score 0.8962399206301646
Confusion Matrix:
 [[1209  144]
 [  25   86]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8846
Precision:  0.3739
Recall:     0.7748
F1-score:   0.5044

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6940, Test Loss: 0.7233, F1: 0.1410, AUC: 0.8472
Epoch [10/30] Train Loss: 0.3795, Test Loss: 0.3625, F1: 0.4478, AUC: 0.8867
Epoch [20/30] Train Loss: 0.2655, Test Loss: 0.6249, F1: 0.3445, AUC: 0.8973
Mejores resultados en la época:  9
f1-score 0.5103448275862069
AUC según el mejor F1-score 0.8857860077372272
Confusion Matrix:
 [[1248  105]
 [  37   74]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.9030
Precision:  0.4134
Recall:     0.6667
F1-score:   0.5103

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6916, Test Loss: 0.6441, F1: 0.0000, AUC: 0.8498
Epoch [10/30] Train Loss: 0.3394, Test Loss: 0.6292, F1: 0.3293, AUC: 0.8830
Epoch [20/30] Train Loss: 0.2801, Test Loss: 0.6831, F1: 0.3311, AUC: 0.8922
Mejores resultados en la época:  16
f1-score 0.5030674846625767
AUC según el mejor F1-score 0.8929838929838929
Confusion Matrix:
 [[1220  133]
 [  29   82]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8893
Precision:  0.3814
Recall:     0.7387
F1-score:   0.5031

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6968, Test Loss: 0.7406, F1: 0.1410, AUC: 0.8386
Epoch [10/30] Train Loss: 0.3775, Test Loss: 0.6599, F1: 0.3027, AUC: 0.8811
Epoch [20/30] Train Loss: 0.3397, Test Loss: 0.6063, F1: 0.3333, AUC: 0.8913
Mejores resultados en la época:  22
f1-score 0.4941860465116279
AUC según el mejor F1-score 0.8930837711325517
Confusion Matrix:
 [[1205  148]
 [  26   85]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8811
Precision:  0.3648
Recall:     0.7658
F1-score:   0.4942
Tiempo total para red 4: 15.49 segundos

Entrenando red 5 con capas [1536, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6943, Test Loss: 0.6507, F1: 0.0000, AUC: 0.8495
Epoch [10/30] Train Loss: 0.3483, Test Loss: 0.7342, F1: 0.3008, AUC: 0.8894
Epoch [20/30] Train Loss: 0.2510, Test Loss: 0.4302, F1: 0.4097, AUC: 0.8951
Mejores resultados en la época:  14
f1-score 0.5292096219931272
AUC según el mejor F1-score 0.8928706977487466
Confusion Matrix:
 [[1250  103]
 [  34   77]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.9064
Precision:  0.4278
Recall:     0.6937
F1-score:   0.5292

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6920, Test Loss: 0.6821, F1: 0.3730, AUC: 0.8526
Epoch [10/30] Train Loss: 0.3418, Test Loss: 0.5061, F1: 0.3562, AUC: 0.8854
Epoch [20/30] Train Loss: 0.2483, Test Loss: 0.3726, F1: 0.4639, AUC: 0.8963
Mejores resultados en la época:  16
f1-score 0.535593220338983
AUC según el mejor F1-score 0.8942090649407722
Confusion Matrix:
 [[1248  105]
 [  32   79]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.9064
Precision:  0.4293
Recall:     0.7117
F1-score:   0.5356

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6815, Test Loss: 0.7043, F1: 0.2204, AUC: 0.8506
Epoch [10/30] Train Loss: 0.3507, Test Loss: 0.9053, F1: 0.2533, AUC: 0.8879
Epoch [20/30] Train Loss: 0.2547, Test Loss: 0.4305, F1: 0.4124, AUC: 0.8960
Mejores resultados en la época:  26
f1-score 0.5418326693227091
AUC según el mejor F1-score 0.8944154797813335
Confusion Matrix:
 [[1281   72]
 [  43   68]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.9214
Precision:  0.4857
Recall:     0.6126
F1-score:   0.5418

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6890, Test Loss: 0.5768, F1: 0.0000, AUC: 0.8533
Epoch [10/30] Train Loss: 0.2970, Test Loss: 0.4025, F1: 0.4198, AUC: 0.8926
Epoch [20/30] Train Loss: 0.2413, Test Loss: 0.5736, F1: 0.3586, AUC: 0.8951
Mejores resultados en la época:  23
f1-score 0.49299719887955185
AUC según el mejor F1-score 0.8951079682787
Confusion Matrix:
 [[1195  158]
 [  23   88]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8764
Precision:  0.3577
Recall:     0.7928
F1-score:   0.4930
Tiempo total para red 5: 16.11 segundos

Entrenando red 6 con capas [1536, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6936, Test Loss: 0.6657, F1: 0.0000, AUC: 0.8580
Epoch [10/30] Train Loss: 0.3214, Test Loss: 0.6795, F1: 0.3239, AUC: 0.8887
Epoch [20/30] Train Loss: 0.2603, Test Loss: 0.4472, F1: 0.3901, AUC: 0.8967
Mejores resultados en la época:  12
f1-score 0.5126582278481012
AUC según el mejor F1-score 0.8921848677946239
Confusion Matrix:
 [[1229  124]
 [  30   81]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8948
Precision:  0.3951
Recall:     0.7297
F1-score:   0.5127

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6940, Test Loss: 0.6943, F1: 0.1410, AUC: 0.8317
Epoch [10/30] Train Loss: 0.3577, Test Loss: 0.4077, F1: 0.3904, AUC: 0.8825
Epoch [20/30] Train Loss: 0.2744, Test Loss: 0.3450, F1: 0.4697, AUC: 0.8957
Mejores resultados en la época:  13
f1-score 0.5121107266435986
AUC según el mejor F1-score 0.8898144263997922
Confusion Matrix:
 [[1249  104]
 [  37   74]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.9037
Precision:  0.4157
Recall:     0.6667
F1-score:   0.5121

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6933, Test Loss: 0.7028, F1: 0.1410, AUC: 0.8521
Epoch [10/30] Train Loss: 0.4413, Test Loss: 0.3765, F1: 0.4462, AUC: 0.8839
Epoch [20/30] Train Loss: 0.2701, Test Loss: 0.2453, F1: 0.5306, AUC: 0.8961
Mejores resultados en la época:  20
f1-score 0.5306122448979592
AUC según el mejor F1-score 0.8961400424815059
Confusion Matrix:
 [[1248  105]
 [  33   78]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.9057
Precision:  0.4262
Recall:     0.7027
F1-score:   0.5306

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6925, Test Loss: 0.7370, F1: 0.1410, AUC: 0.8462
Epoch [10/30] Train Loss: 0.3824, Test Loss: 0.3604, F1: 0.4251, AUC: 0.8894
Epoch [20/30] Train Loss: 0.3065, Test Loss: 0.2479, F1: 0.5165, AUC: 0.8966
Mejores resultados en la época:  27
f1-score 0.5376344086021505
AUC según el mejor F1-score 0.8950014315867975
Confusion Matrix:
 [[1260   93]
 [  36   75]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.9119
Precision:  0.4464
Recall:     0.6757
F1-score:   0.5376
Tiempo total para red 6: 19.47 segundos
Saved on: outputs_only_text_pseudo/2/gpt

==============================
Model: Logistic Regression
Accuracy:  0.8005
Precision: 0.2493
Recall:    0.8108
F1-score:  0.3814
              precision    recall  f1-score   support

           0       0.98      0.80      0.88      1353
           1       0.25      0.81      0.38       111

    accuracy                           0.80      1464
   macro avg       0.62      0.81      0.63      1464
weighted avg       0.93      0.80      0.84      1464

[[1082  271]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8613
Precision:  0.3321
Recall:     0.8198
F1-score:   0.4727
Tiempo total para red 3: 14.85 segundos

Entrenando red 4 con capas [1536, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6859, Test Loss: 0.6182, F1: 0.2192, AUC: 0.8495
Epoch [10/30] Train Loss: 0.3794, Test Loss: 0.5296, F1: 0.3456, AUC: 0.8869
Epoch [20/30] Train Loss: 0.2789, Test Loss: 0.3371, F1: 0.4656, AUC: 0.8944
Mejores resultados en la época:  11
f1-score 0.5015479876160991
AUC según el mejor F1-score 0.8883961566888395
Confusion Matrix:
 [[1222  131]
 [  30   81]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8900
Precision:  0.3821
Recall:     0.7297
F1-score:   0.5015

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6940, Test Loss: 0.6973, F1: 0.1479, AUC: 0.8445
Epoch [10/30] Train Loss: 0.3700, Test Loss: 0.3207, F1: 0.4560, AUC: 0.8857
Epoch [20/30] Train Loss: 0.3007, Test Loss: 0.5024, F1: 0.3609, AUC: 0.8950
Mejores resultados en la época:  26
f1-score 0.5214521452145214
AUC según el mejor F1-score 0.895394285638188
Confusion Matrix:
 [[1240  113]
 [  32   79]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.9010
Precision:  0.4115
Recall:     0.7117
F1-score:   0.5215

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6884, Test Loss: 0.6827, F1: 0.2893, AUC: 0.8498
Epoch [10/30] Train Loss: 0.3697, Test Loss: 0.4433, F1: 0.3905, AUC: 0.8872
Epoch [20/30] Train Loss: 0.2943, Test Loss: 0.4714, F1: 0.3796, AUC: 0.8933
Mejores resultados en la época:  18
f1-score 0.48179271708683474
AUC según el mejor F1-score 0.8951079682787001
Confusion Matrix:
 [[1193  160]
 [  25   86]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8736
Precision:  0.3496
Recall:     0.7748
F1-score:   0.4818

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6916, Test Loss: 0.6640, F1: 0.1550, AUC: 0.8485
Epoch [10/30] Train Loss: 0.3508, Test Loss: 0.4607, F1: 0.3900, AUC: 0.8892
Epoch [20/30] Train Loss: 0.2457, Test Loss: 0.4988, F1: 0.3825, AUC: 0.8951
Mejores resultados en la época:  19
f1-score 0.5014409221902018
AUC según el mejor F1-score 0.8949947730435535
Confusion Matrix:
 [[1204  149]
 [  24   87]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8818
Precision:  0.3686
Recall:     0.7838
F1-score:   0.5014
Tiempo total para red 4: 15.50 segundos

Entrenando red 5 con capas [1536, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6942, Test Loss: 0.6936, F1: 0.2363, AUC: 0.8443
Epoch [10/30] Train Loss: 0.3621, Test Loss: 0.7175, F1: 0.2814, AUC: 0.8888
Epoch [20/30] Train Loss: 0.2869, Test Loss: 0.3818, F1: 0.4259, AUC: 0.8948
Mejores resultados en la época:  16
f1-score 0.4985507246376812
AUC según el mejor F1-score 0.8945553091894556
Confusion Matrix:
 [[1205  148]
 [  25   86]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8818
Precision:  0.3675
Recall:     0.7748
F1-score:   0.4986

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6872, Test Loss: 0.7011, F1: 0.2170, AUC: 0.8510
Epoch [10/30] Train Loss: 0.3668, Test Loss: 0.3550, F1: 0.4491, AUC: 0.8835
Epoch [20/30] Train Loss: 0.2667, Test Loss: 0.5927, F1: 0.3506, AUC: 0.8975
Mejores resultados en la época:  18
f1-score 0.49025069637883006
AUC según el mejor F1-score 0.895480846700359
Confusion Matrix:
 [[1193  160]
 [  23   88]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8750
Precision:  0.3548
Recall:     0.7928
F1-score:   0.4903

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6949, Test Loss: 0.6839, F1: 0.1679, AUC: 0.8407
Epoch [10/30] Train Loss: 0.3154, Test Loss: 0.3859, F1: 0.4303, AUC: 0.8903
Epoch [20/30] Train Loss: 0.3018, Test Loss: 0.5934, F1: 0.3380, AUC: 0.8953
Mejores resultados en la época:  17
f1-score 0.4956772334293948
AUC según el mejor F1-score 0.895760505516603
Confusion Matrix:
 [[1203  150]
 [  25   86]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8805
Precision:  0.3644
Recall:     0.7748
F1-score:   0.4957

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6962, Test Loss: 0.6651, F1: 0.0000, AUC: 0.8533
Epoch [10/30] Train Loss: 0.3792, Test Loss: 0.3593, F1: 0.4324, AUC: 0.8870
Epoch [20/30] Train Loss: 0.3554, Test Loss: 0.5426, F1: 0.3443, AUC: 0.8914
Mejores resultados en la época:  17
f1-score 0.5310344827586206
AUC según el mejor F1-score 0.8922248190540873
Confusion Matrix:
 [[1251  102]
 [  34   77]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.9071
Precision:  0.4302
Recall:     0.6937
F1-score:   0.5310
Tiempo total para red 5: 16.12 segundos

Entrenando red 6 con capas [1536, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6917, Test Loss: 0.5880, F1: 0.0000, AUC: 0.8525
Epoch [10/30] Train Loss: 0.3287, Test Loss: 0.3986, F1: 0.4262, AUC: 0.8912
Epoch [20/30] Train Loss: 0.2535, Test Loss: 0.4721, F1: 0.3909, AUC: 0.8955
Mejores resultados en la época:  14
f1-score 0.5073746312684366
AUC según el mejor F1-score 0.8941225038786016
Confusion Matrix:
 [[1211  142]
 [  25   86]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8859
Precision:  0.3772
Recall:     0.7748
F1-score:   0.5074

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6941, Test Loss: 0.6873, F1: 0.0000, AUC: 0.8602
Epoch [10/30] Train Loss: 0.3763, Test Loss: 0.4085, F1: 0.4149, AUC: 0.8882
Epoch [20/30] Train Loss: 0.2866, Test Loss: 0.3675, F1: 0.4527, AUC: 0.8955
Mejores resultados en la época:  21
f1-score 0.5088757396449705
AUC según el mejor F1-score 0.8965262379896525
Confusion Matrix:
 [[1212  141]
 [  25   86]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8866
Precision:  0.3789
Recall:     0.7748
F1-score:   0.5089

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6944, Test Loss: 0.6361, F1: 0.0000, AUC: 0.5834
Epoch [10/30] Train Loss: 0.3642, Test Loss: 0.4784, F1: 0.3693, AUC: 0.8780
Epoch [20/30] Train Loss: 0.2978, Test Loss: 0.5152, F1: 0.3658, AUC: 0.8939
Mejores resultados en la época:  29
f1-score 0.4943181818181818
AUC según el mejor F1-score 0.8961000912220425
Confusion Matrix:
 [[1199  154]
 [  24   87]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8784
Precision:  0.3610
Recall:     0.7838
F1-score:   0.4943

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6941, Test Loss: 0.6826, F1: 0.0000, AUC: 0.8553
Epoch [10/30] Train Loss: 0.3636, Test Loss: 0.6823, F1: 0.2918, AUC: 0.8889
Epoch [20/30] Train Loss: 0.2483, Test Loss: 0.4545, F1: 0.3884, AUC: 0.8971
Mejores resultados en la época:  11
f1-score 0.49846153846153846
AUC según el mejor F1-score 0.8886758155050838
Confusion Matrix:
 [[1220  133]
 [  30   81]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8887
Precision:  0.3785
Recall:     0.7297
F1-score:   0.4985
Tiempo total para red 6: 19.47 segundos
Saved on: outputs_only_text_pseudo/2/gpt

==============================
Model: Logistic Regression
Accuracy:  0.8005
Precision: 0.2493
Recall:    0.8108
F1-score:  0.3814
              precision    recall  f1-score   support

           0       0.98      0.80      0.88      1353
           1       0.25      0.81      0.38       111

    accuracy                           0.80      1464
   macro avg       0.62      0.81      0.63      1464
weighted avg       0.93      0.80      0.84      1464

[[1082  271]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:09:34] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:09:34] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
 [  21   90]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8258
Precision: 0.2805
Recall:    0.8288
F1-score:  0.4191
              precision    recall  f1-score   support

           0       0.98      0.83      0.90      1353
           1       0.28      0.83      0.42       111

    accuracy                           0.83      1464
   macro avg       0.63      0.83      0.66      1464
weighted avg       0.93      0.83      0.86      1464

[[1117  236]
 [  19   92]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_svm.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6667
Precision: 0.1503
Recall:    0.7297
F1-score:  0.2492
              precision    recall  f1-score   support

           0       0.97      0.66      0.79      1353
           1       0.15      0.73      0.25       111

    accuracy                           0.67      1464
   macro avg       0.56      0.70      0.52      1464
weighted avg       0.91      0.67      0.75      1464

[[895 458]
 [ 30  81]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.7828
Precision: 0.2255
Recall:    0.7658
F1-score:  0.3484
              precision    recall  f1-score   support

           0       0.98      0.78      0.87      1353
           1       0.23      0.77      0.35       111

    accuracy                           0.78      1464
   macro avg       0.60      0.77      0.61      1464
weighted avg       0.92      0.78      0.83      1464

[[1061  292]
 [  26   85]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8026
Precision: 0.2528
Recall:    0.8198
F1-score:  0.3864
              precision    recall  f1-score   support

           0       0.98      0.80      0.88      1353
           1       0.25      0.82      0.39       111

    accuracy                           0.80      1464
   macro avg       0.62      0.81      0.63      1464
weighted avg       0.93      0.80      0.84      1464

[[1084  269]
 [  20   91]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7944
Precision: 0.2376
Recall:    0.7748
F1-score:  0.3636
              precision    recall  f1-score   support

           0       0.98      0.80      0.88      1353
           1       0.24      0.77      0.36       111

    accuracy                           0.79      1464
   macro avg       0.61      0.79      0.62      1464
weighted avg       0.92      0.79      0.84      1464

[[1077  276]
 [  25   86]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/naive_bayes_model.pkl


Resumen de métricas:
SVM: {'accuracy': 0.8258, 'precision': 0.2805, 'recall': 0.8288, 'f1_score': 0.4191}
XGBoost: {'accuracy': 0.8026, 'precision': 0.2528, 'recall': 0.8198, 'f1_score': 0.3864}
Logistic Regression: {'accuracy': 0.8005, 'precision': 0.2493, 'recall': 0.8108, 'f1_score': 0.3814}
Naive Bayes: {'accuracy': 0.7944, 'precision': 0.2376, 'recall': 0.7748, 'f1_score': 0.3636}
Random Forest: {'accuracy': 0.7828, 'precision': 0.2255, 'recall': 0.7658, 'f1_score': 0.3484}
Decision Tree: {'accuracy': 0.6667, 'precision': 0.1503, 'recall': 0.7297, 'f1_score': 0.2492}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: GPT
MLP_959489: {'accuracy': 0.8763661202185792, 'precision': 0.35772357723577236, 'recall': 0.7927927927927928, 'f1_score': 0.5418326693227091, 'f1_score_avg': 0.5249081776335928}
MLP_2273281: {'accuracy': 0.9118852459016393, 'precision': 0.44642857142857145, 'recall': 0.6756756756756757, 'f1_score': 0.5418326693227091, 'f1_score_avg': 0.5232539019979524}
MLP_100481: {'accuracy': 0.8770491803278688, 'precision': 0.35684647302904565, 'recall': 0.7747747747747747, 'f1_score': 0.5245901639344263, 'f1_score_avg': 0.4878808432110948}
MLP_207105: {'accuracy': 0.8907103825136612, 'precision': 0.386046511627907, 'recall': 0.7477477477477478, 'f1_score': 0.5245901639344263, 'f1_score_avg': 0.5050790216097355}
MLP_436737: {'accuracy': 0.8811475409836066, 'precision': 0.3648068669527897, 'recall': 0.7657657657657657, 'f1_score': 0.5245901639344263, 'f1_score_avg': 0.502999296434971}
MLP_49217: {'accuracy': 0.8913934426229508, 'precision': 0.38461538461538464, 'recall': 0.7207207207207207, 'f1_score': 0.5209003215434084, 'f1_score_avg': 0.5073087163049541}
SVM: {'accuracy': 0.8258, 'precision': 0.2805, 'recall': 0.8288, 'f1_score': 0.4191}
XGBoost: {'accuracy': 0.8026, 'precision': 0.2528, 'recall': 0.8198, 'f1_score': 0.3864}
Logistic Regression: {'accuracy': 0.8005, 'precision': 0.2493, 'recall': 0.8108, 'f1_score': 0.3814}
Naive Bayes: {'accuracy': 0.7944, 'precision': 0.2376, 'recall': 0.7748, 'f1_score': 0.3636}
Random Forest: {'accuracy': 0.7828, 'precision': 0.2255, 'recall': 0.7658, 'f1_score': 0.3484}
Decision Tree: {'accuracy': 0.6667, 'precision': 0.1503, 'recall': 0.7297, 'f1_score': 0.2492}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: False
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['lyrics']
Numeric Columns: Not used
====================================

 [  21   90]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8258
Precision: 0.2805
Recall:    0.8288
F1-score:  0.4191
              precision    recall  f1-score   support

           0       0.98      0.83      0.90      1353
           1       0.28      0.83      0.42       111

    accuracy                           0.83      1464
   macro avg       0.63      0.83      0.66      1464
weighted avg       0.93      0.83      0.86      1464

[[1117  236]
 [  19   92]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_svm.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6667
Precision: 0.1503
Recall:    0.7297
F1-score:  0.2492
              precision    recall  f1-score   support

           0       0.97      0.66      0.79      1353
           1       0.15      0.73      0.25       111

    accuracy                           0.67      1464
   macro avg       0.56      0.70      0.52      1464
weighted avg       0.91      0.67      0.75      1464

[[895 458]
 [ 30  81]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.7828
Precision: 0.2255
Recall:    0.7658
F1-score:  0.3484
              precision    recall  f1-score   support

           0       0.98      0.78      0.87      1353
           1       0.23      0.77      0.35       111

    accuracy                           0.78      1464
   macro avg       0.60      0.77      0.61      1464
weighted avg       0.92      0.78      0.83      1464

[[1061  292]
 [  26   85]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8026
Precision: 0.2528
Recall:    0.8198
F1-score:  0.3864
              precision    recall  f1-score   support

           0       0.98      0.80      0.88      1353
           1       0.25      0.82      0.39       111

    accuracy                           0.80      1464
   macro avg       0.62      0.81      0.63      1464
weighted avg       0.93      0.80      0.84      1464

[[1084  269]
 [  20   91]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7944
Precision: 0.2376
Recall:    0.7748
F1-score:  0.3636
              precision    recall  f1-score   support

           0       0.98      0.80      0.88      1353
           1       0.24      0.77      0.36       111

    accuracy                           0.79      1464
   macro avg       0.61      0.79      0.62      1464
weighted avg       0.92      0.79      0.84      1464

[[1077  276]
 [  25   86]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/naive_bayes_model.pkl


Resumen de métricas:
SVM: {'accuracy': 0.8258, 'precision': 0.2805, 'recall': 0.8288, 'f1_score': 0.4191}
XGBoost: {'accuracy': 0.8026, 'precision': 0.2528, 'recall': 0.8198, 'f1_score': 0.3864}
Logistic Regression: {'accuracy': 0.8005, 'precision': 0.2493, 'recall': 0.8108, 'f1_score': 0.3814}
Naive Bayes: {'accuracy': 0.7944, 'precision': 0.2376, 'recall': 0.7748, 'f1_score': 0.3636}
Random Forest: {'accuracy': 0.7828, 'precision': 0.2255, 'recall': 0.7658, 'f1_score': 0.3484}
Decision Tree: {'accuracy': 0.6667, 'precision': 0.1503, 'recall': 0.7297, 'f1_score': 0.2492}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: GPT
MLP_959489: {'accuracy': 0.907103825136612, 'precision': 0.4301675977653631, 'recall': 0.6936936936936937, 'f1_score': 0.5310344827586206, 'f1_score_avg': 0.5038782843011317}
MLP_2273281: {'accuracy': 0.8886612021857924, 'precision': 0.37850467289719625, 'recall': 0.7297297297297297, 'f1_score': 0.5310344827586206, 'f1_score_avg': 0.5022575227982818}
MLP_436737: {'accuracy': 0.8818306010928961, 'precision': 0.3686440677966102, 'recall': 0.7837837837837838, 'f1_score': 0.5214521452145214, 'f1_score_avg': 0.5015584430269142}
MLP_207105: {'accuracy': 0.8613387978142076, 'precision': 0.33211678832116787, 'recall': 0.8198198198198198, 'f1_score': 0.5151515151515151, 'f1_score_avg': 0.48641138206009943}
MLP_100481: {'accuracy': 0.8859289617486339, 'precision': 0.37610619469026546, 'recall': 0.7657657657657657, 'f1_score': 0.5044510385756676, 'f1_score_avg': 0.48876899667638163}
MLP_49217: {'accuracy': 0.8729508196721312, 'precision': 0.3469387755102041, 'recall': 0.7657657657657657, 'f1_score': 0.47752808988764045, 'f1_score_avg': 0.43093124096671465}
SVM: {'accuracy': 0.8258, 'precision': 0.2805, 'recall': 0.8288, 'f1_score': 0.4191}
XGBoost: {'accuracy': 0.8026, 'precision': 0.2528, 'recall': 0.8198, 'f1_score': 0.3864}
Logistic Regression: {'accuracy': 0.8005, 'precision': 0.2493, 'recall': 0.8108, 'f1_score': 0.3814}
Naive Bayes: {'accuracy': 0.7944, 'precision': 0.2376, 'recall': 0.7748, 'f1_score': 0.3636}
Random Forest: {'accuracy': 0.7828, 'precision': 0.2255, 'recall': 0.7658, 'f1_score': 0.3484}
Decision Tree: {'accuracy': 0.6667, 'precision': 0.1503, 'recall': 0.7297, 'f1_score': 0.2492}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: False
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['lyrics']
Numeric Columns: Not used
====================================

