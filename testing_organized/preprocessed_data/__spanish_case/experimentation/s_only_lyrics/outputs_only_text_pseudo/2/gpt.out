2025-10-30 04:40:19.757660: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-30 04:40:19.757660: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__spanish_case/experimentation/s_only_lyrics/main_only_text_gpt.py:273: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__spanish_case/experimentation/s_only_lyrics/main_only_text_gpt.py:273: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
For TF-IDF embbedings you are selecteing this columns:
--> ['lyrics']
Your are not adding cols numercis
You are executing with [ALL] dataset
--> PaTH:  ../../../../../data/spanish/LB_T/LB_fuss/lb_khipu_T.npy

##################################################
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../../../../data/spanish/dataset/oficialDatasetEAIM2026_pseudolabeling.csv
Label distribution: {0: 6077, 1: 1242}
X shape: (7319, 1536)
y shape: (7319,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 4861, 1: 994}
Label distribution en TEST: {0: 1216, 1: 248}


==================================================
Data antes del undersampling ...
X: (5855, 1536)
y: (5855,)
Apliying UNDERSAMPLE
994
Label distribution: {0: 994, 1: 994}
X shape: (1988, 1536)
y shape: (1988,)
Resultados con MLP

Entrenando red 1 con capas [1536, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6825, Test Loss: 0.6858, F1: 0.3974, AUC: 0.8058
Epoch [10/30] Train Loss: 0.5380, Test Loss: 0.4220, F1: 0.5472, AUC: 0.8277
Epoch [20/30] Train Loss: 0.4921, Test Loss: 0.4445, F1: 0.5533, AUC: 0.8458
Mejores resultados en la época:  16
f1-score 0.5756457564575646
AUC según el mejor F1-score 0.8410242466044142
Confusion Matrix:
 [[1078  138]
 [  92  156]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8429
Precision:  0.5306
Recall:     0.6290
F1-score:   0.5756

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6883, Test Loss: 0.7328, F1: 0.3210, AUC: 0.7979
Epoch [10/30] Train Loss: 0.5080, Test Loss: 0.4101, F1: 0.5760, AUC: 0.8416
Epoch [20/30] Train Loss: 0.4659, Test Loss: 0.7298, F1: 0.4412, AUC: 0.8511
Mejores resultados en la época:  10
f1-score 0.5760286225402504
AUC según el mejor F1-score 0.8415829928904923
Confusion Matrix:
 [[1066  150]
 [  87  161]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8381
Precision:  0.5177
Recall:     0.6492
F1-score:   0.5760

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6692, Test Loss: 0.7043, F1: 0.3656, AUC: 0.7971
Epoch [10/30] Train Loss: 0.4974, Test Loss: 0.6061, F1: 0.4682, AUC: 0.8430
Epoch [20/30] Train Loss: 0.4676, Test Loss: 0.4163, F1: 0.5585, AUC: 0.8499
Mejores resultados en la época:  14
f1-score 0.5735294117647058
AUC según el mejor F1-score 0.8465354414261461
Confusion Matrix:
 [[1076  140]
 [  92  156]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8415
Precision:  0.5270
Recall:     0.6290
F1-score:   0.5735

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6946, Test Loss: 0.7284, F1: 0.2897, AUC: 0.7871
Epoch [10/30] Train Loss: 0.6130, Test Loss: 0.6683, F1: 0.4657, AUC: 0.8119
Epoch [20/30] Train Loss: 0.5572, Test Loss: 0.6041, F1: 0.5170, AUC: 0.8202
Mejores resultados en la época:  27
f1-score 0.5678119349005425
AUC según el mejor F1-score 0.8141364468378607
Confusion Matrix:
 [[1068  148]
 [  91  157]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8367
Precision:  0.5148
Recall:     0.6331
F1-score:   0.5678
Tiempo total para red 1: 26.19 segundos

Entrenando red 2 con capas [1536, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6850, Test Loss: 0.5814, F1: 0.1212, AUC: 0.8004
Epoch [10/30] Train Loss: 0.5618, Test Loss: 0.4792, F1: 0.5417, AUC: 0.8421
Epoch [20/30] Train Loss: 0.4503, Test Loss: 0.3269, F1: 0.5568, AUC: 0.8514
Mejores resultados en la época:  16
f1-score 0.575
AUC según el mejor F1-score 0.849065550721562
Confusion Matrix:
 [[1065  151]
 [  87  161]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8374
Precision:  0.5160
Recall:     0.6492
F1-score:   0.5750

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6830, Test Loss: 0.6548, F1: 0.5015, AUC: 0.7978
Epoch [10/30] Train Loss: 0.5062, Test Loss: 0.3536, F1: 0.5536, AUC: 0.8431
Epoch [20/30] Train Loss: 0.4426, Test Loss: 0.4270, F1: 0.5559, AUC: 0.8516
Mejores resultados en la época:  23
f1-score 0.5676691729323309
AUC según el mejor F1-score 0.8519504721986418
Confusion Matrix:
 [[1083  133]
 [  97  151]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8429
Precision:  0.5317
Recall:     0.6089
F1-score:   0.5677

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6821, Test Loss: 0.7841, F1: 0.2902, AUC: 0.8004
Epoch [10/30] Train Loss: 0.5046, Test Loss: 0.6789, F1: 0.4393, AUC: 0.8411
Epoch [20/30] Train Loss: 0.4932, Test Loss: 0.5309, F1: 0.4925, AUC: 0.8504
Mejores resultados en la época:  9
f1-score 0.5693950177935944
AUC según el mejor F1-score 0.8389583775466893
Confusion Matrix:
 [[1062  154]
 [  88  160]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8347
Precision:  0.5096
Recall:     0.6452
F1-score:   0.5694

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6823, Test Loss: 0.6403, F1: 0.5259, AUC: 0.8021
Epoch [10/30] Train Loss: 0.4934, Test Loss: 0.3579, F1: 0.5649, AUC: 0.8470
Epoch [20/30] Train Loss: 0.4772, Test Loss: 0.3275, F1: 0.5571, AUC: 0.8516
Mejores resultados en la época:  11
f1-score 0.5758620689655173
AUC según el mejor F1-score 0.8483227663412565
Confusion Matrix:
 [[1051  165]
 [  81  167]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8320
Precision:  0.5030
Recall:     0.6734
F1-score:   0.5759
Tiempo total para red 2: 25.91 segundos

Entrenando red 3 con capas [1536, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6910, Test Loss: 0.6498, F1: 0.0000, AUC: 0.7991
Epoch [10/30] Train Loss: 0.5776, Test Loss: 0.5006, F1: 0.5118, AUC: 0.8490
Epoch [20/30] Train Loss: 0.4417, Test Loss: 0.3362, F1: 0.5751, AUC: 0.8529
Mejores resultados en la época:  15
f1-score 0.5776173285198556
AUC según el mejor F1-score 0.851048519736842
Confusion Matrix:
 [[1070  146]
 [  88  160]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8402
Precision:  0.5229
Recall:     0.6452
F1-score:   0.5776

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6948, Test Loss: 0.6792, F1: 0.0239, AUC: 0.7966
Epoch [10/30] Train Loss: 0.4917, Test Loss: 0.4162, F1: 0.5613, AUC: 0.8432
Epoch [20/30] Train Loss: 0.4830, Test Loss: 0.3292, F1: 0.5591, AUC: 0.8520
Mejores resultados en la época:  12
f1-score 0.5725338491295938
AUC según el mejor F1-score 0.8483393463497454
Confusion Matrix:
 [[1095  121]
 [ 100  148]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8490
Precision:  0.5502
Recall:     0.5968
F1-score:   0.5725

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6866, Test Loss: 0.6991, F1: 0.3683, AUC: 0.7990
Epoch [10/30] Train Loss: 0.5208, Test Loss: 0.6246, F1: 0.4662, AUC: 0.8483
Epoch [20/30] Train Loss: 0.4533, Test Loss: 0.5042, F1: 0.5091, AUC: 0.8513
Mejores resultados en la época:  14
f1-score 0.5709156193895871
AUC según el mejor F1-score 0.8498945511460101
Confusion Matrix:
 [[1066  150]
 [  89  159]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8367
Precision:  0.5146
Recall:     0.6411
F1-score:   0.5709

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6901, Test Loss: 0.6894, F1: 0.3926, AUC: 0.7971
Epoch [10/30] Train Loss: 0.4907, Test Loss: 0.3501, F1: 0.5584, AUC: 0.8477
Epoch [20/30] Train Loss: 0.4344, Test Loss: 0.5283, F1: 0.5166, AUC: 0.8511
Mejores resultados en la época:  17
f1-score 0.5665236051502146
AUC según el mejor F1-score 0.8499509231748728
Confusion Matrix:
 [[1130   86]
 [ 116  132]]
For TF-IDF embbedings you are selecteing this columns:
--> ['lyrics']
Your are not adding cols numercis
You are executing with [ALL] dataset
--> PaTH:  ../../../../../data/spanish/LB_T/LB_fuss/lb_khipu_T.npy

##################################################
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../../../../data/spanish/dataset/oficialDatasetEAIM2026_pseudolabeling.csv
Label distribution: {0: 6077, 1: 1242}
X shape: (7319, 1536)
y shape: (7319,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 4861, 1: 994}
Label distribution en TEST: {0: 1216, 1: 248}


==================================================
Data antes del undersampling ...
X: (5855, 1536)
y: (5855,)
Apliying UNDERSAMPLE
994
Label distribution: {0: 994, 1: 994}
X shape: (1988, 1536)
y shape: (1988,)
Resultados con MLP

Entrenando red 1 con capas [1536, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6924, Test Loss: 0.5238, F1: 0.0000, AUC: 0.8031
Epoch [10/30] Train Loss: 0.5228, Test Loss: 0.4241, F1: 0.5600, AUC: 0.8403
Epoch [20/30] Train Loss: 0.4831, Test Loss: 0.6235, F1: 0.4562, AUC: 0.8516
Mejores resultados en la época:  15
f1-score 0.5784114052953157
AUC según el mejor F1-score 0.849048970713073
Confusion Matrix:
 [[1115  101]
 [ 106  142]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8586
Precision:  0.5844
Recall:     0.5726
F1-score:   0.5784

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6874, Test Loss: 0.7452, F1: 0.2928, AUC: 0.8050
Epoch [10/30] Train Loss: 0.5259, Test Loss: 0.3834, F1: 0.5657, AUC: 0.8408
Epoch [20/30] Train Loss: 0.4729, Test Loss: 0.4918, F1: 0.5192, AUC: 0.8514
Mejores resultados en la época:  23
f1-score 0.5787545787545788
AUC según el mejor F1-score 0.852232332342954
Confusion Matrix:
 [[1076  140]
 [  90  158]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8429
Precision:  0.5302
Recall:     0.6371
F1-score:   0.5788

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6794, Test Loss: 0.6742, F1: 0.4203, AUC: 0.8029
Epoch [10/30] Train Loss: 0.5044, Test Loss: 0.6403, F1: 0.4578, AUC: 0.8434
Epoch [20/30] Train Loss: 0.4741, Test Loss: 0.4067, F1: 0.5593, AUC: 0.8514
Mejores resultados en la época:  19
f1-score 0.5734513274336284
AUC según el mejor F1-score 0.8514630199490664
Confusion Matrix:
 [[1061  155]
 [  86  162]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8354
Precision:  0.5110
Recall:     0.6532
F1-score:   0.5735

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.7069, Test Loss: 0.6383, F1: 0.4954, AUC: 0.7932
Epoch [10/30] Train Loss: 0.5300, Test Loss: 0.4973, F1: 0.5279, AUC: 0.8361
Epoch [20/30] Train Loss: 0.4648, Test Loss: 0.3805, F1: 0.5634, AUC: 0.8477
Mejores resultados en la época:  17
f1-score 0.5764705882352941
AUC según el mejor F1-score 0.8462900573005093
Confusion Matrix:
 [[1101  115]
 [ 101  147]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8525
Precision:  0.5611
Recall:     0.5927
F1-score:   0.5765
Tiempo total para red 1: 26.18 segundos

Entrenando red 2 con capas [1536, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6847, Test Loss: 0.6084, F1: 0.0849, AUC: 0.8012
Epoch [10/30] Train Loss: 0.4867, Test Loss: 0.3792, F1: 0.5677, AUC: 0.8469
Epoch [20/30] Train Loss: 0.4688, Test Loss: 0.3698, F1: 0.5731, AUC: 0.8524
Mejores resultados en la época:  20
f1-score 0.573055028462998
AUC según el mejor F1-score 0.852394816426146
Confusion Matrix:
 [[1088  128]
 [  97  151]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8463
Precision:  0.5412
Recall:     0.6089
F1-score:   0.5731

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6958, Test Loss: 0.7233, F1: 0.2897, AUC: 0.8044
Epoch [10/30] Train Loss: 0.5003, Test Loss: 0.3752, F1: 0.5720, AUC: 0.8451
Epoch [20/30] Train Loss: 0.4756, Test Loss: 0.4341, F1: 0.5529, AUC: 0.8520
Mejores resultados en la época:  15
f1-score 0.5751633986928104
AUC según el mejor F1-score 0.8502626273344653
Confusion Matrix:
 [[1137   79]
 [ 116  132]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8668
Precision:  0.6256
Recall:     0.5323
F1-score:   0.5752

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6872, Test Loss: 0.7310, F1: 0.2902, AUC: 0.7843
Epoch [10/30] Train Loss: 0.4875, Test Loss: 0.4020, F1: 0.5704, AUC: 0.8453
Epoch [20/30] Train Loss: 0.4507, Test Loss: 0.7392, F1: 0.4383, AUC: 0.8523
Mejores resultados en la época:  17
f1-score 0.5763358778625954
AUC según el mejor F1-score 0.8519173121816639
Confusion Matrix:
 [[1091  125]
 [  97  151]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8484
Precision:  0.5471
Recall:     0.6089
F1-score:   0.5763

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6818, Test Loss: 0.5648, F1: 0.0080, AUC: 0.8018
Epoch [10/30] Train Loss: 0.5012, Test Loss: 0.5503, F1: 0.4835, AUC: 0.8469
Epoch [20/30] Train Loss: 0.4563, Test Loss: 0.5064, F1: 0.5092, AUC: 0.8516
Mejores resultados en la época:  27
f1-score 0.5692599620493358
AUC según el mejor F1-score 0.8505975435059423
Confusion Matrix:
 [[1087  129]
 [  98  150]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8449
Precision:  0.5376
Recall:     0.6048
F1-score:   0.5693
Tiempo total para red 2: 25.91 segundos

Entrenando red 3 con capas [1536, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6932, Test Loss: 0.6492, F1: 0.0000, AUC: 0.7957
Epoch [10/30] Train Loss: 0.4875, Test Loss: 0.3486, F1: 0.5652, AUC: 0.8516
Epoch [20/30] Train Loss: 0.4373, Test Loss: 0.3635, F1: 0.5605, AUC: 0.8520
Mejores resultados en la época:  21
f1-score 0.574468085106383
AUC según el mejor F1-score 0.8515790800084891
Confusion Matrix:
 [[1129   87]
 [ 113  135]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8634
Precision:  0.6081
Recall:     0.5444
F1-score:   0.5745

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6846, Test Loss: 0.7367, F1: 0.2944, AUC: 0.7986
Epoch [10/30] Train Loss: 0.4992, Test Loss: 0.4216, F1: 0.5539, AUC: 0.8472
Epoch [20/30] Train Loss: 0.4871, Test Loss: 0.4126, F1: 0.5623, AUC: 0.8502
Mejores resultados en la época:  7
f1-score 0.5746268656716418
AUC según el mejor F1-score 0.8392535016977929
Confusion Matrix:
 [[1082  134]
 [  94  154]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8443
Precision:  0.5347
Recall:     0.6210
F1-score:   0.5746

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6897, Test Loss: 0.6257, F1: 0.1269, AUC: 0.8042
Epoch [10/30] Train Loss: 0.5227, Test Loss: 0.3780, F1: 0.5725, AUC: 0.8472
Epoch [20/30] Train Loss: 0.4700, Test Loss: 0.5021, F1: 0.5126, AUC: 0.8507
Mejores resultados en la época:  16
f1-score 0.5731225296442688
AUC según el mejor F1-score 0.8501797272920204
Confusion Matrix:
 [[1103  113]
 [ 103  145]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8525
Precision:  0.5620
Recall:     0.5847
F1-score:   0.5731

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6927, Test Loss: 0.6642, F1: 0.0000, AUC: 0.8003
Epoch [10/30] Train Loss: 0.5048, Test Loss: 0.4257, F1: 0.5611, AUC: 0.8463
Epoch [20/30] Train Loss: 0.4433, Test Loss: 0.4229, F1: 0.5466, AUC: 0.8516
Mejores resultados en la época:  11
f1-score 0.5703275529865125
AUC según el mejor F1-score 0.8473080698217317
Confusion Matrix:
 [[1093  123]
 [ 100  148]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8477
Precision:  0.5461
Recall:     0.5968
F1-score:   0.5703
Tiempo total para red 3: 28.63 segundos

Entrenando red 4 con capas [1536, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6937, Test Loss: 0.6986, F1: 0.3082, AUC: 0.8030
Epoch [10/30] Train Loss: 0.5166, Test Loss: 0.5813, F1: 0.4669, AUC: 0.8478
Epoch [20/30] Train Loss: 0.4770, Test Loss: 0.3623, F1: 0.5636, AUC: 0.8528
Mejores resultados en la época:  8
f1-score 0.5807692307692308
AUC según el mejor F1-score 0.8452853087860781
Confusion Matrix:
 [[1095  121]
 [  97  151]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8511
Precision:  0.5551
Recall:     0.6089
F1-score:   0.5808

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6928, Test Loss: 0.7068, F1: 0.2899, AUC: 0.8039
Epoch [10/30] Train Loss: 0.5162, Test Loss: 0.4031, F1: 0.5658, AUC: 0.8436
Epoch [20/30] Train Loss: 0.4592, Test Loss: 0.4121, F1: 0.5657, AUC: 0.8530
Mejores resultados en la época:  13
f1-score 0.5775193798449613
AUC según el mejor F1-score 0.8503919514006792
Confusion Matrix:
 [[1097  119]
 [  99  149]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8511
Precision:  0.5560
Recall:     0.6008
F1-score:   0.5775

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6916, Test Loss: 0.7105, F1: 0.2967, AUC: 0.8027
Epoch [10/30] Train Loss: 0.5032, Test Loss: 0.5141, F1: 0.5074, AUC: 0.8474
Epoch [20/30] Train Loss: 0.4513, Test Loss: 0.4882, F1: 0.5250, AUC: 0.8527
Mejores resultados en la época:  14
f1-score 0.5744234800838575
AUC según el mejor F1-score 0.851642084040747
Confusion Matrix:
 [[1124   92]
 [ 111  137]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8613
Precision:  0.5983
Recall:     0.5524
F1-score:   0.5744

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6917, Test Loss: 0.7574, F1: 0.2897, AUC: 0.7996
Epoch [10/30] Train Loss: 0.4948, Test Loss: 0.3533, F1: 0.5577, AUC: 0.8482
Epoch [20/30] Train Loss: 0.4807, Test Loss: 0.3808, F1: 0.5521, AUC: 0.8511
Mejores resultados en la época:  12
f1-score 0.5735294117647058
AUC según el mejor F1-score 0.851048519736842
Confusion Matrix:
 [[1076  140]
 [  92  156]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8415
Precision:  0.5270
Recall:     0.6290
F1-score:   0.5735
Tiempo total para red 4: 30.47 segundos

Entrenando red 5 con capas [1536, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6927, Test Loss: 0.6966, F1: 0.3551, AUC: 0.8005
Epoch [10/30] Train Loss: 0.4850, Test Loss: 0.6520, F1: 0.4593, AUC: 0.8517
Epoch [20/30] Train Loss: 0.4324, Test Loss: 0.4744, F1: 0.5329, AUC: 0.8485
Mejores resultados en la época:  13
f1-score 0.5676691729323309
AUC según el mejor F1-score 0.8508860356536504
Confusion Matrix:
 [[1083  133]
 [  97  151]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8429
Precision:  0.5317
Recall:     0.6089
F1-score:   0.5677

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6927, Test Loss: 0.6679, F1: 0.0000, AUC: 0.7992
Epoch [10/30] Train Loss: 0.4900, Test Loss: 0.4041, F1: 0.5743, AUC: 0.8507
Epoch [20/30] Train Loss: 0.4363, Test Loss: 0.3266, F1: 0.5425, AUC: 0.8517
Mejores resultados en la época:  10
f1-score 0.5743243243243243
AUC según el mejor F1-score 0.8506539155348048
Confusion Matrix:
 [[1042  174]
 [  78  170]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8279
Precision:  0.4942
Recall:     0.6855
F1-score:   0.5743

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6899, Test Loss: 0.5994, F1: 0.0000, AUC: 0.8031
Epoch [10/30] Train Loss: 0.5760, Test Loss: 0.5265, F1: 0.5087, AUC: 0.8461
Epoch [20/30] Train Loss: 0.4298, Test Loss: 0.5839, F1: 0.4881, AUC: 0.8513
Mejores resultados en la época:  14
f1-score 0.5743801652892562
AUC según el mejor F1-score 0.8500437712224109
Confusion Matrix:
 [[1119   97]
 [ 109  139]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8593
Precision:  0.5890
Recall:     0.5605
F1-score:   0.5744

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6941, Test Loss: 0.7064, F1: 0.2930, AUC: 0.7994
Epoch [10/30] Train Loss: 0.5219, Test Loss: 0.4956, F1: 0.5165, AUC: 0.8481
Epoch [20/30] Train Loss: 0.4717, Test Loss: 0.5176, F1: 0.5070, AUC: 0.8514
Mejores resultados en la época:  12
f1-score 0.5724508050089445
AUC según el mejor F1-score 0.8502924713497454
Confusion Matrix:
 [[1065  151]
 [  88  160]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8367
Precision:  0.5145
Recall:     0.6452
F1-score:   0.5725
Tiempo total para red 5: 31.72 segundos

Entrenando red 6 con capas [1536, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6947, Test Loss: 0.6684, F1: 0.0000, AUC: 0.7999
Epoch [10/30] Train Loss: 0.6136, Test Loss: 0.6858, F1: 0.4579, AUC: 0.8456
Epoch [20/30] Train Loss: 0.4748, Test Loss: 0.7119, F1: 0.4472, AUC: 0.8523
Mejores resultados en la época:  13
f1-score 0.5772811918063314
AUC según el mejor F1-score 0.8497022230475383
Confusion Matrix:
 [[1082  134]
 [  93  155]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8449
Precision:  0.5363
Recall:     0.6250
F1-score:   0.5773

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6946, Test Loss: 0.6827, F1: 0.0000, AUC: 0.7973
Epoch [10/30] Train Loss: 0.5091, Test Loss: 0.4570, F1: 0.5545, AUC: 0.8467
Epoch [20/30] Train Loss: 0.4463, Test Loss: 0.5114, F1: 0.5051, AUC: 0.8523
Mejores resultados en la época:  19
f1-score 0.5648535564853556
AUC según el mejor F1-score 0.8523119163837012
Confusion Matrix:
 [[1121   95]
 [ 113  135]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8579
Precision:  0.5870
Recall:     0.5444
F1-score:   0.5649

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6933, Test Loss: 0.7223, F1: 0.2897, AUC: 0.2037
Epoch [10/30] Train Loss: 0.4713, Test Loss: 0.3426, F1: 0.5651, AUC: 0.8499
Epoch [20/30] Train Loss: 0.5107, Test Loss: 0.5902, F1: 0.4982, AUC: 0.8510
Mejores resultados en la época:  10
f1-score 0.565121412803532
AUC según el mejor F1-score 0.8498945511460102
Confusion Matrix:
 [[1139   77]
 [ 120  128]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8654
Precision:  0.6244
Recall:     0.5161
F1-score:   0.5651

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6856, Test Loss: 0.5178, F1: 0.0000, AUC: 0.8032
Epoch [10/30] Train Loss: 0.6224, Test Loss: 0.5315, F1: 0.5407, AUC: 0.8485
Epoch [20/30] Train Loss: 0.5282, Test Loss: 0.3701, F1: 0.5753, AUC: 0.8516
Mejores resultados en la época:  20
f1-score 0.5753424657534246
AUC según el mejor F1-score 0.8515558679966044
Confusion Matrix:
 [[1100  116]
 [ 101  147]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8518
Precision:  0.5589
Recall:     0.5927
F1-score:   0.5753
Tiempo total para red 6: 34.90 segundos
Saved on: outputs_only_text_pseudo/2/gpt

==============================
Model: Logistic Regression
Accuracy:  0.7432
Precision: 0.3638
Recall:    0.6895
F1-score:  0.4763
              precision    recall  f1-score   support

           0       0.92      0.75      0.83      1216
           1       0.36      0.69      0.48       248

    accuracy                           0.74      1464
   macro avg       0.64      0.72      0.65      1464
weighted avg       0.83      0.74      0.77      1464

[[917 299]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8620
Precision:  0.6055
Recall:     0.5323
F1-score:   0.5665
Tiempo total para red 3: 28.63 segundos

Entrenando red 4 con capas [1536, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6947, Test Loss: 0.6933, F1: 0.3699, AUC: 0.8027
Epoch [10/30] Train Loss: 0.5136, Test Loss: 0.3799, F1: 0.5753, AUC: 0.8463
Epoch [20/30] Train Loss: 0.4514, Test Loss: 0.3352, F1: 0.5672, AUC: 0.8532
Mejores resultados en la época:  11
f1-score 0.5757575757575758
AUC según el mejor F1-score 0.8474937659168083
Confusion Matrix:
 [[1088  128]
 [  96  152]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8470
Precision:  0.5429
Recall:     0.6129
F1-score:   0.5758

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6804, Test Loss: 0.5545, F1: 0.0625, AUC: 0.8002
Epoch [10/30] Train Loss: 0.4955, Test Loss: 0.4690, F1: 0.5330, AUC: 0.8498
Epoch [20/30] Train Loss: 0.4547, Test Loss: 0.3460, F1: 0.3161, AUC: 0.8487
Mejores resultados en la época:  7
f1-score 0.5679012345679012
AUC según el mejor F1-score 0.8452189887521222
Confusion Matrix:
 [[1058  158]
 [  87  161]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8327
Precision:  0.5047
Recall:     0.6492
F1-score:   0.5679

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6895, Test Loss: 0.6152, F1: 0.0472, AUC: 0.8036
Epoch [10/30] Train Loss: 0.5119, Test Loss: 0.5595, F1: 0.4822, AUC: 0.8470
Epoch [20/30] Train Loss: 0.4686, Test Loss: 0.3358, F1: 0.5677, AUC: 0.8509
Mejores resultados en la época:  15
f1-score 0.5758157389635317
AUC según el mejor F1-score 0.850763343590832
Confusion Matrix:
 [[1093  123]
 [  98  150]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8490
Precision:  0.5495
Recall:     0.6048
F1-score:   0.5758

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6901, Test Loss: 0.6447, F1: 0.2525, AUC: 0.8046
Epoch [10/30] Train Loss: 0.4963, Test Loss: 0.5920, F1: 0.4673, AUC: 0.8494
Epoch [20/30] Train Loss: 0.4543, Test Loss: 0.5234, F1: 0.5117, AUC: 0.8511
Mejores resultados en la época:  23
f1-score 0.5815899581589958
AUC según el mejor F1-score 0.8509656196943973
Confusion Matrix:
 [[1125   91]
 [ 109  139]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8634
Precision:  0.6043
Recall:     0.5605
F1-score:   0.5816
Tiempo total para red 4: 30.47 segundos

Entrenando red 5 con capas [1536, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6892, Test Loss: 0.6318, F1: 0.4044, AUC: 0.8054
Epoch [10/30] Train Loss: 0.4831, Test Loss: 0.7381, F1: 0.4395, AUC: 0.8538
Epoch [20/30] Train Loss: 0.4487, Test Loss: 0.3618, F1: 0.5637, AUC: 0.8523
Mejores resultados en la época:  9
f1-score 0.5831842576028623
AUC según el mejor F1-score 0.8523417603989815
Confusion Matrix:
 [[1068  148]
 [  85  163]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8408
Precision:  0.5241
Recall:     0.6573
F1-score:   0.5832

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6956, Test Loss: 0.6662, F1: 0.0000, AUC: 0.8038
Epoch [10/30] Train Loss: 0.5168, Test Loss: 0.6123, F1: 0.4677, AUC: 0.8439
Epoch [20/30] Train Loss: 0.4801, Test Loss: 0.4770, F1: 0.5239, AUC: 0.8509
Mejores resultados en la época:  11
f1-score 0.5734265734265734
AUC según el mejor F1-score 0.845878873089983
Confusion Matrix:
 [[1056  160]
 [  84  164]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8333
Precision:  0.5062
Recall:     0.6613
F1-score:   0.5734

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6948, Test Loss: 0.6715, F1: 0.0472, AUC: 0.8016
Epoch [10/30] Train Loss: 0.5466, Test Loss: 0.4498, F1: 0.5561, AUC: 0.8486
Epoch [20/30] Train Loss: 0.5467, Test Loss: 0.3723, F1: 0.5615, AUC: 0.8501
Mejores resultados en la época:  6
f1-score 0.5663716814159292
AUC según el mejor F1-score 0.8441048121816639
Confusion Matrix:
 [[1140   76]
 [ 120  128]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8661
Precision:  0.6275
Recall:     0.5161
F1-score:   0.5664

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6934, Test Loss: 0.6938, F1: 0.3696, AUC: 0.7961
Epoch [10/30] Train Loss: 0.5097, Test Loss: 0.5574, F1: 0.4721, AUC: 0.8497
Epoch [20/30] Train Loss: 0.4481, Test Loss: 0.5285, F1: 0.5031, AUC: 0.8505
Mejores resultados en la época:  17
f1-score 0.5719237435008665
AUC según el mejor F1-score 0.8507235515704583
Confusion Matrix:
 [[1052  164]
 [  83  165]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8313
Precision:  0.5015
Recall:     0.6653
F1-score:   0.5719
Tiempo total para red 5: 31.71 segundos

Entrenando red 6 con capas [1536, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6944, Test Loss: 0.6907, F1: 0.0000, AUC: 0.6810
Epoch [10/30] Train Loss: 0.4862, Test Loss: 0.3970, F1: 0.5641, AUC: 0.8468
Epoch [20/30] Train Loss: 0.4743, Test Loss: 0.4520, F1: 0.5386, AUC: 0.8523
Mejores resultados en la época:  16
f1-score 0.5780141843971631
AUC según el mejor F1-score 0.8530149087436334
Confusion Matrix:
 [[1063  153]
 [  85  163]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8374
Precision:  0.5158
Recall:     0.6573
F1-score:   0.5780

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6858, Test Loss: 0.6486, F1: 0.4186, AUC: 0.8006
Epoch [10/30] Train Loss: 0.5073, Test Loss: 0.3453, F1: 0.5800, AUC: 0.8479
Epoch [20/30] Train Loss: 0.4652, Test Loss: 0.3867, F1: 0.5699, AUC: 0.8517
Mejores resultados en la época:  10
f1-score 0.58
AUC según el mejor F1-score 0.8479480581494058
Confusion Matrix:
 [[1109  107]
 [ 103  145]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8566
Precision:  0.5754
Recall:     0.5847
F1-score:   0.5800

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6937, Test Loss: 0.7170, F1: 0.2897, AUC: 0.8015
Epoch [10/30] Train Loss: 0.5676, Test Loss: 0.5137, F1: 0.5211, AUC: 0.8473
Epoch [20/30] Train Loss: 0.4538, Test Loss: 0.4932, F1: 0.5409, AUC: 0.8527
Mejores resultados en la época:  18
f1-score 0.5744234800838575
AUC según el mejor F1-score 0.8517216680814941
Confusion Matrix:
 [[1124   92]
 [ 111  137]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8613
Precision:  0.5983
Recall:     0.5524
F1-score:   0.5744

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6950, Test Loss: 0.6860, F1: 0.0000, AUC: 0.8046
Epoch [10/30] Train Loss: 0.5284, Test Loss: 0.4801, F1: 0.5473, AUC: 0.8374
Epoch [20/30] Train Loss: 0.4786, Test Loss: 0.4603, F1: 0.5367, AUC: 0.8504
Mejores resultados en la época:  14
f1-score 0.5786407766990291
AUC según el mejor F1-score 0.8471256897283531
Confusion Matrix:
 [[1098  118]
 [  99  149]]
Matriz de confusión guardada en: outputs_only_text_pseudo/2/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8518
Precision:  0.5581
Recall:     0.6008
F1-score:   0.5786
Tiempo total para red 6: 34.91 segundos
Saved on: outputs_only_text_pseudo/2/gpt

==============================
Model: Logistic Regression
Accuracy:  0.7432
Precision: 0.3638
Recall:    0.6895
F1-score:  0.4763
              precision    recall  f1-score   support

           0       0.92      0.75      0.83      1216
           1       0.36      0.69      0.48       248

    accuracy                           0.74      1464
   macro avg       0.64      0.72      0.65      1464
weighted avg       0.83      0.74      0.77      1464

[[917 299]
 [ 77 171]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:44:06] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:44:06] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
 [ 77 171]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7903
Precision: 0.4322
Recall:    0.7581
F1-score:  0.5505
              precision    recall  f1-score   support

           0       0.94      0.80      0.86      1216
           1       0.43      0.76      0.55       248

    accuracy                           0.79      1464
   macro avg       0.69      0.78      0.71      1464
weighted avg       0.86      0.79      0.81      1464

[[969 247]
 [ 60 188]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_svm.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6086
Precision: 0.2400
Recall:    0.6048
F1-score:  0.3436
              precision    recall  f1-score   support

           0       0.88      0.61      0.72      1216
           1       0.24      0.60      0.34       248

    accuracy                           0.61      1464
   macro avg       0.56      0.61      0.53      1464
weighted avg       0.77      0.61      0.66      1464

[[741 475]
 [ 98 150]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.7794
Precision: 0.4146
Recall:    0.7339
F1-score:  0.5298
              precision    recall  f1-score   support

           0       0.94      0.79      0.86      1216
           1       0.41      0.73      0.53       248

    accuracy                           0.78      1464
   macro avg       0.68      0.76      0.69      1464
weighted avg       0.85      0.78      0.80      1464

[[959 257]
 [ 66 182]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.7650
Precision: 0.3943
Recall:    0.7218
F1-score:  0.5100
              precision    recall  f1-score   support

           0       0.93      0.77      0.85      1216
           1       0.39      0.72      0.51       248

    accuracy                           0.77      1464
   macro avg       0.66      0.75      0.68      1464
weighted avg       0.84      0.77      0.79      1464

[[941 275]
 [ 69 179]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7609
Precision: 0.3886
Recall:    0.7177
F1-score:  0.5042
              precision    recall  f1-score   support

           0       0.93      0.77      0.84      1216
           1       0.39      0.72      0.50       248

    accuracy                           0.76      1464
   macro avg       0.66      0.74      0.67      1464
weighted avg       0.84      0.76      0.79      1464

[[936 280]
 [ 70 178]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/naive_bayes_model.pkl


Resumen de métricas:
SVM: {'accuracy': 0.7903, 'precision': 0.4322, 'recall': 0.7581, 'f1_score': 0.5505}
Random Forest: {'accuracy': 0.7794, 'precision': 0.4146, 'recall': 0.7339, 'f1_score': 0.5298}
XGBoost: {'accuracy': 0.765, 'precision': 0.3943, 'recall': 0.7218, 'f1_score': 0.51}
Naive Bayes: {'accuracy': 0.7609, 'precision': 0.3886, 'recall': 0.7177, 'f1_score': 0.5042}
Logistic Regression: {'accuracy': 0.7432, 'precision': 0.3638, 'recall': 0.6895, 'f1_score': 0.4763}
Decision Tree: {'accuracy': 0.6086, 'precision': 0.24, 'recall': 0.6048, 'f1_score': 0.3436}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: GPT
MLP_436737: {'accuracy': 0.8415300546448088, 'precision': 0.527027027027027, 'recall': 0.6290322580645161, 'f1_score': 0.5807692307692308, 'f1_score_avg': 0.5765603756156888}
MLP_959489: {'accuracy': 0.8367486338797814, 'precision': 0.5144694533762058, 'recall': 0.6451612903225806, 'f1_score': 0.5807692307692308, 'f1_score_avg': 0.572206116888714}
MLP_2273281: {'accuracy': 0.851775956284153, 'precision': 0.55893536121673, 'recall': 0.592741935483871, 'f1_score': 0.5807692307692308, 'f1_score_avg': 0.570649656712161}
MLP_49217: {'accuracy': 0.8524590163934426, 'precision': 0.5610687022900763, 'recall': 0.592741935483871, 'f1_score': 0.5787545787545788, 'f1_score_avg': 0.5767719749297042}
MLP_100481: {'accuracy': 0.8449453551912568, 'precision': 0.5376344086021505, 'recall': 0.6048387096774194, 'f1_score': 0.5787545787545788, 'f1_score_avg': 0.5734535667669349}
MLP_207105: {'accuracy': 0.8476775956284153, 'precision': 0.5461254612546126, 'recall': 0.5967741935483871, 'f1_score': 0.5787545787545788, 'f1_score_avg': 0.5731362583522015}
SVM: {'accuracy': 0.7903, 'precision': 0.4322, 'recall': 0.7581, 'f1_score': 0.5505}
Random Forest: {'accuracy': 0.7794, 'precision': 0.4146, 'recall': 0.7339, 'f1_score': 0.5298}
XGBoost: {'accuracy': 0.765, 'precision': 0.3943, 'recall': 0.7218, 'f1_score': 0.51}
Naive Bayes: {'accuracy': 0.7609, 'precision': 0.3886, 'recall': 0.7177, 'f1_score': 0.5042}
Logistic Regression: {'accuracy': 0.7432, 'precision': 0.3638, 'recall': 0.6895, 'f1_score': 0.4763}
Decision Tree: {'accuracy': 0.6086, 'precision': 0.24, 'recall': 0.6048, 'f1_score': 0.3436}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: False
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['lyrics']
Numeric Columns: Not used
====================================

Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7903
Precision: 0.4322
Recall:    0.7581
F1-score:  0.5505
              precision    recall  f1-score   support

           0       0.94      0.80      0.86      1216
           1       0.43      0.76      0.55       248

    accuracy                           0.79      1464
   macro avg       0.69      0.78      0.71      1464
weighted avg       0.86      0.79      0.81      1464

[[969 247]
 [ 60 188]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_svm.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6086
Precision: 0.2400
Recall:    0.6048
F1-score:  0.3436
              precision    recall  f1-score   support

           0       0.88      0.61      0.72      1216
           1       0.24      0.60      0.34       248

    accuracy                           0.61      1464
   macro avg       0.56      0.61      0.53      1464
weighted avg       0.77      0.61      0.66      1464

[[741 475]
 [ 98 150]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.7794
Precision: 0.4146
Recall:    0.7339
F1-score:  0.5298
              precision    recall  f1-score   support

           0       0.94      0.79      0.86      1216
           1       0.41      0.73      0.53       248

    accuracy                           0.78      1464
   macro avg       0.68      0.76      0.69      1464
weighted avg       0.85      0.78      0.80      1464

[[959 257]
 [ 66 182]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.7650
Precision: 0.3943
Recall:    0.7218
F1-score:  0.5100
              precision    recall  f1-score   support

           0       0.93      0.77      0.85      1216
           1       0.39      0.72      0.51       248

    accuracy                           0.77      1464
   macro avg       0.66      0.75      0.68      1464
weighted avg       0.84      0.77      0.79      1464

[[941 275]
 [ 69 179]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7609
Precision: 0.3886
Recall:    0.7177
F1-score:  0.5042
              precision    recall  f1-score   support

           0       0.93      0.77      0.84      1216
           1       0.39      0.72      0.50       248

    accuracy                           0.76      1464
   macro avg       0.66      0.74      0.67      1464
weighted avg       0.84      0.76      0.79      1464

[[936 280]
 [ 70 178]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_only_text_pseudo/2/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text_pseudo/2/gpt/naive_bayes_model.pkl


Resumen de métricas:
SVM: {'accuracy': 0.7903, 'precision': 0.4322, 'recall': 0.7581, 'f1_score': 0.5505}
Random Forest: {'accuracy': 0.7794, 'precision': 0.4146, 'recall': 0.7339, 'f1_score': 0.5298}
XGBoost: {'accuracy': 0.765, 'precision': 0.3943, 'recall': 0.7218, 'f1_score': 0.51}
Naive Bayes: {'accuracy': 0.7609, 'precision': 0.3886, 'recall': 0.7177, 'f1_score': 0.5042}
Logistic Regression: {'accuracy': 0.7432, 'precision': 0.3638, 'recall': 0.6895, 'f1_score': 0.4763}
Decision Tree: {'accuracy': 0.6086, 'precision': 0.24, 'recall': 0.6048, 'f1_score': 0.3436}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: GPT
MLP_959489: {'accuracy': 0.8312841530054644, 'precision': 0.5015197568389058, 'recall': 0.6653225806451613, 'f1_score': 0.5831842576028623, 'f1_score_avg': 0.5737265639865579}
MLP_2273281: {'accuracy': 0.851775956284153, 'precision': 0.5580524344569289, 'recall': 0.6008064516129032, 'f1_score': 0.5831842576028623, 'f1_score_avg': 0.5777696102950124}
MLP_436737: {'accuracy': 0.8633879781420765, 'precision': 0.6043478260869565, 'recall': 0.5604838709677419, 'f1_score': 0.5815899581589958, 'f1_score_avg': 0.5752661268620012}
MLP_207105: {'accuracy': 0.8620218579234973, 'precision': 0.6055045871559633, 'recall': 0.532258064516129, 'f1_score': 0.5776173285198556, 'f1_score_avg': 0.5718976005473128}
MLP_49217: {'accuracy': 0.8367486338797814, 'precision': 0.5147540983606558, 'recall': 0.6330645161290323, 'f1_score': 0.5760286225402504, 'f1_score_avg': 0.5732539314157659}
MLP_100481: {'accuracy': 0.8319672131147541, 'precision': 0.5030120481927711, 'recall': 0.6733870967741935, 'f1_score': 0.5760286225402504, 'f1_score_avg': 0.5719815649228606}
SVM: {'accuracy': 0.7903, 'precision': 0.4322, 'recall': 0.7581, 'f1_score': 0.5505}
Random Forest: {'accuracy': 0.7794, 'precision': 0.4146, 'recall': 0.7339, 'f1_score': 0.5298}
XGBoost: {'accuracy': 0.765, 'precision': 0.3943, 'recall': 0.7218, 'f1_score': 0.51}
Naive Bayes: {'accuracy': 0.7609, 'precision': 0.3886, 'recall': 0.7177, 'f1_score': 0.5042}
Logistic Regression: {'accuracy': 0.7432, 'precision': 0.3638, 'recall': 0.6895, 'f1_score': 0.4763}
Decision Tree: {'accuracy': 0.6086, 'precision': 0.24, 'recall': 0.6048, 'f1_score': 0.3436}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: False
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['lyrics']
Numeric Columns: Not used
====================================

