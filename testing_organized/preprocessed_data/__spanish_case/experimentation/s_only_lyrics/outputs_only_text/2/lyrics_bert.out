2025-10-30 01:45:59.227555: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-30 01:45:59.227555: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__spanish_case/experimentation/s_only_lyrics/main_only_text_LB.py:273: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__spanish_case/experimentation/s_only_lyrics/main_only_text_LB.py:273: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
For TF-IDF embbedings you are selecteing this columns:
--> ['lyrics']
Your are not adding cols numercis
You are executing with [ALL] dataset
--> PaTH:  ../../../../../data/spanish/LB_T/LB_fuss/lb_khipu_T.npy

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../../../../data/spanish/dataset/oficialDatasetEAIM2026.csv
Label distribution: {False: 6765, True: 554}
X shape: (7319, 300)
y shape: (7319,)

Data con el spliting...
Label distribution en TRAIN: {0: 5412, 1: 443}
Label distribution en TEST: {0: 1353, 1: 111}


==================================================
Data antes del undersampling ...
X: (5855, 300)
y: (5855,)
Apliying UNDERSAMPLE
443
Label distribution: {0: 443, 1: 443}
X shape: (886, 300)
y shape: (886,)
Resultados con MLP

Entrenando red 1 con capas [300, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6943, Test Loss: 0.7882, F1: 0.1410, AUC: 0.6750
Epoch [10/30] Train Loss: 0.6360, Test Loss: 0.6357, F1: 0.2478, AUC: 0.7694
Epoch [20/30] Train Loss: 0.5597, Test Loss: 0.5631, F1: 0.2610, AUC: 0.7801
Mejores resultados en la época:  11
f1-score 0.30976430976430974
AUC según el mejor F1-score 0.7702669409986483
Confusion Matrix:
 [[1213  140]
 [  65   46]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8600
Precision:  0.2473
Recall:     0.4144
F1-score:   0.3098

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6954, Test Loss: 0.6893, F1: 0.1888, AUC: 0.6240
Epoch [10/30] Train Loss: 0.6358, Test Loss: 0.5992, F1: 0.2923, AUC: 0.7639
Epoch [20/30] Train Loss: 0.5490, Test Loss: 0.5673, F1: 0.2540, AUC: 0.7783
Mejores resultados en la época:  6
f1-score 0.3116883116883117
AUC según el mejor F1-score 0.7521557033752156
Confusion Matrix:
 [[1269   84]
 [  75   36]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8914
Precision:  0.3000
Recall:     0.3243
F1-score:   0.3117

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6935, Test Loss: 0.6884, F1: 0.1891, AUC: 0.6413
Epoch [10/30] Train Loss: 0.6208, Test Loss: 0.6978, F1: 0.2139, AUC: 0.7675
Epoch [20/30] Train Loss: 0.5421, Test Loss: 0.5582, F1: 0.2540, AUC: 0.7794
Mejores resultados en la época:  13
f1-score 0.3
AUC según el mejor F1-score 0.7720114793285524
Confusion Matrix:
 [[1158  195]
 [  57   54]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8279
Precision:  0.2169
Recall:     0.4865
F1-score:   0.3000

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6944, Test Loss: 0.6794, F1: 0.2098, AUC: 0.6733
Epoch [10/30] Train Loss: 0.6259, Test Loss: 0.6106, F1: 0.2408, AUC: 0.7653
Epoch [20/30] Train Loss: 0.5496, Test Loss: 0.7652, F1: 0.2147, AUC: 0.7779
Mejores resultados en la época:  26
f1-score 0.30526315789473685
AUC según el mejor F1-score 0.7845428577135894
Confusion Matrix:
 [[1142  211]
 [  53   58]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8197
Precision:  0.2156
Recall:     0.5225
F1-score:   0.3053
Tiempo total para red 1: 17.67 segundos

Entrenando red 2 con capas [300, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6944, Test Loss: 0.6822, F1: 0.1545, AUC: 0.6458
Epoch [10/30] Train Loss: 0.5877, Test Loss: 0.6990, F1: 0.2267, AUC: 0.7766
Epoch [20/30] Train Loss: 0.4772, Test Loss: 0.3919, F1: 0.2857, AUC: 0.7964
Mejores resultados en la época:  9
f1-score 0.3076923076923077
AUC según el mejor F1-score 0.773436407582749
Confusion Matrix:
 [[1222  131]
 [  67   44]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8648
Precision:  0.2514
Recall:     0.3964
F1-score:   0.3077

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6938, Test Loss: 0.7031, F1: 0.1509, AUC: 0.7027
Epoch [10/30] Train Loss: 0.5595, Test Loss: 0.6719, F1: 0.2340, AUC: 0.7747
Epoch [20/30] Train Loss: 0.5091, Test Loss: 0.6687, F1: 0.2524, AUC: 0.7969
Mejores resultados en la época:  28
f1-score 0.3166023166023166
AUC según el mejor F1-score 0.8045384630750485
Confusion Matrix:
 [[1246  107]
 [  70   41]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8791
Precision:  0.2770
Recall:     0.3694
F1-score:   0.3166

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6936, Test Loss: 0.6829, F1: 0.0174, AUC: 0.6728
Epoch [10/30] Train Loss: 0.5864, Test Loss: 0.6631, F1: 0.2326, AUC: 0.7676
Epoch [20/30] Train Loss: 0.5250, Test Loss: 0.6332, F1: 0.2551, AUC: 0.7892
Mejores resultados en la época:  29
f1-score 0.30454545454545456
AUC según el mejor F1-score 0.8009894595260448
Confusion Matrix:
 [[1091  262]
 [  44   67]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.7910
Precision:  0.2036
Recall:     0.6036
F1-score:   0.3045

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6937, Test Loss: 0.7202, F1: 0.1410, AUC: 0.7126
Epoch [10/30] Train Loss: 0.5399, Test Loss: 0.6929, F1: 0.2360, AUC: 0.7805
Epoch [20/30] Train Loss: 0.5062, Test Loss: 0.7444, F1: 0.2391, AUC: 0.8014
Mejores resultados en la época:  19
f1-score 0.31137724550898205
AUC según el mejor F1-score 0.8001371659908245
Confusion Matrix:
 [[1182  171]
 [  59   52]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8429
Precision:  0.2332
Recall:     0.4685
F1-score:   0.3114
Tiempo total para red 2: 16.00 segundos

Entrenando red 3 con capas [300, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6934, Test Loss: 0.6803, F1: 0.0000, AUC: 0.7371
Epoch [10/30] Train Loss: 0.5343, Test Loss: 0.4942, F1: 0.2776, AUC: 0.7896
Epoch [20/30] Train Loss: 0.4510, Test Loss: 0.8450, F1: 0.2288, AUC: 0.8069
Mejores resultados en la época:  13
f1-score 0.32413793103448274
AUC según el mejor F1-score 0.7971807727905289
Confusion Matrix:
 [[1221  132]
 [  64   47]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8661
Precision:  0.2626
Recall:     0.4234
F1-score:   0.3241

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6942, Test Loss: 0.7143, F1: 0.1410, AUC: 0.7313
Epoch [10/30] Train Loss: 0.5152, Test Loss: 0.6611, F1: 0.2459, AUC: 0.7844
Epoch [20/30] Train Loss: 0.4527, Test Loss: 0.5754, F1: 0.2843, AUC: 0.8001
Mejores resultados en la época:  29
f1-score 0.3247863247863248
AUC según el mejor F1-score 0.8086601013430281
Confusion Matrix:
 [[1170  183]
 [  54   57]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8381
Precision:  0.2375
Recall:     0.5135
F1-score:   0.3248

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6943, Test Loss: 0.6884, F1: 0.1140, AUC: 0.6976
Epoch [10/30] Train Loss: 0.5458, Test Loss: 0.7307, F1: 0.2313, AUC: 0.7844
Epoch [20/30] Train Loss: 0.4874, Test Loss: 0.5895, F1: 0.2720, AUC: 0.7988
Mejores resultados en la época:  15
f1-score 0.3262411347517731
AUC según el mejor F1-score 0.7943375748253797
Confusion Matrix:
 [[1228  125]
 [  65   46]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8702
Precision:  0.2690
Recall:     0.4144
F1-score:   0.3262

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6929, Test Loss: 0.6719, F1: 0.0000, AUC: 0.7162
Epoch [10/30] Train Loss: 0.5415, Test Loss: 0.7337, F1: 0.2267, AUC: 0.7853
Epoch [20/30] Train Loss: 0.4526, Test Loss: 0.7672, F1: 0.2443, AUC: 0.8066
Mejores resultados en la época:  26
f1-score 0.32941176470588235
AUC según el mejor F1-score 0.8109106889594695
Confusion Matrix:
 [[1180  173]
 [  55   56]]
For TF-IDF embbedings you are selecteing this columns:
--> ['lyrics']
Your are not adding cols numercis
You are executing with [ALL] dataset
--> PaTH:  ../../../../../data/spanish/LB_T/LB_fuss/lb_khipu_T.npy

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../../../../data/spanish/dataset/oficialDatasetEAIM2026.csv
Label distribution: {False: 6765, True: 554}
X shape: (7319, 300)
y shape: (7319,)

Data con el spliting...
Label distribution en TRAIN: {0: 5412, 1: 443}
Label distribution en TEST: {0: 1353, 1: 111}


==================================================
Data antes del undersampling ...
X: (5855, 300)
y: (5855,)
Apliying UNDERSAMPLE
443
Label distribution: {0: 443, 1: 443}
X shape: (886, 300)
y shape: (886,)
Resultados con MLP

Entrenando red 1 con capas [300, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6888, Test Loss: 0.7411, F1: 0.1418, AUC: 0.7025
Epoch [10/30] Train Loss: 0.6138, Test Loss: 0.4601, F1: 0.2500, AUC: 0.7694
Epoch [20/30] Train Loss: 0.5623, Test Loss: 0.6495, F1: 0.2447, AUC: 0.7824
Mejores resultados en la época:  8
f1-score 0.3063063063063063
AUC según el mejor F1-score 0.7654861069495216
Confusion Matrix:
 [[1182  171]
 [  60   51]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8422
Precision:  0.2297
Recall:     0.4595
F1-score:   0.3063

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6951, Test Loss: 0.6429, F1: 0.0325, AUC: 0.7359
Epoch [10/30] Train Loss: 0.6145, Test Loss: 0.5630, F1: 0.2965, AUC: 0.7755
Epoch [20/30] Train Loss: 0.5301, Test Loss: 0.4925, F1: 0.2773, AUC: 0.7863
Mejores resultados en la época:  9
f1-score 0.3167420814479638
AUC según el mejor F1-score 0.7753007996910435
Confusion Matrix:
 [[1278   75]
 [  76   35]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8969
Precision:  0.3182
Recall:     0.3153
F1-score:   0.3167

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6954, Test Loss: 0.6802, F1: 0.2331, AUC: 0.6613
Epoch [10/30] Train Loss: 0.6145, Test Loss: 0.6061, F1: 0.2528, AUC: 0.7736
Epoch [20/30] Train Loss: 0.5447, Test Loss: 0.4123, F1: 0.3099, AUC: 0.7849
Mejores resultados en la época:  19
f1-score 0.3125
AUC según el mejor F1-score 0.7838037594135155
Confusion Matrix:
 [[1221  132]
 [  66   45]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8648
Precision:  0.2542
Recall:     0.4054
F1-score:   0.3125

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6932, Test Loss: 0.7453, F1: 0.1414, AUC: 0.6396
Epoch [10/30] Train Loss: 0.6070, Test Loss: 0.6727, F1: 0.2388, AUC: 0.7760
Epoch [20/30] Train Loss: 0.5319, Test Loss: 0.5935, F1: 0.2637, AUC: 0.7869
Mejores resultados en la época:  11
f1-score 0.3215434083601286
AUC según el mejor F1-score 0.7770719721939235
Confusion Matrix:
 [[1203  150]
 [  61   50]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8559
Precision:  0.2500
Recall:     0.4505
F1-score:   0.3215
Tiempo total para red 1: 17.68 segundos

Entrenando red 2 con capas [300, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6940, Test Loss: 0.6806, F1: 0.0160, AUC: 0.6816
Epoch [10/30] Train Loss: 0.5950, Test Loss: 0.5986, F1: 0.2606, AUC: 0.7750
Epoch [20/30] Train Loss: 0.4781, Test Loss: 0.5342, F1: 0.2878, AUC: 0.7944
Mejores resultados en la época:  7
f1-score 0.30340557275541796
AUC según el mejor F1-score 0.7700005992688919
Confusion Matrix:
 [[1190  163]
 [  62   49]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8463
Precision:  0.2311
Recall:     0.4414
F1-score:   0.3034

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6936, Test Loss: 0.6602, F1: 0.0000, AUC: 0.7199
Epoch [10/30] Train Loss: 0.5485, Test Loss: 0.7318, F1: 0.2212, AUC: 0.7807
Epoch [20/30] Train Loss: 0.4770, Test Loss: 0.3834, F1: 0.2997, AUC: 0.7996
Mejores resultados en la época:  28
f1-score 0.30403800475059384
AUC según el mejor F1-score 0.8071752461996364
Confusion Matrix:
 [[1107  246]
 [  47   64]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.7999
Precision:  0.2065
Recall:     0.5766
F1-score:   0.3040

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6928, Test Loss: 0.6736, F1: 0.0000, AUC: 0.6878
Epoch [10/30] Train Loss: 0.5750, Test Loss: 0.6592, F1: 0.2364, AUC: 0.7737
Epoch [20/30] Train Loss: 0.5125, Test Loss: 0.6582, F1: 0.2507, AUC: 0.7935
Mejores resultados en la época:  12
f1-score 0.3190883190883191
AUC según el mejor F1-score 0.7782438758048514
Confusion Matrix:
 [[1169  184]
 [  55   56]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8367
Precision:  0.2333
Recall:     0.5045
F1-score:   0.3191

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6929, Test Loss: 0.7073, F1: 0.1418, AUC: 0.7076
Epoch [10/30] Train Loss: 0.5716, Test Loss: 0.8662, F1: 0.1853, AUC: 0.7801
Epoch [20/30] Train Loss: 0.4823, Test Loss: 0.6049, F1: 0.2769, AUC: 0.7990
Mejores resultados en la época:  25
f1-score 0.31213872832369943
AUC según el mejor F1-score 0.8055106103886592
Confusion Matrix:
 [[1026  327]
 [  30   81]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.7561
Precision:  0.1985
Recall:     0.7297
F1-score:   0.3121
Tiempo total para red 2: 15.99 segundos

Entrenando red 3 con capas [300, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6939, Test Loss: 0.7040, F1: 0.1410, AUC: 0.7116
Epoch [10/30] Train Loss: 0.5321, Test Loss: 0.3750, F1: 0.3154, AUC: 0.7798
Epoch [20/30] Train Loss: 0.4656, Test Loss: 0.5336, F1: 0.3125, AUC: 0.8040
Mejores resultados en la época:  18
f1-score 0.32558139534883723
AUC según el mejor F1-score 0.8010693620449718
Confusion Matrix:
 [[1212  141]
 [  62   49]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8613
Precision:  0.2579
Recall:     0.4414
F1-score:   0.3256

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6940, Test Loss: 0.7031, F1: 0.1410, AUC: 0.7369
Epoch [10/30] Train Loss: 0.5810, Test Loss: 0.4282, F1: 0.3052, AUC: 0.7773
Epoch [20/30] Train Loss: 0.4647, Test Loss: 0.4675, F1: 0.3021, AUC: 0.8021
Mejores resultados en la época:  18
f1-score 0.32764505119453924
AUC según el mejor F1-score 0.7997909217421413
Confusion Matrix:
 [[1219  134]
 [  63   48]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8654
Precision:  0.2637
Recall:     0.4324
F1-score:   0.3276

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6942, Test Loss: 0.6878, F1: 0.0000, AUC: 0.6767
Epoch [10/30] Train Loss: 0.5517, Test Loss: 1.0373, F1: 0.1852, AUC: 0.7790
Epoch [20/30] Train Loss: 0.4727, Test Loss: 0.4313, F1: 0.2850, AUC: 0.7953
Mejores resultados en la época:  6
f1-score 0.3045267489711934
AUC según el mejor F1-score 0.7679764021227435
Confusion Matrix:
 [[1258   95]
 [  74   37]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8846
Precision:  0.2803
Recall:     0.3333
F1-score:   0.3045

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6922, Test Loss: 0.6645, F1: 0.0000, AUC: 0.7055
Epoch [10/30] Train Loss: 0.5443, Test Loss: 0.4691, F1: 0.2780, AUC: 0.7729
Epoch [20/30] Train Loss: 0.4601, Test Loss: 0.5418, F1: 0.2995, AUC: 0.8040
Mejores resultados en la época:  27
f1-score 0.3165137614678899
AUC según el mejor F1-score 0.8095057363350047
Confusion Matrix:
 [[1097  256]
 [  42   69]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8443
Precision:  0.2445
Recall:     0.5045
F1-score:   0.3294
Tiempo total para red 3: 17.99 segundos

Entrenando red 4 con capas [300, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6937, Test Loss: 0.7239, F1: 0.1410, AUC: 0.7583
Epoch [10/30] Train Loss: 0.4927, Test Loss: 0.6083, F1: 0.2625, AUC: 0.7942
Epoch [20/30] Train Loss: 0.4385, Test Loss: 0.4498, F1: 0.3060, AUC: 0.8081
Mejores resultados en la época:  28
f1-score 0.3138075313807531
AUC según el mejor F1-score 0.812994812994813
Confusion Matrix:
 [[1061  292]
 [  36   75]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.7760
Precision:  0.2044
Recall:     0.6757
F1-score:   0.3138

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6944, Test Loss: 0.6872, F1: 0.0000, AUC: 0.7218
Epoch [10/30] Train Loss: 0.5514, Test Loss: 0.4967, F1: 0.2570, AUC: 0.7759
Epoch [20/30] Train Loss: 0.4786, Test Loss: 0.4502, F1: 0.2895, AUC: 0.8000
Mejores resultados en la época:  21
f1-score 0.32
AUC según el mejor F1-score 0.8007897032287276
Confusion Matrix:
 [[1233  120]
 [  67   44]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8723
Precision:  0.2683
Recall:     0.3964
F1-score:   0.3200

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6947, Test Loss: 0.6972, F1: 0.1448, AUC: 0.7667
Epoch [10/30] Train Loss: 0.5196, Test Loss: 0.8630, F1: 0.2097, AUC: 0.7979
Epoch [20/30] Train Loss: 0.4577, Test Loss: 0.5978, F1: 0.2907, AUC: 0.8111
Mejores resultados en la época:  29
f1-score 0.3359580052493438
AUC según el mejor F1-score 0.8142732532976436
Confusion Matrix:
 [[1147  206]
 [  47   64]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8272
Precision:  0.2370
Recall:     0.5766
F1-score:   0.3360

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6930, Test Loss: 0.7005, F1: 0.1424, AUC: 0.7366
Epoch [10/30] Train Loss: 0.4873, Test Loss: 0.5768, F1: 0.2852, AUC: 0.7951
Epoch [20/30] Train Loss: 0.4884, Test Loss: 0.3588, F1: 0.3179, AUC: 0.8086
Mejores resultados en la época:  28
f1-score 0.3269230769230769
AUC según el mejor F1-score 0.8125287149677395
Confusion Matrix:
 [[1116  237]
 [  43   68]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8087
Precision:  0.2230
Recall:     0.6126
F1-score:   0.3269
Tiempo total para red 4: 20.36 segundos

Entrenando red 5 con capas [300, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6934, Test Loss: 0.6640, F1: 0.0000, AUC: 0.7221
Epoch [10/30] Train Loss: 0.5124, Test Loss: 0.8203, F1: 0.2261, AUC: 0.8032
Epoch [20/30] Train Loss: 0.4449, Test Loss: 0.5476, F1: 0.2998, AUC: 0.8136
Mejores resultados en la época:  26
f1-score 0.3413173652694611
AUC según el mejor F1-score 0.8141400824327654
Confusion Matrix:
 [[1187  166]
 [  54   57]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8497
Precision:  0.2556
Recall:     0.5135
F1-score:   0.3413

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6942, Test Loss: 0.7013, F1: 0.1410, AUC: 0.7378
Epoch [10/30] Train Loss: 0.5133, Test Loss: 0.6130, F1: 0.2523, AUC: 0.7875
Epoch [20/30] Train Loss: 0.4425, Test Loss: 0.7208, F1: 0.2657, AUC: 0.8089
Mejores resultados en la época:  27
f1-score 0.3237597911227154
AUC según el mejor F1-score 0.8144130827057656
Confusion Matrix:
 [[1143  210]
 [  49   62]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8231
Precision:  0.2279
Recall:     0.5586
F1-score:   0.3238

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6936, Test Loss: 0.6832, F1: 0.0000, AUC: 0.7580
Epoch [10/30] Train Loss: 0.4914, Test Loss: 0.5454, F1: 0.3005, AUC: 0.8037
Epoch [20/30] Train Loss: 0.4203, Test Loss: 0.2994, F1: 0.3275, AUC: 0.8171
Mejores resultados en la época:  17
f1-score 0.33436532507739936
AUC según el mejor F1-score 0.814972400338254
Confusion Matrix:
 [[1195  158]
 [  57   54]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8531
Precision:  0.2547
Recall:     0.4865
F1-score:   0.3344

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6942, Test Loss: 0.6896, F1: 0.2812, AUC: 0.7514
Epoch [10/30] Train Loss: 0.4985, Test Loss: 0.6136, F1: 0.2706, AUC: 0.7976
Epoch [20/30] Train Loss: 0.4505, Test Loss: 0.3808, F1: 0.3085, AUC: 0.8098
Mejores resultados en la época:  25
f1-score 0.3300589390962672
AUC según el mejor F1-score 0.8142066678652045
Confusion Matrix:
 [[1039  314]
 [  27   84]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.7671
Precision:  0.2111
Recall:     0.7568
F1-score:   0.3301
Tiempo total para red 5: 17.43 segundos

Entrenando red 6 con capas [300, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6939, Test Loss: 0.7028, F1: 0.1410, AUC: 0.6117
Epoch [10/30] Train Loss: 0.5025, Test Loss: 0.3932, F1: 0.3289, AUC: 0.7900
Epoch [20/30] Train Loss: 0.4661, Test Loss: 0.6452, F1: 0.2510, AUC: 0.8078
Mejores resultados en la época:  14
f1-score 0.33935018050541516
AUC según el mejor F1-score 0.7952830879660149
Confusion Matrix:
 [[1234  119]
 [  64   47]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8750
Precision:  0.2831
Recall:     0.4234
F1-score:   0.3394

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6963, Test Loss: 0.7171, F1: 0.1410, AUC: 0.7883
Epoch [10/30] Train Loss: 0.5048, Test Loss: 0.4717, F1: 0.3102, AUC: 0.7900
Epoch [20/30] Train Loss: 0.5007, Test Loss: 1.0087, F1: 0.1978, AUC: 0.8109
Mejores resultados en la época:  24
f1-score 0.34576271186440677
AUC según el mejor F1-score 0.8111437379730061
Confusion Matrix:
 [[1220  133]
 [  60   51]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8682
Precision:  0.2772
Recall:     0.4595
F1-score:   0.3458

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6942, Test Loss: 0.6932, F1: 0.1972, AUC: 0.7770
Epoch [10/30] Train Loss: 0.5015, Test Loss: 0.6106, F1: 0.2832, AUC: 0.7944
Epoch [20/30] Train Loss: 0.5001, Test Loss: 0.3857, F1: 0.3345, AUC: 0.8127
Mejores resultados en la época:  16
f1-score 0.3359375
AUC según el mejor F1-score 0.8096455657431267
Confusion Matrix:
 [[1251  102]
 [  68   43]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8839
Precision:  0.2966
Recall:     0.3874
F1-score:   0.3359

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6946, Test Loss: 0.6818, F1: 0.0000, AUC: 0.7556
Epoch [10/30] Train Loss: 0.5626, Test Loss: 0.4776, F1: 0.2794, AUC: 0.7857
Epoch [20/30] Train Loss: 0.5064, Test Loss: 0.3914, F1: 0.3081, AUC: 0.8056
Mejores resultados en la época:  29
f1-score 0.32727272727272727
AUC según el mejor F1-score 0.8151854737220592
Confusion Matrix:
 [[1096  257]
 [  39   72]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.7978
Precision:  0.2188
Recall:     0.6486
F1-score:   0.3273
Tiempo total para red 6: 17.21 segundos
Saved on: outputs_only_text/2/lyrics_bert

==============================
Model: Logistic Regression
Accuracy:  0.7247
Precision: 0.1867
Recall:    0.7838
F1-score:  0.3016
              precision    recall  f1-score   support

           0       0.98      0.72      0.83      1353
           1       0.19      0.78      0.30       111

    accuracy                           0.72      1464
   macro avg       0.58      0.75      0.57      1464
weighted avg       0.92      0.72      0.79      1464

[[974 379]
 [ 24  87]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.7964
Precision:  0.2123
Recall:     0.6216
F1-score:   0.3165
Tiempo total para red 3: 17.99 segundos

Entrenando red 4 con capas [300, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6941, Test Loss: 0.6838, F1: 0.0000, AUC: 0.7381
Epoch [10/30] Train Loss: 0.4794, Test Loss: 0.4704, F1: 0.2851, AUC: 0.7993
Epoch [20/30] Train Loss: 0.4702, Test Loss: 0.6263, F1: 0.2753, AUC: 0.8147
Mejores resultados en la época:  23
f1-score 0.34890965732087226
AUC según el mejor F1-score 0.8149324490787906
Confusion Matrix:
 [[1199  154]
 [  55   56]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8572
Precision:  0.2667
Recall:     0.5045
F1-score:   0.3489

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6938, Test Loss: 0.7026, F1: 0.1410, AUC: 0.7504
Epoch [10/30] Train Loss: 0.4841, Test Loss: 0.5058, F1: 0.2971, AUC: 0.8004
Epoch [20/30] Train Loss: 0.4633, Test Loss: 0.8917, F1: 0.2187, AUC: 0.8118
Mejores resultados en la época:  29
f1-score 0.34173669467787116
AUC según el mejor F1-score 0.8124088611893491
Confusion Matrix:
 [[1168  185]
 [  50   61]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8395
Precision:  0.2480
Recall:     0.5495
F1-score:   0.3417

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6935, Test Loss: 0.6801, F1: 0.0000, AUC: 0.7528
Epoch [10/30] Train Loss: 0.6023, Test Loss: 0.9721, F1: 0.1942, AUC: 0.7903
Epoch [20/30] Train Loss: 0.4657, Test Loss: 0.4211, F1: 0.3029, AUC: 0.8049
Mejores resultados en la época:  22
f1-score 0.3302180685358255
AUC según el mejor F1-score 0.808014222648369
Confusion Matrix:
 [[1196  157]
 [  58   53]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8531
Precision:  0.2524
Recall:     0.4775
F1-score:   0.3302

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6938, Test Loss: 0.7059, F1: 0.1410, AUC: 0.7363
Epoch [10/30] Train Loss: 0.5089, Test Loss: 0.2770, F1: 0.3117, AUC: 0.7965
Epoch [20/30] Train Loss: 0.4672, Test Loss: 0.7958, F1: 0.2289, AUC: 0.8086
Mejores resultados en la época:  19
f1-score 0.32669322709163345
AUC según el mejor F1-score 0.807355026867222
Confusion Matrix:
 [[1254   99]
 [  70   41]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8846
Precision:  0.2929
Recall:     0.3694
F1-score:   0.3267
Tiempo total para red 4: 20.38 segundos

Entrenando red 5 con capas [300, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6947, Test Loss: 0.7056, F1: 0.1410, AUC: 0.7021
Epoch [10/30] Train Loss: 0.5269, Test Loss: 0.5475, F1: 0.2578, AUC: 0.7822
Epoch [20/30] Train Loss: 0.4483, Test Loss: 0.5401, F1: 0.2976, AUC: 0.8065
Mejores resultados en la época:  23
f1-score 0.32098765432098764
AUC según el mejor F1-score 0.809039638307931
Confusion Matrix:
 [[1192  161]
 [  59   52]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8497
Precision:  0.2441
Recall:     0.4685
F1-score:   0.3210

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6944, Test Loss: 0.6884, F1: 0.0175, AUC: 0.7789
Epoch [10/30] Train Loss: 0.5179, Test Loss: 0.4902, F1: 0.2829, AUC: 0.7979
Epoch [20/30] Train Loss: 0.4275, Test Loss: 0.3749, F1: 0.3279, AUC: 0.8132
Mejores resultados en la época:  20
f1-score 0.32786885245901637
AUC según el mejor F1-score 0.8132212034651061
Confusion Matrix:
 [[1158  195]
 [  51   60]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8320
Precision:  0.2353
Recall:     0.5405
F1-score:   0.3279

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6922, Test Loss: 0.7425, F1: 0.1410, AUC: 0.7525
Epoch [10/30] Train Loss: 0.5407, Test Loss: 0.7445, F1: 0.2433, AUC: 0.7844
Epoch [20/30] Train Loss: 0.4499, Test Loss: 0.4290, F1: 0.3173, AUC: 0.8107
Mejores resultados en la época:  29
f1-score 0.3388888888888889
AUC según el mejor F1-score 0.815125546832864
Confusion Matrix:
 [[1165  188]
 [  50   61]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8374
Precision:  0.2450
Recall:     0.5495
F1-score:   0.3389

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6949, Test Loss: 0.6809, F1: 0.0000, AUC: 0.7314
Epoch [10/30] Train Loss: 0.5469, Test Loss: 0.7113, F1: 0.2404, AUC: 0.7802
Epoch [20/30] Train Loss: 0.4482, Test Loss: 0.5626, F1: 0.2975, AUC: 0.8026
Mejores resultados en la época:  22
f1-score 0.33212996389891697
AUC según el mejor F1-score 0.807574758794271
Confusion Matrix:
 [[1233  120]
 [  65   46]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8736
Precision:  0.2771
Recall:     0.4144
F1-score:   0.3321
Tiempo total para red 5: 17.42 segundos

Entrenando red 6 con capas [300, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6956, Test Loss: 0.6850, F1: 0.0000, AUC: 0.7889
Epoch [10/30] Train Loss: 0.6776, Test Loss: 0.6452, F1: 0.2063, AUC: 0.7901
Epoch [20/30] Train Loss: 0.5266, Test Loss: 0.4571, F1: 0.3095, AUC: 0.8097
Mejores resultados en la época:  26
f1-score 0.33986928104575165
AUC según el mejor F1-score 0.813860423616521
Confusion Matrix:
 [[1210  143]
 [  59   52]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8620
Precision:  0.2667
Recall:     0.4685
F1-score:   0.3399

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6966, Test Loss: 0.6823, F1: 0.0000, AUC: 0.7302
Epoch [10/30] Train Loss: 0.5295, Test Loss: 0.4992, F1: 0.2645, AUC: 0.7872
Epoch [20/30] Train Loss: 0.4622, Test Loss: 0.6154, F1: 0.2767, AUC: 0.8053
Mejores resultados en la época:  17
f1-score 0.3164983164983165
AUC según el mejor F1-score 0.8026008269910709
Confusion Matrix:
 [[1214  139]
 [  64   47]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8613
Precision:  0.2527
Recall:     0.4234
F1-score:   0.3165

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6944, Test Loss: 0.6534, F1: 0.0000, AUC: 0.7684
Epoch [10/30] Train Loss: 0.5381, Test Loss: 0.8669, F1: 0.2063, AUC: 0.7981
Epoch [20/30] Train Loss: 0.4655, Test Loss: 0.7550, F1: 0.2307, AUC: 0.8150
Mejores resultados en la época:  15
f1-score 0.3261802575107296
AUC según el mejor F1-score 0.8036262426506329
Confusion Matrix:
 [[1269   84]
 [  73   38]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8928
Precision:  0.3115
Recall:     0.3423
F1-score:   0.3262

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6928, Test Loss: 0.7052, F1: 0.1410, AUC: 0.7674
Epoch [10/30] Train Loss: 0.5137, Test Loss: 0.3532, F1: 0.3333, AUC: 0.8022
Epoch [20/30] Train Loss: 0.4288, Test Loss: 0.5740, F1: 0.2998, AUC: 0.8160
Mejores resultados en la época:  12
f1-score 0.3484848484848485
AUC según el mejor F1-score 0.806109879280611
Confusion Matrix:
 [[1246  107]
 [  65   46]]
Matriz de confusión guardada en: outputs_only_text/2/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8825
Precision:  0.3007
Recall:     0.4144
F1-score:   0.3485
Tiempo total para red 6: 17.22 segundos
Saved on: outputs_only_text/2/lyrics_bert

==============================
Model: Logistic Regression
Accuracy:  0.7247
Precision: 0.1867
Recall:    0.7838
F1-score:  0.3016
              precision    recall  f1-score   support

           0       0.98      0.72      0.83      1353
           1       0.19      0.78      0.30       111

    accuracy                           0.72      1464
   macro avg       0.58      0.75      0.57      1464
weighted avg       0.92      0.72      0.79      1464

[[974 379]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [01:47:59] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [01:47:59] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/2/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text/2/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7418
Precision: 0.1959
Recall:    0.7748
F1-score:  0.3127
              precision    recall  f1-score   support

           0       0.98      0.74      0.84      1353
           1       0.20      0.77      0.31       111

    accuracy                           0.74      1464
   macro avg       0.59      0.76      0.58      1464
weighted avg       0.92      0.74      0.80      1464

[[1000  353]
 [  25   86]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text/2/lyrics_bert/conf_matrix_svm.png
Modelo guardado como: outputs_only_text/2/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6113
Precision: 0.1132
Recall:    0.6036
F1-score:  0.1906
              precision    recall  f1-score   support

           0       0.95      0.61      0.74      1353
           1       0.11      0.60      0.19       111

    accuracy                           0.61      1464
   macro avg       0.53      0.61      0.47      1464
weighted avg       0.89      0.61      0.70      1464

[[828 525]
 [ 44  67]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text/2/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text/2/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.7541
Precision: 0.1783
Recall:    0.6216
F1-score:  0.2771
              precision    recall  f1-score   support

           0       0.96      0.76      0.85      1353
           1       0.18      0.62      0.28       111

    accuracy                           0.75      1464
   macro avg       0.57      0.69      0.56      1464
weighted avg       0.90      0.75      0.81      1464

[[1035  318]
 [  42   69]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/2/lyrics_bert/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text/2/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.7377
Precision: 0.1818
Recall:    0.7027
F1-score:  0.2889
              precision    recall  f1-score   support

           0       0.97      0.74      0.84      1353
           1       0.18      0.70      0.29       111

    accuracy                           0.74      1464
   macro avg       0.57      0.72      0.56      1464
weighted avg       0.91      0.74      0.80      1464

[[1002  351]
 [  33   78]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text/2/lyrics_bert/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text/2/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6045
Precision: 0.1422
Recall:    0.8378
F1-score:  0.2431
              precision    recall  f1-score   support

           0       0.98      0.59      0.73      1353
           1       0.14      0.84      0.24       111

    accuracy                           0.60      1464
   macro avg       0.56      0.71      0.49      1464
weighted avg       0.91      0.60      0.70      1464

[[792 561]
 [ 18  93]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_only_text/2/lyrics_bert/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text/2/lyrics_bert/naive_bayes_model.pkl


Resumen de métricas:
SVM: {'accuracy': 0.7418, 'precision': 0.1959, 'recall': 0.7748, 'f1_score': 0.3127}
Logistic Regression: {'accuracy': 0.7247, 'precision': 0.1867, 'recall': 0.7838, 'f1_score': 0.3016}
XGBoost: {'accuracy': 0.7377, 'precision': 0.1818, 'recall': 0.7027, 'f1_score': 0.2889}
Random Forest: {'accuracy': 0.7541, 'precision': 0.1783, 'recall': 0.6216, 'f1_score': 0.2771}
Naive Bayes: {'accuracy': 0.6045, 'precision': 0.1422, 'recall': 0.8378, 'f1_score': 0.2431}
Decision Tree: {'accuracy': 0.6113, 'precision': 0.1132, 'recall': 0.6036, 'f1_score': 0.1906}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: LYRICS_BERT
MLP_1007617: {'accuracy': 0.7978142076502732, 'precision': 0.2188449848024316, 'recall': 0.6486486486486487, 'f1_score': 0.34576271186440677, 'f1_score_avg': 0.33708077991063734}
MLP_326657: {'accuracy': 0.7670765027322405, 'precision': 0.21105527638190955, 'recall': 0.7567567567567568, 'f1_score': 0.3413173652694611, 'f1_score_avg': 0.33237535514146077}
MLP_120321: {'accuracy': 0.8087431693989071, 'precision': 0.22295081967213115, 'recall': 0.6126126126126126, 'f1_score': 0.3359580052493438, 'f1_score_avg': 0.32417215338829347}
MLP_48897: {'accuracy': 0.8442622950819673, 'precision': 0.2445414847161572, 'recall': 0.5045045045045045, 'f1_score': 0.32941176470588235, 'f1_score_avg': 0.3261442888196157}
MLP_21377: {'accuracy': 0.842896174863388, 'precision': 0.23318385650224216, 'recall': 0.46846846846846846, 'f1_score': 0.3166023166023166, 'f1_score_avg': 0.3100543310872652}
SVM: {'accuracy': 0.7418, 'precision': 0.1959, 'recall': 0.7748, 'f1_score': 0.3127}
MLP_9665: {'accuracy': 0.819672131147541, 'precision': 0.21561338289962825, 'recall': 0.5225225225225225, 'f1_score': 0.3116883116883117, 'f1_score_avg': 0.3066789448368396}
Logistic Regression: {'accuracy': 0.7247, 'precision': 0.1867, 'recall': 0.7838, 'f1_score': 0.3016}
XGBoost: {'accuracy': 0.7377, 'precision': 0.1818, 'recall': 0.7027, 'f1_score': 0.2889}
Random Forest: {'accuracy': 0.7541, 'precision': 0.1783, 'recall': 0.6216, 'f1_score': 0.2771}
Naive Bayes: {'accuracy': 0.6045, 'precision': 0.1422, 'recall': 0.8378, 'f1_score': 0.2431}
Decision Tree: {'accuracy': 0.6113, 'precision': 0.1132, 'recall': 0.6036, 'f1_score': 0.1906}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: False
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['lyrics']
Numeric Columns: Not used
====================================

 [ 24  87]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/2/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text/2/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7418
Precision: 0.1959
Recall:    0.7748
F1-score:  0.3127
              precision    recall  f1-score   support

           0       0.98      0.74      0.84      1353
           1       0.20      0.77      0.31       111

    accuracy                           0.74      1464
   macro avg       0.59      0.76      0.58      1464
weighted avg       0.92      0.74      0.80      1464

[[1000  353]
 [  25   86]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text/2/lyrics_bert/conf_matrix_svm.png
Modelo guardado como: outputs_only_text/2/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6113
Precision: 0.1132
Recall:    0.6036
F1-score:  0.1906
              precision    recall  f1-score   support

           0       0.95      0.61      0.74      1353
           1       0.11      0.60      0.19       111

    accuracy                           0.61      1464
   macro avg       0.53      0.61      0.47      1464
weighted avg       0.89      0.61      0.70      1464

[[828 525]
 [ 44  67]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text/2/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text/2/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.7541
Precision: 0.1783
Recall:    0.6216
F1-score:  0.2771
              precision    recall  f1-score   support

           0       0.96      0.76      0.85      1353
           1       0.18      0.62      0.28       111

    accuracy                           0.75      1464
   macro avg       0.57      0.69      0.56      1464
weighted avg       0.90      0.75      0.81      1464

[[1035  318]
 [  42   69]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/2/lyrics_bert/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text/2/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.7377
Precision: 0.1818
Recall:    0.7027
F1-score:  0.2889
              precision    recall  f1-score   support

           0       0.97      0.74      0.84      1353
           1       0.18      0.70      0.29       111

    accuracy                           0.74      1464
   macro avg       0.57      0.72      0.56      1464
weighted avg       0.91      0.74      0.80      1464

[[1002  351]
 [  33   78]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text/2/lyrics_bert/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text/2/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6045
Precision: 0.1422
Recall:    0.8378
F1-score:  0.2431
              precision    recall  f1-score   support

           0       0.98      0.59      0.73      1353
           1       0.14      0.84      0.24       111

    accuracy                           0.60      1464
   macro avg       0.56      0.71      0.49      1464
weighted avg       0.91      0.60      0.70      1464

[[792 561]
 [ 18  93]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_only_text/2/lyrics_bert/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text/2/lyrics_bert/naive_bayes_model.pkl


Resumen de métricas:
SVM: {'accuracy': 0.7418, 'precision': 0.1959, 'recall': 0.7748, 'f1_score': 0.3127}
Logistic Regression: {'accuracy': 0.7247, 'precision': 0.1867, 'recall': 0.7838, 'f1_score': 0.3016}
XGBoost: {'accuracy': 0.7377, 'precision': 0.1818, 'recall': 0.7027, 'f1_score': 0.2889}
Random Forest: {'accuracy': 0.7541, 'precision': 0.1783, 'recall': 0.6216, 'f1_score': 0.2771}
Naive Bayes: {'accuracy': 0.6045, 'precision': 0.1422, 'recall': 0.8378, 'f1_score': 0.2431}
Decision Tree: {'accuracy': 0.6113, 'precision': 0.1132, 'recall': 0.6036, 'f1_score': 0.1906}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: LYRICS_BERT
MLP_120321: {'accuracy': 0.8845628415300546, 'precision': 0.29285714285714287, 'recall': 0.36936936936936937, 'f1_score': 0.34890965732087226, 'f1_score_avg': 0.3368894119065506}
MLP_326657: {'accuracy': 0.8736338797814208, 'precision': 0.27710843373493976, 'recall': 0.4144144144144144, 'f1_score': 0.34890965732087226, 'f1_score_avg': 0.32996883989195247}
MLP_1007617: {'accuracy': 0.8825136612021858, 'precision': 0.3006535947712418, 'recall': 0.4144144144144144, 'f1_score': 0.34890965732087226, 'f1_score_avg': 0.3327581758849115}
MLP_48897: {'accuracy': 0.796448087431694, 'precision': 0.2123076923076923, 'recall': 0.6216216216216216, 'f1_score': 0.32764505119453924, 'f1_score_avg': 0.31856673924561496}
MLP_9665: {'accuracy': 0.8558743169398907, 'precision': 0.25, 'recall': 0.45045045045045046, 'f1_score': 0.3215434083601286, 'f1_score_avg': 0.3142729490285997}
MLP_21377: {'accuracy': 0.7561475409836066, 'precision': 0.19852941176470587, 'recall': 0.7297297297297297, 'f1_score': 0.3215434083601286, 'f1_score_avg': 0.3096676562295076}
SVM: {'accuracy': 0.7418, 'precision': 0.1959, 'recall': 0.7748, 'f1_score': 0.3127}
Logistic Regression: {'accuracy': 0.7247, 'precision': 0.1867, 'recall': 0.7838, 'f1_score': 0.3016}
XGBoost: {'accuracy': 0.7377, 'precision': 0.1818, 'recall': 0.7027, 'f1_score': 0.2889}
Random Forest: {'accuracy': 0.7541, 'precision': 0.1783, 'recall': 0.6216, 'f1_score': 0.2771}
Naive Bayes: {'accuracy': 0.6045, 'precision': 0.1422, 'recall': 0.8378, 'f1_score': 0.2431}
Decision Tree: {'accuracy': 0.6113, 'precision': 0.1132, 'recall': 0.6036, 'f1_score': 0.1906}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: False
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['lyrics']
Numeric Columns: Not used
====================================

