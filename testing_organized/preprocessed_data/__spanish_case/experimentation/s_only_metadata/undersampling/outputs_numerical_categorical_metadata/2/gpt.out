2025-10-30 00:26:36.017819: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-30 00:26:36.017819: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__spanish_case/experimentation/s_only_metadata/undersampling/metadata_GPT.py:267: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__spanish_case/experimentation/s_only_metadata/undersampling/metadata_GPT.py:267: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
For TF-IDF embbedings you are selecteing this columns:
--> ['artist', 'genre', 'title_songs_new']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 3 ['title_songs_new', 'artist', 'genre']

CAT_LOW 1 ['title_songs_new']
CAT_HIGH 2 ['artist', 'genre']
For both embbedings your are adding this columns: 
--> ['lyrics_word_count', 'popularity', 'duration_ms']
You are executing with [ALL] dataset
--> PaTH:  ../../../../../../data/spanish/LB_M/LB_fuss/lb_khipu_M.npy
Loading Lb vectors from:  ../../../../../../data/spanish/dataset/oficialDatasetEAIM2026.csv
Label distribution: {False: 6765, True: 554}
X shape: (7319, 1536)
y shape: (7319,)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (5855, 3)
X_train_Numeric:  (5855, 3)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (5855, 1539)
Shape of X_test after concatenation:  (1464, 1539)
Shape of y_train:  (5855,)
Shape of y_test:  (1464,)

Data con el spliting...
Label distribution en TRAIN: {0: 5412, 1: 443}
Label distribution en TEST: {0: 1353, 1: 111}


==================================================
Data antes del undersampling ...
X: (5855, 1539)
y: (5855,)
Apliying UNDERSAMPLE
443
Label distribution: {0: 443, 1: 443}
X shape: (886, 1539)
y shape: (886,)
Resultados con MLP

Entrenando red 1 con capas [1539, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.7004, Test Loss: 0.6787, F1: 0.2809, AUC: 0.7746
Epoch [10/30] Train Loss: 0.4147, Test Loss: 0.4047, F1: 0.4219, AUC: 0.8666
Epoch [20/30] Train Loss: 0.3193, Test Loss: 0.3941, F1: 0.4221, AUC: 0.8850
Mejores resultados en la época:  27
f1-score 0.46706586826347307
AUC según el mejor F1-score 0.8923646484622095
Confusion Matrix:
 [[1208  145]
 [  33   78]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_49313.png
Accuracy:   0.8784
Precision:  0.3498
Recall:     0.7027
F1-score:   0.4671

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.7022, Test Loss: 0.6276, F1: 0.0000, AUC: 0.7607
Epoch [10/30] Train Loss: 0.5329, Test Loss: 0.7126, F1: 0.2379, AUC: 0.8496
Epoch [20/30] Train Loss: 0.4116, Test Loss: 0.4088, F1: 0.4290, AUC: 0.8663
Mejores resultados en la época:  21
f1-score 0.4476744186046512
AUC según el mejor F1-score 0.8677480140894775
Confusion Matrix:
 [[1197  156]
 [  34   77]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_49313.png
Accuracy:   0.8702
Precision:  0.3305
Recall:     0.6937
F1-score:   0.4477

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6945, Test Loss: 0.7102, F1: 0.1452, AUC: 0.8114
Epoch [10/30] Train Loss: 0.4403, Test Loss: 0.4428, F1: 0.4062, AUC: 0.8627
Epoch [20/30] Train Loss: 0.3373, Test Loss: 0.5663, F1: 0.3501, AUC: 0.8814
Mejores resultados en la época:  22
f1-score 0.4536082474226804
AUC según el mejor F1-score 0.8836486153559324
Confusion Matrix:
 [[1164  189]
 [  23   88]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_49313.png
Accuracy:   0.8552
Precision:  0.3177
Recall:     0.7928
F1-score:   0.4536

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6841, Test Loss: 0.6933, F1: 0.2302, AUC: 0.8205
Epoch [10/30] Train Loss: 0.4156, Test Loss: 0.5689, F1: 0.3398, AUC: 0.8674
Epoch [20/30] Train Loss: 0.3189, Test Loss: 0.3455, F1: 0.4562, AUC: 0.8856
Mejores resultados en la época:  26
f1-score 0.47592067988668557
AUC según el mejor F1-score 0.8919784529540626
Confusion Matrix:
 [[1195  158]
 [  27   84]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_49313.png
Accuracy:   0.8736
Precision:  0.3471
Recall:     0.7568
F1-score:   0.4759
Tiempo total para red 1: 18.58 segundos

Entrenando red 2 con capas [1539, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6925, Test Loss: 0.7107, F1: 0.1422, AUC: 0.8357
Epoch [10/30] Train Loss: 0.3785, Test Loss: 0.3103, F1: 0.4526, AUC: 0.8764
Epoch [20/30] Train Loss: 0.2974, Test Loss: 0.6524, F1: 0.3361, AUC: 0.8926
Mejores resultados en la época:  24
f1-score 0.4838709677419355
AUC según el mejor F1-score 0.8953609929219685
Confusion Matrix:
 [[1229  124]
 [  36   75]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_100673.png
Accuracy:   0.8907
Precision:  0.3769
Recall:     0.6757
F1-score:   0.4839

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6970, Test Loss: 0.6672, F1: 0.0000, AUC: 0.7975
Epoch [10/30] Train Loss: 0.3946, Test Loss: 0.3810, F1: 0.4241, AUC: 0.8722
Epoch [20/30] Train Loss: 0.2845, Test Loss: 0.5061, F1: 0.3851, AUC: 0.8944
Mejores resultados en la época:  19
f1-score 0.4722222222222222
AUC según el mejor F1-score 0.8929772344406491
Confusion Matrix:
 [[1189  164]
 [  26   85]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_100673.png
Accuracy:   0.8702
Precision:  0.3414
Recall:     0.7658
F1-score:   0.4722

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6924, Test Loss: 0.6995, F1: 0.1647, AUC: 0.7854
Epoch [10/30] Train Loss: 0.3449, Test Loss: 0.3099, F1: 0.4597, AUC: 0.8773
Epoch [20/30] Train Loss: 0.2658, Test Loss: 0.4685, F1: 0.4025, AUC: 0.8954
Mejores resultados en la época:  22
f1-score 0.4835820895522388
AUC según el mejor F1-score 0.8972786533762144
Confusion Matrix:
 [[1210  143]
 [  30   81]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_100673.png
Accuracy:   0.8818
Precision:  0.3616
Recall:     0.7297
F1-score:   0.4836

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6927, Test Loss: 0.7321, F1: 0.1410, AUC: 0.8127
Epoch [10/30] Train Loss: 0.3837, Test Loss: 0.4762, F1: 0.3770, AUC: 0.8762
Epoch [20/30] Train Loss: 0.2793, Test Loss: 0.4362, F1: 0.4079, AUC: 0.8927
Mejores resultados en la época:  21
f1-score 0.47592067988668557
AUC según el mejor F1-score 0.8940892111623818
Confusion Matrix:
 [[1195  158]
 [  27   84]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_100673.png
Accuracy:   0.8736
Precision:  0.3471
Recall:     0.7568
F1-score:   0.4759
Tiempo total para red 2: 17.40 segundos

Entrenando red 3 con capas [1539, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6909, Test Loss: 0.7904, F1: 0.1410, AUC: 0.8412
Epoch [10/30] Train Loss: 0.3306, Test Loss: 0.3938, F1: 0.4211, AUC: 0.8860
Epoch [20/30] Train Loss: 0.2455, Test Loss: 0.5136, F1: 0.4016, AUC: 0.8975
Mejores resultados en la época:  11
f1-score 0.4782608695652174
AUC según el mejor F1-score 0.8887224253077912
Confusion Matrix:
 [[1254   99]
 [  45   66]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_207489.png
Accuracy:   0.9016
Precision:  0.4000
Recall:     0.5946
F1-score:   0.4783

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6929, Test Loss: 0.7502, F1: 0.1410, AUC: 0.8229
Epoch [10/30] Train Loss: 0.3680, Test Loss: 0.3705, F1: 0.4315, AUC: 0.8805
Epoch [20/30] Train Loss: 0.2382, Test Loss: 0.5174, F1: 0.4049, AUC: 0.8973
Mejores resultados en la época:  22
f1-score 0.5034013605442177
AUC según el mejor F1-score 0.8990298502493624
Confusion Matrix:
 [[1244  109]
 [  37   74]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_207489.png
Accuracy:   0.9003
Precision:  0.4044
Recall:     0.6667
F1-score:   0.5034

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6926, Test Loss: 0.8026, F1: 0.1410, AUC: 0.8409
Epoch [10/30] Train Loss: 0.3669, Test Loss: 0.7189, F1: 0.2990, AUC: 0.8813
Epoch [20/30] Train Loss: 0.2641, Test Loss: 0.3122, F1: 0.4709, AUC: 0.8974
Mejores resultados en la época:  26
f1-score 0.4968553459119497
AUC según el mejor F1-score 0.9020395117956094
Confusion Matrix:
 [[1225  128]
 [  32   79]]
For TF-IDF embbedings you are selecteing this columns:
--> ['artist', 'genre', 'title_songs_new']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 3 ['title_songs_new', 'artist', 'genre']

CAT_LOW 1 ['title_songs_new']
CAT_HIGH 2 ['artist', 'genre']
For both embbedings your are adding this columns: 
--> ['lyrics_word_count', 'popularity', 'duration_ms']
You are executing with [ALL] dataset
--> PaTH:  ../../../../../../data/spanish/LB_M/LB_fuss/lb_khipu_M.npy
Loading Lb vectors from:  ../../../../../../data/spanish/dataset/oficialDatasetEAIM2026.csv
Label distribution: {False: 6765, True: 554}
X shape: (7319, 1536)
y shape: (7319,)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (5855, 3)
X_train_Numeric:  (5855, 3)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (5855, 1539)
Shape of X_test after concatenation:  (1464, 1539)
Shape of y_train:  (5855,)
Shape of y_test:  (1464,)

Data con el spliting...
Label distribution en TRAIN: {0: 5412, 1: 443}
Label distribution en TEST: {0: 1353, 1: 111}


==================================================
Data antes del undersampling ...
X: (5855, 1539)
y: (5855,)
Apliying UNDERSAMPLE
443
Label distribution: {0: 443, 1: 443}
X shape: (886, 1539)
y shape: (886,)
Resultados con MLP

Entrenando red 1 con capas [1539, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.7062, Test Loss: 0.6137, F1: 0.0000, AUC: 0.7590
Epoch [10/30] Train Loss: 0.6235, Test Loss: 0.4582, F1: 0.2553, AUC: 0.8532
Epoch [20/30] Train Loss: 0.5173, Test Loss: 0.3595, F1: 0.4218, AUC: 0.8667
Mejores resultados en la época:  27
f1-score 0.4476744186046512
AUC según el mejor F1-score 0.8704180899302849
Confusion Matrix:
 [[1197  156]
 [  34   77]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_49313.png
Accuracy:   0.8702
Precision:  0.3305
Recall:     0.6937
F1-score:   0.4477

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.7014, Test Loss: 0.7339, F1: 0.1410, AUC: 0.8186
Epoch [10/30] Train Loss: 0.5080, Test Loss: 0.5147, F1: 0.3844, AUC: 0.8532
Epoch [20/30] Train Loss: 0.3698, Test Loss: 0.5234, F1: 0.3626, AUC: 0.8731
Mejores resultados en la época:  21
f1-score 0.43967828418230565
AUC según el mejor F1-score 0.8744065573333866
Confusion Matrix:
 [[1173  180]
 [  29   82]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_49313.png
Accuracy:   0.8572
Precision:  0.3130
Recall:     0.7387
F1-score:   0.4397

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6867, Test Loss: 0.6305, F1: 0.4040, AUC: 0.8415
Epoch [10/30] Train Loss: 0.3738, Test Loss: 0.4055, F1: 0.4229, AUC: 0.8758
Epoch [20/30] Train Loss: 0.2842, Test Loss: 0.3520, F1: 0.4405, AUC: 0.8930
Mejores resultados en la época:  27
f1-score 0.48580441640378547
AUC según el mejor F1-score 0.8978912393546539
Confusion Matrix:
 [[1224  129]
 [  34   77]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_49313.png
Accuracy:   0.8887
Precision:  0.3738
Recall:     0.6937
F1-score:   0.4858

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.7040, Test Loss: 0.6395, F1: 0.0000, AUC: 0.6873
Epoch [10/30] Train Loss: 0.6475, Test Loss: 0.5560, F1: 0.0000, AUC: 0.8474
Epoch [20/30] Train Loss: 0.5497, Test Loss: 0.3521, F1: 0.3942, AUC: 0.8645
Mejores resultados en la época:  29
f1-score 0.44047619047619047
AUC según el mejor F1-score 0.8657204876717072
Confusion Matrix:
 [[1202  151]
 [  37   74]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_49313.png
Accuracy:   0.8716
Precision:  0.3289
Recall:     0.6667
F1-score:   0.4405
Tiempo total para red 1: 18.58 segundos

Entrenando red 2 con capas [1539, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6912, Test Loss: 0.7201, F1: 0.1450, AUC: 0.8410
Epoch [10/30] Train Loss: 0.3494, Test Loss: 0.4626, F1: 0.3859, AUC: 0.8823
Epoch [20/30] Train Loss: 0.2378, Test Loss: 0.5720, F1: 0.3713, AUC: 0.8988
Mejores resultados en la época:  15
f1-score 0.46111111111111114
AUC según el mejor F1-score 0.8928773562919906
Confusion Matrix:
 [[1187  166]
 [  28   83]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_100673.png
Accuracy:   0.8675
Precision:  0.3333
Recall:     0.7477
F1-score:   0.4611

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6951, Test Loss: 0.6324, F1: 0.0000, AUC: 0.8163
Epoch [10/30] Train Loss: 0.3852, Test Loss: 0.5438, F1: 0.3599, AUC: 0.8707
Epoch [20/30] Train Loss: 0.2859, Test Loss: 0.4610, F1: 0.4043, AUC: 0.8893
Mejores resultados en la época:  24
f1-score 0.4742857142857143
AUC según el mejor F1-score 0.893929406124528
Confusion Matrix:
 [[1197  156]
 [  28   83]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_100673.png
Accuracy:   0.8743
Precision:  0.3473
Recall:     0.7477
F1-score:   0.4743

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6960, Test Loss: 0.8128, F1: 0.1410, AUC: 0.8004
Epoch [10/30] Train Loss: 0.4177, Test Loss: 0.4037, F1: 0.4216, AUC: 0.8637
Epoch [20/30] Train Loss: 0.3128, Test Loss: 0.3097, F1: 0.4642, AUC: 0.8894
Mejores resultados en la época:  20
f1-score 0.46418338108882523
AUC según el mejor F1-score 0.8894282308916454
Confusion Matrix:
 [[1196  157]
 [  30   81]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_100673.png
Accuracy:   0.8723
Precision:  0.3403
Recall:     0.7297
F1-score:   0.4642

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6913, Test Loss: 0.8444, F1: 0.1410, AUC: 0.8352
Epoch [10/30] Train Loss: 0.3485, Test Loss: 0.5306, F1: 0.3702, AUC: 0.8775
Epoch [20/30] Train Loss: 0.2420, Test Loss: 0.3884, F1: 0.4303, AUC: 0.8968
Mejores resultados en la época:  23
f1-score 0.455026455026455
AUC según el mejor F1-score 0.8981708981708981
Confusion Matrix:
 [[1172  181]
 [  25   86]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_100673.png
Accuracy:   0.8593
Precision:  0.3221
Recall:     0.7748
F1-score:   0.4550
Tiempo total para red 2: 17.39 segundos

Entrenando red 3 con capas [1539, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6984, Test Loss: 0.6113, F1: 0.0000, AUC: 0.7190
Epoch [10/30] Train Loss: 0.3300, Test Loss: 0.4725, F1: 0.3867, AUC: 0.8798
Epoch [20/30] Train Loss: 0.2969, Test Loss: 0.3595, F1: 0.4373, AUC: 0.8952
Mejores resultados en la época:  28
f1-score 0.4984025559105431
AUC según el mejor F1-score 0.8992628992628993
Confusion Matrix:
 [[1229  124]
 [  33   78]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_207489.png
Accuracy:   0.8928
Precision:  0.3861
Recall:     0.7027
F1-score:   0.4984

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6893, Test Loss: 0.7416, F1: 0.1410, AUC: 0.8412
Epoch [10/30] Train Loss: 0.3283, Test Loss: 0.7411, F1: 0.2960, AUC: 0.8856
Epoch [20/30] Train Loss: 0.2774, Test Loss: 0.3890, F1: 0.4365, AUC: 0.8997
Mejores resultados en la época:  27
f1-score 0.4850498338870432
AUC según el mejor F1-score 0.8990564844223381
Confusion Matrix:
 [[1236  117]
 [  38   73]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_207489.png
Accuracy:   0.8941
Precision:  0.3842
Recall:     0.6577
F1-score:   0.4850

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6918, Test Loss: 0.6284, F1: 0.0000, AUC: 0.8485
Epoch [10/30] Train Loss: 0.4169, Test Loss: 0.4012, F1: 0.4027, AUC: 0.8877
Epoch [20/30] Train Loss: 0.2550, Test Loss: 0.4127, F1: 0.4253, AUC: 0.8982
Mejores resultados en la época:  17
f1-score 0.5037593984962406
AUC según el mejor F1-score 0.8950214072165291
Confusion Matrix:
 [[1265   88]
 [  44   67]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_207489.png
Accuracy:   0.8907
Precision:  0.3816
Recall:     0.7117
F1-score:   0.4969

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6936, Test Loss: 0.6704, F1: 0.2090, AUC: 0.8275
Epoch [10/30] Train Loss: 0.3068, Test Loss: 0.4732, F1: 0.3910, AUC: 0.8883
Epoch [20/30] Train Loss: 0.2728, Test Loss: 0.3591, F1: 0.4505, AUC: 0.9007
Mejores resultados en la época:  15
f1-score 0.48
AUC según el mejor F1-score 0.898483849703362
Confusion Matrix:
 [[1217  136]
 [  33   78]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_207489.png
Accuracy:   0.8846
Precision:  0.3645
Recall:     0.7027
F1-score:   0.4800
Tiempo total para red 3: 18.48 segundos

Entrenando red 4 con capas [1539, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6947, Test Loss: 0.6451, F1: 0.0000, AUC: 0.8319
Epoch [10/30] Train Loss: 0.3321, Test Loss: 0.4288, F1: 0.4069, AUC: 0.8851
Epoch [20/30] Train Loss: 0.3188, Test Loss: 0.1994, F1: 0.4314, AUC: 0.9023
Mejores resultados en la época:  21
f1-score 0.4925373134328358
AUC según el mejor F1-score 0.90096748633334
Confusion Matrix:
 [[1262   91]
 [  45   66]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_437505.png
Accuracy:   0.9071
Precision:  0.4204
Recall:     0.5946
F1-score:   0.4925

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6954, Test Loss: 0.7140, F1: 0.1410, AUC: 0.8290
Epoch [10/30] Train Loss: 0.3943, Test Loss: 1.0956, F1: 0.2004, AUC: 0.8881
Epoch [20/30] Train Loss: 0.2446, Test Loss: 0.5213, F1: 0.3938, AUC: 0.8988
Mejores resultados en la época:  27
f1-score 0.47863247863247865
AUC según el mejor F1-score 0.9016333406577309
Confusion Matrix:
 [[1197  156]
 [  27   84]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_437505.png
Accuracy:   0.8750
Precision:  0.3500
Recall:     0.7568
F1-score:   0.4786

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6945, Test Loss: 0.6960, F1: 0.1410, AUC: 0.7931
Epoch [10/30] Train Loss: 0.3609, Test Loss: 0.3423, F1: 0.4450, AUC: 0.8822
Epoch [20/30] Train Loss: 0.2966, Test Loss: 0.3100, F1: 0.4661, AUC: 0.8999
Mejores resultados en la época:  18
f1-score 0.484375
AUC según el mejor F1-score 0.898543776592557
Confusion Matrix:
 [[1270   83]
 [  49   62]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_437505.png
Accuracy:   0.9098
Precision:  0.4276
Recall:     0.5586
F1-score:   0.4844

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6875, Test Loss: 0.5697, F1: 0.0000, AUC: 0.8172
Epoch [10/30] Train Loss: 0.3539, Test Loss: 0.3574, F1: 0.4456, AUC: 0.8868
Epoch [20/30] Train Loss: 0.2226, Test Loss: 0.7238, F1: 0.3361, AUC: 0.8987
Mejores resultados en la época:  29
f1-score 0.4897959183673469
AUC según el mejor F1-score 0.9005413395657299
Confusion Matrix:
 [[1242  111]
 [  39   72]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_437505.png
Accuracy:   0.8975
Precision:  0.3934
Recall:     0.6486
F1-score:   0.4898
Tiempo total para red 4: 19.37 segundos

Entrenando red 5 con capas [1539, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6893, Test Loss: 0.5647, F1: 0.0000, AUC: 0.8398
Epoch [10/30] Train Loss: 0.3007, Test Loss: 0.5317, F1: 0.3706, AUC: 0.8955
Epoch [20/30] Train Loss: 0.2532, Test Loss: 0.3318, F1: 0.4580, AUC: 0.8990
Mejores resultados en la época:  24
f1-score 0.4983388704318937
AUC según el mejor F1-score 0.900015314649461
Confusion Matrix:
 [[1238  115]
 [  36   75]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_961025.png
Accuracy:   0.8969
Precision:  0.3947
Recall:     0.6757
F1-score:   0.4983

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6931, Test Loss: 0.7235, F1: 0.1410, AUC: 0.8482
Epoch [10/30] Train Loss: 0.3744, Test Loss: 0.2677, F1: 0.4669, AUC: 0.8896
Epoch [20/30] Train Loss: 0.2315, Test Loss: 0.4246, F1: 0.4251, AUC: 0.9041
Mejores resultados en la época:  14
f1-score 0.46808510638297873
AUC según el mejor F1-score 0.899356118868314
Confusion Matrix:
 [[1176  177]
 [  23   88]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_961025.png
Accuracy:   0.8634
Precision:  0.3321
Recall:     0.7928
F1-score:   0.4681

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6940, Test Loss: 0.6978, F1: 0.1428, AUC: 0.8393
Epoch [10/30] Train Loss: 0.3348, Test Loss: 0.2897, F1: 0.4615, AUC: 0.8873
Epoch [20/30] Train Loss: 0.2416, Test Loss: 0.5399, F1: 0.3848, AUC: 0.9003
Mejores resultados en la época:  28
f1-score 0.49504950495049505
AUC según el mejor F1-score 0.9022659022659022
Confusion Matrix:
 [[1236  117]
 [  36   75]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_961025.png
Accuracy:   0.8955
Precision:  0.3906
Recall:     0.6757
F1-score:   0.4950

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6911, Test Loss: 0.6755, F1: 0.3799, AUC: 0.8358
Epoch [10/30] Train Loss: 0.3340, Test Loss: 0.6228, F1: 0.3265, AUC: 0.8859
Epoch [20/30] Train Loss: 0.2279, Test Loss: 0.6257, F1: 0.3601, AUC: 0.9006
Mejores resultados en la época:  23
f1-score 0.50814332247557
AUC según el mejor F1-score 0.9027253417497321
Confusion Matrix:
 [[1235  118]
 [  33   78]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_961025.png
Accuracy:   0.8969
Precision:  0.3980
Recall:     0.7027
F1-score:   0.5081
Tiempo total para red 5: 20.11 segundos

Entrenando red 6 con capas [1539, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6950, Test Loss: 0.6739, F1: 0.0000, AUC: 0.3888
Epoch [10/30] Train Loss: 0.4686, Test Loss: 0.4435, F1: 0.3862, AUC: 0.8788
Epoch [20/30] Train Loss: 0.2808, Test Loss: 0.3584, F1: 0.4348, AUC: 0.8982
Mejores resultados en la época:  26
f1-score 0.48598130841121495
AUC según el mejor F1-score 0.9017132431766577
Confusion Matrix:
 [[1221  132]
 [  33   78]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_2276353.png
Accuracy:   0.8873
Precision:  0.3714
Recall:     0.7027
F1-score:   0.4860

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6899, Test Loss: 0.6319, F1: 0.0000, AUC: 0.8363
Epoch [10/30] Train Loss: 0.3526, Test Loss: 0.6002, F1: 0.3280, AUC: 0.8885
Epoch [20/30] Train Loss: 0.2178, Test Loss: 0.6730, F1: 0.3394, AUC: 0.9005
Mejores resultados en la época:  22
f1-score 0.5064935064935064
AUC según el mejor F1-score 0.9010140961360473
Confusion Matrix:
 [[1234  119]
 [  33   78]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_2276353.png
Accuracy:   0.8962
Precision:  0.3959
Recall:     0.7027
F1-score:   0.5065

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6939, Test Loss: 0.6449, F1: 0.0000, AUC: 0.6514
Epoch [10/30] Train Loss: 0.3569, Test Loss: 0.5444, F1: 0.3306, AUC: 0.8874
Epoch [20/30] Train Loss: 0.3189, Test Loss: 0.1839, F1: 0.2914, AUC: 0.8972
Mejores resultados en la época:  29
f1-score 0.4892086330935252
AUC según el mejor F1-score 0.8985171424195814
Confusion Matrix:
 [[1254   99]
 [  43   68]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_2276353.png
Accuracy:   0.9030
Precision:  0.4072
Recall:     0.6126
F1-score:   0.4892

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6946, Test Loss: 0.7105, F1: 0.1410, AUC: 0.7774
Epoch [10/30] Train Loss: 0.3535, Test Loss: 0.3783, F1: 0.4112, AUC: 0.8765
Epoch [20/30] Train Loss: 0.2438, Test Loss: 0.3315, F1: 0.4615, AUC: 0.8987
Mejores resultados en la época:  17
f1-score 0.4876543209876543
AUC según el mejor F1-score 0.8948216509192118
Confusion Matrix:
 [[1219  134]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_207489.png
Accuracy:   0.9098
Precision:  0.4323
Recall:     0.6036
F1-score:   0.5038

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6881, Test Loss: 0.7453, F1: 0.1410, AUC: 0.8206
Epoch [10/30] Train Loss: 0.3350, Test Loss: 0.3206, F1: 0.4568, AUC: 0.8840
Epoch [20/30] Train Loss: 0.2507, Test Loss: 0.8457, F1: 0.3019, AUC: 0.8975
Mejores resultados en la época:  29
f1-score 0.48951048951048953
AUC según el mejor F1-score 0.8973119460924339
Confusion Matrix:
 [[1248  105]
 [  41   70]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_207489.png
Accuracy:   0.9003
Precision:  0.4000
Recall:     0.6306
F1-score:   0.4895
Tiempo total para red 3: 18.49 segundos

Entrenando red 4 con capas [1539, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6948, Test Loss: 0.6647, F1: 0.0000, AUC: 0.8418
Epoch [10/30] Train Loss: 0.3262, Test Loss: 0.6384, F1: 0.3394, AUC: 0.8876
Epoch [20/30] Train Loss: 0.2643, Test Loss: 0.4773, F1: 0.4008, AUC: 0.8959
Mejores resultados en la época:  22
f1-score 0.5
AUC según el mejor F1-score 0.8973585558951412
Confusion Matrix:
 [[1254   99]
 [  41   70]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_437505.png
Accuracy:   0.9044
Precision:  0.4142
Recall:     0.6306
F1-score:   0.5000

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6924, Test Loss: 0.7610, F1: 0.1410, AUC: 0.8346
Epoch [10/30] Train Loss: 0.3459, Test Loss: 0.3447, F1: 0.4462, AUC: 0.8857
Epoch [20/30] Train Loss: 0.3307, Test Loss: 0.5911, F1: 0.3394, AUC: 0.8949
Mejores resultados en la época:  18
f1-score 0.4846153846153846
AUC según el mejor F1-score 0.8942623332867236
Confusion Matrix:
 [[1267   86]
 [  48   63]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_437505.png
Accuracy:   0.9085
Precision:  0.4228
Recall:     0.5676
F1-score:   0.4846

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6903, Test Loss: 0.6493, F1: 0.3000, AUC: 0.8358
Epoch [10/30] Train Loss: 0.3050, Test Loss: 0.4066, F1: 0.4107, AUC: 0.8896
Epoch [20/30] Train Loss: 0.2298, Test Loss: 0.2809, F1: 0.4909, AUC: 0.9012
Mejores resultados en la época:  20
f1-score 0.4909090909090909
AUC según el mejor F1-score 0.9011872182603891
Confusion Matrix:
 [[1215  138]
 [  30   81]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_437505.png
Accuracy:   0.8852
Precision:  0.3699
Recall:     0.7297
F1-score:   0.4909

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6911, Test Loss: 0.6841, F1: 0.3810, AUC: 0.8343
Epoch [10/30] Train Loss: 0.3156, Test Loss: 0.4966, F1: 0.3968, AUC: 0.8907
Epoch [20/30] Train Loss: 0.2688, Test Loss: 0.4426, F1: 0.4165, AUC: 0.9015
Mejores resultados en la época:  21
f1-score 0.5058365758754864
AUC según el mejor F1-score 0.9021260728577801
Confusion Matrix:
 [[1272   81]
 [  46   65]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_437505.png
Accuracy:   0.9133
Precision:  0.4452
Recall:     0.5856
F1-score:   0.5058
Tiempo total para red 4: 19.36 segundos

Entrenando red 5 con capas [1539, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6934, Test Loss: 0.6962, F1: 0.1835, AUC: 0.8332
Epoch [10/30] Train Loss: 0.4132, Test Loss: 0.6516, F1: 0.2918, AUC: 0.8882
Epoch [20/30] Train Loss: 0.2165, Test Loss: 0.4986, F1: 0.4025, AUC: 0.9011
Mejores resultados en la época:  15
f1-score 0.49469964664310956
AUC según el mejor F1-score 0.8940625769894062
Confusion Matrix:
 [[1251  102]
 [  41   70]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_961025.png
Accuracy:   0.9023
Precision:  0.4070
Recall:     0.6306
F1-score:   0.4947

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6946, Test Loss: 0.7338, F1: 0.1410, AUC: 0.8239
Epoch [10/30] Train Loss: 0.3223, Test Loss: 0.2964, F1: 0.4509, AUC: 0.8829
Epoch [20/30] Train Loss: 0.2517, Test Loss: 0.4178, F1: 0.4276, AUC: 0.9007
Mejores resultados en la época:  25
f1-score 0.5048543689320388
AUC según el mejor F1-score 0.9017864871523409
Confusion Matrix:
 [[1233  120]
 [  33   78]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_961025.png
Accuracy:   0.8955
Precision:  0.3939
Recall:     0.7027
F1-score:   0.5049

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6963, Test Loss: 0.7508, F1: 0.1410, AUC: 0.8225
Epoch [10/30] Train Loss: 0.3109, Test Loss: 0.3489, F1: 0.4536, AUC: 0.8851
Epoch [20/30] Train Loss: 0.2394, Test Loss: 0.2971, F1: 0.4789, AUC: 0.8997
Mejores resultados en la época:  21
f1-score 0.5
AUC según el mejor F1-score 0.8999753633899974
Confusion Matrix:
 [[1257   96]
 [  42   69]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_961025.png
Accuracy:   0.9057
Precision:  0.4182
Recall:     0.6216
F1-score:   0.5000

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6919, Test Loss: 0.7177, F1: 0.1410, AUC: 0.8306
Epoch [10/30] Train Loss: 0.3249, Test Loss: 0.3635, F1: 0.4363, AUC: 0.8942
Epoch [20/30] Train Loss: 0.2072, Test Loss: 0.4185, F1: 0.4327, AUC: 0.9008
Mejores resultados en la época:  15
f1-score 0.4928571428571429
AUC según el mejor F1-score 0.8990498258790941
Confusion Matrix:
 [[1253  100]
 [  42   69]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_961025.png
Accuracy:   0.9030
Precision:  0.4083
Recall:     0.6216
F1-score:   0.4929
Tiempo total para red 5: 20.11 segundos

Entrenando red 6 con capas [1539, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6943, Test Loss: 0.6783, F1: 0.0000, AUC: 0.6701
Epoch [10/30] Train Loss: 0.3201, Test Loss: 0.8494, F1: 0.2810, AUC: 0.8877
Epoch [20/30] Train Loss: 0.3203, Test Loss: 0.2654, F1: 0.4800, AUC: 0.8977
Mejores resultados en la época:  21
f1-score 0.48125
AUC según el mejor F1-score 0.8989366306439477
Confusion Matrix:
 [[1221  132]
 [  34   77]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_2276353.png
Accuracy:   0.8866
Precision:  0.3684
Recall:     0.6937
F1-score:   0.4813

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6944, Test Loss: 0.6928, F1: 0.3322, AUC: 0.8566
Epoch [10/30] Train Loss: 0.4151, Test Loss: 0.5951, F1: 0.3140, AUC: 0.8849
Epoch [20/30] Train Loss: 0.2797, Test Loss: 0.2538, F1: 0.4889, AUC: 0.9011
Mejores resultados en la época:  20
f1-score 0.4888888888888889
AUC según el mejor F1-score 0.9010673644819986
Confusion Matrix:
 [[1226  127]
 [  34   77]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_2276353.png
Accuracy:   0.8900
Precision:  0.3775
Recall:     0.6937
F1-score:   0.4889

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6932, Test Loss: 0.6345, F1: 0.0000, AUC: 0.1803
Epoch [10/30] Train Loss: 0.3914, Test Loss: 0.5897, F1: 0.2987, AUC: 0.8670
Epoch [20/30] Train Loss: 0.2705, Test Loss: 0.2812, F1: 0.4611, AUC: 0.8953
Mejores resultados en la época:  29
f1-score 0.48854961832061067
AUC según el mejor F1-score 0.9011539255441694
Confusion Matrix:
 [[1266   87]
 [  47   64]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_2276353.png
Accuracy:   0.9085
Precision:  0.4238
Recall:     0.5766
F1-score:   0.4885

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6942, Test Loss: 0.7078, F1: 0.1410, AUC: 0.8406
Epoch [10/30] Train Loss: 0.3658, Test Loss: 0.2280, F1: 0.4797, AUC: 0.8888
Epoch [20/30] Train Loss: 0.2449, Test Loss: 0.3220, F1: 0.4574, AUC: 0.8992
Mejores resultados en la época:  22
f1-score 0.5049180327868853
AUC según el mejor F1-score 0.8994493384737288
Confusion Matrix:
 [[1236  117]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [00:28:57] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [00:28:58] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
 [  34   77]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_2276353.png
Accuracy:   0.8969
Precision:  0.3969
Recall:     0.6937
F1-score:   0.5049
Tiempo total para red 6: 22.92 segundos
Saved on: outputs_numerical_categorical_metadata/2/gpt

==============================
Model: Logistic Regression
Accuracy:  0.7862
Precision: 0.2342
Recall:    0.8018
F1-score:  0.3625
              precision    recall  f1-score   support

           0       0.98      0.78      0.87      1353
           1       0.23      0.80      0.36       111

    accuracy                           0.79      1464
   macro avg       0.61      0.79      0.62      1464
weighted avg       0.92      0.79      0.83      1464

[[1062  291]
 [  22   89]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8210
Precision: 0.2799
Recall:    0.8649
F1-score:  0.4229
              precision    recall  f1-score   support

           0       0.99      0.82      0.89      1353
           1       0.28      0.86      0.42       111

    accuracy                           0.82      1464
   macro avg       0.63      0.84      0.66      1464
weighted avg       0.93      0.82      0.86      1464

[[1106  247]
 [  15   96]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_svm.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6885
Precision: 0.1557
Recall:    0.7027
F1-score:  0.2549
              precision    recall  f1-score   support

           0       0.97      0.69      0.80      1353
           1       0.16      0.70      0.25       111

    accuracy                           0.69      1464
   macro avg       0.56      0.70      0.53      1464
weighted avg       0.90      0.69      0.76      1464

[[930 423]
 [ 33  78]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8367
Precision: 0.2852
Recall:    0.7658
F1-score:  0.4156
              precision    recall  f1-score   support

           0       0.98      0.84      0.91      1353
           1       0.29      0.77      0.42       111

    accuracy                           0.84      1464
   macro avg       0.63      0.80      0.66      1464
weighted avg       0.93      0.84      0.87      1464

[[1140  213]
 [  26   85]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8415
Precision: 0.3067
Recall:    0.8649
F1-score:  0.4528
              precision    recall  f1-score   support

           0       0.99      0.84      0.91      1353
           1       0.31      0.86      0.45       111

    accuracy                           0.84      1464
   macro avg       0.65      0.85      0.68      1464
weighted avg       0.94      0.84      0.87      1464

[[1136  217]
 [  15   96]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8415
Precision: 0.2784
Recall:    0.6847
F1-score:  0.3958
              precision    recall  f1-score   support

           0       0.97      0.85      0.91      1353
           1       0.28      0.68      0.40       111

    accuracy                           0.84      1464
   macro avg       0.62      0.77      0.65      1464
weighted avg       0.92      0.84      0.87      1464

[[1156  197]
 [  35   76]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.8415, 'precision': 0.3067, 'recall': 0.8649, 'f1_score': 0.4528}
SVM: {'accuracy': 0.821, 'precision': 0.2799, 'recall': 0.8649, 'f1_score': 0.4229}
Random Forest: {'accuracy': 0.8367, 'precision': 0.2852, 'recall': 0.7658, 'f1_score': 0.4156}
Naive Bayes: {'accuracy': 0.8415, 'precision': 0.2784, 'recall': 0.6847, 'f1_score': 0.3958}
Logistic Regression: {'accuracy': 0.7862, 'precision': 0.2342, 'recall': 0.8018, 'f1_score': 0.3625}
Decision Tree: {'accuracy': 0.6885, 'precision': 0.1557, 'recall': 0.7027, 'f1_score': 0.2549}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: GPT
MLP_437505: {'accuracy': 0.9132513661202186, 'precision': 0.4452054794520548, 'recall': 0.5855855855855856, 'f1_score': 0.5058365758754864, 'f1_score_avg': 0.4953402628499905}
MLP_961025: {'accuracy': 0.9030054644808743, 'precision': 0.40828402366863903, 'recall': 0.6216216216216216, 'f1_score': 0.5058365758754864, 'f1_score_avg': 0.4981027896080728}
MLP_2276353: {'accuracy': 0.8968579234972678, 'precision': 0.39690721649484534, 'recall': 0.6936936936936937, 'f1_score': 0.5058365758754864, 'f1_score_avg': 0.4909016349990962}
MLP_207489: {'accuracy': 0.9002732240437158, 'precision': 0.4, 'recall': 0.6306306306306306, 'f1_score': 0.5037593984962406, 'f1_score_avg': 0.49418056945107913}
MLP_49313: {'accuracy': 0.8715846994535519, 'precision': 0.3288888888888889, 'recall': 0.6666666666666666, 'f1_score': 0.48580441640378547, 'f1_score_avg': 0.45340832741673315}
MLP_100673: {'accuracy': 0.8592896174863388, 'precision': 0.32209737827715357, 'recall': 0.7747747747747747, 'f1_score': 0.48580441640378547, 'f1_score_avg': 0.4636516653780264}
XGBoost: {'accuracy': 0.8415, 'precision': 0.3067, 'recall': 0.8649, 'f1_score': 0.4528}
SVM: {'accuracy': 0.821, 'precision': 0.2799, 'recall': 0.8649, 'f1_score': 0.4229}
Random Forest: {'accuracy': 0.8367, 'precision': 0.2852, 'recall': 0.7658, 'f1_score': 0.4156}
Naive Bayes: {'accuracy': 0.8415, 'precision': 0.2784, 'recall': 0.6847, 'f1_score': 0.3958}
Logistic Regression: {'accuracy': 0.7862, 'precision': 0.2342, 'recall': 0.8018, 'f1_score': 0.3625}
Decision Tree: {'accuracy': 0.6885, 'precision': 0.1557, 'recall': 0.7027, 'f1_score': 0.2549}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['artist', 'genre', 'title_songs_new']
Numeric Columns: ['lyrics_word_count', 'popularity', 'duration_ms']
====================================

 [  32   79]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/gpt/confusion_matrix_param_2276353.png
Accuracy:   0.8866
Precision:  0.3709
Recall:     0.7117
F1-score:   0.4877
Tiempo total para red 6: 22.91 segundos
Saved on: outputs_numerical_categorical_metadata/2/gpt

==============================
Model: Logistic Regression
Accuracy:  0.7862
Precision: 0.2342
Recall:    0.8018
F1-score:  0.3625
              precision    recall  f1-score   support

           0       0.98      0.78      0.87      1353
           1       0.23      0.80      0.36       111

    accuracy                           0.79      1464
   macro avg       0.61      0.79      0.62      1464
weighted avg       0.92      0.79      0.83      1464

[[1062  291]
 [  22   89]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8210
Precision: 0.2799
Recall:    0.8649
F1-score:  0.4229
              precision    recall  f1-score   support

           0       0.99      0.82      0.89      1353
           1       0.28      0.86      0.42       111

    accuracy                           0.82      1464
   macro avg       0.63      0.84      0.66      1464
weighted avg       0.93      0.82      0.86      1464

[[1106  247]
 [  15   96]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_svm.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6885
Precision: 0.1557
Recall:    0.7027
F1-score:  0.2549
              precision    recall  f1-score   support

           0       0.97      0.69      0.80      1353
           1       0.16      0.70      0.25       111

    accuracy                           0.69      1464
   macro avg       0.56      0.70      0.53      1464
weighted avg       0.90      0.69      0.76      1464

[[930 423]
 [ 33  78]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8367
Precision: 0.2852
Recall:    0.7658
F1-score:  0.4156
              precision    recall  f1-score   support

           0       0.98      0.84      0.91      1353
           1       0.29      0.77      0.42       111

    accuracy                           0.84      1464
   macro avg       0.63      0.80      0.66      1464
weighted avg       0.93      0.84      0.87      1464

[[1140  213]
 [  26   85]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8415
Precision: 0.3067
Recall:    0.8649
F1-score:  0.4528
              precision    recall  f1-score   support

           0       0.99      0.84      0.91      1353
           1       0.31      0.86      0.45       111

    accuracy                           0.84      1464
   macro avg       0.65      0.85      0.68      1464
weighted avg       0.94      0.84      0.87      1464

[[1136  217]
 [  15   96]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8415
Precision: 0.2784
Recall:    0.6847
F1-score:  0.3958
              precision    recall  f1-score   support

           0       0.97      0.85      0.91      1353
           1       0.28      0.68      0.40       111

    accuracy                           0.84      1464
   macro avg       0.62      0.77      0.65      1464
weighted avg       0.92      0.84      0.87      1464

[[1156  197]
 [  35   76]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/gpt/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.8415, 'precision': 0.3067, 'recall': 0.8649, 'f1_score': 0.4528}
SVM: {'accuracy': 0.821, 'precision': 0.2799, 'recall': 0.8649, 'f1_score': 0.4229}
Random Forest: {'accuracy': 0.8367, 'precision': 0.2852, 'recall': 0.7658, 'f1_score': 0.4156}
Naive Bayes: {'accuracy': 0.8415, 'precision': 0.2784, 'recall': 0.6847, 'f1_score': 0.3958}
Logistic Regression: {'accuracy': 0.7862, 'precision': 0.2342, 'recall': 0.8018, 'f1_score': 0.3625}
Decision Tree: {'accuracy': 0.6885, 'precision': 0.1557, 'recall': 0.7027, 'f1_score': 0.2549}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: GPT
MLP_961025: {'accuracy': 0.8968579234972678, 'precision': 0.3979591836734694, 'recall': 0.7027027027027027, 'f1_score': 0.50814332247557, 'f1_score_avg': 0.49240420106023436}
MLP_2276353: {'accuracy': 0.8866120218579235, 'precision': 0.37089201877934275, 'recall': 0.7117117117117117, 'f1_score': 0.50814332247557, 'f1_score_avg': 0.4923344422464752}
MLP_207489: {'accuracy': 0.8845628415300546, 'precision': 0.3644859813084112, 'recall': 0.7027027027027027, 'f1_score': 0.5034013605442177, 'f1_score_avg': 0.4896293940053462}
MLP_437505: {'accuracy': 0.8975409836065574, 'precision': 0.39344262295081966, 'recall': 0.6486486486486487, 'f1_score': 0.5034013605442177, 'f1_score_avg': 0.48633517760816536}
MLP_100673: {'accuracy': 0.8736338797814208, 'precision': 0.34710743801652894, 'recall': 0.7567567567567568, 'f1_score': 0.4838709677419355, 'f1_score_avg': 0.4788989898507705}
MLP_49313: {'accuracy': 0.8736338797814208, 'precision': 0.34710743801652894, 'recall': 0.7567567567567568, 'f1_score': 0.47592067988668557, 'f1_score_avg': 0.46106730354437253}
XGBoost: {'accuracy': 0.8415, 'precision': 0.3067, 'recall': 0.8649, 'f1_score': 0.4528}
SVM: {'accuracy': 0.821, 'precision': 0.2799, 'recall': 0.8649, 'f1_score': 0.4229}
Random Forest: {'accuracy': 0.8367, 'precision': 0.2852, 'recall': 0.7658, 'f1_score': 0.4156}
Naive Bayes: {'accuracy': 0.8415, 'precision': 0.2784, 'recall': 0.6847, 'f1_score': 0.3958}
Logistic Regression: {'accuracy': 0.7862, 'precision': 0.2342, 'recall': 0.8018, 'f1_score': 0.3625}
Decision Tree: {'accuracy': 0.6885, 'precision': 0.1557, 'recall': 0.7027, 'f1_score': 0.2549}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['artist', 'genre', 'title_songs_new']
Numeric Columns: ['lyrics_word_count', 'popularity', 'duration_ms']
====================================

