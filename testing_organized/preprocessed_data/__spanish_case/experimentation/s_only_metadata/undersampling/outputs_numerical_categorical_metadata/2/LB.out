2025-10-30 00:26:44.905338: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-30 00:26:44.905337: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__spanish_case/experimentation/s_only_metadata/undersampling/metadata_LB.py:267: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__spanish_case/experimentation/s_only_metadata/undersampling/metadata_LB.py:267: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
For TF-IDF embbedings you are selecteing this columns:
--> ['artist', 'genre', 'title_songs_new']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 3 ['title_songs_new', 'artist', 'genre']

CAT_LOW 1 ['title_songs_new']
CAT_HIGH 2 ['artist', 'genre']
For both embbedings your are adding this columns: 
--> ['lyrics_word_count', 'popularity', 'duration_ms']
You are executing with [ALL] dataset
--> PaTH:  ../../../../../../data/spanish/LB_M/LB_fuss/lb_khipu_M.npy
Loading Lb vectors from:  ../../../../../../data/spanish/dataset/oficialDatasetEAIM2026.csv
Label distribution: {False: 6765, True: 554}
X shape: (7319, 300)
y shape: (7319,)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (5855, 3)
X_train_Numeric:  (5855, 3)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (5855, 303)
Shape of X_test after concatenation:  (1464, 303)
Shape of y_train:  (5855,)
Shape of y_test:  (1464,)

Data con el spliting...
Label distribution en TRAIN: {0: 5412, 1: 443}
Label distribution en TEST: {0: 1353, 1: 111}


==================================================
Data antes del undersampling ...
X: (5855, 303)
y: (5855,)
Apliying UNDERSAMPLE
443
Label distribution: {0: 443, 1: 443}
X shape: (886, 303)
y shape: (886,)
Resultados con MLP

Entrenando red 1 con capas [303, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6913, Test Loss: 0.7910, F1: 0.1410, AUC: 0.7511
Epoch [10/30] Train Loss: 0.5670, Test Loss: 0.6830, F1: 0.2399, AUC: 0.7927
Epoch [20/30] Train Loss: 0.4693, Test Loss: 0.5475, F1: 0.2985, AUC: 0.8225
Mejores resultados en la época:  29
f1-score 0.375
AUC según el mejor F1-score 0.8365394219052754
Confusion Matrix:
 [[1165  188]
 [  42   69]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_9761.png
Accuracy:   0.8429
Precision:  0.2685
Recall:     0.6216
F1-score:   0.3750

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6903, Test Loss: 0.7442, F1: 0.1414, AUC: 0.6695
Epoch [10/30] Train Loss: 0.5794, Test Loss: 0.5890, F1: 0.2748, AUC: 0.7839
Epoch [20/30] Train Loss: 0.4766, Test Loss: 0.5375, F1: 0.2903, AUC: 0.8159
Mejores resultados en la época:  6
f1-score 0.363013698630137
AUC según el mejor F1-score 0.7661386441874246
Confusion Matrix:
 [[1225  128]
 [  58   53]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_9761.png
Accuracy:   0.8730
Precision:  0.2928
Recall:     0.4775
F1-score:   0.3630

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6962, Test Loss: 0.6607, F1: 0.2320, AUC: 0.6840
Epoch [10/30] Train Loss: 0.5678, Test Loss: 0.4292, F1: 0.3750, AUC: 0.7874
Epoch [20/30] Train Loss: 0.4700, Test Loss: 0.5195, F1: 0.2972, AUC: 0.8202
Mejores resultados en la época:  9
f1-score 0.38545454545454544
AUC según el mejor F1-score 0.7815331961673425
Confusion Matrix:
 [[1242  111]
 [  58   53]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_9761.png
Accuracy:   0.8846
Precision:  0.3232
Recall:     0.4775
F1-score:   0.3855

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6906, Test Loss: 0.6580, F1: 0.1860, AUC: 0.6593
Epoch [10/30] Train Loss: 0.6010, Test Loss: 0.7494, F1: 0.1888, AUC: 0.7721
Epoch [20/30] Train Loss: 0.4925, Test Loss: 0.5414, F1: 0.2823, AUC: 0.8098
Mejores resultados en la época:  8
f1-score 0.37992831541218636
AUC según el mejor F1-score 0.7634286170871537
Confusion Matrix:
 [[1238  115]
 [  58   53]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_9761.png
Accuracy:   0.8818
Precision:  0.3155
Recall:     0.4775
F1-score:   0.3799
Tiempo total para red 1: 16.57 segundos

Entrenando red 2 con capas [303, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6911, Test Loss: 0.6055, F1: 0.0000, AUC: 0.7344
Epoch [10/30] Train Loss: 0.4804, Test Loss: 0.4839, F1: 0.3160, AUC: 0.8144
Epoch [20/30] Train Loss: 0.3748, Test Loss: 0.6217, F1: 0.3079, AUC: 0.8464
Mejores resultados en la época:  28
f1-score 0.3894230769230769
AUC según el mejor F1-score 0.8530059993474627
Confusion Matrix:
 [[1129  224]
 [  30   81]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_21569.png
Accuracy:   0.8265
Precision:  0.2656
Recall:     0.7297
F1-score:   0.3894

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6951, Test Loss: 0.7005, F1: 0.1498, AUC: 0.6984
Epoch [10/30] Train Loss: 0.4663, Test Loss: 0.4924, F1: 0.3015, AUC: 0.8147
Epoch [20/30] Train Loss: 0.3887, Test Loss: 0.4019, F1: 0.3781, AUC: 0.8437
Mejores resultados en la época:  27
f1-score 0.3821656050955414
AUC según el mejor F1-score 0.8494170445389957
Confusion Matrix:
 [[1210  143]
 [  51   60]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_21569.png
Accuracy:   0.8675
Precision:  0.2956
Recall:     0.5405
F1-score:   0.3822

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6920, Test Loss: 0.6732, F1: 0.2416, AUC: 0.6664
Epoch [10/30] Train Loss: 0.4621, Test Loss: 0.4613, F1: 0.3302, AUC: 0.8157
Epoch [20/30] Train Loss: 0.3887, Test Loss: 0.7682, F1: 0.2684, AUC: 0.8437
Mejores resultados en la época:  23
f1-score 0.37889688249400477
AUC según el mejor F1-score 0.8470133104279445
Confusion Matrix:
 [[1126  227]
 [  32   79]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_21569.png
Accuracy:   0.8231
Precision:  0.2582
Recall:     0.7117
F1-score:   0.3789

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6940, Test Loss: 0.6631, F1: 0.0000, AUC: 0.6425
Epoch [10/30] Train Loss: 0.5252, Test Loss: 0.5611, F1: 0.2788, AUC: 0.7947
Epoch [20/30] Train Loss: 0.4251, Test Loss: 0.5575, F1: 0.3203, AUC: 0.8361
Mejores resultados en la época:  27
f1-score 0.3697916666666667
AUC según el mejor F1-score 0.846833529760359
Confusion Matrix:
 [[1151  202]
 [  40   71]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_21569.png
Accuracy:   0.8347
Precision:  0.2601
Recall:     0.6396
F1-score:   0.3698
Tiempo total para red 2: 15.33 segundos

Entrenando red 3 con capas [303, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6937, Test Loss: 0.7102, F1: 0.1410, AUC: 0.7404
Epoch [10/30] Train Loss: 0.4630, Test Loss: 0.5054, F1: 0.3214, AUC: 0.8267
Epoch [20/30] Train Loss: 0.3781, Test Loss: 0.8033, F1: 0.2714, AUC: 0.8478
Mejores resultados en la época:  26
f1-score 0.40588235294117647
AUC según el mejor F1-score 0.851814120106803
Confusion Matrix:
 [[1193  160]
 [  42   69]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_49281.png
Accuracy:   0.8620
Precision:  0.3013
Recall:     0.6216
F1-score:   0.4059

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6920, Test Loss: 0.7002, F1: 0.1495, AUC: 0.7673
Epoch [10/30] Train Loss: 0.5262, Test Loss: 0.7466, F1: 0.2547, AUC: 0.8305
Epoch [20/30] Train Loss: 0.3600, Test Loss: 0.5059, F1: 0.3490, AUC: 0.8498
Mejores resultados en la época:  22
f1-score 0.4
AUC según el mejor F1-score 0.8521936570717058
Confusion Matrix:
 [[1244  109]
 [  56   55]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_49281.png
Accuracy:   0.8873
Precision:  0.3354
Recall:     0.4955
F1-score:   0.4000

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6928, Test Loss: 0.7075, F1: 0.1413, AUC: 0.7109
Epoch [10/30] Train Loss: 0.4358, Test Loss: 0.4843, F1: 0.3208, AUC: 0.8278
Epoch [20/30] Train Loss: 0.3712, Test Loss: 0.7480, F1: 0.2729, AUC: 0.8485
Mejores resultados en la época:  25
f1-score 0.3963963963963964
AUC según el mejor F1-score 0.8515810710932663
Confusion Matrix:
 [[1197  156]
For TF-IDF embbedings you are selecteing this columns:
--> ['artist', 'genre', 'title_songs_new']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 3 ['title_songs_new', 'artist', 'genre']

CAT_LOW 1 ['title_songs_new']
CAT_HIGH 2 ['artist', 'genre']
For both embbedings your are adding this columns: 
--> ['lyrics_word_count', 'popularity', 'duration_ms']
You are executing with [ALL] dataset
--> PaTH:  ../../../../../../data/spanish/LB_M/LB_fuss/lb_khipu_M.npy
Loading Lb vectors from:  ../../../../../../data/spanish/dataset/oficialDatasetEAIM2026.csv
Label distribution: {False: 6765, True: 554}
X shape: (7319, 300)
y shape: (7319,)

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (5855, 3)
X_train_Numeric:  (5855, 3)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (5855, 303)
Shape of X_test after concatenation:  (1464, 303)
Shape of y_train:  (5855,)
Shape of y_test:  (1464,)

Data con el spliting...
Label distribution en TRAIN: {0: 5412, 1: 443}
Label distribution en TEST: {0: 1353, 1: 111}


==================================================
Data antes del undersampling ...
X: (5855, 303)
y: (5855,)
Apliying UNDERSAMPLE
443
Label distribution: {0: 443, 1: 443}
X shape: (886, 303)
y shape: (886,)
Resultados con MLP

Entrenando red 1 con capas [303, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6954, Test Loss: 0.6849, F1: 0.1931, AUC: 0.6621
Epoch [10/30] Train Loss: 0.5741, Test Loss: 0.6276, F1: 0.2576, AUC: 0.7886
Epoch [20/30] Train Loss: 0.4780, Test Loss: 0.6907, F1: 0.2600, AUC: 0.8220
Mejores resultados en la época:  25
f1-score 0.3767313019390582
AUC según el mejor F1-score 0.8311659775074409
Confusion Matrix:
 [[1171  182]
 [  43   68]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_9761.png
Accuracy:   0.8463
Precision:  0.2720
Recall:     0.6126
F1-score:   0.3767

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6878, Test Loss: 0.6454, F1: 0.1364, AUC: 0.7021
Epoch [10/30] Train Loss: 0.5583, Test Loss: 0.5807, F1: 0.2958, AUC: 0.7892
Epoch [20/30] Train Loss: 0.4583, Test Loss: 0.5340, F1: 0.3008, AUC: 0.8242
Mejores resultados en la época:  28
f1-score 0.3770491803278688
AUC según el mejor F1-score 0.8365460804485194
Confusion Matrix:
 [[1167  186]
 [  42   69]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_9761.png
Accuracy:   0.8443
Precision:  0.2706
Recall:     0.6216
F1-score:   0.3770

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6945, Test Loss: 0.7201, F1: 0.1439, AUC: 0.6229
Epoch [10/30] Train Loss: 0.5643, Test Loss: 0.4810, F1: 0.3763, AUC: 0.7860
Epoch [20/30] Train Loss: 0.4587, Test Loss: 0.6214, F1: 0.2857, AUC: 0.8219
Mejores resultados en la época:  10
f1-score 0.37630662020905925
AUC según el mejor F1-score 0.7860210543137371
Confusion Matrix:
 [[1231  122]
 [  57   54]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_9761.png
Accuracy:   0.8777
Precision:  0.3068
Recall:     0.4865
F1-score:   0.3763

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6970, Test Loss: 0.6843, F1: 0.2061, AUC: 0.6258
Epoch [10/30] Train Loss: 0.5828, Test Loss: 0.4953, F1: 0.3551, AUC: 0.7758
Epoch [20/30] Train Loss: 0.4926, Test Loss: 0.6162, F1: 0.2812, AUC: 0.8121
Mejores resultados en la época:  11
f1-score 0.3563636363636364
AUC según el mejor F1-score 0.7807408295213174
Confusion Matrix:
 [[1238  115]
 [  62   49]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_9761.png
Accuracy:   0.8791
Precision:  0.2988
Recall:     0.4414
F1-score:   0.3564
Tiempo total para red 1: 16.57 segundos

Entrenando red 2 con capas [303, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6896, Test Loss: 0.7224, F1: 0.1430, AUC: 0.6865
Epoch [10/30] Train Loss: 0.4604, Test Loss: 0.6183, F1: 0.2921, AUC: 0.8205
Epoch [20/30] Train Loss: 0.3790, Test Loss: 0.7330, F1: 0.2798, AUC: 0.8463
Mejores resultados en la época:  26
f1-score 0.391304347826087
AUC según el mejor F1-score 0.8512414853878268
Confusion Matrix:
 [[1131  222]
 [  30   81]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_21569.png
Accuracy:   0.8279
Precision:  0.2673
Recall:     0.7297
F1-score:   0.3913

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6922, Test Loss: 0.6813, F1: 0.2254, AUC: 0.6796
Epoch [10/30] Train Loss: 0.4964, Test Loss: 0.6118, F1: 0.2755, AUC: 0.8087
Epoch [20/30] Train Loss: 0.4141, Test Loss: 0.4859, F1: 0.3313, AUC: 0.8394
Mejores resultados en la época:  19
f1-score 0.361323155216285
AUC según el mejor F1-score 0.8376181059107889
Confusion Matrix:
 [[1142  211]
 [  40   71]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_21569.png
Accuracy:   0.8286
Precision:  0.2518
Recall:     0.6396
F1-score:   0.3613

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6901, Test Loss: 0.7365, F1: 0.1411, AUC: 0.7283
Epoch [10/30] Train Loss: 0.4870, Test Loss: 0.5876, F1: 0.2823, AUC: 0.8133
Epoch [20/30] Train Loss: 0.3828, Test Loss: 0.3975, F1: 0.3673, AUC: 0.8443
Mejores resultados en la época:  11
f1-score 0.3783783783783784
AUC según el mejor F1-score 0.819733258757649
Confusion Matrix:
 [[1224  129]
 [  55   56]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_21569.png
Accuracy:   0.8743
Precision:  0.3027
Recall:     0.5045
F1-score:   0.3784

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6908, Test Loss: 0.7099, F1: 0.1457, AUC: 0.6839
Epoch [10/30] Train Loss: 0.4562, Test Loss: 0.3881, F1: 0.3728, AUC: 0.8202
Epoch [20/30] Train Loss: 0.3879, Test Loss: 0.6364, F1: 0.3014, AUC: 0.8432
Mejores resultados en la época:  12
f1-score 0.38910505836575876
AUC según el mejor F1-score 0.8283827064314869
Confusion Matrix:
 [[1257   96]
 [  61   50]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_21569.png
Accuracy:   0.8928
Precision:  0.3425
Recall:     0.4505
F1-score:   0.3891
Tiempo total para red 2: 15.33 segundos

Entrenando red 3 con capas [303, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6929, Test Loss: 0.6840, F1: 0.2222, AUC: 0.7129
Epoch [10/30] Train Loss: 0.4631, Test Loss: 0.5216, F1: 0.3109, AUC: 0.8270
Epoch [20/30] Train Loss: 0.3570, Test Loss: 0.4767, F1: 0.3492, AUC: 0.8483
Mejores resultados en la época:  25
f1-score 0.4040920716112532
AUC según el mejor F1-score 0.8508419727931924
Confusion Matrix:
 [[1152  201]
 [  32   79]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_49281.png
Accuracy:   0.8408
Precision:  0.2821
Recall:     0.7117
F1-score:   0.4041

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6966, Test Loss: 0.6912, F1: 0.2453, AUC: 0.6436
Epoch [10/30] Train Loss: 0.4485, Test Loss: 0.4078, F1: 0.3538, AUC: 0.8213
Epoch [20/30] Train Loss: 0.3856, Test Loss: 0.7273, F1: 0.2793, AUC: 0.8480
Mejores resultados en la época:  27
f1-score 0.37986270022883295
AUC según el mejor F1-score 0.8512414853878267
Confusion Matrix:
 [[1110  243]
 [  28   83]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_49281.png
Accuracy:   0.8149
Precision:  0.2546
Recall:     0.7477
F1-score:   0.3799

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6909, Test Loss: 0.7309, F1: 0.1410, AUC: 0.7513
Epoch [10/30] Train Loss: 0.4075, Test Loss: 0.3642, F1: 0.3607, AUC: 0.8395
Epoch [20/30] Train Loss: 0.3418, Test Loss: 0.5171, F1: 0.3422, AUC: 0.8498
Mejores resultados en la época:  23
f1-score 0.3948220064724919
AUC según el mejor F1-score 0.8513613391662173
Confusion Matrix:
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
 [[1216  137]
 [  50   61]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_49281.png
Accuracy:   0.8723
Precision:  0.3081
Recall:     0.5495
F1-score:   0.3948

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6923, Test Loss: 0.6670, F1: 0.0000, AUC: 0.7083
Epoch [10/30] Train Loss: 0.4199, Test Loss: 0.3657, F1: 0.3761, AUC: 0.8369
Epoch [20/30] Train Loss: 0.3534, Test Loss: 0.3998, F1: 0.3920, AUC: 0.8502
Mejores resultados en la época:  29
f1-score 0.39800995024875624
AUC según el mejor F1-score 0.8522069741581937
Confusion Matrix:
 [[1142  211]
 [  31   80]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_49281.png
Accuracy:   0.8347
Precision:  0.2749
Recall:     0.7207
F1-score:   0.3980
Tiempo total para red 3: 16.34 segundos

Entrenando red 4 con capas [303, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6960, Test Loss: 0.7183, F1: 0.1410, AUC: 0.7172
Epoch [10/30] Train Loss: 0.4266, Test Loss: 0.5679, F1: 0.3130, AUC: 0.8351
Epoch [20/30] Train Loss: 0.3518, Test Loss: 0.5311, F1: 0.3377, AUC: 0.8493
Mejores resultados en la época:  22
f1-score 0.40437158469945356
AUC según el mejor F1-score 0.8504824114580213
Confusion Matrix:
 [[1172  181]
 [  37   74]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_121089.png
Accuracy:   0.8511
Precision:  0.2902
Recall:     0.6667
F1-score:   0.4044

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6931, Test Loss: 0.6829, F1: 0.0992, AUC: 0.7511
Epoch [10/30] Train Loss: 0.4161, Test Loss: 0.4960, F1: 0.3471, AUC: 0.8444
Epoch [20/30] Train Loss: 0.3489, Test Loss: 0.5366, F1: 0.3346, AUC: 0.8544
Mejores resultados en la época:  26
f1-score 0.4057971014492754
AUC según el mejor F1-score 0.8542311713043421
Confusion Matrix:
 [[1189  164]
 [  41   70]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_121089.png
Accuracy:   0.8600
Precision:  0.2991
Recall:     0.6306
F1-score:   0.4058

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6943, Test Loss: 0.7136, F1: 0.1410, AUC: 0.7546
Epoch [10/30] Train Loss: 0.4327, Test Loss: 0.6956, F1: 0.2788, AUC: 0.8406
Epoch [20/30] Train Loss: 0.3446, Test Loss: 0.3538, F1: 0.4011, AUC: 0.8546
Mejores resultados en la época:  14
f1-score 0.4011142061281337
AUC según el mejor F1-score 0.8485514339172875
Confusion Matrix:
 [[1177  176]
 [  39   72]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_121089.png
Accuracy:   0.8531
Precision:  0.2903
Recall:     0.6486
F1-score:   0.4011

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6956, Test Loss: 0.7309, F1: 0.1410, AUC: 0.7483
Epoch [10/30] Train Loss: 0.4639, Test Loss: 0.3516, F1: 0.3832, AUC: 0.8359
Epoch [20/30] Train Loss: 0.3519, Test Loss: 0.3924, F1: 0.3970, AUC: 0.8501
Mejores resultados en la época:  28
f1-score 0.40594059405940597
AUC según el mejor F1-score 0.8522868766771207
Confusion Matrix:
 [[1142  211]
 [  29   82]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_121089.png
Accuracy:   0.8361
Precision:  0.2799
Recall:     0.7387
F1-score:   0.4059
Tiempo total para red 4: 17.36 segundos

Entrenando red 5 con capas [303, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6910, Test Loss: 0.7632, F1: 0.1410, AUC: 0.7523
Epoch [10/30] Train Loss: 0.3820, Test Loss: 0.3409, F1: 0.3801, AUC: 0.8448
Epoch [20/30] Train Loss: 0.3079, Test Loss: 0.7577, F1: 0.2796, AUC: 0.8533
Mejores resultados en la época:  24
f1-score 0.4061302681992337
AUC según el mejor F1-score 0.8498898011093133
Confusion Matrix:
 [[1256   97]
 [  58   53]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_328193.png
Accuracy:   0.8941
Precision:  0.3533
Recall:     0.4775
F1-score:   0.4061

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6932, Test Loss: 0.6514, F1: 0.0000, AUC: 0.7409
Epoch [10/30] Train Loss: 0.4011, Test Loss: 0.5708, F1: 0.3163, AUC: 0.8412
Epoch [20/30] Train Loss: 0.3898, Test Loss: 0.4831, F1: 0.3398, AUC: 0.8495
Mejores resultados en la época:  28
f1-score 0.4021164021164021
AUC según el mejor F1-score 0.8517808273905834
Confusion Matrix:
 [[1162  191]
 [  35   76]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_328193.png
Accuracy:   0.8456
Precision:  0.2846
Recall:     0.6847
F1-score:   0.4021

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6951, Test Loss: 0.6960, F1: 0.1410, AUC: 0.7019
Epoch [10/30] Train Loss: 0.3818, Test Loss: 0.4648, F1: 0.3532, AUC: 0.8432
Epoch [20/30] Train Loss: 0.3280, Test Loss: 0.4015, F1: 0.3913, AUC: 0.8534
Mejores resultados en la época:  16
f1-score 0.4063745019920319
AUC según el mejor F1-score 0.8504557772850455
Confusion Matrix:
 [[1264   89]
 [  60   51]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_328193.png
Accuracy:   0.8982
Precision:  0.3643
Recall:     0.4595
F1-score:   0.4064

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6906, Test Loss: 0.6646, F1: 0.3158, AUC: 0.7615
Epoch [10/30] Train Loss: 0.4301, Test Loss: 0.4081, F1: 0.3646, AUC: 0.8407
Epoch [20/30] Train Loss: 0.3629, Test Loss: 0.3106, F1: 0.4214, AUC: 0.8545
Mejores resultados en la época:  20
f1-score 0.42138364779874216
AUC según el mejor F1-score 0.8544775374043667
Confusion Matrix:
 [[1213  140]
 [  44   67]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_328193.png
Accuracy:   0.8743
Precision:  0.3237
Recall:     0.6036
F1-score:   0.4214
Tiempo total para red 5: 18.03 segundos

Entrenando red 6 con capas [303, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6934, Test Loss: 0.7191, F1: 0.1410, AUC: 0.7261
Epoch [10/30] Train Loss: 0.4344, Test Loss: 0.6623, F1: 0.2644, AUC: 0.8432
Epoch [20/30] Train Loss: 0.3597, Test Loss: 0.4755, F1: 0.3472, AUC: 0.8530
Mejores resultados en la época:  18
f1-score 0.413265306122449
AUC según el mejor F1-score 0.8504491187418017
Confusion Matrix:
 [[1153  200]
 [  30   81]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_1010689.png
Accuracy:   0.8429
Precision:  0.2883
Recall:     0.7297
F1-score:   0.4133

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6941, Test Loss: 0.7148, F1: 0.1410, AUC: 0.7490
Epoch [10/30] Train Loss: 0.4556, Test Loss: 0.3387, F1: 0.3758, AUC: 0.8455
Epoch [20/30] Train Loss: 0.3569, Test Loss: 0.7519, F1: 0.2805, AUC: 0.8536
Mejores resultados en la época:  9
f1-score 0.39849624060150374
AUC según el mejor F1-score 0.8423123788977447
Confusion Matrix:
 [[1251  102]
 [  58   53]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_1010689.png
Accuracy:   0.8907
Precision:  0.3419
Recall:     0.4775
F1-score:   0.3985

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6935, Test Loss: 0.6945, F1: 0.1410, AUC: 0.6521
Epoch [10/30] Train Loss: 0.4312, Test Loss: 0.4350, F1: 0.3392, AUC: 0.8364
Epoch [20/30] Train Loss: 0.3250, Test Loss: 0.9929, F1: 0.2407, AUC: 0.8535
Mejores resultados en la época:  22
f1-score 0.41055718475073316
AUC según el mejor F1-score 0.8554030749152701
Confusion Matrix:
 [[1193  160]
 [  41   70]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_1010689.png
Accuracy:   0.8627
Precision:  0.3043
Recall:     0.6306
F1-score:   0.4106

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6958, Test Loss: 0.6594, F1: 0.0000, AUC: 0.7489
 [  45   66]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_49281.png
Accuracy:   0.8627
Precision:  0.2973
Recall:     0.5946
F1-score:   0.3964

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6912, Test Loss: 0.6809, F1: 0.3265, AUC: 0.7491
Epoch [10/30] Train Loss: 0.4137, Test Loss: 0.5042, F1: 0.3373, AUC: 0.8372
Epoch [20/30] Train Loss: 0.3409, Test Loss: 0.3986, F1: 0.3860, AUC: 0.8490
Mejores resultados en la época:  24
f1-score 0.40233236151603496
AUC según el mejor F1-score 0.8512348268445831
Confusion Matrix:
 [[1190  163]
 [  42   69]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_49281.png
Accuracy:   0.8600
Precision:  0.2974
Recall:     0.6216
F1-score:   0.4023
Tiempo total para red 3: 16.34 segundos

Entrenando red 4 con capas [303, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6916, Test Loss: 0.7034, F1: 0.1460, AUC: 0.7482
Epoch [10/30] Train Loss: 0.4268, Test Loss: 0.8066, F1: 0.2541, AUC: 0.8402
Epoch [20/30] Train Loss: 0.3411, Test Loss: 0.6841, F1: 0.2964, AUC: 0.8535
Mejores resultados en la época:  21
f1-score 0.41131105398457585
AUC según el mejor F1-score 0.8541113175259518
Confusion Matrix:
 [[1155  198]
 [  31   80]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_121089.png
Accuracy:   0.8436
Precision:  0.2878
Recall:     0.7207
F1-score:   0.4113

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6925, Test Loss: 0.6915, F1: 0.2240, AUC: 0.7093
Epoch [10/30] Train Loss: 0.4361, Test Loss: 0.2772, F1: 0.3891, AUC: 0.8409
Epoch [20/30] Train Loss: 0.3809, Test Loss: 0.8389, F1: 0.2494, AUC: 0.8532
Mejores resultados en la época:  27
f1-score 0.40555555555555556
AUC según el mejor F1-score 0.8541379516989275
Confusion Matrix:
 [[1177  176]
 [  38   73]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_121089.png
Accuracy:   0.8538
Precision:  0.2932
Recall:     0.6577
F1-score:   0.4056

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6922, Test Loss: 0.6912, F1: 0.2386, AUC: 0.7610
Epoch [10/30] Train Loss: 0.4268, Test Loss: 0.5767, F1: 0.3262, AUC: 0.8374
Epoch [20/30] Train Loss: 0.4047, Test Loss: 1.2469, F1: 0.1825, AUC: 0.8515
Mejores resultados en la época:  24
f1-score 0.4222222222222222
AUC según el mejor F1-score 0.8551966600747088
Confusion Matrix:
 [[1180  173]
 [  35   76]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_121089.png
Accuracy:   0.8579
Precision:  0.3052
Recall:     0.6847
F1-score:   0.4222

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6928, Test Loss: 0.6912, F1: 0.2349, AUC: 0.7154
Epoch [10/30] Train Loss: 0.4081, Test Loss: 0.3357, F1: 0.3810, AUC: 0.8361
Epoch [20/30] Train Loss: 0.3438, Test Loss: 0.5018, F1: 0.3442, AUC: 0.8501
Mejores resultados en la época:  16
f1-score 0.40117994100294985
AUC según el mejor F1-score 0.8471331642063349
Confusion Matrix:
 [[1193  160]
 [  43   68]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_121089.png
Accuracy:   0.8613
Precision:  0.2982
Recall:     0.6126
F1-score:   0.4012
Tiempo total para red 4: 17.36 segundos

Entrenando red 5 con capas [303, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6946, Test Loss: 0.6802, F1: 0.0000, AUC: 0.7594
Epoch [10/30] Train Loss: 0.4128, Test Loss: 0.5882, F1: 0.3108, AUC: 0.8397
Epoch [20/30] Train Loss: 0.2997, Test Loss: 0.6183, F1: 0.3174, AUC: 0.8528
Mejores resultados en la época:  27
f1-score 0.40093240093240096
AUC según el mejor F1-score 0.8539981222908053
Confusion Matrix:
 [[1121  232]
 [  25   86]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_328193.png
Accuracy:   0.8245
Precision:  0.2704
Recall:     0.7748
F1-score:   0.4009

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6943, Test Loss: 0.7072, F1: 0.1410, AUC: 0.7086
Epoch [10/30] Train Loss: 0.4248, Test Loss: 0.4173, F1: 0.3548, AUC: 0.8422
Epoch [20/30] Train Loss: 0.3219, Test Loss: 0.4655, F1: 0.3648, AUC: 0.8514
Mejores resultados en la época:  18
f1-score 0.4028776978417266
AUC según el mejor F1-score 0.8507420946445337
Confusion Matrix:
 [[1131  222]
 [  27   84]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_328193.png
Accuracy:   0.8299
Precision:  0.2745
Recall:     0.7568
F1-score:   0.4029

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6934, Test Loss: 0.6908, F1: 0.2428, AUC: 0.7588
Epoch [10/30] Train Loss: 0.3618, Test Loss: 0.5541, F1: 0.3339, AUC: 0.8480
Epoch [20/30] Train Loss: 0.2798, Test Loss: 0.4591, F1: 0.3769, AUC: 0.8517
Mejores resultados en la época:  23
f1-score 0.3948220064724919
AUC según el mejor F1-score 0.8504757529147774
Confusion Matrix:
 [[1216  137]
 [  50   61]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_328193.png
Accuracy:   0.8723
Precision:  0.3081
Recall:     0.5495
F1-score:   0.3948

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6931, Test Loss: 0.6717, F1: 0.0179, AUC: 0.7660
Epoch [10/30] Train Loss: 0.4563, Test Loss: 1.0897, F1: 0.1976, AUC: 0.8447
Epoch [20/30] Train Loss: 0.3272, Test Loss: 0.5356, F1: 0.3408, AUC: 0.8530
Mejores resultados en la época:  21
f1-score 0.3888888888888889
AUC según el mejor F1-score 0.8532989752501949
Confusion Matrix:
 [[1116  237]
 [  27   84]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_328193.png
Accuracy:   0.8197
Precision:  0.2617
Recall:     0.7568
F1-score:   0.3889
Tiempo total para red 5: 18.02 segundos

Entrenando red 6 con capas [303, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6953, Test Loss: 0.6949, F1: 0.1443, AUC: 0.7611
Epoch [10/30] Train Loss: 0.4074, Test Loss: 0.3532, F1: 0.3636, AUC: 0.8447
Epoch [20/30] Train Loss: 0.3532, Test Loss: 0.3886, F1: 0.3918, AUC: 0.8552
Mejores resultados en la época:  23
f1-score 0.43352601156069365
AUC según el mejor F1-score 0.8528195601366333
Confusion Matrix:
 [[1193  160]
 [  36   75]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_1010689.png
Accuracy:   0.8661
Precision:  0.3191
Recall:     0.6757
F1-score:   0.4335

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6940, Test Loss: 0.7050, F1: 0.1410, AUC: 0.6736
Epoch [10/30] Train Loss: 0.4996, Test Loss: 0.5756, F1: 0.3011, AUC: 0.8399
Epoch [20/30] Train Loss: 0.3439, Test Loss: 0.8920, F1: 0.2430, AUC: 0.8513
Mejores resultados en la época:  26
f1-score 0.41414141414141414
AUC según el mejor F1-score 0.856215417191027
Confusion Matrix:
 [[1150  203]
 [  29   82]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_1010689.png
Accuracy:   0.8415
Precision:  0.2877
Recall:     0.7387
F1-score:   0.4141

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6952, Test Loss: 0.6961, F1: 0.1444, AUC: 0.7654
Epoch [10/30] Train Loss: 0.4529, Test Loss: 0.7418, F1: 0.2609, AUC: 0.8394
Epoch [20/30] Train Loss: 0.3273, Test Loss: 0.7360, F1: 0.2833, AUC: 0.8544
Mejores resultados en la época:  27
f1-score 0.4207492795389049
AUC según el mejor F1-score 0.8585459073263951
Confusion Matrix:
 [[1190  163]
 [  38   73]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_1010689.png
Accuracy:   0.8627
Precision:  0.3093
Recall:     0.6577
F1-score:   0.4207

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6940, Test Loss: 0.6951, F1: 0.1617, AUC: 0.7764
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [00:28:45] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [00:28:45] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [10/30] Train Loss: 0.4667, Test Loss: 0.3604, F1: 0.3844, AUC: 0.8414
Epoch [20/30] Train Loss: 0.3085, Test Loss: 0.4676, F1: 0.3793, AUC: 0.8528
Mejores resultados en la época:  14
f1-score 0.4110429447852761
AUC según el mejor F1-score 0.8486712876956779
Confusion Matrix:
 [[1205  148]
 [  44   67]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_1010689.png
Accuracy:   0.8689
Precision:  0.3116
Recall:     0.6036
F1-score:   0.4110
Tiempo total para red 6: 21.96 segundos
Saved on: outputs_numerical_categorical_metadata/2/lyrics_bert

==============================
Model: Logistic Regression
Accuracy:  0.7862
Precision: 0.2314
Recall:    0.7838
F1-score:  0.3573
              precision    recall  f1-score   support

           0       0.98      0.79      0.87      1353
           1       0.23      0.78      0.36       111

    accuracy                           0.79      1464
   macro avg       0.60      0.79      0.61      1464
weighted avg       0.92      0.79      0.83      1464

[[1064  289]
 [  24   87]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8005
Precision: 0.2534
Recall:    0.8378
F1-score:  0.3891
              precision    recall  f1-score   support

           0       0.98      0.80      0.88      1353
           1       0.25      0.84      0.39       111

    accuracy                           0.80      1464
   macro avg       0.62      0.82      0.63      1464
weighted avg       0.93      0.80      0.84      1464

[[1079  274]
 [  18   93]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_svm.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6776
Precision: 0.1495
Recall:    0.6937
F1-score:  0.2460
              precision    recall  f1-score   support

           0       0.96      0.68      0.79      1353
           1       0.15      0.69      0.25       111

    accuracy                           0.68      1464
   macro avg       0.56      0.68      0.52      1464
weighted avg       0.90      0.68      0.75      1464

[[915 438]
 [ 34  77]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8005
Precision: 0.2407
Recall:    0.7568
F1-score:  0.3652
              precision    recall  f1-score   support

           0       0.98      0.80      0.88      1353
           1       0.24      0.76      0.37       111

    accuracy                           0.80      1464
   macro avg       0.61      0.78      0.62      1464
weighted avg       0.92      0.80      0.84      1464

[[1088  265]
 [  27   84]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_random_forest.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8067
Precision: 0.2529
Recall:    0.7928
F1-score:  0.3834
              precision    recall  f1-score   support

           0       0.98      0.81      0.89      1353
           1       0.25      0.79      0.38       111

    accuracy                           0.81      1464
   macro avg       0.62      0.80      0.63      1464
weighted avg       0.92      0.81      0.85      1464

[[1093  260]
 [  23   88]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_xgboost.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7753
Precision: 0.1955
Recall:    0.6306
F1-score:  0.2985
              precision    recall  f1-score   support

           0       0.96      0.79      0.87      1353
           1       0.20      0.63      0.30       111

    accuracy                           0.78      1464
   macro avg       0.58      0.71      0.58      1464
weighted avg       0.90      0.78      0.82      1464

[[1065  288]
 [  41   70]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/naive_bayes_model.pkl


Resumen de métricas:
SVM: {'accuracy': 0.8005, 'precision': 0.2534, 'recall': 0.8378, 'f1_score': 0.3891}
XGBoost: {'accuracy': 0.8067, 'precision': 0.2529, 'recall': 0.7928, 'f1_score': 0.3834}
Random Forest: {'accuracy': 0.8005, 'precision': 0.2407, 'recall': 0.7568, 'f1_score': 0.3652}
Logistic Regression: {'accuracy': 0.7862, 'precision': 0.2314, 'recall': 0.7838, 'f1_score': 0.3573}
Naive Bayes: {'accuracy': 0.7753, 'precision': 0.1955, 'recall': 0.6306, 'f1_score': 0.2985}
Decision Tree: {'accuracy': 0.6776, 'precision': 0.1495, 'recall': 0.6937, 'f1_score': 0.246}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: LYRICS_BERT
MLP_328193: {'accuracy': 0.8743169398907104, 'precision': 0.32367149758454106, 'recall': 0.6036036036036037, 'f1_score': 0.42138364779874216, 'f1_score_avg': 0.4090012050266024}
MLP_1010689: {'accuracy': 0.8688524590163934, 'precision': 0.3116279069767442, 'recall': 0.6036036036036037, 'f1_score': 0.42138364779874216, 'f1_score_avg': 0.4083404190649905}
MLP_121089: {'accuracy': 0.8360655737704918, 'precision': 0.27986348122866894, 'recall': 0.7387387387387387, 'f1_score': 0.40594059405940597, 'f1_score_avg': 0.40430587158406717}
Epoch [10/30] Train Loss: 0.4682, Test Loss: 0.4081, F1: 0.3867, AUC: 0.8387
Epoch [20/30] Train Loss: 0.3162, Test Loss: 0.4602, F1: 0.3633, AUC: 0.8571
Mejores resultados en la época:  28
f1-score 0.4024767801857585
AUC según el mejor F1-score 0.8566681981316128
Confusion Matrix:
 [[1206  147]
 [  46   65]]
Matriz de confusión guardada en: outputs_numerical_categorical_metadata/2/lyrics_bert/confusion_matrix_param_1010689.png
Accuracy:   0.8682
Precision:  0.3066
Recall:     0.5856
F1-score:   0.4025
Tiempo total para red 6: 21.97 segundos
Saved on: outputs_numerical_categorical_metadata/2/lyrics_bert

==============================
Model: Logistic Regression
Accuracy:  0.7862
Precision: 0.2314
Recall:    0.7838
F1-score:  0.3573
              precision    recall  f1-score   support

           0       0.98      0.79      0.87      1353
           1       0.23      0.78      0.36       111

    accuracy                           0.79      1464
   macro avg       0.60      0.79      0.61      1464
weighted avg       0.92      0.79      0.83      1464

[[1064  289]
 [  24   87]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8005
Precision: 0.2534
Recall:    0.8378
F1-score:  0.3891
              precision    recall  f1-score   support

           0       0.98      0.80      0.88      1353
           1       0.25      0.84      0.39       111

    accuracy                           0.80      1464
   macro avg       0.62      0.82      0.63      1464
weighted avg       0.93      0.80      0.84      1464

[[1079  274]
 [  18   93]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_svm.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6776
Precision: 0.1495
Recall:    0.6937
F1-score:  0.2460
              precision    recall  f1-score   support

           0       0.96      0.68      0.79      1353
           1       0.15      0.69      0.25       111

    accuracy                           0.68      1464
   macro avg       0.56      0.68      0.52      1464
weighted avg       0.90      0.68      0.75      1464

[[915 438]
 [ 34  77]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8005
Precision: 0.2407
Recall:    0.7568
F1-score:  0.3652
              precision    recall  f1-score   support

           0       0.98      0.80      0.88      1353
           1       0.24      0.76      0.37       111

    accuracy                           0.80      1464
   macro avg       0.61      0.78      0.62      1464
weighted avg       0.92      0.80      0.84      1464

[[1088  265]
 [  27   84]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_random_forest.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8067
Precision: 0.2529
Recall:    0.7928
F1-score:  0.3834
              precision    recall  f1-score   support

           0       0.98      0.81      0.89      1353
           1       0.25      0.79      0.38       111

    accuracy                           0.81      1464
   macro avg       0.62      0.80      0.63      1464
weighted avg       0.92      0.81      0.85      1464

[[1093  260]
 [  23   88]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_xgboost.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7753
Precision: 0.1955
Recall:    0.6306
F1-score:  0.2985
              precision    recall  f1-score   support

           0       0.96      0.79      0.87      1353
           1       0.20      0.63      0.30       111

    accuracy                           0.78      1464
   macro avg       0.58      0.71      0.58      1464
weighted avg       0.90      0.78      0.82      1464

[[1065  288]
 [  41   70]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_numerical_categorical_metadata/2/lyrics_bert/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_numerical_categorical_metadata/2/lyrics_bert/naive_bayes_model.pkl


Resumen de métricas:
SVM: {'accuracy': 0.8005, 'precision': 0.2534, 'recall': 0.8378, 'f1_score': 0.3891}
XGBoost: {'accuracy': 0.8067, 'precision': 0.2529, 'recall': 0.7928, 'f1_score': 0.3834}
Random Forest: {'accuracy': 0.8005, 'precision': 0.2407, 'recall': 0.7568, 'f1_score': 0.3652}
Logistic Regression: {'accuracy': 0.7862, 'precision': 0.2314, 'recall': 0.7838, 'f1_score': 0.3573}
Naive Bayes: {'accuracy': 0.7753, 'precision': 0.1955, 'recall': 0.6306, 'f1_score': 0.2985}
Decision Tree: {'accuracy': 0.6776, 'precision': 0.1495, 'recall': 0.6937, 'f1_score': 0.246}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: LYRICS_BERT
MLP_1010689: {'accuracy': 0.8681693989071039, 'precision': 0.30660377358490565, 'recall': 0.5855855855855856, 'f1_score': 0.43352601156069365, 'f1_score_avg': 0.4177233713566928}
MLP_121089: {'accuracy': 0.8613387978142076, 'precision': 0.2982456140350877, 'recall': 0.6126126126126126, 'f1_score': 0.4222222222222222, 'f1_score_avg': 0.4100671931913259}
MLP_328193: {'accuracy': 0.819672131147541, 'precision': 0.2616822429906542, 'recall': 0.7567567567567568, 'f1_score': 0.4222222222222222, 'f1_score_avg': 0.39688024853387704}
MLP_49281: {'accuracy': 0.8346994535519126, 'precision': 0.27491408934707906, 'recall': 0.7207207207207207, 'f1_score': 0.4040920716112532, 'f1_score_avg': 0.39419668214033354}
MLP_21569: {'accuracy': 0.89275956284153, 'precision': 0.3424657534246575, 'recall': 0.45045045045045046, 'f1_score': 0.391304347826087, 'f1_score_avg': 0.3800277349466272}
SVM: {'accuracy': 0.8005, 'precision': 0.2534, 'recall': 0.8378, 'f1_score': 0.3891}
XGBoost: {'accuracy': 0.8067, 'precision': 0.2529, 'recall': 0.7928, 'f1_score': 0.3834}
MLP_9761: {'accuracy': 0.8790983606557377, 'precision': 0.29878048780487804, 'recall': 0.44144144144144143, 'f1_score': 0.3770491803278688, 'f1_score_avg': 0.37161268470990566}
Random Forest: {'accuracy': 0.8005, 'precision': 0.2407, 'recall': 0.7568, 'f1_score': 0.3652}
Logistic Regression: {'accuracy': 0.7862, 'precision': 0.2314, 'recall': 0.7838, 'f1_score': 0.3573}
Naive Bayes: {'accuracy': 0.7753, 'precision': 0.1955, 'recall': 0.6306, 'f1_score': 0.2985}
Decision Tree: {'accuracy': 0.6776, 'precision': 0.1495, 'recall': 0.6937, 'f1_score': 0.246}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['artist', 'genre', 'title_songs_new']
Numeric Columns: ['lyrics_word_count', 'popularity', 'duration_ms']
====================================

MLP_49281: {'accuracy': 0.8599726775956285, 'precision': 0.2974137931034483, 'recall': 0.6216216216216216, 'f1_score': 0.40588235294117647, 'f1_score_avg': 0.40115277771340196}
MLP_21569: {'accuracy': 0.8346994535519126, 'precision': 0.2600732600732601, 'recall': 0.6396396396396397, 'f1_score': 0.3894230769230769, 'f1_score_avg': 0.38006930779482245}
SVM: {'accuracy': 0.8005, 'precision': 0.2534, 'recall': 0.8378, 'f1_score': 0.3891}
MLP_9761: {'accuracy': 0.8818306010928961, 'precision': 0.31547619047619047, 'recall': 0.4774774774774775, 'f1_score': 0.38545454545454544, 'f1_score_avg': 0.3758491398742172}
XGBoost: {'accuracy': 0.8067, 'precision': 0.2529, 'recall': 0.7928, 'f1_score': 0.3834}
Random Forest: {'accuracy': 0.8005, 'precision': 0.2407, 'recall': 0.7568, 'f1_score': 0.3652}
Logistic Regression: {'accuracy': 0.7862, 'precision': 0.2314, 'recall': 0.7838, 'f1_score': 0.3573}
Naive Bayes: {'accuracy': 0.7753, 'precision': 0.1955, 'recall': 0.6306, 'f1_score': 0.2985}
Decision Tree: {'accuracy': 0.6776, 'precision': 0.1495, 'recall': 0.6937, 'f1_score': 0.246}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['artist', 'genre', 'title_songs_new']
Numeric Columns: ['lyrics_word_count', 'popularity', 'duration_ms']
====================================

