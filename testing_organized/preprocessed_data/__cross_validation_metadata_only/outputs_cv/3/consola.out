2025-10-23 01:44:27.081449: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-23 01:44:27.081604: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-23 01:44:27.174090: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-23 01:44:27.174090: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-23 01:44:30.838310: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-23 01:44:30.838310: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_2.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_2.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 15 ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']

CAT_LOW 3 ['emotion', 'Key', 'Time signature']
CAT_HIGH 12 ['Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
After removing some columns that ignorated, for TF-IDF embbedings you are selecteing this columns:
--> ['text']
After removing some columns that ignorated, for numeric cols you are selecteing this columns:
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
--> PaTH:  ../../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy
Running experiment with TFIDF embeddings
Contaning the categorical cols
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 5023)
Shape of X_test after concatenation:  (21625, 5023)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [5023, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3205, Test Loss: 0.2107, F1: 0.9133, AUC: 0.9737
Epoch [10/30] Train Loss: 0.0774, Test Loss: 0.2184, F1: 0.9295, AUC: 0.9797
Epoch [20/30] Train Loss: 0.0424, Test Loss: 0.2981, F1: 0.9274, AUC: 0.9780
Mejores resultados en la época:  5
f1-score 0.9345928494559369
AUC según el mejor F1-score 0.9811906527458537

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3242, Test Loss: 0.1937, F1: 0.9284, AUC: 0.9791
Epoch [10/30] Train Loss: 0.0708, Test Loss: 0.1999, F1: 0.9363, AUC: 0.9827
Epoch [20/30] Train Loss: 0.0329, Test Loss: 0.2698, F1: 0.9314, AUC: 0.9807
Mejores resultados en la época:  2
f1-score 0.9388593523441277
AUC según el mejor F1-score 0.9844494701527852

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3282, Test Loss: 0.1995, F1: 0.9210, AUC: 0.9775
Epoch [10/30] Train Loss: 0.0777, Test Loss: 0.2031, F1: 0.9291, AUC: 0.9818
Epoch [20/30] Train Loss: 0.0422, Test Loss: 0.2717, F1: 0.9264, AUC: 0.9798
Mejores resultados en la época:  4
f1-score 0.9342200725513906
AUC según el mejor F1-score 0.9834006674966951

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3194, Test Loss: 0.2047, F1: 0.9172, AUC: 0.9759
Epoch [10/30] Train Loss: 0.0837, Test Loss: 0.2236, F1: 0.9249, AUC: 0.9791
Epoch [20/30] Train Loss: 0.0681, Test Loss: 0.2840, F1: 0.9216, AUC: 0.9768
Mejores resultados en la época:  3
f1-score 0.9327485380116959
AUC según el mejor F1-score 0.9815319164022893

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3214, Test Loss: 0.2066, F1: 0.9190, AUC: 0.9749
Epoch [10/30] Train Loss: 0.0769, Test Loss: 0.2244, F1: 0.9292, AUC: 0.9797
Epoch [20/30] Train Loss: 0.0381, Test Loss: 0.2965, F1: 0.9289, AUC: 0.9785
Mejores resultados en la época:  5
f1-score 0.93396340724585
AUC según el mejor F1-score 0.9807298622420326
Epoch [0/30] Train Loss: 0.2965, Test Loss: 0.2097, F1: 0.8490, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0939, Test Loss: 0.1949, F1: 0.8669, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0831, Test Loss: 0.2272, F1: 0.8619, AUC: 0.9813
Mejores resultados en la época:  2
f1-score 0.8795347549155359
AUC según el mejor F1-score 0.9837098013874862
Confusion matrix Test saved: outputs_cv/3/tfidf/cm_mlp_1.png

========================================
Entrenando red 5 con capas [5023, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2354, Test Loss: 0.1870, F1: 0.9261, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.4005, F1: 0.9436, AUC: 0.9842
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5823, F1: 0.9453, AUC: 0.9843
Mejores resultados en la época:  20
f1-score 0.9452581032412966
AUC según el mejor F1-score 0.9843039922105041

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2391, Test Loss: 0.1810, F1: 0.9321, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0033, Test Loss: 0.3174, F1: 0.9440, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5303, F1: 0.9457, AUC: 0.9853
Mejores resultados en la época:  29
f1-score 0.946164037095026
AUC según el mejor F1-score 0.9833634324203024

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2378, Test Loss: 0.1809, F1: 0.9265, AUC: 0.9816
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.4680, F1: 0.9436, AUC: 0.9847
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7790, F1: 0.9433, AUC: 0.9807
Mejores resultados en la época:  29
f1-score 0.9447573397243859
AUC según el mejor F1-score 0.9785908580463913

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2334, Test Loss: 0.1797, F1: 0.9312, AUC: 0.9802
Epoch [10/30] Train Loss: 0.0035, Test Loss: 0.4039, F1: 0.9428, AUC: 0.9838
Epoch [20/30] Train Loss: 0.0019, Test Loss: 0.2808, F1: 0.9380, AUC: 0.9843
Mejores resultados en la época:  28
f1-score 0.9456508403876825
AUC según el mejor F1-score 0.9821880171324029

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2337, Test Loss: 0.1852, F1: 0.9270, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0037, Test Loss: 0.3826, F1: 0.9399, AUC: 0.9832
Epoch [20/30] Train Loss: 0.0001, Test Loss: 0.6181, F1: 0.9429, AUC: 0.9820
Mejores resultados en la época:  29
f1-score 0.9452386721589532
AUC según el mejor F1-score 0.9796926625192763
Epoch [0/30] Train Loss: 0.2250, Test Loss: 0.1615, F1: 0.8698, AUC: 0.9829
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 15 ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']

CAT_LOW 3 ['emotion', 'Key', 'Time signature']
CAT_HIGH 12 ['Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
After removing some columns that ignorated, for TF-IDF embbedings you are selecteing this columns:
--> ['text']
After removing some columns that ignorated, for numeric cols you are selecteing this columns:
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
--> PaTH:  ../../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy
Running experiment with TFIDF embeddings
Contaning the categorical cols
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 5023)
Shape of X_test after concatenation:  (21625, 5023)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [5023, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3205, Test Loss: 0.2107, F1: 0.9133, AUC: 0.9737
Epoch [10/30] Train Loss: 0.0774, Test Loss: 0.2184, F1: 0.9295, AUC: 0.9797
Epoch [20/30] Train Loss: 0.0424, Test Loss: 0.2981, F1: 0.9274, AUC: 0.9780
Mejores resultados en la época:  5
f1-score 0.9345928494559369
AUC según el mejor F1-score 0.9811906527458537

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3242, Test Loss: 0.1937, F1: 0.9284, AUC: 0.9791
Epoch [10/30] Train Loss: 0.0708, Test Loss: 0.1999, F1: 0.9363, AUC: 0.9827
Epoch [20/30] Train Loss: 0.0329, Test Loss: 0.2698, F1: 0.9314, AUC: 0.9807
Mejores resultados en la época:  2
f1-score 0.9388593523441277
AUC según el mejor F1-score 0.9844494701527852

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3282, Test Loss: 0.1995, F1: 0.9210, AUC: 0.9775
Epoch [10/30] Train Loss: 0.0777, Test Loss: 0.2031, F1: 0.9291, AUC: 0.9818
Epoch [20/30] Train Loss: 0.0422, Test Loss: 0.2717, F1: 0.9264, AUC: 0.9798
Mejores resultados en la época:  4
f1-score 0.9342200725513906
AUC según el mejor F1-score 0.9834006674966951

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3194, Test Loss: 0.2047, F1: 0.9172, AUC: 0.9759
Epoch [10/30] Train Loss: 0.0837, Test Loss: 0.2236, F1: 0.9249, AUC: 0.9791
Epoch [20/30] Train Loss: 0.0681, Test Loss: 0.2840, F1: 0.9216, AUC: 0.9768
Mejores resultados en la época:  3
f1-score 0.9327485380116959
AUC según el mejor F1-score 0.9815319164022893

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3214, Test Loss: 0.2066, F1: 0.9190, AUC: 0.9749
Epoch [10/30] Train Loss: 0.0769, Test Loss: 0.2244, F1: 0.9292, AUC: 0.9797
Epoch [20/30] Train Loss: 0.0381, Test Loss: 0.2965, F1: 0.9289, AUC: 0.9785
Mejores resultados en la época:  5
f1-score 0.93396340724585
AUC según el mejor F1-score 0.9807298622420326
Epoch [0/30] Train Loss: 0.2965, Test Loss: 0.2097, F1: 0.8490, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0939, Test Loss: 0.1949, F1: 0.8669, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0831, Test Loss: 0.2272, F1: 0.8619, AUC: 0.9813
Mejores resultados en la época:  2
f1-score 0.8795347549155359
AUC según el mejor F1-score 0.9837098013874862
Confusion matrix Test saved: outputs_cv/3/tfidf/cm_mlp_1.png

========================================
Entrenando red 5 con capas [5023, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2354, Test Loss: 0.1870, F1: 0.9261, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.4005, F1: 0.9436, AUC: 0.9842
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5823, F1: 0.9453, AUC: 0.9843
Mejores resultados en la época:  20
f1-score 0.9452581032412966
AUC según el mejor F1-score 0.9843039922105041

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2391, Test Loss: 0.1810, F1: 0.9321, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0033, Test Loss: 0.3174, F1: 0.9440, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5303, F1: 0.9457, AUC: 0.9853
Mejores resultados en la época:  29
f1-score 0.946164037095026
AUC según el mejor F1-score 0.9833634324203024

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2378, Test Loss: 0.1809, F1: 0.9265, AUC: 0.9816
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.4680, F1: 0.9436, AUC: 0.9847
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7790, F1: 0.9433, AUC: 0.9807
Mejores resultados en la época:  29
f1-score 0.9447573397243859
AUC según el mejor F1-score 0.9785908580463913

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2334, Test Loss: 0.1797, F1: 0.9312, AUC: 0.9802
Epoch [10/30] Train Loss: 0.0035, Test Loss: 0.4039, F1: 0.9428, AUC: 0.9838
Epoch [20/30] Train Loss: 0.0019, Test Loss: 0.2808, F1: 0.9380, AUC: 0.9843
Mejores resultados en la época:  28
f1-score 0.9456508403876825
AUC según el mejor F1-score 0.9821880171324029

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2337, Test Loss: 0.1852, F1: 0.9270, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0037, Test Loss: 0.3826, F1: 0.9399, AUC: 0.9832
Epoch [20/30] Train Loss: 0.0001, Test Loss: 0.6181, F1: 0.9429, AUC: 0.9820
Mejores resultados en la época:  29
f1-score 0.9452386721589532
AUC según el mejor F1-score 0.9796926625192763
Epoch [0/30] Train Loss: 0.2250, Test Loss: 0.1615, F1: 0.8698, AUC: 0.9829
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Epoch [10/30] Train Loss: 0.0035, Test Loss: 0.3334, F1: 0.9089, AUC: 0.9874
Epoch [20/30] Train Loss: 0.0023, Test Loss: 0.3607, F1: 0.8906, AUC: 0.9880
Mejores resultados en la época:  18
f1-score 0.9097542242703534
AUC según el mejor F1-score 0.9875491646598259
Confusion matrix Test saved: outputs_cv/3/tfidf/cm_mlp_5.png

========================================
Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2358, Test Loss: 0.1881, F1: 0.9289, AUC: 0.9798
Epoch [10/30] Train Loss: 0.0020, Test Loss: 0.3595, F1: 0.9447, AUC: 0.9848
Epoch [20/30] Train Loss: 0.0025, Test Loss: 0.4757, F1: 0.9419, AUC: 0.9843
Mejores resultados en la época:  29
f1-score 0.9463449765934462
AUC según el mejor F1-score 0.9794359387675772

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2440, Test Loss: 0.1688, F1: 0.9347, AUC: 0.9825
Epoch [10/30] Train Loss: 0.0039, Test Loss: 0.3678, F1: 0.9455, AUC: 0.9864
Epoch [20/30] Train Loss: 0.0021, Test Loss: 0.3759, F1: 0.9449, AUC: 0.9871
Mejores resultados en la época:  21
f1-score 0.948952564411269
AUC según el mejor F1-score 0.9858812453991647

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2443, Test Loss: 0.1787, F1: 0.9237, AUC: 0.9819
Epoch [10/30] Train Loss: 0.0018, Test Loss: 0.3953, F1: 0.9461, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0000, Test Loss: 1.0212, F1: 0.9448, AUC: 0.9762
Mejores resultados en la época:  10
f1-score 0.9461380889263766
AUC según el mejor F1-score 0.9858784872453579

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2391, Test Loss: 0.1853, F1: 0.9248, AUC: 0.9790
Epoch [10/30] Train Loss: 0.0041, Test Loss: 0.4510, F1: 0.9416, AUC: 0.9841
Epoch [20/30] Train Loss: 0.0000, Test Loss: 1.1695, F1: 0.9423, AUC: 0.9783
Mejores resultados en la época:  7
f1-score 0.9427312775330396
AUC según el mejor F1-score 0.9841767463461456

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2405, Test Loss: 0.1988, F1: 0.9086, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0019, Test Loss: 0.5812, F1: 0.9430, AUC: 0.9816
Epoch [20/30] Train Loss: 0.0033, Test Loss: 0.6290, F1: 0.9440, AUC: 0.9842
Mejores resultados en la época:  8
f1-score 0.9475327170128467
AUC según el mejor F1-score 0.9847086707314094
Epoch [0/30] Train Loss: 0.2271, Test Loss: 0.1785, F1: 0.8722, AUC: 0.9824
Epoch [10/30] Train Loss: 0.0057, Test Loss: 0.3283, F1: 0.9063, AUC: 0.9878
Epoch [20/30] Train Loss: 0.0035, Test Loss: 0.4600, F1: 0.8978, AUC: 0.9869
Mejores resultados en la época:  21
f1-score 0.9127657546000572
AUC según el mejor F1-score 0.9873417067446334
Confusion matrix Test saved: outputs_cv/3/tfidf/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9347, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9329, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.937, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9349, 'f1_cv_std': 0.0021, 'params': 160801, 'accuracy_test': 0.9397, 'precision_test': 0.8398, 'recall_test': 0.9233, 'f1_score_test': 0.8795}, 'MLP_2744833': {'accuracy_cv_mean': 0.9453, 'accuracy_cv_std': 0.0008, 'precision_cv_mean': 0.9432, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9478, 'recall_cv_std': 0.0078, 'f1_cv_mean': 0.9454, 'f1_cv_std': 0.0005, 'params': 2744833, 'accuracy_test': 0.9565, 'precision_test': 0.9014, 'recall_test': 0.9182, 'f1_score_test': 0.9098}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9428, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.95, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.9463, 'f1_cv_std': 0.0021, 'params': 5843969, 'accuracy_test': 0.9577, 'precision_test': 0.8983, 'recall_test': 0.9277, 'f1_score_test': 0.9128}}}
Saved on: outputs_cv/3/tfidf

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.936, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0032}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 45, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.82      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.94     21625

Confusion matrix Test saved as: outputs_cv/3/tfidf/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/3/tfidf/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8184, 'accuracy_cv_std': 0.027, 'precision_cv_mean': 0.7823, 'precision_cv_std': 0.0468, 'recall_cv_mean': 0.8925, 'recall_cv_std': 0.0401, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0154}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 45, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.71      0.81     16465
           1       0.49      0.87      0.62      5160

    accuracy                           0.75     21625
   macro avg       0.72      0.79      0.72     21625
weighted avg       0.84      0.75      0.77     21625

Confusion matrix Test saved as: outputs_cv/3/tfidf/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/3/tfidf/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8316, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8608, 'precision_cv_std': 0.0152, 'recall_cv_mean': 0.7917, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8245, 'f1_cv_std': 0.0024}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 45, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.86      0.90     16465
           1       0.65      0.80      0.72      5160

    accuracy                           0.85     21625
   macro avg       0.79      0.83      0.81     21625
weighted avg       0.86      0.85      0.85     21625

Confusion matrix Test saved as: outputs_cv/3/tfidf/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/3/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8374, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.7903, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8294, 'f1_cv_std': 0.0029}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 45, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.70      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

Confusion matrix Test saved as: outputs_cv/3/tfidf/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/3/tfidf/random_forest_model.pkl

Epoch [10/30] Train Loss: 0.0035, Test Loss: 0.3334, F1: 0.9089, AUC: 0.9874
Epoch [20/30] Train Loss: 0.0023, Test Loss: 0.3607, F1: 0.8906, AUC: 0.9880
Mejores resultados en la época:  18
f1-score 0.9097542242703534
AUC según el mejor F1-score 0.9875491646598259
Confusion matrix Test saved: outputs_cv/3/tfidf/cm_mlp_5.png

========================================
Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2358, Test Loss: 0.1881, F1: 0.9289, AUC: 0.9798
Epoch [10/30] Train Loss: 0.0020, Test Loss: 0.3595, F1: 0.9447, AUC: 0.9848
Epoch [20/30] Train Loss: 0.0025, Test Loss: 0.4757, F1: 0.9419, AUC: 0.9843
Mejores resultados en la época:  29
f1-score 0.9463449765934462
AUC según el mejor F1-score 0.9794359387675772

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2440, Test Loss: 0.1688, F1: 0.9347, AUC: 0.9825
Epoch [10/30] Train Loss: 0.0039, Test Loss: 0.3678, F1: 0.9455, AUC: 0.9864
Epoch [20/30] Train Loss: 0.0021, Test Loss: 0.3759, F1: 0.9449, AUC: 0.9871
Mejores resultados en la época:  21
f1-score 0.948952564411269
AUC según el mejor F1-score 0.9858812453991647

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2443, Test Loss: 0.1787, F1: 0.9237, AUC: 0.9819
Epoch [10/30] Train Loss: 0.0018, Test Loss: 0.3953, F1: 0.9461, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0000, Test Loss: 1.0212, F1: 0.9448, AUC: 0.9762
Mejores resultados en la época:  10
f1-score 0.9461380889263766
AUC según el mejor F1-score 0.9858784872453579

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2391, Test Loss: 0.1853, F1: 0.9248, AUC: 0.9790
Epoch [10/30] Train Loss: 0.0041, Test Loss: 0.4510, F1: 0.9416, AUC: 0.9841
Epoch [20/30] Train Loss: 0.0000, Test Loss: 1.1695, F1: 0.9423, AUC: 0.9783
Mejores resultados en la época:  7
f1-score 0.9427312775330396
AUC según el mejor F1-score 0.9841767463461456

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2405, Test Loss: 0.1988, F1: 0.9086, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0019, Test Loss: 0.5812, F1: 0.9430, AUC: 0.9816
Epoch [20/30] Train Loss: 0.0033, Test Loss: 0.6290, F1: 0.9440, AUC: 0.9842
Mejores resultados en la época:  8
f1-score 0.9475327170128467
AUC según el mejor F1-score 0.9847086707314094
Epoch [0/30] Train Loss: 0.2271, Test Loss: 0.1785, F1: 0.8722, AUC: 0.9824
Epoch [10/30] Train Loss: 0.0057, Test Loss: 0.3283, F1: 0.9063, AUC: 0.9878
Epoch [20/30] Train Loss: 0.0035, Test Loss: 0.4600, F1: 0.8978, AUC: 0.9869
Mejores resultados en la época:  21
f1-score 0.9127657546000572
AUC según el mejor F1-score 0.9873417067446334
Confusion matrix Test saved: outputs_cv/3/tfidf/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9347, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9329, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.937, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9349, 'f1_cv_std': 0.0021, 'params': 160801, 'accuracy_test': 0.9397, 'precision_test': 0.8398, 'recall_test': 0.9233, 'f1_score_test': 0.8795}, 'MLP_2744833': {'accuracy_cv_mean': 0.9453, 'accuracy_cv_std': 0.0008, 'precision_cv_mean': 0.9432, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9478, 'recall_cv_std': 0.0078, 'f1_cv_mean': 0.9454, 'f1_cv_std': 0.0005, 'params': 2744833, 'accuracy_test': 0.9565, 'precision_test': 0.9014, 'recall_test': 0.9182, 'f1_score_test': 0.9098}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9428, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.95, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.9463, 'f1_cv_std': 0.0021, 'params': 5843969, 'accuracy_test': 0.9577, 'precision_test': 0.8983, 'recall_test': 0.9277, 'f1_score_test': 0.9128}}}
Saved on: outputs_cv/3/tfidf

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.936, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0032}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 45, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.82      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.94     21625

Confusion matrix Test saved as: outputs_cv/3/tfidf/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/3/tfidf/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8184, 'accuracy_cv_std': 0.027, 'precision_cv_mean': 0.7823, 'precision_cv_std': 0.0468, 'recall_cv_mean': 0.8925, 'recall_cv_std': 0.0401, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0154}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 45, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.71      0.81     16465
           1       0.49      0.87      0.62      5160

    accuracy                           0.75     21625
   macro avg       0.72      0.79      0.72     21625
weighted avg       0.84      0.75      0.77     21625

Confusion matrix Test saved as: outputs_cv/3/tfidf/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/3/tfidf/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8316, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8608, 'precision_cv_std': 0.0152, 'recall_cv_mean': 0.7917, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8245, 'f1_cv_std': 0.0024}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 45, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.86      0.90     16465
           1       0.65      0.80      0.72      5160

    accuracy                           0.85     21625
   macro avg       0.79      0.83      0.81     21625
weighted avg       0.86      0.85      0.85     21625

Confusion matrix Test saved as: outputs_cv/3/tfidf/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/3/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8374, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.7903, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8294, 'f1_cv_std': 0.0029}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 45, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.70      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

Confusion matrix Test saved as: outputs_cv/3/tfidf/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/3/tfidf/random_forest_model.pkl

/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:15:12] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:15:18] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:19:10] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:19:17] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:23:10] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:23:17] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:27:09] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:27:16] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:31:04] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:31:11] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:35:08] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:35:16] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9065, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9249, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8848, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.9044, 'f1_cv_std': 0.0019}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 45, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.89      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/3/tfidf/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/3/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8739, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.901, 'f1_cv_std': 0.0033}
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.81      0.87      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.90      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/3/tfidf/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/3/tfidf/naive_bayes_model.pkl


Resumen Final:
Logistic Regression: {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.936, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0032, 'accuracy_test': 0.9338, 'precision_test': 0.8225, 'recall_test': 0.9213, 'f1_score_test': 0.8691}
XGBoost: {'accuracy_cv_mean': 0.9065, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9249, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8848, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.9044, 'f1_cv_std': 0.0019, 'accuracy_test': 0.9227, 'precision_test': 0.8049, 'recall_test': 0.8924, 'f1_score_test': 0.8464}
Naive Bayes: {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8739, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.901, 'f1_cv_std': 0.0033, 'accuracy_test': 0.9195, 'precision_test': 0.805, 'recall_test': 0.8746, 'f1_score_test': 0.8384}
Random Forest: {'accuracy_cv_mean': 0.8374, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.7903, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8294, 'f1_cv_std': 0.0029, 'accuracy_test': 0.8689, 'precision_test': 0.7005, 'recall_test': 0.7868, 'f1_score_test': 0.7411}
Decision Tree: {'accuracy_cv_mean': 0.8316, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8608, 'precision_cv_std': 0.0152, 'recall_cv_mean': 0.7917, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8245, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8486, 'precision_test': 0.6479, 'recall_test': 0.8012, 'f1_score_test': 0.7164}
SVM: {'accuracy_cv_mean': 0.8184, 'accuracy_cv_std': 0.027, 'precision_cv_mean': 0.7823, 'precision_cv_std': 0.0468, 'recall_cv_mean': 0.8925, 'recall_cv_std': 0.0401, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0154, 'accuracy_test': 0.7489, 'precision_test': 0.4854, 'recall_test': 0.8742, 'f1_score_test': 0.6242}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9347, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9329, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.937, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9349, 'f1_cv_std': 0.0021, 'params': 160801, 'accuracy_test': 0.9397, 'precision_test': 0.8398, 'recall_test': 0.9233, 'f1_score_test': 0.8795}, 'MLP_2744833': {'accuracy_cv_mean': 0.9453, 'accuracy_cv_std': 0.0008, 'precision_cv_mean': 0.9432, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9478, 'recall_cv_std': 0.0078, 'f1_cv_mean': 0.9454, 'f1_cv_std': 0.0005, 'params': 2744833, 'accuracy_test': 0.9565, 'precision_test': 0.9014, 'recall_test': 0.9182, 'f1_score_test': 0.9098}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9428, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.95, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.9463, 'f1_cv_std': 0.0021, 'params': 5843969, 'accuracy_test': 0.9577, 'precision_test': 0.8983, 'recall_test': 0.9277, 'f1_score_test': 0.9128}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.936, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0032, 'accuracy_test': 0.9338, 'precision_test': 0.8225, 'recall_test': 0.9213, 'f1_score_test': 0.8691}, 'SVM': {'accuracy_cv_mean': 0.8184, 'accuracy_cv_std': 0.027, 'precision_cv_mean': 0.7823, 'precision_cv_std': 0.0468, 'recall_cv_mean': 0.8925, 'recall_cv_std': 0.0401, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0154, 'accuracy_test': 0.7489, 'precision_test': 0.4854, 'recall_test': 0.8742, 'f1_score_test': 0.6242}, 'Decision Tree': {'accuracy_cv_mean': 0.8316, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8608, 'precision_cv_std': 0.0152, 'recall_cv_mean': 0.7917, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8245, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8486, 'precision_test': 0.6479, 'recall_test': 0.8012, 'f1_score_test': 0.7164}, 'Random Forest': {'accuracy_cv_mean': 0.8374, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.7903, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8294, 'f1_cv_std': 0.0029, 'accuracy_test': 0.8689, 'precision_test': 0.7005, 'recall_test': 0.7868, 'f1_score_test': 0.7411}, 'XGBoost': {'accuracy_cv_mean': 0.9065, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9249, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8848, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.9044, 'f1_cv_std': 0.0019, 'accuracy_test': 0.9227, 'precision_test': 0.8049, 'recall_test': 0.8924, 'f1_score_test': 0.8464}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8739, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.901, 'f1_cv_std': 0.0033, 'accuracy_test': 0.9195, 'precision_test': 0.805, 'recall_test': 0.8746, 'f1_score_test': 0.8384}}}
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_2.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9065, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9249, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8848, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.9044, 'f1_cv_std': 0.0019}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 45, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.89      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/3/tfidf/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/3/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8739, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.901, 'f1_cv_std': 0.0033}
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.81      0.87      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.90      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/3/tfidf/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/3/tfidf/naive_bayes_model.pkl


Resumen Final:
Logistic Regression: {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.936, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0032, 'accuracy_test': 0.9338, 'precision_test': 0.8225, 'recall_test': 0.9213, 'f1_score_test': 0.8691}
XGBoost: {'accuracy_cv_mean': 0.9065, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9249, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8848, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.9044, 'f1_cv_std': 0.0019, 'accuracy_test': 0.9227, 'precision_test': 0.8049, 'recall_test': 0.8924, 'f1_score_test': 0.8464}
Naive Bayes: {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8739, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.901, 'f1_cv_std': 0.0033, 'accuracy_test': 0.9195, 'precision_test': 0.805, 'recall_test': 0.8746, 'f1_score_test': 0.8384}
Random Forest: {'accuracy_cv_mean': 0.8374, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.7903, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8294, 'f1_cv_std': 0.0029, 'accuracy_test': 0.8689, 'precision_test': 0.7005, 'recall_test': 0.7868, 'f1_score_test': 0.7411}
Decision Tree: {'accuracy_cv_mean': 0.8316, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8608, 'precision_cv_std': 0.0152, 'recall_cv_mean': 0.7917, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8245, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8486, 'precision_test': 0.6479, 'recall_test': 0.8012, 'f1_score_test': 0.7164}
SVM: {'accuracy_cv_mean': 0.8184, 'accuracy_cv_std': 0.027, 'precision_cv_mean': 0.7823, 'precision_cv_std': 0.0468, 'recall_cv_mean': 0.8925, 'recall_cv_std': 0.0401, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0154, 'accuracy_test': 0.7489, 'precision_test': 0.4854, 'recall_test': 0.8742, 'f1_score_test': 0.6242}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9347, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9329, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.937, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9349, 'f1_cv_std': 0.0021, 'params': 160801, 'accuracy_test': 0.9397, 'precision_test': 0.8398, 'recall_test': 0.9233, 'f1_score_test': 0.8795}, 'MLP_2744833': {'accuracy_cv_mean': 0.9453, 'accuracy_cv_std': 0.0008, 'precision_cv_mean': 0.9432, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9478, 'recall_cv_std': 0.0078, 'f1_cv_mean': 0.9454, 'f1_cv_std': 0.0005, 'params': 2744833, 'accuracy_test': 0.9565, 'precision_test': 0.9014, 'recall_test': 0.9182, 'f1_score_test': 0.9098}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9428, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.95, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.9463, 'f1_cv_std': 0.0021, 'params': 5843969, 'accuracy_test': 0.9577, 'precision_test': 0.8983, 'recall_test': 0.9277, 'f1_score_test': 0.9128}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.936, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0032, 'accuracy_test': 0.9338, 'precision_test': 0.8225, 'recall_test': 0.9213, 'f1_score_test': 0.8691}, 'SVM': {'accuracy_cv_mean': 0.8184, 'accuracy_cv_std': 0.027, 'precision_cv_mean': 0.7823, 'precision_cv_std': 0.0468, 'recall_cv_mean': 0.8925, 'recall_cv_std': 0.0401, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0154, 'accuracy_test': 0.7489, 'precision_test': 0.4854, 'recall_test': 0.8742, 'f1_score_test': 0.6242}, 'Decision Tree': {'accuracy_cv_mean': 0.8316, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8608, 'precision_cv_std': 0.0152, 'recall_cv_mean': 0.7917, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8245, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8486, 'precision_test': 0.6479, 'recall_test': 0.8012, 'f1_score_test': 0.7164}, 'Random Forest': {'accuracy_cv_mean': 0.8374, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.7903, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8294, 'f1_cv_std': 0.0029, 'accuracy_test': 0.8689, 'precision_test': 0.7005, 'recall_test': 0.7868, 'f1_score_test': 0.7411}, 'XGBoost': {'accuracy_cv_mean': 0.9065, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9249, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8848, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.9044, 'f1_cv_std': 0.0019, 'accuracy_test': 0.9227, 'precision_test': 0.8049, 'recall_test': 0.8924, 'f1_score_test': 0.8464}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8739, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.901, 'f1_cv_std': 0.0033, 'accuracy_test': 0.9195, 'precision_test': 0.805, 'recall_test': 0.8746, 'f1_score_test': 0.8384}}}
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_2.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 323)
Shape of X_test after concatenation:  (21625, 323)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 323)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 323)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [323, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.5459, Test Loss: 0.4533, F1: 0.7866, AUC: 0.8792
Epoch [10/30] Train Loss: 0.3592, Test Loss: 0.3716, F1: 0.8380, AUC: 0.9152
Epoch [20/30] Train Loss: 0.3473, Test Loss: 0.3639, F1: 0.8320, AUC: 0.9189
Mejores resultados en la época:  29
f1-score 0.8429028057298449
AUC según el mejor F1-score 0.9215863914803799

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.5757, Test Loss: 0.4560, F1: 0.7890, AUC: 0.8854
Epoch [10/30] Train Loss: 0.3622, Test Loss: 0.3537, F1: 0.8482, AUC: 0.9243
Epoch [20/30] Train Loss: 0.3508, Test Loss: 0.3554, F1: 0.8552, AUC: 0.9288
Mejores resultados en la época:  22
f1-score 0.8576542178737621
AUC según el mejor F1-score 0.9294208980267111

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.5317, Test Loss: 0.4301, F1: 0.8018, AUC: 0.8878
Epoch [10/30] Train Loss: 0.3574, Test Loss: 0.3595, F1: 0.8329, AUC: 0.9223
Epoch [20/30] Train Loss: 0.3412, Test Loss: 0.3392, F1: 0.8475, AUC: 0.9284
Mejores resultados en la época:  27
f1-score 0.8513198281154082
AUC según el mejor F1-score 0.9313594987061324

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.5640, Test Loss: 0.4476, F1: 0.7940, AUC: 0.8835
Epoch [10/30] Train Loss: 0.3621, Test Loss: 0.3598, F1: 0.8320, AUC: 0.9242
Epoch [20/30] Train Loss: 0.3466, Test Loss: 0.3620, F1: 0.8449, AUC: 0.9280
Mejores resultados en la época:  28
f1-score 0.8532879542968275
AUC según el mejor F1-score 0.9295109207093389

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.5848, Test Loss: 0.4720, F1: 0.7741, AUC: 0.8757
Epoch [10/30] Train Loss: 0.3641, Test Loss: 0.3745, F1: 0.8405, AUC: 0.9193
Epoch [20/30] Train Loss: 0.3506, Test Loss: 0.3509, F1: 0.8426, AUC: 0.9235
Mejores resultados en la época:  26
f1-score 0.8484484002886697
AUC según el mejor F1-score 0.9254913168715002
Epoch [0/30] Train Loss: 0.5265, Test Loss: 0.4445, F1: 0.6639, AUC: 0.8924
Epoch [10/30] Train Loss: 0.3576, Test Loss: 0.3216, F1: 0.7465, AUC: 0.9228
Epoch [20/30] Train Loss: 0.3449, Test Loss: 0.2897, F1: 0.7560, AUC: 0.9264
Mejores resultados en la época:  28
f1-score 0.757412143601402
AUC según el mejor F1-score 0.9270129026334931
Confusion matrix Test saved: outputs_cv/3/lyrics_bert/cm_mlp_1.png

========================================
Entrenando red 5 con capas [323, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4772, Test Loss: 0.3994, F1: 0.8173, AUC: 0.9002
Epoch [10/30] Train Loss: 0.3462, Test Loss: 0.3640, F1: 0.8126, AUC: 0.9215
Epoch [20/30] Train Loss: 0.3285, Test Loss: 0.3781, F1: 0.8027, AUC: 0.9240
Mejores resultados en la época:  28
f1-score 0.8478155339805825
AUC según el mejor F1-score 0.9292029451918457

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4761, Test Loss: 0.4071, F1: 0.8315, AUC: 0.9105
Epoch [10/30] Train Loss: 0.3594, Test Loss: 0.3364, F1: 0.8488, AUC: 0.9311
Epoch [20/30] Train Loss: 0.3401, Test Loss: 0.3317, F1: 0.8429, AUC: 0.9359
Mejores resultados en la época:  27
f1-score 0.8620857699805068
AUC según el mejor F1-score 0.9386184607107445

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4706, Test Loss: 0.4031, F1: 0.8018, AUC: 0.9069
Epoch [10/30] Train Loss: 0.3576, Test Loss: 0.3484, F1: 0.8286, AUC: 0.9262
Epoch [20/30] Train Loss: 0.3331, Test Loss: 0.3377, F1: 0.8510, AUC: 0.9319
Mejores resultados en la época:  29
f1-score 0.8553658536585366
AUC según el mejor F1-score 0.9342152442104591

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4726, Test Loss: 0.4425, F1: 0.7479, AUC: 0.9093
Epoch [10/30] Train Loss: 0.3614, Test Loss: 0.3442, F1: 0.8496, AUC: 0.9288
Epoch [20/30] Train Loss: 0.3353, Test Loss: 0.3335, F1: 0.8512, AUC: 0.9318
Mejores resultados en la época:  24
f1-score 0.8571765260428942
AUC según el mejor F1-score 0.9351279706057481

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4809, Test Loss: 0.3841, F1: 0.8233, AUC: 0.9069
Epoch [10/30] Train Loss: 0.3530, Test Loss: 0.3488, F1: 0.8461, AUC: 0.9273
Epoch [20/30] Train Loss: 0.3292, Test Loss: 0.3379, F1: 0.8485, AUC: 0.9328
Mejores resultados en la época:  25
f1-score 0.8569748294518936
AUC según el mejor F1-score 0.9357428064006552
Epoch [0/30] Train Loss: 0.4590, Test Loss: 0.4481, F1: 0.6850, AUC: 0.9106
Epoch [10/30] Train Loss: 0.3484, Test Loss: 0.3016, F1: 0.7525, AUC: 0.9277
Epoch [20/30] Train Loss: 0.3318, Test Loss: 0.3219, F1: 0.7611, AUC: 0.9334
Mejores resultados en la época:  24
f1-score 0.768976897689769
AUC según el mejor F1-score 0.9344382905246507
Confusion matrix Test saved: outputs_cv/3/lyrics_bert/cm_mlp_5.png

========================================
Entrenando red 6 con capas [323, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.5808, Test Loss: 0.4163, F1: 0.8077, AUC: 0.8884
Epoch [10/30] Train Loss: 0.3476, Test Loss: 0.3592, F1: 0.8314, AUC: 0.9195
Epoch [20/30] Train Loss: 0.3374, Test Loss: 0.3727, F1: 0.8405, AUC: 0.9237
Mejores resultados en la época:  29
f1-score 0.8488315481986368
AUC según el mejor F1-score 0.9287713821472567

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4979, Test Loss: 0.4008, F1: 0.8297, AUC: 0.9082
Epoch [10/30] Train Loss: 0.3572, Test Loss: 0.3775, F1: 0.8002, AUC: 0.9299
Epoch [20/30] Train Loss: 0.3442, Test Loss: 0.3283, F1: 0.8582, AUC: 0.9358
Mejores resultados en la época:  28
f1-score 0.8650231876983159
AUC según el mejor F1-score 0.9406399527146807

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.5280, Test Loss: 0.4127, F1: 0.7952, AUC: 0.8987
Epoch [10/30] Train Loss: 0.3511, Test Loss: 0.3659, F1: 0.8435, AUC: 0.9270
Epoch [20/30] Train Loss: 0.3362, Test Loss: 0.3375, F1: 0.8531, AUC: 0.9323
Mejores resultados en la época:  27
f1-score 0.8550810363184668
AUC según el mejor F1-score 0.9349082450254642

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4973, Test Loss: 0.4306, F1: 0.8169, AUC: 0.9063
Epoch [10/30] Train Loss: 0.3526, Test Loss: 0.3384, F1: 0.8466, AUC: 0.9292
Epoch [20/30] Train Loss: 0.3322, Test Loss: 0.3566, F1: 0.8500, AUC: 0.9309
Mejores resultados en la época:  23
f1-score 0.8545281673401474
AUC según el mejor F1-score 0.9322307377865183

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.5000, Test Loss: 0.4006, F1: 0.8256, AUC: 0.9043
Epoch [10/30] Train Loss: 0.3589, Test Loss: 0.3783, F1: 0.7948, AUC: 0.9255
Epoch [20/30] Train Loss: 0.3368, Test Loss: 0.3444, F1: 0.8502, AUC: 0.9321
Mejores resultados en la época:  21
f1-score 0.8538052278510294
AUC según el mejor F1-score 0.9330692142686751
Epoch [0/30] Train Loss: 0.5343, Test Loss: 0.3496, F1: 0.7257, AUC: 0.9004
Epoch [10/30] Train Loss: 0.3576, Test Loss: 0.3421, F1: 0.7404, AUC: 0.9258
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 323)
Shape of X_test after concatenation:  (21625, 323)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 323)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 323)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [323, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.5459, Test Loss: 0.4533, F1: 0.7866, AUC: 0.8792
Epoch [10/30] Train Loss: 0.3592, Test Loss: 0.3716, F1: 0.8380, AUC: 0.9152
Epoch [20/30] Train Loss: 0.3473, Test Loss: 0.3639, F1: 0.8320, AUC: 0.9189
Mejores resultados en la época:  29
f1-score 0.8429028057298449
AUC según el mejor F1-score 0.9215863914803799

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.5757, Test Loss: 0.4560, F1: 0.7890, AUC: 0.8854
Epoch [10/30] Train Loss: 0.3622, Test Loss: 0.3537, F1: 0.8482, AUC: 0.9243
Epoch [20/30] Train Loss: 0.3508, Test Loss: 0.3554, F1: 0.8552, AUC: 0.9288
Mejores resultados en la época:  22
f1-score 0.8576542178737621
AUC según el mejor F1-score 0.9294208980267111

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.5317, Test Loss: 0.4301, F1: 0.8018, AUC: 0.8878
Epoch [10/30] Train Loss: 0.3574, Test Loss: 0.3595, F1: 0.8329, AUC: 0.9223
Epoch [20/30] Train Loss: 0.3412, Test Loss: 0.3392, F1: 0.8475, AUC: 0.9284
Mejores resultados en la época:  27
f1-score 0.8513198281154082
AUC según el mejor F1-score 0.9313594987061324

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.5640, Test Loss: 0.4476, F1: 0.7940, AUC: 0.8835
Epoch [10/30] Train Loss: 0.3621, Test Loss: 0.3598, F1: 0.8320, AUC: 0.9242
Epoch [20/30] Train Loss: 0.3466, Test Loss: 0.3620, F1: 0.8449, AUC: 0.9280
Mejores resultados en la época:  28
f1-score 0.8532879542968275
AUC según el mejor F1-score 0.9295109207093389

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.5848, Test Loss: 0.4720, F1: 0.7741, AUC: 0.8757
Epoch [10/30] Train Loss: 0.3641, Test Loss: 0.3745, F1: 0.8405, AUC: 0.9193
Epoch [20/30] Train Loss: 0.3506, Test Loss: 0.3509, F1: 0.8426, AUC: 0.9235
Mejores resultados en la época:  26
f1-score 0.8484484002886697
AUC según el mejor F1-score 0.9254913168715002
Epoch [0/30] Train Loss: 0.5265, Test Loss: 0.4445, F1: 0.6639, AUC: 0.8924
Epoch [10/30] Train Loss: 0.3576, Test Loss: 0.3216, F1: 0.7465, AUC: 0.9228
Epoch [20/30] Train Loss: 0.3449, Test Loss: 0.2897, F1: 0.7560, AUC: 0.9264
Mejores resultados en la época:  28
f1-score 0.757412143601402
AUC según el mejor F1-score 0.9270129026334931
Confusion matrix Test saved: outputs_cv/3/lyrics_bert/cm_mlp_1.png

========================================
Entrenando red 5 con capas [323, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4772, Test Loss: 0.3994, F1: 0.8173, AUC: 0.9002
Epoch [10/30] Train Loss: 0.3462, Test Loss: 0.3640, F1: 0.8126, AUC: 0.9215
Epoch [20/30] Train Loss: 0.3285, Test Loss: 0.3781, F1: 0.8027, AUC: 0.9240
Mejores resultados en la época:  28
f1-score 0.8478155339805825
AUC según el mejor F1-score 0.9292029451918457

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4761, Test Loss: 0.4071, F1: 0.8315, AUC: 0.9105
Epoch [10/30] Train Loss: 0.3594, Test Loss: 0.3364, F1: 0.8488, AUC: 0.9311
Epoch [20/30] Train Loss: 0.3401, Test Loss: 0.3317, F1: 0.8429, AUC: 0.9359
Mejores resultados en la época:  27
f1-score 0.8620857699805068
AUC según el mejor F1-score 0.9386184607107445

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4706, Test Loss: 0.4031, F1: 0.8018, AUC: 0.9069
Epoch [10/30] Train Loss: 0.3576, Test Loss: 0.3484, F1: 0.8286, AUC: 0.9262
Epoch [20/30] Train Loss: 0.3331, Test Loss: 0.3377, F1: 0.8510, AUC: 0.9319
Mejores resultados en la época:  29
f1-score 0.8553658536585366
AUC según el mejor F1-score 0.9342152442104591

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4726, Test Loss: 0.4425, F1: 0.7479, AUC: 0.9093
Epoch [10/30] Train Loss: 0.3614, Test Loss: 0.3442, F1: 0.8496, AUC: 0.9288
Epoch [20/30] Train Loss: 0.3353, Test Loss: 0.3335, F1: 0.8512, AUC: 0.9318
Mejores resultados en la época:  24
f1-score 0.8571765260428942
AUC según el mejor F1-score 0.9351279706057481

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4809, Test Loss: 0.3841, F1: 0.8233, AUC: 0.9069
Epoch [10/30] Train Loss: 0.3530, Test Loss: 0.3488, F1: 0.8461, AUC: 0.9273
Epoch [20/30] Train Loss: 0.3292, Test Loss: 0.3379, F1: 0.8485, AUC: 0.9328
Mejores resultados en la época:  25
f1-score 0.8569748294518936
AUC según el mejor F1-score 0.9357428064006552
Epoch [0/30] Train Loss: 0.4590, Test Loss: 0.4481, F1: 0.6850, AUC: 0.9106
Epoch [10/30] Train Loss: 0.3484, Test Loss: 0.3016, F1: 0.7525, AUC: 0.9277
Epoch [20/30] Train Loss: 0.3318, Test Loss: 0.3219, F1: 0.7611, AUC: 0.9334
Mejores resultados en la época:  24
f1-score 0.768976897689769
AUC según el mejor F1-score 0.9344382905246507
Confusion matrix Test saved: outputs_cv/3/lyrics_bert/cm_mlp_5.png

========================================
Entrenando red 6 con capas [323, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.5808, Test Loss: 0.4163, F1: 0.8077, AUC: 0.8884
Epoch [10/30] Train Loss: 0.3476, Test Loss: 0.3592, F1: 0.8314, AUC: 0.9195
Epoch [20/30] Train Loss: 0.3374, Test Loss: 0.3727, F1: 0.8405, AUC: 0.9237
Mejores resultados en la época:  29
f1-score 0.8488315481986368
AUC según el mejor F1-score 0.9287713821472567

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4979, Test Loss: 0.4008, F1: 0.8297, AUC: 0.9082
Epoch [10/30] Train Loss: 0.3572, Test Loss: 0.3775, F1: 0.8002, AUC: 0.9299
Epoch [20/30] Train Loss: 0.3442, Test Loss: 0.3283, F1: 0.8582, AUC: 0.9358
Mejores resultados en la época:  28
f1-score 0.8650231876983159
AUC según el mejor F1-score 0.9406399527146807

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.5280, Test Loss: 0.4127, F1: 0.7952, AUC: 0.8987
Epoch [10/30] Train Loss: 0.3511, Test Loss: 0.3659, F1: 0.8435, AUC: 0.9270
Epoch [20/30] Train Loss: 0.3362, Test Loss: 0.3375, F1: 0.8531, AUC: 0.9323
Mejores resultados en la época:  27
f1-score 0.8550810363184668
AUC según el mejor F1-score 0.9349082450254642

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4973, Test Loss: 0.4306, F1: 0.8169, AUC: 0.9063
Epoch [10/30] Train Loss: 0.3526, Test Loss: 0.3384, F1: 0.8466, AUC: 0.9292
Epoch [20/30] Train Loss: 0.3322, Test Loss: 0.3566, F1: 0.8500, AUC: 0.9309
Mejores resultados en la época:  23
f1-score 0.8545281673401474
AUC según el mejor F1-score 0.9322307377865183

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.5000, Test Loss: 0.4006, F1: 0.8256, AUC: 0.9043
Epoch [10/30] Train Loss: 0.3589, Test Loss: 0.3783, F1: 0.7948, AUC: 0.9255
Epoch [20/30] Train Loss: 0.3368, Test Loss: 0.3444, F1: 0.8502, AUC: 0.9321
Mejores resultados en la época:  21
f1-score 0.8538052278510294
AUC según el mejor F1-score 0.9330692142686751
Epoch [0/30] Train Loss: 0.5343, Test Loss: 0.3496, F1: 0.7257, AUC: 0.9004
Epoch [10/30] Train Loss: 0.3576, Test Loss: 0.3421, F1: 0.7404, AUC: 0.9258
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Epoch [20/30] Train Loss: 0.3367, Test Loss: 0.3615, F1: 0.7360, AUC: 0.9308
Mejores resultados en la época:  14
f1-score 0.7527839643652561
AUC según el mejor F1-score 0.9264923127988192
Confusion matrix Test saved: outputs_cv/3/lyrics_bert/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9347, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9329, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.937, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9349, 'f1_cv_std': 0.0021, 'params': 160801, 'accuracy_test': 0.9397, 'precision_test': 0.8398, 'recall_test': 0.9233, 'f1_score_test': 0.8795}, 'MLP_2744833': {'accuracy_cv_mean': 0.9453, 'accuracy_cv_std': 0.0008, 'precision_cv_mean': 0.9432, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9478, 'recall_cv_std': 0.0078, 'f1_cv_mean': 0.9454, 'f1_cv_std': 0.0005, 'params': 2744833, 'accuracy_test': 0.9565, 'precision_test': 0.9014, 'recall_test': 0.9182, 'f1_score_test': 0.9098}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9428, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.95, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.9463, 'f1_cv_std': 0.0021, 'params': 5843969, 'accuracy_test': 0.9577, 'precision_test': 0.8983, 'recall_test': 0.9277, 'f1_score_test': 0.9128}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.936, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0032, 'accuracy_test': 0.9338, 'precision_test': 0.8225, 'recall_test': 0.9213, 'f1_score_test': 0.8691}, 'SVM': {'accuracy_cv_mean': 0.8184, 'accuracy_cv_std': 0.027, 'precision_cv_mean': 0.7823, 'precision_cv_std': 0.0468, 'recall_cv_mean': 0.8925, 'recall_cv_std': 0.0401, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0154, 'accuracy_test': 0.7489, 'precision_test': 0.4854, 'recall_test': 0.8742, 'f1_score_test': 0.6242}, 'Decision Tree': {'accuracy_cv_mean': 0.8316, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8608, 'precision_cv_std': 0.0152, 'recall_cv_mean': 0.7917, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8245, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8486, 'precision_test': 0.6479, 'recall_test': 0.8012, 'f1_score_test': 0.7164}, 'Random Forest': {'accuracy_cv_mean': 0.8374, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.7903, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8294, 'f1_cv_std': 0.0029, 'accuracy_test': 0.8689, 'precision_test': 0.7005, 'recall_test': 0.7868, 'f1_score_test': 0.7411}, 'XGBoost': {'accuracy_cv_mean': 0.9065, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9249, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8848, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.9044, 'f1_cv_std': 0.0019, 'accuracy_test': 0.9227, 'precision_test': 0.8049, 'recall_test': 0.8924, 'f1_score_test': 0.8464}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8739, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.901, 'f1_cv_std': 0.0033, 'accuracy_test': 0.9195, 'precision_test': 0.805, 'recall_test': 0.8746, 'f1_score_test': 0.8384}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8498, 'accuracy_cv_std': 0.006, 'precision_cv_mean': 0.8462, 'precision_cv_std': 0.0133, 'recall_cv_mean': 0.8556, 'recall_cv_std': 0.0105, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0049, 'params': 10401, 'accuracy_test': 0.8816, 'precision_test': 0.7408, 'recall_test': 0.7748, 'f1_score_test': 0.7574}, 'MLP_338433': {'accuracy_cv_mean': 0.8546, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.849, 'precision_cv_std': 0.0138, 'recall_cv_mean': 0.8633, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8559, 'f1_cv_std': 0.0046, 'params': 338433, 'accuracy_test': 0.8899, 'precision_test': 0.7703, 'recall_test': 0.7676, 'f1_score_test': 0.769}, 'MLP_1031169': {'accuracy_cv_mean': 0.8532, 'accuracy_cv_std': 0.0067, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0183, 'recall_cv_mean': 0.8688, 'recall_cv_std': 0.0166, 'f1_cv_mean': 0.8555, 'f1_cv_std': 0.0053, 'params': 1031169, 'accuracy_test': 0.8871, 'precision_test': 0.788, 'recall_test': 0.7205, 'f1_score_test': 0.7528}}}
Saved on: outputs_cv/3/lyrics_bert

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8493, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8605, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8339, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.847, 'f1_cv_std': 0.0035}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 45, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.87      0.90     16465
           1       0.66      0.83      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.85      0.82     21625
weighted avg       0.87      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/3/lyrics_bert/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/3/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.6611, 'accuracy_cv_std': 0.0215, 'precision_cv_mean': 0.6399, 'precision_cv_std': 0.0203, 'recall_cv_mean': 0.7394, 'recall_cv_std': 0.0406, 'f1_cv_mean': 0.6854, 'f1_cv_std': 0.0216}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 45, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.88      0.34      0.49     16465
           1       0.29      0.85      0.43      5160

    accuracy                           0.46     21625
   macro avg       0.58      0.60      0.46     21625
weighted avg       0.74      0.46      0.48     21625

Confusion matrix Test saved as: outputs_cv/3/lyrics_bert/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/3/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7958, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8131, 'precision_cv_std': 0.0115, 'recall_cv_mean': 0.7687, 'recall_cv_std': 0.0142, 'f1_cv_mean': 0.7901, 'f1_cv_std': 0.0053}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 45, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.82      0.87     16465
           1       0.58      0.78      0.67      5160

    accuracy                           0.81     21625
   macro avg       0.75      0.80      0.77     21625
weighted avg       0.84      0.81      0.82     21625

Confusion matrix Test saved as: outputs_cv/3/lyrics_bert/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/3/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8547, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.8486, 'f1_cv_std': 0.0047}
Epoch [20/30] Train Loss: 0.3367, Test Loss: 0.3615, F1: 0.7360, AUC: 0.9308
Mejores resultados en la época:  14
f1-score 0.7527839643652561
AUC según el mejor F1-score 0.9264923127988192
Confusion matrix Test saved: outputs_cv/3/lyrics_bert/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9347, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9329, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.937, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9349, 'f1_cv_std': 0.0021, 'params': 160801, 'accuracy_test': 0.9397, 'precision_test': 0.8398, 'recall_test': 0.9233, 'f1_score_test': 0.8795}, 'MLP_2744833': {'accuracy_cv_mean': 0.9453, 'accuracy_cv_std': 0.0008, 'precision_cv_mean': 0.9432, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9478, 'recall_cv_std': 0.0078, 'f1_cv_mean': 0.9454, 'f1_cv_std': 0.0005, 'params': 2744833, 'accuracy_test': 0.9565, 'precision_test': 0.9014, 'recall_test': 0.9182, 'f1_score_test': 0.9098}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9428, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.95, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.9463, 'f1_cv_std': 0.0021, 'params': 5843969, 'accuracy_test': 0.9577, 'precision_test': 0.8983, 'recall_test': 0.9277, 'f1_score_test': 0.9128}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.936, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0032, 'accuracy_test': 0.9338, 'precision_test': 0.8225, 'recall_test': 0.9213, 'f1_score_test': 0.8691}, 'SVM': {'accuracy_cv_mean': 0.8184, 'accuracy_cv_std': 0.027, 'precision_cv_mean': 0.7823, 'precision_cv_std': 0.0468, 'recall_cv_mean': 0.8925, 'recall_cv_std': 0.0401, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0154, 'accuracy_test': 0.7489, 'precision_test': 0.4854, 'recall_test': 0.8742, 'f1_score_test': 0.6242}, 'Decision Tree': {'accuracy_cv_mean': 0.8316, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8608, 'precision_cv_std': 0.0152, 'recall_cv_mean': 0.7917, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8245, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8486, 'precision_test': 0.6479, 'recall_test': 0.8012, 'f1_score_test': 0.7164}, 'Random Forest': {'accuracy_cv_mean': 0.8374, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.7903, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8294, 'f1_cv_std': 0.0029, 'accuracy_test': 0.8689, 'precision_test': 0.7005, 'recall_test': 0.7868, 'f1_score_test': 0.7411}, 'XGBoost': {'accuracy_cv_mean': 0.9065, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9249, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8848, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.9044, 'f1_cv_std': 0.0019, 'accuracy_test': 0.9227, 'precision_test': 0.8049, 'recall_test': 0.8924, 'f1_score_test': 0.8464}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8739, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.901, 'f1_cv_std': 0.0033, 'accuracy_test': 0.9195, 'precision_test': 0.805, 'recall_test': 0.8746, 'f1_score_test': 0.8384}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8498, 'accuracy_cv_std': 0.006, 'precision_cv_mean': 0.8462, 'precision_cv_std': 0.0133, 'recall_cv_mean': 0.8556, 'recall_cv_std': 0.0105, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0049, 'params': 10401, 'accuracy_test': 0.8816, 'precision_test': 0.7408, 'recall_test': 0.7748, 'f1_score_test': 0.7574}, 'MLP_338433': {'accuracy_cv_mean': 0.8546, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.849, 'precision_cv_std': 0.0138, 'recall_cv_mean': 0.8633, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8559, 'f1_cv_std': 0.0046, 'params': 338433, 'accuracy_test': 0.8899, 'precision_test': 0.7703, 'recall_test': 0.7676, 'f1_score_test': 0.769}, 'MLP_1031169': {'accuracy_cv_mean': 0.8532, 'accuracy_cv_std': 0.0067, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0183, 'recall_cv_mean': 0.8688, 'recall_cv_std': 0.0166, 'f1_cv_mean': 0.8555, 'f1_cv_std': 0.0053, 'params': 1031169, 'accuracy_test': 0.8871, 'precision_test': 0.788, 'recall_test': 0.7205, 'f1_score_test': 0.7528}}}
Saved on: outputs_cv/3/lyrics_bert

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8493, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8605, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8339, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.847, 'f1_cv_std': 0.0035}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 45, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.87      0.90     16465
           1       0.66      0.83      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.85      0.82     21625
weighted avg       0.87      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/3/lyrics_bert/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/3/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.6611, 'accuracy_cv_std': 0.0215, 'precision_cv_mean': 0.6399, 'precision_cv_std': 0.0203, 'recall_cv_mean': 0.7394, 'recall_cv_std': 0.0406, 'f1_cv_mean': 0.6854, 'f1_cv_std': 0.0216}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 45, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.88      0.34      0.49     16465
           1       0.29      0.85      0.43      5160

    accuracy                           0.46     21625
   macro avg       0.58      0.60      0.46     21625
weighted avg       0.74      0.46      0.48     21625

Confusion matrix Test saved as: outputs_cv/3/lyrics_bert/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/3/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7958, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8131, 'precision_cv_std': 0.0115, 'recall_cv_mean': 0.7687, 'recall_cv_std': 0.0142, 'f1_cv_mean': 0.7901, 'f1_cv_std': 0.0053}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 45, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.82      0.87     16465
           1       0.58      0.78      0.67      5160

    accuracy                           0.81     21625
   macro avg       0.75      0.80      0.77     21625
weighted avg       0.84      0.81      0.82     21625

Confusion matrix Test saved as: outputs_cv/3/lyrics_bert/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/3/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8547, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.8486, 'f1_cv_std': 0.0047}
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:26:00] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:26:11] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:26:49] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:27:01] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:27:37] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:27:49] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:28:27] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:28:38] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:29:16] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:29:26] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:30:05] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:30:15] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 45, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.90      0.92     16465
           1       0.72      0.81      0.76      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.85      0.84     21625
weighted avg       0.88      0.88      0.88     21625

Confusion matrix Test saved as: outputs_cv/3/lyrics_bert/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/3/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.878, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8925, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8596, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.8757, 'f1_cv_std': 0.0023}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 45, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.90      0.93     16465
           1       0.74      0.86      0.79      5160

    accuracy                           0.89     21625
   macro avg       0.85      0.88      0.86     21625
weighted avg       0.90      0.89      0.90     21625

Confusion matrix Test saved as: outputs_cv/3/lyrics_bert/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/3/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7801, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.7642, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.7866, 'f1_cv_std': 0.0033}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.75      0.83     16465
           1       0.50      0.81      0.62      5160

    accuracy                           0.76     21625
   macro avg       0.71      0.78      0.72     21625
weighted avg       0.82      0.76      0.78     21625

Confusion matrix Test saved as: outputs_cv/3/lyrics_bert/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/3/lyrics_bert/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.878, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8925, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8596, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.8757, 'f1_cv_std': 0.0023, 'accuracy_test': 0.8931, 'precision_test': 0.7375, 'recall_test': 0.8574, 'f1_score_test': 0.7929}
Random Forest: {'accuracy_cv_mean': 0.8547, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.8486, 'f1_cv_std': 0.0047, 'accuracy_test': 0.8775, 'precision_test': 0.7163, 'recall_test': 0.8062, 'f1_score_test': 0.7586}
Logistic Regression: {'accuracy_cv_mean': 0.8493, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8605, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8339, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.847, 'f1_cv_std': 0.0035, 'accuracy_test': 0.8575, 'precision_test': 0.6614, 'recall_test': 0.8256, 'f1_score_test': 0.7344}
Decision Tree: {'accuracy_cv_mean': 0.7958, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8131, 'precision_cv_std': 0.0115, 'recall_cv_mean': 0.7687, 'recall_cv_std': 0.0142, 'f1_cv_mean': 0.7901, 'f1_cv_std': 0.0053, 'accuracy_test': 0.813, 'precision_test': 0.58, 'recall_test': 0.7841, 'f1_score_test': 0.6668}
Naive Bayes: {'accuracy_cv_mean': 0.7801, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.7642, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.7866, 'f1_cv_std': 0.0033, 'accuracy_test': 0.7636, 'precision_test': 0.5029, 'recall_test': 0.8083, 'f1_score_test': 0.62}
SVM: {'accuracy_cv_mean': 0.6611, 'accuracy_cv_std': 0.0215, 'precision_cv_mean': 0.6399, 'precision_cv_std': 0.0203, 'recall_cv_mean': 0.7394, 'recall_cv_std': 0.0406, 'f1_cv_mean': 0.6854, 'f1_cv_std': 0.0216, 'accuracy_test': 0.4647, 'precision_test': 0.2887, 'recall_test': 0.8496, 'f1_score_test': 0.431}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9347, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9329, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.937, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9349, 'f1_cv_std': 0.0021, 'params': 160801, 'accuracy_test': 0.9397, 'precision_test': 0.8398, 'recall_test': 0.9233, 'f1_score_test': 0.8795}, 'MLP_2744833': {'accuracy_cv_mean': 0.9453, 'accuracy_cv_std': 0.0008, 'precision_cv_mean': 0.9432, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9478, 'recall_cv_std': 0.0078, 'f1_cv_mean': 0.9454, 'f1_cv_std': 0.0005, 'params': 2744833, 'accuracy_test': 0.9565, 'precision_test': 0.9014, 'recall_test': 0.9182, 'f1_score_test': 0.9098}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9428, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.95, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.9463, 'f1_cv_std': 0.0021, 'params': 5843969, 'accuracy_test': 0.9577, 'precision_test': 0.8983, 'recall_test': 0.9277, 'f1_score_test': 0.9128}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.936, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0032, 'accuracy_test': 0.9338, 'precision_test': 0.8225, 'recall_test': 0.9213, 'f1_score_test': 0.8691}, 'SVM': {'accuracy_cv_mean': 0.8184, 'accuracy_cv_std': 0.027, 'precision_cv_mean': 0.7823, 'precision_cv_std': 0.0468, 'recall_cv_mean': 0.8925, 'recall_cv_std': 0.0401, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0154, 'accuracy_test': 0.7489, 'precision_test': 0.4854, 'recall_test': 0.8742, 'f1_score_test': 0.6242}, 'Decision Tree': {'accuracy_cv_mean': 0.8316, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8608, 'precision_cv_std': 0.0152, 'recall_cv_mean': 0.7917, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8245, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8486, 'precision_test': 0.6479, 'recall_test': 0.8012, 'f1_score_test': 0.7164}, 'Random Forest': {'accuracy_cv_mean': 0.8374, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.7903, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8294, 'f1_cv_std': 0.0029, 'accuracy_test': 0.8689, 'precision_test': 0.7005, 'recall_test': 0.7868, 'f1_score_test': 0.7411}, 'XGBoost': {'accuracy_cv_mean': 0.9065, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9249, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8848, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.9044, 'f1_cv_std': 0.0019, 'accuracy_test': 0.9227, 'precision_test': 0.8049, 'recall_test': 0.8924, 'f1_score_test': 0.8464}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8739, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.901, 'f1_cv_std': 0.0033, 'accuracy_test': 0.9195, 'precision_test': 0.805, 'recall_test': 0.8746, 'f1_score_test': 0.8384}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8498, 'accuracy_cv_std': 0.006, 'precision_cv_mean': 0.8462, 'precision_cv_std': 0.0133, 'recall_cv_mean': 0.8556, 'recall_cv_std': 0.0105, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0049, 'params': 10401, 'accuracy_test': 0.8816, 'precision_test': 0.7408, 'recall_test': 0.7748, 'f1_score_test': 0.7574}, 'MLP_338433': {'accuracy_cv_mean': 0.8546, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.849, 'precision_cv_std': 0.0138, 'recall_cv_mean': 0.8633, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8559, 'f1_cv_std': 0.0046, 'params': 338433, 'accuracy_test': 0.8899, 'precision_test': 0.7703, 'recall_test': 0.7676, 'f1_score_test': 0.769}, 'MLP_1031169': {'accuracy_cv_mean': 0.8532, 'accuracy_cv_std': 0.0067, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0183, 'recall_cv_mean': 0.8688, 'recall_cv_std': 0.0166, 'f1_cv_mean': 0.8555, 'f1_cv_std': 0.0053, 'params': 1031169, 'accuracy_test': 0.8871, 'precision_test': 0.788, 'recall_test': 0.7205, 'f1_score_test': 0.7528}, 'Logistic Regression': {'accuracy_cv_mean': 0.8493, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8605, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8339, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.847, 'f1_cv_std': 0.0035, 'accuracy_test': 0.8575, 'precision_test': 0.6614, 'recall_test': 0.8256, 'f1_score_test': 0.7344}, 'SVM': {'accuracy_cv_mean': 0.6611, 'accuracy_cv_std': 0.0215, 'precision_cv_mean': 0.6399, 'precision_cv_std': 0.0203, 'recall_cv_mean': 0.7394, 'recall_cv_std': 0.0406, 'f1_cv_mean': 0.6854, 'f1_cv_std': 0.0216, 'accuracy_test': 0.4647, 'precision_test': 0.2887, 'recall_test': 0.8496, 'f1_score_test': 0.431}, 'Decision Tree': {'accuracy_cv_mean': 0.7958, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8131, 'precision_cv_std': 0.0115, 'recall_cv_mean': 0.7687, 'recall_cv_std': 0.0142, 'f1_cv_mean': 0.7901, 'f1_cv_std': 0.0053, 'accuracy_test': 0.813, 'precision_test': 0.58, 'recall_test': 0.7841, 'f1_score_test': 0.6668}, 'Random Forest': {'accuracy_cv_mean': 0.8547, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0077, 'recall_cv_mean': /home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_2.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 45, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.90      0.92     16465
           1       0.72      0.81      0.76      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.85      0.84     21625
weighted avg       0.88      0.88      0.88     21625

Confusion matrix Test saved as: outputs_cv/3/lyrics_bert/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/3/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.878, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8925, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8596, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.8757, 'f1_cv_std': 0.0023}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 45, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.90      0.93     16465
           1       0.74      0.86      0.79      5160

    accuracy                           0.89     21625
   macro avg       0.85      0.88      0.86     21625
weighted avg       0.90      0.89      0.90     21625

Confusion matrix Test saved as: outputs_cv/3/lyrics_bert/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/3/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7801, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.7642, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.7866, 'f1_cv_std': 0.0033}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.75      0.83     16465
           1       0.50      0.81      0.62      5160

    accuracy                           0.76     21625
   macro avg       0.71      0.78      0.72     21625
weighted avg       0.82      0.76      0.78     21625

Confusion matrix Test saved as: outputs_cv/3/lyrics_bert/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/3/lyrics_bert/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.878, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8925, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8596, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.8757, 'f1_cv_std': 0.0023, 'accuracy_test': 0.8931, 'precision_test': 0.7375, 'recall_test': 0.8574, 'f1_score_test': 0.7929}
Random Forest: {'accuracy_cv_mean': 0.8547, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.8486, 'f1_cv_std': 0.0047, 'accuracy_test': 0.8775, 'precision_test': 0.7163, 'recall_test': 0.8062, 'f1_score_test': 0.7586}
Logistic Regression: {'accuracy_cv_mean': 0.8493, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8605, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8339, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.847, 'f1_cv_std': 0.0035, 'accuracy_test': 0.8575, 'precision_test': 0.6614, 'recall_test': 0.8256, 'f1_score_test': 0.7344}
Decision Tree: {'accuracy_cv_mean': 0.7958, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8131, 'precision_cv_std': 0.0115, 'recall_cv_mean': 0.7687, 'recall_cv_std': 0.0142, 'f1_cv_mean': 0.7901, 'f1_cv_std': 0.0053, 'accuracy_test': 0.813, 'precision_test': 0.58, 'recall_test': 0.7841, 'f1_score_test': 0.6668}
Naive Bayes: {'accuracy_cv_mean': 0.7801, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.7642, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.7866, 'f1_cv_std': 0.0033, 'accuracy_test': 0.7636, 'precision_test': 0.5029, 'recall_test': 0.8083, 'f1_score_test': 0.62}
SVM: {'accuracy_cv_mean': 0.6611, 'accuracy_cv_std': 0.0215, 'precision_cv_mean': 0.6399, 'precision_cv_std': 0.0203, 'recall_cv_mean': 0.7394, 'recall_cv_std': 0.0406, 'f1_cv_mean': 0.6854, 'f1_cv_std': 0.0216, 'accuracy_test': 0.4647, 'precision_test': 0.2887, 'recall_test': 0.8496, 'f1_score_test': 0.431}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9347, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9329, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.937, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9349, 'f1_cv_std': 0.0021, 'params': 160801, 'accuracy_test': 0.9397, 'precision_test': 0.8398, 'recall_test': 0.9233, 'f1_score_test': 0.8795}, 'MLP_2744833': {'accuracy_cv_mean': 0.9453, 'accuracy_cv_std': 0.0008, 'precision_cv_mean': 0.9432, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9478, 'recall_cv_std': 0.0078, 'f1_cv_mean': 0.9454, 'f1_cv_std': 0.0005, 'params': 2744833, 'accuracy_test': 0.9565, 'precision_test': 0.9014, 'recall_test': 0.9182, 'f1_score_test': 0.9098}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9428, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.95, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.9463, 'f1_cv_std': 0.0021, 'params': 5843969, 'accuracy_test': 0.9577, 'precision_test': 0.8983, 'recall_test': 0.9277, 'f1_score_test': 0.9128}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.936, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0032, 'accuracy_test': 0.9338, 'precision_test': 0.8225, 'recall_test': 0.9213, 'f1_score_test': 0.8691}, 'SVM': {'accuracy_cv_mean': 0.8184, 'accuracy_cv_std': 0.027, 'precision_cv_mean': 0.7823, 'precision_cv_std': 0.0468, 'recall_cv_mean': 0.8925, 'recall_cv_std': 0.0401, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0154, 'accuracy_test': 0.7489, 'precision_test': 0.4854, 'recall_test': 0.8742, 'f1_score_test': 0.6242}, 'Decision Tree': {'accuracy_cv_mean': 0.8316, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8608, 'precision_cv_std': 0.0152, 'recall_cv_mean': 0.7917, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8245, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8486, 'precision_test': 0.6479, 'recall_test': 0.8012, 'f1_score_test': 0.7164}, 'Random Forest': {'accuracy_cv_mean': 0.8374, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.7903, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8294, 'f1_cv_std': 0.0029, 'accuracy_test': 0.8689, 'precision_test': 0.7005, 'recall_test': 0.7868, 'f1_score_test': 0.7411}, 'XGBoost': {'accuracy_cv_mean': 0.9065, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9249, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8848, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.9044, 'f1_cv_std': 0.0019, 'accuracy_test': 0.9227, 'precision_test': 0.8049, 'recall_test': 0.8924, 'f1_score_test': 0.8464}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8739, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.901, 'f1_cv_std': 0.0033, 'accuracy_test': 0.9195, 'precision_test': 0.805, 'recall_test': 0.8746, 'f1_score_test': 0.8384}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8498, 'accuracy_cv_std': 0.006, 'precision_cv_mean': 0.8462, 'precision_cv_std': 0.0133, 'recall_cv_mean': 0.8556, 'recall_cv_std': 0.0105, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0049, 'params': 10401, 'accuracy_test': 0.8816, 'precision_test': 0.7408, 'recall_test': 0.7748, 'f1_score_test': 0.7574}, 'MLP_338433': {'accuracy_cv_mean': 0.8546, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.849, 'precision_cv_std': 0.0138, 'recall_cv_mean': 0.8633, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8559, 'f1_cv_std': 0.0046, 'params': 338433, 'accuracy_test': 0.8899, 'precision_test': 0.7703, 'recall_test': 0.7676, 'f1_score_test': 0.769}, 'MLP_1031169': {'accuracy_cv_mean': 0.8532, 'accuracy_cv_std': 0.0067, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0183, 'recall_cv_mean': 0.8688, 'recall_cv_std': 0.0166, 'f1_cv_mean': 0.8555, 'f1_cv_std': 0.0053, 'params': 1031169, 'accuracy_test': 0.8871, 'precision_test': 0.788, 'recall_test': 0.7205, 'f1_score_test': 0.7528}, 'Logistic Regression': {'accuracy_cv_mean': 0.8493, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8605, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8339, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.847, 'f1_cv_std': 0.0035, 'accuracy_test': 0.8575, 'precision_test': 0.6614, 'recall_test': 0.8256, 'f1_score_test': 0.7344}, 'SVM': {'accuracy_cv_mean': 0.6611, 'accuracy_cv_std': 0.0215, 'precision_cv_mean': 0.6399, 'precision_cv_std': 0.0203, 'recall_cv_mean': 0.7394, 'recall_cv_std': 0.0406, 'f1_cv_mean': 0.6854, 'f1_cv_std': 0.0216, 'accuracy_test': 0.4647, 'precision_test': 0.2887, 'recall_test': 0.8496, 'f1_score_test': 0.431}, 'Decision Tree': {'accuracy_cv_mean': 0.7958, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8131, 'precision_cv_std': 0.0115, 'recall_cv_mean': 0.7687, 'recall_cv_std': 0.0142, 'f1_cv_mean': 0.7901, 'f1_cv_std': 0.0053, 'accuracy_test': 0.813, 'precision_test': 0.58, 'recall_test': 0.7841, 'f1_score_test': 0.6668}, 'Random Forest': {'accuracy_cv_mean': 0.8547, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0077, 'recall_cv_mean': /home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_2.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
0.8143, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.8486, 'f1_cv_std': 0.0047, 'accuracy_test': 0.8775, 'precision_test': 0.7163, 'recall_test': 0.8062, 'f1_score_test': 0.7586}, 'XGBoost': {'accuracy_cv_mean': 0.878, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8925, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8596, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.8757, 'f1_cv_std': 0.0023, 'accuracy_test': 0.8931, 'precision_test': 0.7375, 'recall_test': 0.8574, 'f1_score_test': 0.7929}, 'Naive Bayes': {'accuracy_cv_mean': 0.7801, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.7642, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.7866, 'f1_cv_std': 0.0033, 'accuracy_test': 0.7636, 'precision_test': 0.5029, 'recall_test': 0.8083, 'f1_score_test': 0.62}}}
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 1559)
Shape of X_test after concatenation:  (21625, 1559)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1559)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1559)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [1559, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4315, Test Loss: 0.3351, F1: 0.8507, AUC: 0.9305
Epoch [10/30] Train Loss: 0.2589, Test Loss: 0.2899, F1: 0.8703, AUC: 0.9557
Epoch [20/30] Train Loss: 0.2449, Test Loss: 0.2628, F1: 0.8864, AUC: 0.9585
Mejores resultados en la época:  24
f1-score 0.8932584269662921
AUC según el mejor F1-score 0.9591216958491076

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4094, Test Loss: 0.3312, F1: 0.8455, AUC: 0.9448
Epoch [10/30] Train Loss: 0.2625, Test Loss: 0.2604, F1: 0.8879, AUC: 0.9631
Epoch [20/30] Train Loss: 0.2516, Test Loss: 0.2456, F1: 0.8973, AUC: 0.9652
Mejores resultados en la época:  27
f1-score 0.9047503045066991
AUC según el mejor F1-score 0.9658197550008263

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3904, Test Loss: 0.3055, F1: 0.8685, AUC: 0.9426
Epoch [10/30] Train Loss: 0.2622, Test Loss: 0.2525, F1: 0.8906, AUC: 0.9612
Epoch [20/30] Train Loss: 0.2446, Test Loss: 0.2450, F1: 0.8937, AUC: 0.9634
Mejores resultados en la época:  28
f1-score 0.8972520908004779
AUC según el mejor F1-score 0.9644775610690465

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3956, Test Loss: 0.3102, F1: 0.8626, AUC: 0.9420
Epoch [10/30] Train Loss: 0.2603, Test Loss: 0.2666, F1: 0.8914, AUC: 0.9605
Epoch [20/30] Train Loss: 0.2461, Test Loss: 0.2627, F1: 0.8866, AUC: 0.9625
Mejores resultados en la época:  23
f1-score 0.8997197514317047
AUC según el mejor F1-score 0.9628389007537805

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4348, Test Loss: 0.3647, F1: 0.8210, AUC: 0.9297
Epoch [10/30] Train Loss: 0.2589, Test Loss: 0.2668, F1: 0.8861, AUC: 0.9567
Epoch [20/30] Train Loss: 0.2440, Test Loss: 0.2582, F1: 0.8850, AUC: 0.9596
Mejores resultados en la época:  28
f1-score 0.8933160438524107
AUC según el mejor F1-score 0.9605469065503596
Epoch [0/30] Train Loss: 0.3932, Test Loss: 0.2894, F1: 0.7779, AUC: 0.9402
Epoch [10/30] Train Loss: 0.2565, Test Loss: 0.2792, F1: 0.7922, AUC: 0.9597
Epoch [20/30] Train Loss: 0.2437, Test Loss: 0.3232, F1: 0.7663, AUC: 0.9620
Mejores resultados en la época:  29
f1-score 0.828574291153191
AUC según el mejor F1-score 0.9628827063279637
Confusion matrix Test saved: outputs_cv/3/gpt/cm_mlp_1.png

========================================
Entrenando red 5 con capas [1559, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3775, Test Loss: 0.3414, F1: 0.8551, AUC: 0.9429
Epoch [10/30] Train Loss: 0.2547, Test Loss: 0.2638, F1: 0.8896, AUC: 0.9575
Epoch [20/30] Train Loss: 0.2410, Test Loss: 0.2542, F1: 0.8895, AUC: 0.9606
Mejores resultados en la época:  29
f1-score 0.8921568627450981
AUC según el mejor F1-score 0.9625090021445526

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3985, Test Loss: 0.2909, F1: 0.8739, AUC: 0.9492
Epoch [10/30] Train Loss: 0.2600, Test Loss: 0.2715, F1: 0.8825, AUC: 0.9641
Epoch [20/30] Train Loss: 0.2440, Test Loss: 0.2397, F1: 0.9026, AUC: 0.9669
Mejores resultados en la época:  22
f1-score 0.905784720527215
AUC según el mejor F1-score 0.9671468084287302

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3976, Test Loss: 0.3116, F1: 0.8695, AUC: 0.9466
Epoch [10/30] Train Loss: 0.2593, Test Loss: 0.3000, F1: 0.8761, AUC: 0.9619
Epoch [20/30] Train Loss: 0.2440, Test Loss: 0.2478, F1: 0.8932, AUC: 0.9655
Mejores resultados en la época:  29
f1-score 0.8972868217054264
AUC según el mejor F1-score 0.9669341958491077

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3916, Test Loss: 0.2910, F1: 0.8737, AUC: 0.9473
Epoch [10/30] Train Loss: 0.2605, Test Loss: 0.2710, F1: 0.8813, AUC: 0.9619
Epoch [20/30] Train Loss: 0.2410, Test Loss: 0.2553, F1: 0.8847, AUC: 0.9648
Mejores resultados en la época:  23
f1-score 0.8985542759127665
AUC según el mejor F1-score 0.9652703035220885

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3817, Test Loss: 0.3852, F1: 0.8587, AUC: 0.9428
Epoch [10/30] Train Loss: 0.2646, Test Loss: 0.2725, F1: 0.8897, AUC: 0.9587
Epoch [20/30] Train Loss: 0.2397, Test Loss: 0.2565, F1: 0.8914, AUC: 0.9618
Mejores resultados en la época:  29
f1-score 0.8943302032568644
AUC según el mejor F1-score 0.963252606676021
Epoch [0/30] Train Loss: 0.3765, Test Loss: 0.2420, F1: 0.8029, AUC: 0.9475
Epoch [10/30] Train Loss: 0.2595, Test Loss: 0.3489, F1: 0.7461, AUC: 0.9613
Epoch [20/30] Train Loss: 0.2423, Test Loss: 0.1991, F1: 0.8278, AUC: 0.9638
Mejores resultados en la época:  23
f1-score 0.8287948742108735
AUC según el mejor F1-score 0.9648589855860563
Confusion matrix Test saved: outputs_cv/3/gpt/cm_mlp_5.png

========================================
Entrenando red 6 con capas [1559, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4186, Test Loss: 0.3287, F1: 0.8648, AUC: 0.9388
Epoch [10/30] Train Loss: 0.2598, Test Loss: 0.2666, F1: 0.8844, AUC: 0.9580
Epoch [20/30] Train Loss: 0.2407, Test Loss: 0.2581, F1: 0.8861, AUC: 0.9610
Mejores resultados en la época:  25
f1-score 0.8939821344616832
AUC según el mejor F1-score 0.9622825987958957

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4125, Test Loss: 0.2919, F1: 0.8798, AUC: 0.9507
Epoch [10/30] Train Loss: 0.2618, Test Loss: 0.2447, F1: 0.8991, AUC: 0.9645
Epoch [20/30] Train Loss: 0.2466, Test Loss: 0.2401, F1: 0.8980, AUC: 0.9665
Mejores resultados en la época:  27
f1-score 0.9058077110785749
AUC según el mejor F1-score 0.967569246092107

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4021, Test Loss: 0.2931, F1: 0.8754, AUC: 0.9464
Epoch [10/30] Train Loss: 0.2714, Test Loss: 0.2542, F1: 0.8879, AUC: 0.9620
Epoch [20/30] Train Loss: 0.2496, Test Loss: 0.2518, F1: 0.8911, AUC: 0.9651
Mejores resultados en la época:  29
f1-score 0.8959487747814309
AUC según el mejor F1-score 0.9669496297736013
0.8143, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.8486, 'f1_cv_std': 0.0047, 'accuracy_test': 0.8775, 'precision_test': 0.7163, 'recall_test': 0.8062, 'f1_score_test': 0.7586}, 'XGBoost': {'accuracy_cv_mean': 0.878, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8925, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8596, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.8757, 'f1_cv_std': 0.0023, 'accuracy_test': 0.8931, 'precision_test': 0.7375, 'recall_test': 0.8574, 'f1_score_test': 0.7929}, 'Naive Bayes': {'accuracy_cv_mean': 0.7801, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.7642, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.7866, 'f1_cv_std': 0.0033, 'accuracy_test': 0.7636, 'precision_test': 0.5029, 'recall_test': 0.8083, 'f1_score_test': 0.62}}}
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 1559)
Shape of X_test after concatenation:  (21625, 1559)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1559)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1559)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [1559, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4315, Test Loss: 0.3351, F1: 0.8507, AUC: 0.9305
Epoch [10/30] Train Loss: 0.2589, Test Loss: 0.2899, F1: 0.8703, AUC: 0.9557
Epoch [20/30] Train Loss: 0.2449, Test Loss: 0.2628, F1: 0.8864, AUC: 0.9585
Mejores resultados en la época:  24
f1-score 0.8932584269662921
AUC según el mejor F1-score 0.9591216958491076

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4094, Test Loss: 0.3312, F1: 0.8455, AUC: 0.9448
Epoch [10/30] Train Loss: 0.2625, Test Loss: 0.2604, F1: 0.8879, AUC: 0.9631
Epoch [20/30] Train Loss: 0.2516, Test Loss: 0.2456, F1: 0.8973, AUC: 0.9652
Mejores resultados en la época:  27
f1-score 0.9047503045066991
AUC según el mejor F1-score 0.9658197550008263

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3904, Test Loss: 0.3055, F1: 0.8685, AUC: 0.9426
Epoch [10/30] Train Loss: 0.2622, Test Loss: 0.2525, F1: 0.8906, AUC: 0.9612
Epoch [20/30] Train Loss: 0.2446, Test Loss: 0.2450, F1: 0.8937, AUC: 0.9634
Mejores resultados en la época:  28
f1-score 0.8972520908004779
AUC según el mejor F1-score 0.9644775610690465

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3956, Test Loss: 0.3102, F1: 0.8626, AUC: 0.9420
Epoch [10/30] Train Loss: 0.2603, Test Loss: 0.2666, F1: 0.8914, AUC: 0.9605
Epoch [20/30] Train Loss: 0.2461, Test Loss: 0.2627, F1: 0.8866, AUC: 0.9625
Mejores resultados en la época:  23
f1-score 0.8997197514317047
AUC según el mejor F1-score 0.9628389007537805

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4348, Test Loss: 0.3647, F1: 0.8210, AUC: 0.9297
Epoch [10/30] Train Loss: 0.2589, Test Loss: 0.2668, F1: 0.8861, AUC: 0.9567
Epoch [20/30] Train Loss: 0.2440, Test Loss: 0.2582, F1: 0.8850, AUC: 0.9596
Mejores resultados en la época:  28
f1-score 0.8933160438524107
AUC según el mejor F1-score 0.9605469065503596
Epoch [0/30] Train Loss: 0.3932, Test Loss: 0.2894, F1: 0.7779, AUC: 0.9402
Epoch [10/30] Train Loss: 0.2565, Test Loss: 0.2792, F1: 0.7922, AUC: 0.9597
Epoch [20/30] Train Loss: 0.2437, Test Loss: 0.3232, F1: 0.7663, AUC: 0.9620
Mejores resultados en la época:  29
f1-score 0.828574291153191
AUC según el mejor F1-score 0.9628827063279637
Confusion matrix Test saved: outputs_cv/3/gpt/cm_mlp_1.png

========================================
Entrenando red 5 con capas [1559, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3775, Test Loss: 0.3414, F1: 0.8551, AUC: 0.9429
Epoch [10/30] Train Loss: 0.2547, Test Loss: 0.2638, F1: 0.8896, AUC: 0.9575
Epoch [20/30] Train Loss: 0.2410, Test Loss: 0.2542, F1: 0.8895, AUC: 0.9606
Mejores resultados en la época:  29
f1-score 0.8921568627450981
AUC según el mejor F1-score 0.9625090021445526

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3985, Test Loss: 0.2909, F1: 0.8739, AUC: 0.9492
Epoch [10/30] Train Loss: 0.2600, Test Loss: 0.2715, F1: 0.8825, AUC: 0.9641
Epoch [20/30] Train Loss: 0.2440, Test Loss: 0.2397, F1: 0.9026, AUC: 0.9669
Mejores resultados en la época:  22
f1-score 0.905784720527215
AUC según el mejor F1-score 0.9671468084287302

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3976, Test Loss: 0.3116, F1: 0.8695, AUC: 0.9466
Epoch [10/30] Train Loss: 0.2593, Test Loss: 0.3000, F1: 0.8761, AUC: 0.9619
Epoch [20/30] Train Loss: 0.2440, Test Loss: 0.2478, F1: 0.8932, AUC: 0.9655
Mejores resultados en la época:  29
f1-score 0.8972868217054264
AUC según el mejor F1-score 0.9669341958491077

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3916, Test Loss: 0.2910, F1: 0.8737, AUC: 0.9473
Epoch [10/30] Train Loss: 0.2605, Test Loss: 0.2710, F1: 0.8813, AUC: 0.9619
Epoch [20/30] Train Loss: 0.2410, Test Loss: 0.2553, F1: 0.8847, AUC: 0.9648
Mejores resultados en la época:  23
f1-score 0.8985542759127665
AUC según el mejor F1-score 0.9652703035220885

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3817, Test Loss: 0.3852, F1: 0.8587, AUC: 0.9428
Epoch [10/30] Train Loss: 0.2646, Test Loss: 0.2725, F1: 0.8897, AUC: 0.9587
Epoch [20/30] Train Loss: 0.2397, Test Loss: 0.2565, F1: 0.8914, AUC: 0.9618
Mejores resultados en la época:  29
f1-score 0.8943302032568644
AUC según el mejor F1-score 0.963252606676021
Epoch [0/30] Train Loss: 0.3765, Test Loss: 0.2420, F1: 0.8029, AUC: 0.9475
Epoch [10/30] Train Loss: 0.2595, Test Loss: 0.3489, F1: 0.7461, AUC: 0.9613
Epoch [20/30] Train Loss: 0.2423, Test Loss: 0.1991, F1: 0.8278, AUC: 0.9638
Mejores resultados en la época:  23
f1-score 0.8287948742108735
AUC según el mejor F1-score 0.9648589855860563
Confusion matrix Test saved: outputs_cv/3/gpt/cm_mlp_5.png

========================================
Entrenando red 6 con capas [1559, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4186, Test Loss: 0.3287, F1: 0.8648, AUC: 0.9388
Epoch [10/30] Train Loss: 0.2598, Test Loss: 0.2666, F1: 0.8844, AUC: 0.9580
Epoch [20/30] Train Loss: 0.2407, Test Loss: 0.2581, F1: 0.8861, AUC: 0.9610
Mejores resultados en la época:  25
f1-score 0.8939821344616832
AUC según el mejor F1-score 0.9622825987958957

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4125, Test Loss: 0.2919, F1: 0.8798, AUC: 0.9507
Epoch [10/30] Train Loss: 0.2618, Test Loss: 0.2447, F1: 0.8991, AUC: 0.9645
Epoch [20/30] Train Loss: 0.2466, Test Loss: 0.2401, F1: 0.8980, AUC: 0.9665
Mejores resultados en la época:  27
f1-score 0.9058077110785749
AUC según el mejor F1-score 0.967569246092107

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4021, Test Loss: 0.2931, F1: 0.8754, AUC: 0.9464
Epoch [10/30] Train Loss: 0.2714, Test Loss: 0.2542, F1: 0.8879, AUC: 0.9620
Epoch [20/30] Train Loss: 0.2496, Test Loss: 0.2518, F1: 0.8911, AUC: 0.9651
Mejores resultados en la época:  29
f1-score 0.8959487747814309
AUC según el mejor F1-score 0.9669496297736013

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3948, Test Loss: 0.3404, F1: 0.8679, AUC: 0.9476
Epoch [10/30] Train Loss: 0.2606, Test Loss: 0.2546, F1: 0.8912, AUC: 0.9618
Epoch [20/30] Train Loss: 0.2462, Test Loss: 0.2564, F1: 0.8925, AUC: 0.9641
Mejores resultados en la época:  25
f1-score 0.9026208223130561
AUC según el mejor F1-score 0.9655757110012905

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4042, Test Loss: 0.4264, F1: 0.7587, AUC: 0.9404
Epoch [10/30] Train Loss: 0.2575, Test Loss: 0.2611, F1: 0.8854, AUC: 0.9592
Epoch [20/30] Train Loss: 0.2422, Test Loss: 0.2675, F1: 0.8890, AUC: 0.9615
Mejores resultados en la época:  28
f1-score 0.8945686900958466
AUC según el mejor F1-score 0.9628831886536572
Epoch [0/30] Train Loss: 0.3949, Test Loss: 0.4115, F1: 0.7132, AUC: 0.9465
Epoch [10/30] Train Loss: 0.2638, Test Loss: 0.3305, F1: 0.7536, AUC: 0.9604
Epoch [20/30] Train Loss: 0.2377, Test Loss: 0.3093, F1: 0.7436, AUC: 0.9640
Mejores resultados en la época:  22
f1-score 0.8294072948328267
AUC según el mejor F1-score 0.9642189740040537
Confusion matrix Test saved: outputs_cv/3/gpt/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9347, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9329, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.937, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9349, 'f1_cv_std': 0.0021, 'params': 160801, 'accuracy_test': 0.9397, 'precision_test': 0.8398, 'recall_test': 0.9233, 'f1_score_test': 0.8795}, 'MLP_2744833': {'accuracy_cv_mean': 0.9453, 'accuracy_cv_std': 0.0008, 'precision_cv_mean': 0.9432, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9478, 'recall_cv_std': 0.0078, 'f1_cv_mean': 0.9454, 'f1_cv_std': 0.0005, 'params': 2744833, 'accuracy_test': 0.9565, 'precision_test': 0.9014, 'recall_test': 0.9182, 'f1_score_test': 0.9098}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9428, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.95, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.9463, 'f1_cv_std': 0.0021, 'params': 5843969, 'accuracy_test': 0.9577, 'precision_test': 0.8983, 'recall_test': 0.9277, 'f1_score_test': 0.9128}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.936, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0032, 'accuracy_test': 0.9338, 'precision_test': 0.8225, 'recall_test': 0.9213, 'f1_score_test': 0.8691}, 'SVM': {'accuracy_cv_mean': 0.8184, 'accuracy_cv_std': 0.027, 'precision_cv_mean': 0.7823, 'precision_cv_std': 0.0468, 'recall_cv_mean': 0.8925, 'recall_cv_std': 0.0401, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0154, 'accuracy_test': 0.7489, 'precision_test': 0.4854, 'recall_test': 0.8742, 'f1_score_test': 0.6242}, 'Decision Tree': {'accuracy_cv_mean': 0.8316, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8608, 'precision_cv_std': 0.0152, 'recall_cv_mean': 0.7917, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8245, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8486, 'precision_test': 0.6479, 'recall_test': 0.8012, 'f1_score_test': 0.7164}, 'Random Forest': {'accuracy_cv_mean': 0.8374, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.7903, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8294, 'f1_cv_std': 0.0029, 'accuracy_test': 0.8689, 'precision_test': 0.7005, 'recall_test': 0.7868, 'f1_score_test': 0.7411}, 'XGBoost': {'accuracy_cv_mean': 0.9065, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9249, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8848, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.9044, 'f1_cv_std': 0.0019, 'accuracy_test': 0.9227, 'precision_test': 0.8049, 'recall_test': 0.8924, 'f1_score_test': 0.8464}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8739, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.901, 'f1_cv_std': 0.0033, 'accuracy_test': 0.9195, 'precision_test': 0.805, 'recall_test': 0.8746, 'f1_score_test': 0.8384}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8498, 'accuracy_cv_std': 0.006, 'precision_cv_mean': 0.8462, 'precision_cv_std': 0.0133, 'recall_cv_mean': 0.8556, 'recall_cv_std': 0.0105, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0049, 'params': 10401, 'accuracy_test': 0.8816, 'precision_test': 0.7408, 'recall_test': 0.7748, 'f1_score_test': 0.7574}, 'MLP_338433': {'accuracy_cv_mean': 0.8546, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.849, 'precision_cv_std': 0.0138, 'recall_cv_mean': 0.8633, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8559, 'f1_cv_std': 0.0046, 'params': 338433, 'accuracy_test': 0.8899, 'precision_test': 0.7703, 'recall_test': 0.7676, 'f1_score_test': 0.769}, 'MLP_1031169': {'accuracy_cv_mean': 0.8532, 'accuracy_cv_std': 0.0067, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0183, 'recall_cv_mean': 0.8688, 'recall_cv_std': 0.0166, 'f1_cv_mean': 0.8555, 'f1_cv_std': 0.0053, 'params': 1031169, 'accuracy_test': 0.8871, 'precision_test': 0.788, 'recall_test': 0.7205, 'f1_score_test': 0.7528}, 'Logistic Regression': {'accuracy_cv_mean': 0.8493, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8605, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8339, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.847, 'f1_cv_std': 0.0035, 'accuracy_test': 0.8575, 'precision_test': 0.6614, 'recall_test': 0.8256, 'f1_score_test': 0.7344}, 'SVM': {'accuracy_cv_mean': 0.6611, 'accuracy_cv_std': 0.0215, 'precision_cv_mean': 0.6399, 'precision_cv_std': 0.0203, 'recall_cv_mean': 0.7394, 'recall_cv_std': 0.0406, 'f1_cv_mean': 0.6854, 'f1_cv_std': 0.0216, 'accuracy_test': 0.4647, 'precision_test': 0.2887, 'recall_test': 0.8496, 'f1_score_test': 0.431}, 'Decision Tree': {'accuracy_cv_mean': 0.7958, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8131, 'precision_cv_std': 0.0115, 'recall_cv_mean': 0.7687, 'recall_cv_std': 0.0142, 'f1_cv_mean': 0.7901, 'f1_cv_std': 0.0053, 'accuracy_test': 0.813, 'precision_test': 0.58, 'recall_test': 0.7841, 'f1_score_test': 0.6668}, 'Random Forest': {'accuracy_cv_mean': 0.8547, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.8486, 'f1_cv_std': 0.0047, 'accuracy_test': 0.8775, 'precision_test': 0.7163, 'recall_test': 0.8062, 'f1_score_test': 0.7586}, 'XGBoost': {'accuracy_cv_mean': 0.878, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8925, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8596, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.8757, 'f1_cv_std': 0.0023, 'accuracy_test': 0.8931, 'precision_test': 0.7375, 'recall_test': 0.8574, 'f1_score_test': 0.7929}, 'Naive Bayes': {'accuracy_cv_mean': 0.7801, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.7642, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.7866, 'f1_cv_std': 0.0033, 'accuracy_test': 0.7636, 'precision_test': 0.5029, 'recall_test': 0.8083, 'f1_score_test': 0.62}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8972, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8941, 'precision_cv_std': 0.0146, 'recall_cv_mean': 0.9015, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8977, 'f1_cv_std': 0.0043, 'params': 49953, 'accuracy_test': 0.9209, 'precision_test': 0.8577, 'recall_test': 0.8014, 'f1_score_test': 0.8286}, 'MLP_971265': {'accuracy_cv_mean': 0.8968, 'accuracy_cv_std': 0.0063, 'precision_cv_mean': 0.8915, 'precision_cv_std': 0.0195, 'recall_cv_mean': 0.9044, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8976, 'f1_cv_std': 0.0047, 'params': 971265, 'accuracy_test': 0.916, 'precision_test': 0.8065, 'recall_test': 0.8523, 'f1_score_test': 0.8288}, 'MLP_2296833': {'accuracy_cv_mean': 0.8978, 'accuracy_cv_std': 0.0059, 'precision_cv_mean': 0.8924, 'precision_cv_std': 0.0183, 'recall_cv_mean': 0.9054, 'recall_cv_std': 0.0141, 'f1_cv_mean': 0.8986, 'f1_cv_std': 0.0047, 'params': 2296833, 'accuracy_test': 0.9169, 'precision_test': 0.8133, 'recall_test': 0.8461, 'f1_score_test': 0.8294}}}
Saved on: outputs_cv/3/gpt

==============================
Model: Logistic Regression


--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3948, Test Loss: 0.3404, F1: 0.8679, AUC: 0.9476
Epoch [10/30] Train Loss: 0.2606, Test Loss: 0.2546, F1: 0.8912, AUC: 0.9618
Epoch [20/30] Train Loss: 0.2462, Test Loss: 0.2564, F1: 0.8925, AUC: 0.9641
Mejores resultados en la época:  25
f1-score 0.9026208223130561
AUC según el mejor F1-score 0.9655757110012905

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4042, Test Loss: 0.4264, F1: 0.7587, AUC: 0.9404
Epoch [10/30] Train Loss: 0.2575, Test Loss: 0.2611, F1: 0.8854, AUC: 0.9592
Epoch [20/30] Train Loss: 0.2422, Test Loss: 0.2675, F1: 0.8890, AUC: 0.9615
Mejores resultados en la época:  28
f1-score 0.8945686900958466
AUC según el mejor F1-score 0.9628831886536572
Epoch [0/30] Train Loss: 0.3949, Test Loss: 0.4115, F1: 0.7132, AUC: 0.9465
Epoch [10/30] Train Loss: 0.2638, Test Loss: 0.3305, F1: 0.7536, AUC: 0.9604
Epoch [20/30] Train Loss: 0.2377, Test Loss: 0.3093, F1: 0.7436, AUC: 0.9640
Mejores resultados en la época:  22
f1-score 0.8294072948328267
AUC según el mejor F1-score 0.9642189740040537
Confusion matrix Test saved: outputs_cv/3/gpt/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9347, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9329, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.937, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9349, 'f1_cv_std': 0.0021, 'params': 160801, 'accuracy_test': 0.9397, 'precision_test': 0.8398, 'recall_test': 0.9233, 'f1_score_test': 0.8795}, 'MLP_2744833': {'accuracy_cv_mean': 0.9453, 'accuracy_cv_std': 0.0008, 'precision_cv_mean': 0.9432, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9478, 'recall_cv_std': 0.0078, 'f1_cv_mean': 0.9454, 'f1_cv_std': 0.0005, 'params': 2744833, 'accuracy_test': 0.9565, 'precision_test': 0.9014, 'recall_test': 0.9182, 'f1_score_test': 0.9098}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9428, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.95, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.9463, 'f1_cv_std': 0.0021, 'params': 5843969, 'accuracy_test': 0.9577, 'precision_test': 0.8983, 'recall_test': 0.9277, 'f1_score_test': 0.9128}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.936, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0032, 'accuracy_test': 0.9338, 'precision_test': 0.8225, 'recall_test': 0.9213, 'f1_score_test': 0.8691}, 'SVM': {'accuracy_cv_mean': 0.8184, 'accuracy_cv_std': 0.027, 'precision_cv_mean': 0.7823, 'precision_cv_std': 0.0468, 'recall_cv_mean': 0.8925, 'recall_cv_std': 0.0401, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0154, 'accuracy_test': 0.7489, 'precision_test': 0.4854, 'recall_test': 0.8742, 'f1_score_test': 0.6242}, 'Decision Tree': {'accuracy_cv_mean': 0.8316, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8608, 'precision_cv_std': 0.0152, 'recall_cv_mean': 0.7917, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8245, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8486, 'precision_test': 0.6479, 'recall_test': 0.8012, 'f1_score_test': 0.7164}, 'Random Forest': {'accuracy_cv_mean': 0.8374, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.7903, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8294, 'f1_cv_std': 0.0029, 'accuracy_test': 0.8689, 'precision_test': 0.7005, 'recall_test': 0.7868, 'f1_score_test': 0.7411}, 'XGBoost': {'accuracy_cv_mean': 0.9065, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9249, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8848, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.9044, 'f1_cv_std': 0.0019, 'accuracy_test': 0.9227, 'precision_test': 0.8049, 'recall_test': 0.8924, 'f1_score_test': 0.8464}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8739, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.901, 'f1_cv_std': 0.0033, 'accuracy_test': 0.9195, 'precision_test': 0.805, 'recall_test': 0.8746, 'f1_score_test': 0.8384}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8498, 'accuracy_cv_std': 0.006, 'precision_cv_mean': 0.8462, 'precision_cv_std': 0.0133, 'recall_cv_mean': 0.8556, 'recall_cv_std': 0.0105, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0049, 'params': 10401, 'accuracy_test': 0.8816, 'precision_test': 0.7408, 'recall_test': 0.7748, 'f1_score_test': 0.7574}, 'MLP_338433': {'accuracy_cv_mean': 0.8546, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.849, 'precision_cv_std': 0.0138, 'recall_cv_mean': 0.8633, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8559, 'f1_cv_std': 0.0046, 'params': 338433, 'accuracy_test': 0.8899, 'precision_test': 0.7703, 'recall_test': 0.7676, 'f1_score_test': 0.769}, 'MLP_1031169': {'accuracy_cv_mean': 0.8532, 'accuracy_cv_std': 0.0067, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0183, 'recall_cv_mean': 0.8688, 'recall_cv_std': 0.0166, 'f1_cv_mean': 0.8555, 'f1_cv_std': 0.0053, 'params': 1031169, 'accuracy_test': 0.8871, 'precision_test': 0.788, 'recall_test': 0.7205, 'f1_score_test': 0.7528}, 'Logistic Regression': {'accuracy_cv_mean': 0.8493, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8605, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8339, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.847, 'f1_cv_std': 0.0035, 'accuracy_test': 0.8575, 'precision_test': 0.6614, 'recall_test': 0.8256, 'f1_score_test': 0.7344}, 'SVM': {'accuracy_cv_mean': 0.6611, 'accuracy_cv_std': 0.0215, 'precision_cv_mean': 0.6399, 'precision_cv_std': 0.0203, 'recall_cv_mean': 0.7394, 'recall_cv_std': 0.0406, 'f1_cv_mean': 0.6854, 'f1_cv_std': 0.0216, 'accuracy_test': 0.4647, 'precision_test': 0.2887, 'recall_test': 0.8496, 'f1_score_test': 0.431}, 'Decision Tree': {'accuracy_cv_mean': 0.7958, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8131, 'precision_cv_std': 0.0115, 'recall_cv_mean': 0.7687, 'recall_cv_std': 0.0142, 'f1_cv_mean': 0.7901, 'f1_cv_std': 0.0053, 'accuracy_test': 0.813, 'precision_test': 0.58, 'recall_test': 0.7841, 'f1_score_test': 0.6668}, 'Random Forest': {'accuracy_cv_mean': 0.8547, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.8486, 'f1_cv_std': 0.0047, 'accuracy_test': 0.8775, 'precision_test': 0.7163, 'recall_test': 0.8062, 'f1_score_test': 0.7586}, 'XGBoost': {'accuracy_cv_mean': 0.878, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8925, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8596, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.8757, 'f1_cv_std': 0.0023, 'accuracy_test': 0.8931, 'precision_test': 0.7375, 'recall_test': 0.8574, 'f1_score_test': 0.7929}, 'Naive Bayes': {'accuracy_cv_mean': 0.7801, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.7642, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.7866, 'f1_cv_std': 0.0033, 'accuracy_test': 0.7636, 'precision_test': 0.5029, 'recall_test': 0.8083, 'f1_score_test': 0.62}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8972, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8941, 'precision_cv_std': 0.0146, 'recall_cv_mean': 0.9015, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8977, 'f1_cv_std': 0.0043, 'params': 49953, 'accuracy_test': 0.9209, 'precision_test': 0.8577, 'recall_test': 0.8014, 'f1_score_test': 0.8286}, 'MLP_971265': {'accuracy_cv_mean': 0.8968, 'accuracy_cv_std': 0.0063, 'precision_cv_mean': 0.8915, 'precision_cv_std': 0.0195, 'recall_cv_mean': 0.9044, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8976, 'f1_cv_std': 0.0047, 'params': 971265, 'accuracy_test': 0.916, 'precision_test': 0.8065, 'recall_test': 0.8523, 'f1_score_test': 0.8288}, 'MLP_2296833': {'accuracy_cv_mean': 0.8978, 'accuracy_cv_std': 0.0059, 'precision_cv_mean': 0.8924, 'precision_cv_std': 0.0183, 'recall_cv_mean': 0.9054, 'recall_cv_std': 0.0141, 'f1_cv_mean': 0.8986, 'f1_cv_std': 0.0047, 'params': 2296833, 'accuracy_test': 0.9169, 'precision_test': 0.8133, 'recall_test': 0.8461, 'f1_score_test': 0.8294}}}
Saved on: outputs_cv/3/gpt

==============================
Model: Logistic Regression

/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:56:48] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:56:57] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:01:26] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:01:37] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:05:57] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:06:13] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:10:33] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:10:53] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:15:10] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:15:31] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:19:41] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:20:05] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9005, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9051, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8949, 'recall_cv_std': 0.0025, 'f1_cv_mean': 0.8999, 'f1_cv_std': 0.0016}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 45, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.91      0.93     16465
           1       0.75      0.89      0.82      5160

    accuracy                           0.90     21625
   macro avg       0.86      0.90      0.88     21625
weighted avg       0.91      0.90      0.91     21625

Confusion matrix Test saved as: outputs_cv/3/gpt/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/3/gpt/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8489, 'accuracy_cv_std': 0.0084, 'precision_cv_mean': 0.8445, 'precision_cv_std': 0.0231, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0211, 'f1_cv_mean': 0.8501, 'f1_cv_std': 0.0065}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 45, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.82      0.88     16465
           1       0.59      0.85      0.70      5160

    accuracy                           0.83     21625
   macro avg       0.77      0.83      0.79     21625
weighted avg       0.86      0.83      0.83     21625

Confusion matrix Test saved as: outputs_cv/3/gpt/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/3/gpt/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7972, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.8136, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7713, 'recall_cv_std': 0.0117, 'f1_cv_mean': 0.7918, 'f1_cv_std': 0.0041}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 45, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.83      0.87     16465
           1       0.59      0.78      0.67      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.80      0.77     21625
weighted avg       0.84      0.82      0.83     21625

Confusion matrix Test saved as: outputs_cv/3/gpt/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/3/gpt/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.875, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9056, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8372, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.8701, 'f1_cv_std': 0.0031}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 45, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.91      0.93     16465
           1       0.75      0.83      0.79      5160

    accuracy                           0.89     21625
   macro avg       0.85      0.87      0.86     21625
weighted avg       0.90      0.89      0.89     21625

Confusion matrix Test saved as: outputs_cv/3/gpt/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/3/gpt/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.921, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8922, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.9064, 'f1_cv_std': 0.0029}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 45, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.92      0.94     16465
           1       0.78      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.87      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/3/gpt/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/3/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8334, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8642, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.7911, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.826, 'f1_cv_std': 0.0034}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.79      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/3/gpt/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/3/gpt/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.921, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8922, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.9064, 'f1_cv_std': 0.0029, 'accuracy_test': 0.9162, 'precision_test': 0.7846, 'recall_test': 0.8942, 'f1_score_test': 0.8358}
Logistic Regression: {'accuracy_cv_mean': 0.9005, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9051, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8949, 'recall_cv_std': 0.0025, 'f1_cv_mean': 0.8999, 'f1_cv_std': 0.0016, 'accuracy_test': 0.9037, 'precision_test': 0.7499, 'recall_test': 0.895, 'f1_score_test': 0.816}
Random Forest: {'accuracy_cv_mean': 0.875, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9056, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8372, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.8701, 'f1_cv_std': 0.0031, 'accuracy_test': 0.8922, 'precision_test': 0.7452, 'recall_test': 0.8328, 'f1_score_test': 0.7866}
Naive Bayes: {'accuracy_cv_mean': 0.8334, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8642, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.7911, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.826, 'f1_cv_std': 0.0034, 'accuracy_test': 0.8562, 'precision_test': 0.6679, 'recall_test': 0.7903, 'f1_score_test': 0.7239}
SVM: {'accuracy_cv_mean': 0.8489, 'accuracy_cv_std': 0.0084, 'precision_cv_mean': 0.8445, 'precision_cv_std': 0.0231, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0211, 'f1_cv_mean': 0.8501, 'f1_cv_std': 0.0065, 'accuracy_test': 0.8256, 'precision_test': 0.5944, 'recall_test': 0.8473, 'f1_score_test': 0.6987}
Decision Tree: {'accuracy_cv_mean': 0.7972, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.8136, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7713, 'recall_cv_std': 0.0117, 'f1_cv_mean': 0.7918, 'f1_cv_std': 0.0041, 'accuracy_test': 0.8188, 'precision_test': 0.5913, 'recall_test': 0.7785, 'f1_score_test': 0.6721}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9347, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9329, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.937, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9349, 'f1_cv_std': 0.0021, 'params': 160801, 'accuracy_test': 0.9397, 'precision_test': 0.8398, 'recall_test': 0.9233, 'f1_score_test': 0.8795}, 'MLP_2744833': {'accuracy_cv_mean': 0.9453, 'accuracy_cv_std': 0.0008, 'precision_cv_mean': 0.9432, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9478, 'recall_cv_std': 0.0078, 'f1_cv_mean': 0.9454, 'f1_cv_std': 0.0005, 'params': 2744833, 'accuracy_test': 0.9565, 'precision_test': 0.9014, 'recall_test': 0.9182, 'f1_score_test': 0.9098}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9428, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.95, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.9463, 'f1_cv_std': 0.0021, 'params': 5843969, 'accuracy_test': 0.9577, 'precision_test': 0.8983, 'recall_test': 0.9277, 'f1_score_test': 0.9128}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.936, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0032, 'accuracy_test': 0.9338, 'precision_test': 0.8225, 'recall_test': 0.9213, 'f1_score_test': 0.8691}, 'SVM': {'accuracy_cv_mean': 0.8184, 'accuracy_cv_std': 0.027, 'precision_cv_mean': 0.7823, 'precision_cv_std': 0.0468, 'recall_cv_mean': 0.8925, 'recall_cv_std': 0.0401, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0154, 'accuracy_test': 0.7489, 'precision_test': 0.4854, 'recall_test': 0.8742, 'f1_score_test': 0.6242}, 'Decision Tree': {'accuracy_cv_mean': 0.8316, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8608, 'precision_cv_std': 0.0152, 'recall_cv_mean': 0.7917, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8245, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8486, 'precision_test': 0.6479, 'recall_test': 0.8012, 'f1_score_test': 0.7164}, 'Random Forest': {'accuracy_cv_mean': 0.8374, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.7903, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8294, 'f1_cv_std': 0.0029, 'accuracy_test': 0.8689, 'precision_test': 0.7005, 'recall_test': 0.7868, 'f1_score_test': 0.7411}, 'XGBoost': {'accuracy_cv_mean': 0.9065, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9249, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8848, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.9044, 'f1_cv_std': 0.0019, 'accuracy_test': 0.9227, 'precision_test': 0.8049, 'recall_test': 0.8924, 'f1_score_test': 0.8464}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8739, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.901, 'f1_cv_std': 0.0033, 'accuracy_test': 0.9195, 'precision_test': 0.805, 'recall_test': 0.8746, 'f1_score_test': 0.8384}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8498, 'accuracy_cv_std': 0.006, 'precision_cv_mean': 0.8462, 'precision_cv_std': 0.0133, 'recall_cv_mean': 0.8556, 'recall_cv_std': 0.0105, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0049, 'params': 10401, 'accuracy_test': 0.8816, 'precision_test': 0.7408, 'recall_test': 0.7748, 'f1_score_test': 0.7574}, 'MLP_338433': {'accuracy_cv_mean': 0.8546, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.849, 'precision_cv_std': 0.0138, 'recall_cv_mean': 0.8633, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8559, 'f1_cv_std': 0.0046, 'params': 338433, 'accuracy_test': 0.8899, 'precision_test': 0.7703, 'recall_test': 0.7676, 'f1_score_test': 0.769}, 'MLP_1031169': {'accuracy_cv_mean': 0.8532, 'accuracy_cv_std': 0.0067, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0183, 'recall_cv_mean': 0.8688, 'recall_cv_std': 0.0166, 'f1_cv_mean': 0.8555, 'f1_cv_std': 0.0053, 'params': 1031169, 'accuracy_test': 0.8871, 'precision_test': 0.788, 'recall_test': 0.7205, 'f1_score_test': 0.7528}, 'Logistic Regression': {'accuracy_cv_mean': 0.8493, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8605, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8339, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.847, 'f1_cv_std': 0.0035, 'accuracy_test': 0.8575, 'precision_test': 0.6614, 'recall_test': 0.8256, 'f1_score_test': 0.7344}, 'SVM': {'accuracy_cv_mean': 0.6611, 'accuracy_cv_std': 0.0215, 'precision_cv_mean': 0.6399, 'precision_cv_std': 0.0203, 'recall_cv_mean': 0.7394, 'recall_cv_std': 0.0406, 'f1_cv_mean': 0.6854, 'f1_cv_std': 0.0216, 'accuracy_test': 0.4647, 'precision_test': 0.2887, 'recall_test': 0.8496, 'f1_score_test': 0.431}, 'Decision Tree': {'accuracy_cv_mean': 0.7958, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8131, 'precision_cv_std': 0.0115, 'recall_cv_mean': 0.7687, 'recall_cv_std': 0.0142, 'f1_cv_mean': 0.7901, 'f1_cv_std': 0.0053, 'accuracy_test': 0.813, 'precision_test': 0.58, 'recall_test': 0.7841, 'f1_score_test': 0.6668}, 'Random Forest': {'accuracy_cv_mean': 0.8547, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.8486, 'f1_cv_std': 0.0047, 'accuracy_test': 0.8775, 'precision_test': 0.7163, 'recall_test': 0.8062, 'f1_score_test': 0.7586}, 'XGBoost': {'accuracy_cv_mean': 0.878, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8925, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8596, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.8757, 'f1_cv_std': 0.0023, 'accuracy_test': 0.8931, 'precision_test': 0.7375, 'recall_test': 0.8574, 'f1_score_test': 0.7929}, 'Naive Bayes': {'accuracy_cv_mean': 0.7801, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.7642, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.7866, 'f1_cv_std': 0.0033, 'accuracy_test': 0.7636, 'precision_test': 0.5029, 'recall_test': 0.8083, 'f1_score_test': 0.62}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8972, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8941, 'precision_cv_std': 0.0146, 'recall_cv_mean': 0.9015, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8977, 'f1_cv_std': 0.0043, 'params': 49953, 'accuracy_test': 0.9209, 'precision_test': 0.8577, 'recall_test': 0.8014, 'f1_score_test': 0.8286}, 'MLP_971265': {'accuracy_cv_mean': 0.8968, 'accuracy_cv_std': 0.0063, 'precision_cv_mean': 0.8915, 'precision_cv_std': 0.0195, 'recall_cv_mean': 0.9044, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8976, 'f1_cv_std': 0.0047, 'params': 971265, 'accuracy_test': 0.916, 'precision_test': 0.8065, 'recall_test': 0.8523, 'f1_score_test': 0.8288}, 'MLP_2296833': {'accuracy_cv_mean': 0.8978, 'accuracy_cv_std': 0.0059, 'precision_cv_mean': 0.8924, 'precision_cv_std': 0.0183, 'recall_cv_mean': 0.9054, 'recall_cv_std': 0.0141, 'f1_cv_mean': 0.8986, 'f1_cv_std': 0.0047, 'params': 2296833, 'accuracy_test': 0.9169, 'precision_test': 0.8133, 'recall_test': 0.8461, 'f1_score_test': 0.8294}, 'Logistic Regression': {'accuracy_cv_mean': 0.9005, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9051, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8949, 'recall_cv_std': 0.0025, 'f1_cv_mean': 0.8999, 'f1_cv_std': 0.0016, 'accuracy_test': 0.9037, 'precision_test': 0.7499, 'recall_test': 0.895, 'f1_score_test': 0.816}, 'SVM': {'accuracy_cv_mean': 0.8489, 'accuracy_cv_std': 0.0084, 'precision_cv_mean': 0.8445, 'precision_cv_std': 0.0231, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0211, 'f1_cv_mean': 0.8501, 'f1_cv_std': 0.0065, 'accuracy_test': 0.8256, 'precision_test': 0.5944, 'recall_test': 0.8473, 'f1_score_test': 0.6987}, 'Decision Tree': {'accuracy_cv_mean': 0.7972, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.8136, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7713, 'recall_cv_std': 0.0117, 'f1_cv_mean': 0.7918, 'f1_cv_std': 0.0041, 'accuracy_test': 0.8188, 'precision_test': 0.5913, 'recall_test': 0.7785, 'f1_score_test': 0.6721}, 'Random Forest': {'accuracy_cv_mean': 0.875, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9056, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8372, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.8701, 'f1_cv_std': 0.0031, 'accuracy_test': 0.8922, 'precision_test': 0.7452, 'recall_test': 0.8328, 'f1_score_test': 0.7866}, 'XGBoost': {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.921, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8922, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.9064, 'f1_cv_std': 0.0029, 'accuracy_test': 0.9162, 'precision_test': 0.7846, 'recall_test': 0.8942, 'f1_score_test': 0.8358}, 'Naive Bayes': {'accuracy_cv_mean': 0.8334, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8642, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.7911, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.826, 'f1_cv_std': 0.0034, 'accuracy_test': 0.8562, 'precision_test': 0.6679, 'recall_test': 0.7903, 'f1_score_test': 0.7239}}}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
MLP_5843969: {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9428, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.95, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.9463, 'f1_cv_std': 0.0021, 'params': 5843969, 'accuracy_test': 0.9577, 'precision_test': 0.8983, 'recall_test': 0.9277, 'f1_score_test': 0.9128}
MLP_2744833: {'accuracy_cv_mean': 0.9453, 'accuracy_cv_std': 0.0008, 'precision_cv_mean': 0.9432, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9478, 'recall_cv_std': 0.0078, 'f1_cv_mean': 0.9454, 'f1_cv_std': 0.0005, 'params': 2744833, 'accuracy_test': 0.9565, 'precision_test': 0.9014, 'recall_test': 0.9182, 'f1_score_test': 0.9098}
MLP_160801: {'accuracy_cv_mean': 0.9347, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9329, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.937, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9349, 'f1_cv_std': 0.0021, 'params': 160801, 'accuracy_test': 0.9397, 'precision_test': 0.8398, 'recall_test': 0.9233, 'f1_score_test': 0.8795}
Logistic Regression: {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.936, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0032, 'accuracy_test': 0.9338, 'precision_test': 0.8225, 'recall_test': 0.9213, 'f1_score_test': 0.8691}
XGBoost: {'accuracy_cv_mean': 0.9065, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9249, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8848, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.9044, 'f1_cv_std': 0.0019, 'accuracy_test': 0.9227, 'precision_test': 0.8049, 'recall_test': 0.8924, 'f1_score_test': 0.8464}
Naive Bayes: {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8739, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.901, 'f1_cv_std': 0.0033, 'accuracy_test': 0.9195, 'precision_test': 0.805, 'recall_test': 0.8746, 'f1_score_test': 0.8384}
Random Forest: {'accuracy_cv_mean': 0.8374, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.7903, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8294, 'f1_cv_std': 0.0029, 'accuracy_test': 0.8689, 'precision_test': 0.7005, 'recall_test': 0.7868, 'f1_score_test': 0.7411}
Decision Tree: {'accuracy_cv_mean': 0.8316, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8608, 'precision_cv_std': 0.0152, 'recall_cv_mean': 0.7917, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8245, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8486, 'precision_test': 0.6479, 'recall_test': 0.8012, 'f1_score_test': 0.7164}
SVM: {'accuracy_cv_mean': 0.8184, 'accuracy_cv_std': 0.027, 'precision_cv_mean': 0.7823, 'precision_cv_std': 0.0468, 'recall_cv_mean': 0.8925, 'recall_cv_std': 0.0401, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0154, 'accuracy_test': 0.7489, 'precision_test': 0.4854, 'recall_test': 0.8742, 'f1_score_test': 0.6242}


EMBEDDINGS TYPE: LYRICS_BERT
XGBoost: {'accuracy_cv_mean': 0.878, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8925, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8596, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.8757, 'f1_cv_std': 0.0023, 'accuracy_test': 0.8931, 'precision_test': 0.7375, 'recall_test': 0.8574, 'f1_score_test': 0.7929}
MLP_338433: {'accuracy_cv_mean': 0.8546, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.849, 'precision_cv_std': 0.0138, 'recall_cv_mean': 0.8633, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8559, 'f1_cv_std': 0.0046, 'params': 338433, 'accuracy_test': 0.8899, 'precision_test': 0.7703, 'recall_test': 0.7676, 'f1_score_test': 0.769}
Random Forest: {'accuracy_cv_mean': 0.8547, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.8486, 'f1_cv_std': 0.0047, 'accuracy_test': 0.8775, 'precision_test': 0.7163, 'recall_test': 0.8062, 'f1_score_test': 0.7586}
MLP_10401: {'accuracy_cv_mean': 0.8498, 'accuracy_cv_std': 0.006, 'precision_cv_mean': 0.8462, 'precision_cv_std': 0.0133, 'recall_cv_mean': 0.8556, 'recall_cv_std': 0.0105, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0049, 'params': 10401, 'accuracy_test': 0.8816, 'precision_test': 0.7408, 'recall_test': 0.7748, 'f1_score_test': 0.7574}
MLP_1031169: {'accuracy_cv_mean': 0.8532, 'accuracy_cv_std': 0.0067, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0183, 'recall_cv_mean': 0.8688, 'recall_cv_std': 0.0166, 'f1_cv_mean': 0.8555, 'f1_cv_std': 0.0053, 'params': 1031169, 'accuracy_test': 0.8871, 'precision_test': 0.788, 'recall_test': 0.7205, 'f1_score_test': 0.7528}
Logistic Regression: {'accuracy_cv_mean': 0.8493, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8605, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8339, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.847, 'f1_cv_std': 0.0035, 'accuracy_test': 0.8575, 'precision_test': 0.6614, 'recall_test': 0.8256, 'f1_score_test': 0.7344}
Decision Tree: {'accuracy_cv_mean': 0.7958, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8131, 'precision_cv_std': 0.0115, 'recall_cv_mean': 0.7687, 'recall_cv_std': 0.0142, 'f1_cv_mean': 0.7901, 'f1_cv_std': 0.0053, 'accuracy_test': 0.813, 'precision_test': 0.58, 'recall_test': 0.7841, 'f1_score_test': 0.6668}
Naive Bayes: {'accuracy_cv_mean': 0.7801, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.7642, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.7866, 'f1_cv_std': 0.0033, 'accuracy_test': 0.7636, 'precision_test': 0.5029, 'recall_test': 0.8083, 'f1_score_test': 0.62}
SVM: {'accuracy_cv_mean': 0.6611, 'accuracy_cv_std': 0.0215, 'precision_cv_mean': 0.6399, 'precision_cv_std': 0.0203, 'recall_cv_mean': 0.7394, 'recall_cv_std': 0.0406, 'f1_cv_mean': 0.6854, 'f1_cv_std': 0.0216, 'accuracy_test': 0.4647, 'precision_test': 0.2887, 'recall_test': 0.8496, 'f1_score_test': 0.431}


EMBEDDINGS TYPE: GPT
XGBoost: {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.921, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8922, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.9064, 'f1_cv_std': 0.0029, 'accuracy_test': 0.9162, 'precision_test': 0.7846, 'recall_test': 0.8942, 'f1_score_test': 0.8358}
MLP_2296833: {'accuracy_cv_mean': 0.8978, 'accuracy_cv_std': 0.0059, 'precision_cv_mean': 0.8924, 'precision_cv_std': 0.0183, 'recall_cv_mean': 0.9054, 'recall_cv_std': 0.0141, 'f1_cv_mean': 0.8986, 'f1_cv_std': 0.0047, 'params': 2296833, 'accuracy_test': 0.9169, 'precision_test': 0.8133, 'recall_test': 0.8461, 'f1_score_test': 0.8294}
MLP_971265: {'accuracy_cv_mean': 0.8968, 'accuracy_cv_std': 0.0063, 'precision_cv_mean': 0.8915, 'precision_cv_std': 0.0195, 'recall_cv_mean': 0.9044, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8976, 'f1_cv_std': 0.0047, 'params': 971265, 'accuracy_test': 0.916, 'precision_test': 0.8065, 'recall_test': 0.8523, 'f1_score_test': 0.8288}
MLP_49953: {'accuracy_cv_mean': 0.8972, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8941, 'precision_cv_std': 0.0146, 'recall_cv_mean': 0.9015, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8977, 'f1_cv_std': 0.0043, 'params': 49953, 'accuracy_test': 0.9209, 'precision_test': 0.8577, 'recall_test': 0.8014, 'f1_score_test': 0.8286}
Logistic Regression: {'accuracy_cv_mean': 0.9005, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9051, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8949, 'recall_cv_std': 0.0025, 'f1_cv_mean': 0.8999, 'f1_cv_std': 0.0016, 'accuracy_test': 0.9037, 'precision_test': 0.7499, 'recall_test': 0.895, 'f1_score_test': 0.816}
Random Forest: {'accuracy_cv_mean': 0.875, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9056, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8372, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.8701, 'f1_cv_std': 0.0031, 'accuracy_test': 0.8922, 'precision_test': 0.7452, 'recall_test': 0.8328, 'f1_score_test': 0.7866}
Naive Bayes: {'accuracy_cv_mean': 0.8334, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8642, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.7911, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.826, 'f1_cv_std': 0.0034, 'accuracy_test': 0.8562, 'precision_test': 0.6679, 'recall_test': 0.7903, 'f1_score_test': 0.7239}
SVM: {'accuracy_cv_mean': 0.8489, 'accuracy_cv_std': 0.0084, 'precision_cv_mean': 0.8445, 'precision_cv_std': 0.0231, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0211, 'f1_cv_mean': 0.8501, 'f1_cv_std': 0.0065, 'accuracy_test': 0.8256, 'precision_test': 0.5944, 'recall_test': 0.8473, 'f1_score_test': 0.6987}
Decision Tree: {'accuracy_cv_mean': 0.7972, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.8136, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7713, 'recall_cv_std': 0.0117, 'f1_cv_mean': 0.7918, 'f1_cv_std': 0.0041, 'accuracy_test': 0.8188, 'precision_test': 0.5913, 'recall_test': 0.7785, 'f1_score_test': 0.6721}
Diccionario global guardado en: outputs_cv/3/gpt/global_metrics.json

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
====================================

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9005, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9051, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8949, 'recall_cv_std': 0.0025, 'f1_cv_mean': 0.8999, 'f1_cv_std': 0.0016}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 45, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.91      0.93     16465
           1       0.75      0.89      0.82      5160

    accuracy                           0.90     21625
   macro avg       0.86      0.90      0.88     21625
weighted avg       0.91      0.90      0.91     21625

Confusion matrix Test saved as: outputs_cv/3/gpt/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/3/gpt/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8489, 'accuracy_cv_std': 0.0084, 'precision_cv_mean': 0.8445, 'precision_cv_std': 0.0231, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0211, 'f1_cv_mean': 0.8501, 'f1_cv_std': 0.0065}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 45, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.82      0.88     16465
           1       0.59      0.85      0.70      5160

    accuracy                           0.83     21625
   macro avg       0.77      0.83      0.79     21625
weighted avg       0.86      0.83      0.83     21625

Confusion matrix Test saved as: outputs_cv/3/gpt/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/3/gpt/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7972, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.8136, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7713, 'recall_cv_std': 0.0117, 'f1_cv_mean': 0.7918, 'f1_cv_std': 0.0041}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 45, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.83      0.87     16465
           1       0.59      0.78      0.67      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.80      0.77     21625
weighted avg       0.84      0.82      0.83     21625

Confusion matrix Test saved as: outputs_cv/3/gpt/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/3/gpt/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.875, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9056, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8372, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.8701, 'f1_cv_std': 0.0031}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 45, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.91      0.93     16465
           1       0.75      0.83      0.79      5160

    accuracy                           0.89     21625
   macro avg       0.85      0.87      0.86     21625
weighted avg       0.90      0.89      0.89     21625

Confusion matrix Test saved as: outputs_cv/3/gpt/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/3/gpt/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.921, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8922, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.9064, 'f1_cv_std': 0.0029}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 45, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.92      0.94     16465
           1       0.78      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.87      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/3/gpt/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/3/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8334, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8642, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.7911, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.826, 'f1_cv_std': 0.0034}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.79      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/3/gpt/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/3/gpt/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.921, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8922, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.9064, 'f1_cv_std': 0.0029, 'accuracy_test': 0.9162, 'precision_test': 0.7846, 'recall_test': 0.8942, 'f1_score_test': 0.8358}
Logistic Regression: {'accuracy_cv_mean': 0.9005, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9051, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8949, 'recall_cv_std': 0.0025, 'f1_cv_mean': 0.8999, 'f1_cv_std': 0.0016, 'accuracy_test': 0.9037, 'precision_test': 0.7499, 'recall_test': 0.895, 'f1_score_test': 0.816}
Random Forest: {'accuracy_cv_mean': 0.875, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9056, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8372, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.8701, 'f1_cv_std': 0.0031, 'accuracy_test': 0.8922, 'precision_test': 0.7452, 'recall_test': 0.8328, 'f1_score_test': 0.7866}
Naive Bayes: {'accuracy_cv_mean': 0.8334, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8642, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.7911, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.826, 'f1_cv_std': 0.0034, 'accuracy_test': 0.8562, 'precision_test': 0.6679, 'recall_test': 0.7903, 'f1_score_test': 0.7239}
SVM: {'accuracy_cv_mean': 0.8489, 'accuracy_cv_std': 0.0084, 'precision_cv_mean': 0.8445, 'precision_cv_std': 0.0231, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0211, 'f1_cv_mean': 0.8501, 'f1_cv_std': 0.0065, 'accuracy_test': 0.8256, 'precision_test': 0.5944, 'recall_test': 0.8473, 'f1_score_test': 0.6987}
Decision Tree: {'accuracy_cv_mean': 0.7972, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.8136, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7713, 'recall_cv_std': 0.0117, 'f1_cv_mean': 0.7918, 'f1_cv_std': 0.0041, 'accuracy_test': 0.8188, 'precision_test': 0.5913, 'recall_test': 0.7785, 'f1_score_test': 0.6721}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9347, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9329, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.937, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9349, 'f1_cv_std': 0.0021, 'params': 160801, 'accuracy_test': 0.9397, 'precision_test': 0.8398, 'recall_test': 0.9233, 'f1_score_test': 0.8795}, 'MLP_2744833': {'accuracy_cv_mean': 0.9453, 'accuracy_cv_std': 0.0008, 'precision_cv_mean': 0.9432, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9478, 'recall_cv_std': 0.0078, 'f1_cv_mean': 0.9454, 'f1_cv_std': 0.0005, 'params': 2744833, 'accuracy_test': 0.9565, 'precision_test': 0.9014, 'recall_test': 0.9182, 'f1_score_test': 0.9098}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9428, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.95, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.9463, 'f1_cv_std': 0.0021, 'params': 5843969, 'accuracy_test': 0.9577, 'precision_test': 0.8983, 'recall_test': 0.9277, 'f1_score_test': 0.9128}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.936, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0032, 'accuracy_test': 0.9338, 'precision_test': 0.8225, 'recall_test': 0.9213, 'f1_score_test': 0.8691}, 'SVM': {'accuracy_cv_mean': 0.8184, 'accuracy_cv_std': 0.027, 'precision_cv_mean': 0.7823, 'precision_cv_std': 0.0468, 'recall_cv_mean': 0.8925, 'recall_cv_std': 0.0401, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0154, 'accuracy_test': 0.7489, 'precision_test': 0.4854, 'recall_test': 0.8742, 'f1_score_test': 0.6242}, 'Decision Tree': {'accuracy_cv_mean': 0.8316, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8608, 'precision_cv_std': 0.0152, 'recall_cv_mean': 0.7917, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8245, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8486, 'precision_test': 0.6479, 'recall_test': 0.8012, 'f1_score_test': 0.7164}, 'Random Forest': {'accuracy_cv_mean': 0.8374, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.7903, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8294, 'f1_cv_std': 0.0029, 'accuracy_test': 0.8689, 'precision_test': 0.7005, 'recall_test': 0.7868, 'f1_score_test': 0.7411}, 'XGBoost': {'accuracy_cv_mean': 0.9065, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9249, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8848, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.9044, 'f1_cv_std': 0.0019, 'accuracy_test': 0.9227, 'precision_test': 0.8049, 'recall_test': 0.8924, 'f1_score_test': 0.8464}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8739, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.901, 'f1_cv_std': 0.0033, 'accuracy_test': 0.9195, 'precision_test': 0.805, 'recall_test': 0.8746, 'f1_score_test': 0.8384}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8498, 'accuracy_cv_std': 0.006, 'precision_cv_mean': 0.8462, 'precision_cv_std': 0.0133, 'recall_cv_mean': 0.8556, 'recall_cv_std': 0.0105, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0049, 'params': 10401, 'accuracy_test': 0.8816, 'precision_test': 0.7408, 'recall_test': 0.7748, 'f1_score_test': 0.7574}, 'MLP_338433': {'accuracy_cv_mean': 0.8546, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.849, 'precision_cv_std': 0.0138, 'recall_cv_mean': 0.8633, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8559, 'f1_cv_std': 0.0046, 'params': 338433, 'accuracy_test': 0.8899, 'precision_test': 0.7703, 'recall_test': 0.7676, 'f1_score_test': 0.769}, 'MLP_1031169': {'accuracy_cv_mean': 0.8532, 'accuracy_cv_std': 0.0067, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0183, 'recall_cv_mean': 0.8688, 'recall_cv_std': 0.0166, 'f1_cv_mean': 0.8555, 'f1_cv_std': 0.0053, 'params': 1031169, 'accuracy_test': 0.8871, 'precision_test': 0.788, 'recall_test': 0.7205, 'f1_score_test': 0.7528}, 'Logistic Regression': {'accuracy_cv_mean': 0.8493, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8605, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8339, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.847, 'f1_cv_std': 0.0035, 'accuracy_test': 0.8575, 'precision_test': 0.6614, 'recall_test': 0.8256, 'f1_score_test': 0.7344}, 'SVM': {'accuracy_cv_mean': 0.6611, 'accuracy_cv_std': 0.0215, 'precision_cv_mean': 0.6399, 'precision_cv_std': 0.0203, 'recall_cv_mean': 0.7394, 'recall_cv_std': 0.0406, 'f1_cv_mean': 0.6854, 'f1_cv_std': 0.0216, 'accuracy_test': 0.4647, 'precision_test': 0.2887, 'recall_test': 0.8496, 'f1_score_test': 0.431}, 'Decision Tree': {'accuracy_cv_mean': 0.7958, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8131, 'precision_cv_std': 0.0115, 'recall_cv_mean': 0.7687, 'recall_cv_std': 0.0142, 'f1_cv_mean': 0.7901, 'f1_cv_std': 0.0053, 'accuracy_test': 0.813, 'precision_test': 0.58, 'recall_test': 0.7841, 'f1_score_test': 0.6668}, 'Random Forest': {'accuracy_cv_mean': 0.8547, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.8486, 'f1_cv_std': 0.0047, 'accuracy_test': 0.8775, 'precision_test': 0.7163, 'recall_test': 0.8062, 'f1_score_test': 0.7586}, 'XGBoost': {'accuracy_cv_mean': 0.878, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8925, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8596, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.8757, 'f1_cv_std': 0.0023, 'accuracy_test': 0.8931, 'precision_test': 0.7375, 'recall_test': 0.8574, 'f1_score_test': 0.7929}, 'Naive Bayes': {'accuracy_cv_mean': 0.7801, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.7642, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.7866, 'f1_cv_std': 0.0033, 'accuracy_test': 0.7636, 'precision_test': 0.5029, 'recall_test': 0.8083, 'f1_score_test': 0.62}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8972, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8941, 'precision_cv_std': 0.0146, 'recall_cv_mean': 0.9015, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8977, 'f1_cv_std': 0.0043, 'params': 49953, 'accuracy_test': 0.9209, 'precision_test': 0.8577, 'recall_test': 0.8014, 'f1_score_test': 0.8286}, 'MLP_971265': {'accuracy_cv_mean': 0.8968, 'accuracy_cv_std': 0.0063, 'precision_cv_mean': 0.8915, 'precision_cv_std': 0.0195, 'recall_cv_mean': 0.9044, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8976, 'f1_cv_std': 0.0047, 'params': 971265, 'accuracy_test': 0.916, 'precision_test': 0.8065, 'recall_test': 0.8523, 'f1_score_test': 0.8288}, 'MLP_2296833': {'accuracy_cv_mean': 0.8978, 'accuracy_cv_std': 0.0059, 'precision_cv_mean': 0.8924, 'precision_cv_std': 0.0183, 'recall_cv_mean': 0.9054, 'recall_cv_std': 0.0141, 'f1_cv_mean': 0.8986, 'f1_cv_std': 0.0047, 'params': 2296833, 'accuracy_test': 0.9169, 'precision_test': 0.8133, 'recall_test': 0.8461, 'f1_score_test': 0.8294}, 'Logistic Regression': {'accuracy_cv_mean': 0.9005, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9051, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8949, 'recall_cv_std': 0.0025, 'f1_cv_mean': 0.8999, 'f1_cv_std': 0.0016, 'accuracy_test': 0.9037, 'precision_test': 0.7499, 'recall_test': 0.895, 'f1_score_test': 0.816}, 'SVM': {'accuracy_cv_mean': 0.8489, 'accuracy_cv_std': 0.0084, 'precision_cv_mean': 0.8445, 'precision_cv_std': 0.0231, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0211, 'f1_cv_mean': 0.8501, 'f1_cv_std': 0.0065, 'accuracy_test': 0.8256, 'precision_test': 0.5944, 'recall_test': 0.8473, 'f1_score_test': 0.6987}, 'Decision Tree': {'accuracy_cv_mean': 0.7972, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.8136, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7713, 'recall_cv_std': 0.0117, 'f1_cv_mean': 0.7918, 'f1_cv_std': 0.0041, 'accuracy_test': 0.8188, 'precision_test': 0.5913, 'recall_test': 0.7785, 'f1_score_test': 0.6721}, 'Random Forest': {'accuracy_cv_mean': 0.875, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9056, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8372, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.8701, 'f1_cv_std': 0.0031, 'accuracy_test': 0.8922, 'precision_test': 0.7452, 'recall_test': 0.8328, 'f1_score_test': 0.7866}, 'XGBoost': {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.921, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8922, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.9064, 'f1_cv_std': 0.0029, 'accuracy_test': 0.9162, 'precision_test': 0.7846, 'recall_test': 0.8942, 'f1_score_test': 0.8358}, 'Naive Bayes': {'accuracy_cv_mean': 0.8334, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8642, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.7911, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.826, 'f1_cv_std': 0.0034, 'accuracy_test': 0.8562, 'precision_test': 0.6679, 'recall_test': 0.7903, 'f1_score_test': 0.7239}}}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
MLP_5843969: {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9428, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.95, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.9463, 'f1_cv_std': 0.0021, 'params': 5843969, 'accuracy_test': 0.9577, 'precision_test': 0.8983, 'recall_test': 0.9277, 'f1_score_test': 0.9128}
MLP_2744833: {'accuracy_cv_mean': 0.9453, 'accuracy_cv_std': 0.0008, 'precision_cv_mean': 0.9432, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9478, 'recall_cv_std': 0.0078, 'f1_cv_mean': 0.9454, 'f1_cv_std': 0.0005, 'params': 2744833, 'accuracy_test': 0.9565, 'precision_test': 0.9014, 'recall_test': 0.9182, 'f1_score_test': 0.9098}
MLP_160801: {'accuracy_cv_mean': 0.9347, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9329, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.937, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9349, 'f1_cv_std': 0.0021, 'params': 160801, 'accuracy_test': 0.9397, 'precision_test': 0.8398, 'recall_test': 0.9233, 'f1_score_test': 0.8795}
Logistic Regression: {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.936, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0032, 'accuracy_test': 0.9338, 'precision_test': 0.8225, 'recall_test': 0.9213, 'f1_score_test': 0.8691}
XGBoost: {'accuracy_cv_mean': 0.9065, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9249, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8848, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.9044, 'f1_cv_std': 0.0019, 'accuracy_test': 0.9227, 'precision_test': 0.8049, 'recall_test': 0.8924, 'f1_score_test': 0.8464}
Naive Bayes: {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0024, 'recall_cv_mean': 0.8739, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.901, 'f1_cv_std': 0.0033, 'accuracy_test': 0.9195, 'precision_test': 0.805, 'recall_test': 0.8746, 'f1_score_test': 0.8384}
Random Forest: {'accuracy_cv_mean': 0.8374, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8725, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.7903, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.8294, 'f1_cv_std': 0.0029, 'accuracy_test': 0.8689, 'precision_test': 0.7005, 'recall_test': 0.7868, 'f1_score_test': 0.7411}
Decision Tree: {'accuracy_cv_mean': 0.8316, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8608, 'precision_cv_std': 0.0152, 'recall_cv_mean': 0.7917, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8245, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8486, 'precision_test': 0.6479, 'recall_test': 0.8012, 'f1_score_test': 0.7164}
SVM: {'accuracy_cv_mean': 0.8184, 'accuracy_cv_std': 0.027, 'precision_cv_mean': 0.7823, 'precision_cv_std': 0.0468, 'recall_cv_mean': 0.8925, 'recall_cv_std': 0.0401, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0154, 'accuracy_test': 0.7489, 'precision_test': 0.4854, 'recall_test': 0.8742, 'f1_score_test': 0.6242}


EMBEDDINGS TYPE: LYRICS_BERT
XGBoost: {'accuracy_cv_mean': 0.878, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.8925, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8596, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.8757, 'f1_cv_std': 0.0023, 'accuracy_test': 0.8931, 'precision_test': 0.7375, 'recall_test': 0.8574, 'f1_score_test': 0.7929}
MLP_338433: {'accuracy_cv_mean': 0.8546, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.849, 'precision_cv_std': 0.0138, 'recall_cv_mean': 0.8633, 'recall_cv_std': 0.0156, 'f1_cv_mean': 0.8559, 'f1_cv_std': 0.0046, 'params': 338433, 'accuracy_test': 0.8899, 'precision_test': 0.7703, 'recall_test': 0.7676, 'f1_score_test': 0.769}
Random Forest: {'accuracy_cv_mean': 0.8547, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.8486, 'f1_cv_std': 0.0047, 'accuracy_test': 0.8775, 'precision_test': 0.7163, 'recall_test': 0.8062, 'f1_score_test': 0.7586}
MLP_10401: {'accuracy_cv_mean': 0.8498, 'accuracy_cv_std': 0.006, 'precision_cv_mean': 0.8462, 'precision_cv_std': 0.0133, 'recall_cv_mean': 0.8556, 'recall_cv_std': 0.0105, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0049, 'params': 10401, 'accuracy_test': 0.8816, 'precision_test': 0.7408, 'recall_test': 0.7748, 'f1_score_test': 0.7574}
MLP_1031169: {'accuracy_cv_mean': 0.8532, 'accuracy_cv_std': 0.0067, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0183, 'recall_cv_mean': 0.8688, 'recall_cv_std': 0.0166, 'f1_cv_mean': 0.8555, 'f1_cv_std': 0.0053, 'params': 1031169, 'accuracy_test': 0.8871, 'precision_test': 0.788, 'recall_test': 0.7205, 'f1_score_test': 0.7528}
Logistic Regression: {'accuracy_cv_mean': 0.8493, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8605, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8339, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.847, 'f1_cv_std': 0.0035, 'accuracy_test': 0.8575, 'precision_test': 0.6614, 'recall_test': 0.8256, 'f1_score_test': 0.7344}
Decision Tree: {'accuracy_cv_mean': 0.7958, 'accuracy_cv_std': 0.0048, 'precision_cv_mean': 0.8131, 'precision_cv_std': 0.0115, 'recall_cv_mean': 0.7687, 'recall_cv_std': 0.0142, 'f1_cv_mean': 0.7901, 'f1_cv_std': 0.0053, 'accuracy_test': 0.813, 'precision_test': 0.58, 'recall_test': 0.7841, 'f1_score_test': 0.6668}
Naive Bayes: {'accuracy_cv_mean': 0.7801, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.7642, 'precision_cv_std': 0.0047, 'recall_cv_mean': 0.8103, 'recall_cv_std': 0.0064, 'f1_cv_mean': 0.7866, 'f1_cv_std': 0.0033, 'accuracy_test': 0.7636, 'precision_test': 0.5029, 'recall_test': 0.8083, 'f1_score_test': 0.62}
SVM: {'accuracy_cv_mean': 0.6611, 'accuracy_cv_std': 0.0215, 'precision_cv_mean': 0.6399, 'precision_cv_std': 0.0203, 'recall_cv_mean': 0.7394, 'recall_cv_std': 0.0406, 'f1_cv_mean': 0.6854, 'f1_cv_std': 0.0216, 'accuracy_test': 0.4647, 'precision_test': 0.2887, 'recall_test': 0.8496, 'f1_score_test': 0.431}


EMBEDDINGS TYPE: GPT
XGBoost: {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.921, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8922, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.9064, 'f1_cv_std': 0.0029, 'accuracy_test': 0.9162, 'precision_test': 0.7846, 'recall_test': 0.8942, 'f1_score_test': 0.8358}
MLP_2296833: {'accuracy_cv_mean': 0.8978, 'accuracy_cv_std': 0.0059, 'precision_cv_mean': 0.8924, 'precision_cv_std': 0.0183, 'recall_cv_mean': 0.9054, 'recall_cv_std': 0.0141, 'f1_cv_mean': 0.8986, 'f1_cv_std': 0.0047, 'params': 2296833, 'accuracy_test': 0.9169, 'precision_test': 0.8133, 'recall_test': 0.8461, 'f1_score_test': 0.8294}
MLP_971265: {'accuracy_cv_mean': 0.8968, 'accuracy_cv_std': 0.0063, 'precision_cv_mean': 0.8915, 'precision_cv_std': 0.0195, 'recall_cv_mean': 0.9044, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8976, 'f1_cv_std': 0.0047, 'params': 971265, 'accuracy_test': 0.916, 'precision_test': 0.8065, 'recall_test': 0.8523, 'f1_score_test': 0.8288}
MLP_49953: {'accuracy_cv_mean': 0.8972, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8941, 'precision_cv_std': 0.0146, 'recall_cv_mean': 0.9015, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8977, 'f1_cv_std': 0.0043, 'params': 49953, 'accuracy_test': 0.9209, 'precision_test': 0.8577, 'recall_test': 0.8014, 'f1_score_test': 0.8286}
Logistic Regression: {'accuracy_cv_mean': 0.9005, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9051, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8949, 'recall_cv_std': 0.0025, 'f1_cv_mean': 0.8999, 'f1_cv_std': 0.0016, 'accuracy_test': 0.9037, 'precision_test': 0.7499, 'recall_test': 0.895, 'f1_score_test': 0.816}
Random Forest: {'accuracy_cv_mean': 0.875, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9056, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8372, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.8701, 'f1_cv_std': 0.0031, 'accuracy_test': 0.8922, 'precision_test': 0.7452, 'recall_test': 0.8328, 'f1_score_test': 0.7866}
Naive Bayes: {'accuracy_cv_mean': 0.8334, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8642, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.7911, 'recall_cv_std': 0.0044, 'f1_cv_mean': 0.826, 'f1_cv_std': 0.0034, 'accuracy_test': 0.8562, 'precision_test': 0.6679, 'recall_test': 0.7903, 'f1_score_test': 0.7239}
SVM: {'accuracy_cv_mean': 0.8489, 'accuracy_cv_std': 0.0084, 'precision_cv_mean': 0.8445, 'precision_cv_std': 0.0231, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0211, 'f1_cv_mean': 0.8501, 'f1_cv_std': 0.0065, 'accuracy_test': 0.8256, 'precision_test': 0.5944, 'recall_test': 0.8473, 'f1_score_test': 0.6987}
Decision Tree: {'accuracy_cv_mean': 0.7972, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.8136, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7713, 'recall_cv_std': 0.0117, 'f1_cv_mean': 0.7918, 'f1_cv_std': 0.0041, 'accuracy_test': 0.8188, 'precision_test': 0.5913, 'recall_test': 0.7785, 'f1_score_test': 0.6721}
Diccionario global guardado en: outputs_cv/3/gpt/global_metrics.json

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
====================================

