2025-10-23 01:44:40.527417: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-23 01:44:40.527417: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-23 01:44:40.578461: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-23 01:44:40.578461: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-23 01:44:43.399556: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-23 01:44:43.399556: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_4.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_4.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 15 ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']

CAT_LOW 3 ['emotion', 'Key', 'Time signature']
CAT_HIGH 12 ['Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
After removing some columns that ignorated, for TF-IDF embbedings you are selecteing this columns:
--> ['text']
After removing some columns that ignorated, for numeric cols you are selecteing this columns:
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
--> PaTH:  ../../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy
Running experiment with TFIDF embeddings
Contaning the categorical cols
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 5023)
Shape of X_test after concatenation:  (21625, 5023)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [5023, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3170, Test Loss: 0.2031, F1: 0.9221, AUC: 0.9757
Epoch [10/30] Train Loss: 0.0736, Test Loss: 0.2188, F1: 0.9285, AUC: 0.9805
Epoch [20/30] Train Loss: 0.0446, Test Loss: 0.2883, F1: 0.9261, AUC: 0.9787
Mejores resultados en la época:  8
f1-score 0.9300480769230769
AUC según el mejor F1-score 0.9812654456613183

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3150, Test Loss: 0.2035, F1: 0.9143, AUC: 0.9768
Epoch [10/30] Train Loss: 0.0845, Test Loss: 0.2046, F1: 0.9312, AUC: 0.9810
Epoch [20/30] Train Loss: 0.0658, Test Loss: 0.2615, F1: 0.9279, AUC: 0.9791
Mejores resultados en la época:  4
f1-score 0.9331870275542551
AUC según el mejor F1-score 0.9830132935971396

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3212, Test Loss: 0.1994, F1: 0.9235, AUC: 0.9774
Epoch [10/30] Train Loss: 0.0854, Test Loss: 0.1975, F1: 0.9354, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0659, Test Loss: 0.2617, F1: 0.9291, AUC: 0.9798
Mejores resultados en la época:  8
f1-score 0.9368344627299129
AUC según el mejor F1-score 0.9827612159444293

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3228, Test Loss: 0.1993, F1: 0.9208, AUC: 0.9775
Epoch [10/30] Train Loss: 0.0766, Test Loss: 0.2011, F1: 0.9303, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0441, Test Loss: 0.2577, F1: 0.9256, AUC: 0.9807
Mejores resultados en la época:  2
f1-score 0.9333494617152535
AUC según el mejor F1-score 0.9826500024418509

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3272, Test Loss: 0.2008, F1: 0.9222, AUC: 0.9771
Epoch [10/30] Train Loss: 0.0782, Test Loss: 0.2065, F1: 0.9342, AUC: 0.9815
Epoch [20/30] Train Loss: 0.0517, Test Loss: 0.2659, F1: 0.9317, AUC: 0.9795
Mejores resultados en la época:  5
f1-score 0.9377289377289377
AUC según el mejor F1-score 0.9829708769344627
Epoch [0/30] Train Loss: 0.2972, Test Loss: 0.1730, F1: 0.8696, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0826, Test Loss: 0.1736, F1: 0.8784, AUC: 0.9836
Epoch [20/30] Train Loss: 0.0354, Test Loss: 0.2390, F1: 0.8696, AUC: 0.9830
Mejores resultados en la época:  1
f1-score 0.8790964636178485
AUC según el mejor F1-score 0.982786142557504
Confusion matrix Test saved: outputs_cv/5/tfidf/cm_mlp_1.png

========================================
Entrenando red 5 con capas [5023, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2338, Test Loss: 0.1816, F1: 0.9252, AUC: 0.9803
Epoch [10/30] Train Loss: 0.0022, Test Loss: 0.4745, F1: 0.9399, AUC: 0.9822
Epoch [20/30] Train Loss: 0.0007, Test Loss: 0.4610, F1: 0.9452, AUC: 0.9847
Mejores resultados en la época:  20
f1-score 0.9451640237259412
AUC según el mejor F1-score 0.9846678924606393

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2358, Test Loss: 0.1893, F1: 0.9257, AUC: 0.9807
Epoch [10/30] Train Loss: 0.0035, Test Loss: 0.3336, F1: 0.9406, AUC: 0.9854
Epoch [20/30] Train Loss: 0.0023, Test Loss: 0.4693, F1: 0.9439, AUC: 0.9846
Mejores resultados en la época:  23
f1-score 0.9459393346379648
AUC según el mejor F1-score 0.9858345328368188

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2385, Test Loss: 0.1831, F1: 0.9276, AUC: 0.9816
Epoch [10/30] Train Loss: 0.0040, Test Loss: 0.3405, F1: 0.9424, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6194, F1: 0.9452, AUC: 0.9833
Mejores resultados en la época:  6
f1-score 0.9464991023339318
AUC según el mejor F1-score 0.9850058249861036

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2374, Test Loss: 0.1748, F1: 0.9299, AUC: 0.9816
Epoch [10/30] Train Loss: 0.0049, Test Loss: 0.3105, F1: 0.9430, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.9600, F1: 0.9456, AUC: 0.9789
Mejores resultados en la época:  15
f1-score 0.9461193849134277
AUC según el mejor F1-score 0.9849934750921798

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2353, Test Loss: 0.1730, F1: 0.9322, AUC: 0.9814
Epoch [10/30] Train Loss: 0.0036, Test Loss: 0.4340, F1: 0.9435, AUC: 0.9851
Epoch [20/30] Train Loss: 0.0025, Test Loss: 0.3192, F1: 0.9470, AUC: 0.9864
Target F1-score 0.95 alcanzado en la época 26. Deteniendo entrenamiento.
Mejores resultados en la época:  26
f1-score 0.9500600240096039
AUC según el mejor F1-score 0.9865673831151633
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 15 ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']

CAT_LOW 3 ['emotion', 'Key', 'Time signature']
CAT_HIGH 12 ['Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
After removing some columns that ignorated, for TF-IDF embbedings you are selecteing this columns:
--> ['text']
After removing some columns that ignorated, for numeric cols you are selecteing this columns:
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
--> PaTH:  ../../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy
Running experiment with TFIDF embeddings
Contaning the categorical cols
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 5023)
Shape of X_test after concatenation:  (21625, 5023)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [5023, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3170, Test Loss: 0.2031, F1: 0.9221, AUC: 0.9757
Epoch [10/30] Train Loss: 0.0736, Test Loss: 0.2188, F1: 0.9285, AUC: 0.9805
Epoch [20/30] Train Loss: 0.0446, Test Loss: 0.2883, F1: 0.9261, AUC: 0.9787
Mejores resultados en la época:  8
f1-score 0.9300480769230769
AUC según el mejor F1-score 0.9812654456613183

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3150, Test Loss: 0.2035, F1: 0.9143, AUC: 0.9768
Epoch [10/30] Train Loss: 0.0845, Test Loss: 0.2046, F1: 0.9312, AUC: 0.9810
Epoch [20/30] Train Loss: 0.0658, Test Loss: 0.2615, F1: 0.9279, AUC: 0.9791
Mejores resultados en la época:  4
f1-score 0.9331870275542551
AUC según el mejor F1-score 0.9830132935971396

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3212, Test Loss: 0.1994, F1: 0.9235, AUC: 0.9774
Epoch [10/30] Train Loss: 0.0854, Test Loss: 0.1975, F1: 0.9354, AUC: 0.9823
Epoch [20/30] Train Loss: 0.0659, Test Loss: 0.2617, F1: 0.9291, AUC: 0.9798
Mejores resultados en la época:  8
f1-score 0.9368344627299129
AUC según el mejor F1-score 0.9827612159444293

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3228, Test Loss: 0.1993, F1: 0.9208, AUC: 0.9775
Epoch [10/30] Train Loss: 0.0766, Test Loss: 0.2011, F1: 0.9303, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0441, Test Loss: 0.2577, F1: 0.9256, AUC: 0.9807
Mejores resultados en la época:  2
f1-score 0.9333494617152535
AUC según el mejor F1-score 0.9826500024418509

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3272, Test Loss: 0.2008, F1: 0.9222, AUC: 0.9771
Epoch [10/30] Train Loss: 0.0782, Test Loss: 0.2065, F1: 0.9342, AUC: 0.9815
Epoch [20/30] Train Loss: 0.0517, Test Loss: 0.2659, F1: 0.9317, AUC: 0.9795
Mejores resultados en la época:  5
f1-score 0.9377289377289377
AUC según el mejor F1-score 0.9829708769344627
Epoch [0/30] Train Loss: 0.2972, Test Loss: 0.1730, F1: 0.8696, AUC: 0.9793
Epoch [10/30] Train Loss: 0.0826, Test Loss: 0.1736, F1: 0.8784, AUC: 0.9836
Epoch [20/30] Train Loss: 0.0354, Test Loss: 0.2390, F1: 0.8696, AUC: 0.9830
Mejores resultados en la época:  1
f1-score 0.8790964636178485
AUC según el mejor F1-score 0.982786142557504
Confusion matrix Test saved: outputs_cv/5/tfidf/cm_mlp_1.png

========================================
Entrenando red 5 con capas [5023, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2338, Test Loss: 0.1816, F1: 0.9252, AUC: 0.9803
Epoch [10/30] Train Loss: 0.0022, Test Loss: 0.4745, F1: 0.9399, AUC: 0.9822
Epoch [20/30] Train Loss: 0.0007, Test Loss: 0.4610, F1: 0.9452, AUC: 0.9847
Mejores resultados en la época:  20
f1-score 0.9451640237259412
AUC según el mejor F1-score 0.9846678924606393

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2358, Test Loss: 0.1893, F1: 0.9257, AUC: 0.9807
Epoch [10/30] Train Loss: 0.0035, Test Loss: 0.3336, F1: 0.9406, AUC: 0.9854
Epoch [20/30] Train Loss: 0.0023, Test Loss: 0.4693, F1: 0.9439, AUC: 0.9846
Mejores resultados en la época:  23
f1-score 0.9459393346379648
AUC según el mejor F1-score 0.9858345328368188

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2385, Test Loss: 0.1831, F1: 0.9276, AUC: 0.9816
Epoch [10/30] Train Loss: 0.0040, Test Loss: 0.3405, F1: 0.9424, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6194, F1: 0.9452, AUC: 0.9833
Mejores resultados en la época:  6
f1-score 0.9464991023339318
AUC según el mejor F1-score 0.9850058249861036

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2374, Test Loss: 0.1748, F1: 0.9299, AUC: 0.9816
Epoch [10/30] Train Loss: 0.0049, Test Loss: 0.3105, F1: 0.9430, AUC: 0.9860
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.9600, F1: 0.9456, AUC: 0.9789
Mejores resultados en la época:  15
f1-score 0.9461193849134277
AUC según el mejor F1-score 0.9849934750921798

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2353, Test Loss: 0.1730, F1: 0.9322, AUC: 0.9814
Epoch [10/30] Train Loss: 0.0036, Test Loss: 0.4340, F1: 0.9435, AUC: 0.9851
Epoch [20/30] Train Loss: 0.0025, Test Loss: 0.3192, F1: 0.9470, AUC: 0.9864
Target F1-score 0.95 alcanzado en la época 26. Deteniendo entrenamiento.
Mejores resultados en la época:  26
f1-score 0.9500600240096039
AUC según el mejor F1-score 0.9865673831151633
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Epoch [0/30] Train Loss: 0.2225, Test Loss: 0.2052, F1: 0.8483, AUC: 0.9825
Epoch [10/30] Train Loss: 0.0040, Test Loss: 0.3785, F1: 0.8787, AUC: 0.9861
Epoch [20/30] Train Loss: 0.0011, Test Loss: 0.5512, F1: 0.8939, AUC: 0.9860
Mejores resultados en la época:  17
f1-score 0.9085256712199717
AUC según el mejor F1-score 0.9875410549038718
Confusion matrix Test saved: outputs_cv/5/tfidf/cm_mlp_5.png

========================================
Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2314, Test Loss: 0.1865, F1: 0.9256, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0062, Test Loss: 0.3871, F1: 0.9407, AUC: 0.9851
Epoch [20/30] Train Loss: 0.0000, Test Loss: 1.3405, F1: 0.9442, AUC: 0.9769
Mejores resultados en la época:  29
f1-score 0.9447808186001453
AUC según el mejor F1-score 0.9758876032371102

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2333, Test Loss: 0.1814, F1: 0.9261, AUC: 0.9806
Epoch [10/30] Train Loss: 0.0048, Test Loss: 0.5288, F1: 0.9417, AUC: 0.9829
Epoch [20/30] Train Loss: 0.0011, Test Loss: 0.8005, F1: 0.9467, AUC: 0.9785
Mejores resultados en la época:  20
f1-score 0.9467069486404834
AUC según el mejor F1-score 0.9784943226631513

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2434, Test Loss: 0.1712, F1: 0.9300, AUC: 0.9827
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.6568, F1: 0.9388, AUC: 0.9850
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5016, F1: 0.9449, AUC: 0.9858
Mejores resultados en la época:  16
f1-score 0.9486959715929962
AUC según el mejor F1-score 0.9858939505119133

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2392, Test Loss: 0.1915, F1: 0.9218, AUC: 0.9814
Epoch [10/30] Train Loss: 0.0028, Test Loss: 0.3150, F1: 0.9408, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7454, F1: 0.9472, AUC: 0.9842
Mejores resultados en la época:  29
f1-score 0.948190888327924
AUC según el mejor F1-score 0.9824197581910016

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2377, Test Loss: 0.1819, F1: 0.9305, AUC: 0.9812
Epoch [10/30] Train Loss: 0.0041, Test Loss: 0.3818, F1: 0.9462, AUC: 0.9856
Epoch [20/30] Train Loss: 0.0002, Test Loss: 0.6759, F1: 0.9469, AUC: 0.9822
Mejores resultados en la época:  29
f1-score 0.9478028647730031
AUC según el mejor F1-score 0.9812896389911023
Epoch [0/30] Train Loss: 0.2260, Test Loss: 0.1506, F1: 0.8759, AUC: 0.9833
Epoch [10/30] Train Loss: 0.0053, Test Loss: 0.2501, F1: 0.9114, AUC: 0.9882
Epoch [20/30] Train Loss: 0.0011, Test Loss: 0.9639, F1: 0.8924, AUC: 0.9778
Mejores resultados en la época:  10
f1-score 0.9113536281830366
AUC según el mejor F1-score 0.9881538534876659
Confusion matrix Test saved: outputs_cv/5/tfidf/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9343, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.9334, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.9342, 'f1_cv_std': 0.0028, 'params': 160801, 'accuracy_test': 0.9396, 'precision_test': 0.8415, 'recall_test': 0.9202, 'f1_score_test': 0.8791}, 'MLP_2744833': {'accuracy_cv_mean': 0.9466, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9445, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9491, 'recall_cv_std': 0.0082, 'f1_cv_mean': 0.9468, 'f1_cv_std': 0.0017, 'params': 2744833, 'accuracy_test': 0.9551, 'precision_test': 0.884, 'recall_test': 0.9345, 'f1_score_test': 0.9085}, 'MLP_5843969': {'accuracy_cv_mean': 0.9473, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9478, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9468, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.9472, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9572, 'precision_test': 0.9005, 'recall_test': 0.9225, 'f1_score_test': 0.9114}}}
Saved on: outputs_cv/5/tfidf

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.928, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9345, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0048, 'f1_cv_mean': 0.9274, 'f1_cv_std': 0.0015}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 47, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.82      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.93     21625

Confusion matrix Test saved as: outputs_cv/5/tfidf/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/5/tfidf/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.832, 'accuracy_cv_std': 0.0121, 'precision_cv_mean': 0.7958, 'precision_cv_std': 0.0282, 'recall_cv_mean': 0.8967, 'recall_cv_std': 0.028, 'f1_cv_mean': 0.8423, 'f1_cv_std': 0.0075}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 47, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.60      0.74     16465
           1       0.42      0.92      0.58      5160

    accuracy                           0.68     21625
   macro avg       0.69      0.76      0.66     21625
weighted avg       0.83      0.68      0.70     21625

Confusion matrix Test saved as: outputs_cv/5/tfidf/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/5/tfidf/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8289, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8831, 'precision_cv_std': 0.0196, 'recall_cv_mean': 0.7595, 'recall_cv_std': 0.0255, 'f1_cv_mean': 0.816, 'f1_cv_std': 0.0067}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 47, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/5/tfidf/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/5/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8379, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.8738, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.7901, 'recall_cv_std': 0.0052, 'f1_cv_mean': 0.8298, 'f1_cv_std': 0.0038}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 47, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.87      0.87     21625

Confusion matrix Test saved as: outputs_cv/5/tfidf/conf_matrix_test_random_forest.png
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:16:34] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [0/30] Train Loss: 0.2225, Test Loss: 0.2052, F1: 0.8483, AUC: 0.9825
Epoch [10/30] Train Loss: 0.0040, Test Loss: 0.3785, F1: 0.8787, AUC: 0.9861
Epoch [20/30] Train Loss: 0.0011, Test Loss: 0.5512, F1: 0.8939, AUC: 0.9860
Mejores resultados en la época:  17
f1-score 0.9085256712199717
AUC según el mejor F1-score 0.9875410549038718
Confusion matrix Test saved: outputs_cv/5/tfidf/cm_mlp_5.png

========================================
Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2314, Test Loss: 0.1865, F1: 0.9256, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0062, Test Loss: 0.3871, F1: 0.9407, AUC: 0.9851
Epoch [20/30] Train Loss: 0.0000, Test Loss: 1.3405, F1: 0.9442, AUC: 0.9769
Mejores resultados en la época:  29
f1-score 0.9447808186001453
AUC según el mejor F1-score 0.9758876032371102

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2333, Test Loss: 0.1814, F1: 0.9261, AUC: 0.9806
Epoch [10/30] Train Loss: 0.0048, Test Loss: 0.5288, F1: 0.9417, AUC: 0.9829
Epoch [20/30] Train Loss: 0.0011, Test Loss: 0.8005, F1: 0.9467, AUC: 0.9785
Mejores resultados en la época:  20
f1-score 0.9467069486404834
AUC según el mejor F1-score 0.9784943226631513

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2434, Test Loss: 0.1712, F1: 0.9300, AUC: 0.9827
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.6568, F1: 0.9388, AUC: 0.9850
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5016, F1: 0.9449, AUC: 0.9858
Mejores resultados en la época:  16
f1-score 0.9486959715929962
AUC según el mejor F1-score 0.9858939505119133

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2392, Test Loss: 0.1915, F1: 0.9218, AUC: 0.9814
Epoch [10/30] Train Loss: 0.0028, Test Loss: 0.3150, F1: 0.9408, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7454, F1: 0.9472, AUC: 0.9842
Mejores resultados en la época:  29
f1-score 0.948190888327924
AUC según el mejor F1-score 0.9824197581910016

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2377, Test Loss: 0.1819, F1: 0.9305, AUC: 0.9812
Epoch [10/30] Train Loss: 0.0041, Test Loss: 0.3818, F1: 0.9462, AUC: 0.9856
Epoch [20/30] Train Loss: 0.0002, Test Loss: 0.6759, F1: 0.9469, AUC: 0.9822
Mejores resultados en la época:  29
f1-score 0.9478028647730031
AUC según el mejor F1-score 0.9812896389911023
Epoch [0/30] Train Loss: 0.2260, Test Loss: 0.1506, F1: 0.8759, AUC: 0.9833
Epoch [10/30] Train Loss: 0.0053, Test Loss: 0.2501, F1: 0.9114, AUC: 0.9882
Epoch [20/30] Train Loss: 0.0011, Test Loss: 0.9639, F1: 0.8924, AUC: 0.9778
Mejores resultados en la época:  10
f1-score 0.9113536281830366
AUC según el mejor F1-score 0.9881538534876659
Confusion matrix Test saved: outputs_cv/5/tfidf/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9343, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.9334, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.9342, 'f1_cv_std': 0.0028, 'params': 160801, 'accuracy_test': 0.9396, 'precision_test': 0.8415, 'recall_test': 0.9202, 'f1_score_test': 0.8791}, 'MLP_2744833': {'accuracy_cv_mean': 0.9466, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9445, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9491, 'recall_cv_std': 0.0082, 'f1_cv_mean': 0.9468, 'f1_cv_std': 0.0017, 'params': 2744833, 'accuracy_test': 0.9551, 'precision_test': 0.884, 'recall_test': 0.9345, 'f1_score_test': 0.9085}, 'MLP_5843969': {'accuracy_cv_mean': 0.9473, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9478, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9468, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.9472, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9572, 'precision_test': 0.9005, 'recall_test': 0.9225, 'f1_score_test': 0.9114}}}
Saved on: outputs_cv/5/tfidf

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.928, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9345, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0048, 'f1_cv_mean': 0.9274, 'f1_cv_std': 0.0015}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 47, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.82      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.93     21625

Confusion matrix Test saved as: outputs_cv/5/tfidf/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/5/tfidf/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.832, 'accuracy_cv_std': 0.0121, 'precision_cv_mean': 0.7958, 'precision_cv_std': 0.0282, 'recall_cv_mean': 0.8967, 'recall_cv_std': 0.028, 'f1_cv_mean': 0.8423, 'f1_cv_std': 0.0075}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 47, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.60      0.74     16465
           1       0.42      0.92      0.58      5160

    accuracy                           0.68     21625
   macro avg       0.69      0.76      0.66     21625
weighted avg       0.83      0.68      0.70     21625

Confusion matrix Test saved as: outputs_cv/5/tfidf/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/5/tfidf/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8289, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8831, 'precision_cv_std': 0.0196, 'recall_cv_mean': 0.7595, 'recall_cv_std': 0.0255, 'f1_cv_mean': 0.816, 'f1_cv_std': 0.0067}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 47, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.79      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/5/tfidf/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/5/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8379, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.8738, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.7901, 'recall_cv_std': 0.0052, 'f1_cv_mean': 0.8298, 'f1_cv_std': 0.0038}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 47, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.87      0.87     21625

Confusion matrix Test saved as: outputs_cv/5/tfidf/conf_matrix_test_random_forest.png
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:16:47] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:20:32] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:20:45] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:24:34] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:24:48] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:28:34] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:28:48] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:32:27] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:32:40] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:36:33] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:36:47] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Modelo guardado como: outputs_cv/5/tfidf/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9067, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9261, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.8839, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.9045, 'f1_cv_std': 0.0028}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 47, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.93      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/5/tfidf/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/5/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9038, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9303, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.873, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.003}
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/5/tfidf/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/5/tfidf/naive_bayes_model.pkl


Resumen Final:
Logistic Regression: {'accuracy_cv_mean': 0.928, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9345, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0048, 'f1_cv_mean': 0.9274, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9329, 'precision_test': 0.8194, 'recall_test': 0.9217, 'f1_score_test': 0.8676}
XGBoost: {'accuracy_cv_mean': 0.9067, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9261, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.8839, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.9045, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9206, 'precision_test': 0.7976, 'recall_test': 0.8938, 'f1_score_test': 0.843}
Naive Bayes: {'accuracy_cv_mean': 0.9038, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9303, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.873, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.003, 'accuracy_test': 0.9193, 'precision_test': 0.8014, 'recall_test': 0.8797, 'f1_score_test': 0.8387}
Random Forest: {'accuracy_cv_mean': 0.8379, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.8738, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.7901, 'recall_cv_std': 0.0052, 'f1_cv_mean': 0.8298, 'f1_cv_std': 0.0038, 'accuracy_test': 0.866, 'precision_test': 0.6928, 'recall_test': 0.788, 'f1_score_test': 0.7373}
Decision Tree: {'accuracy_cv_mean': 0.8289, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8831, 'precision_cv_std': 0.0196, 'recall_cv_mean': 0.7595, 'recall_cv_std': 0.0255, 'f1_cv_mean': 0.816, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8574, 'precision_test': 0.6715, 'recall_test': 0.788, 'f1_score_test': 0.7251}
SVM: {'accuracy_cv_mean': 0.832, 'accuracy_cv_std': 0.0121, 'precision_cv_mean': 0.7958, 'precision_cv_std': 0.0282, 'recall_cv_mean': 0.8967, 'recall_cv_std': 0.028, 'f1_cv_mean': 0.8423, 'f1_cv_std': 0.0075, 'accuracy_test': 0.6783, 'precision_test': 0.4203, 'recall_test': 0.9178, 'f1_score_test': 0.5765}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9343, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.9334, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.9342, 'f1_cv_std': 0.0028, 'params': 160801, 'accuracy_test': 0.9396, 'precision_test': 0.8415, 'recall_test': 0.9202, 'f1_score_test': 0.8791}, 'MLP_2744833': {'accuracy_cv_mean': 0.9466, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9445, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9491, 'recall_cv_std': 0.0082, 'f1_cv_mean': 0.9468, 'f1_cv_std': 0.0017, 'params': 2744833, 'accuracy_test': 0.9551, 'precision_test': 0.884, 'recall_test': 0.9345, 'f1_score_test': 0.9085}, 'MLP_5843969': {'accuracy_cv_mean': 0.9473, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9478, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9468, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.9472, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9572, 'precision_test': 0.9005, 'recall_test': 0.9225, 'f1_score_test': 0.9114}, 'Logistic Regression': {'accuracy_cv_mean': 0.928, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9345, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0048, 'f1_cv_mean': 0.9274, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9329, 'precision_test': 0.8194, 'recall_test': 0.9217, 'f1_score_test': 0.8676}, 'SVM': {'accuracy_cv_mean': 0.832, 'accuracy_cv_std': 0.0121, 'precision_cv_mean': 0.7958, 'precision_cv_std': 0.0282, 'recall_cv_mean': 0.8967, 'recall_cv_std': 0.028, 'f1_cv_mean': 0.8423, 'f1_cv_std': 0.0075, 'accuracy_test': 0.6783, 'precision_test': 0.4203, 'recall_test': 0.9178, 'f1_score_test': 0.5765}, 'Decision Tree': {'accuracy_cv_mean': 0.8289, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8831, 'precision_cv_std': 0.0196, 'recall_cv_mean': 0.7595, 'recall_cv_std': 0.0255, 'f1_cv_mean': 0.816, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8574, 'precision_test': 0.6715, 'recall_test': 0.788, 'f1_score_test': 0.7251}, 'Random Forest': {'accuracy_cv_mean': 0.8379, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.8738, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.7901, 'recall_cv_std': 0.0052, 'f1_cv_mean': 0.8298, 'f1_cv_std': 0.0038, 'accuracy_test': 0.866, 'precision_test': 0.6928, 'recall_test': 0.788, 'f1_score_test': 0.7373}, 'XGBoost': {'accuracy_cv_mean': 0.9067, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9261, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.8839, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.9045, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9206, 'precision_test': 0.7976, 'recall_test': 0.8938, 'f1_score_test': 0.843}, 'Naive Bayes': {'accuracy_cv_mean': 0.9038, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9303, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.873, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.003, 'accuracy_test': 0.9193, 'precision_test': 0.8014, 'recall_test': 0.8797, 'f1_score_test': 0.8387}}}
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_4.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Modelo guardado como: outputs_cv/5/tfidf/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9067, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9261, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.8839, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.9045, 'f1_cv_std': 0.0028}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 47, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.93      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/5/tfidf/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/5/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9038, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9303, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.873, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.003}
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/5/tfidf/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/5/tfidf/naive_bayes_model.pkl


Resumen Final:
Logistic Regression: {'accuracy_cv_mean': 0.928, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9345, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0048, 'f1_cv_mean': 0.9274, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9329, 'precision_test': 0.8194, 'recall_test': 0.9217, 'f1_score_test': 0.8676}
XGBoost: {'accuracy_cv_mean': 0.9067, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9261, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.8839, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.9045, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9206, 'precision_test': 0.7976, 'recall_test': 0.8938, 'f1_score_test': 0.843}
Naive Bayes: {'accuracy_cv_mean': 0.9038, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9303, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.873, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.003, 'accuracy_test': 0.9193, 'precision_test': 0.8014, 'recall_test': 0.8797, 'f1_score_test': 0.8387}
Random Forest: {'accuracy_cv_mean': 0.8379, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.8738, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.7901, 'recall_cv_std': 0.0052, 'f1_cv_mean': 0.8298, 'f1_cv_std': 0.0038, 'accuracy_test': 0.866, 'precision_test': 0.6928, 'recall_test': 0.788, 'f1_score_test': 0.7373}
Decision Tree: {'accuracy_cv_mean': 0.8289, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8831, 'precision_cv_std': 0.0196, 'recall_cv_mean': 0.7595, 'recall_cv_std': 0.0255, 'f1_cv_mean': 0.816, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8574, 'precision_test': 0.6715, 'recall_test': 0.788, 'f1_score_test': 0.7251}
SVM: {'accuracy_cv_mean': 0.832, 'accuracy_cv_std': 0.0121, 'precision_cv_mean': 0.7958, 'precision_cv_std': 0.0282, 'recall_cv_mean': 0.8967, 'recall_cv_std': 0.028, 'f1_cv_mean': 0.8423, 'f1_cv_std': 0.0075, 'accuracy_test': 0.6783, 'precision_test': 0.4203, 'recall_test': 0.9178, 'f1_score_test': 0.5765}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9343, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.9334, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.9342, 'f1_cv_std': 0.0028, 'params': 160801, 'accuracy_test': 0.9396, 'precision_test': 0.8415, 'recall_test': 0.9202, 'f1_score_test': 0.8791}, 'MLP_2744833': {'accuracy_cv_mean': 0.9466, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9445, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9491, 'recall_cv_std': 0.0082, 'f1_cv_mean': 0.9468, 'f1_cv_std': 0.0017, 'params': 2744833, 'accuracy_test': 0.9551, 'precision_test': 0.884, 'recall_test': 0.9345, 'f1_score_test': 0.9085}, 'MLP_5843969': {'accuracy_cv_mean': 0.9473, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9478, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9468, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.9472, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9572, 'precision_test': 0.9005, 'recall_test': 0.9225, 'f1_score_test': 0.9114}, 'Logistic Regression': {'accuracy_cv_mean': 0.928, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9345, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0048, 'f1_cv_mean': 0.9274, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9329, 'precision_test': 0.8194, 'recall_test': 0.9217, 'f1_score_test': 0.8676}, 'SVM': {'accuracy_cv_mean': 0.832, 'accuracy_cv_std': 0.0121, 'precision_cv_mean': 0.7958, 'precision_cv_std': 0.0282, 'recall_cv_mean': 0.8967, 'recall_cv_std': 0.028, 'f1_cv_mean': 0.8423, 'f1_cv_std': 0.0075, 'accuracy_test': 0.6783, 'precision_test': 0.4203, 'recall_test': 0.9178, 'f1_score_test': 0.5765}, 'Decision Tree': {'accuracy_cv_mean': 0.8289, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8831, 'precision_cv_std': 0.0196, 'recall_cv_mean': 0.7595, 'recall_cv_std': 0.0255, 'f1_cv_mean': 0.816, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8574, 'precision_test': 0.6715, 'recall_test': 0.788, 'f1_score_test': 0.7251}, 'Random Forest': {'accuracy_cv_mean': 0.8379, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.8738, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.7901, 'recall_cv_std': 0.0052, 'f1_cv_mean': 0.8298, 'f1_cv_std': 0.0038, 'accuracy_test': 0.866, 'precision_test': 0.6928, 'recall_test': 0.788, 'f1_score_test': 0.7373}, 'XGBoost': {'accuracy_cv_mean': 0.9067, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9261, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.8839, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.9045, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9206, 'precision_test': 0.7976, 'recall_test': 0.8938, 'f1_score_test': 0.843}, 'Naive Bayes': {'accuracy_cv_mean': 0.9038, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9303, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.873, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.003, 'accuracy_test': 0.9193, 'precision_test': 0.8014, 'recall_test': 0.8797, 'f1_score_test': 0.8387}}}
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_4.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 323)
Shape of X_test after concatenation:  (21625, 323)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 323)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 323)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [323, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.5468, Test Loss: 0.4335, F1: 0.8086, AUC: 0.8910
Epoch [10/30] Train Loss: 0.3591, Test Loss: 0.3485, F1: 0.8475, AUC: 0.9241
Epoch [20/30] Train Loss: 0.3535, Test Loss: 0.3426, F1: 0.8493, AUC: 0.9272
Mejores resultados en la época:  29
f1-score 0.8497023809523809
AUC según el mejor F1-score 0.9283894658711915

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.5399, Test Loss: 0.4545, F1: 0.7999, AUC: 0.8847
Epoch [10/30] Train Loss: 0.3562, Test Loss: 0.3810, F1: 0.8150, AUC: 0.9201
Epoch [20/30] Train Loss: 0.3420, Test Loss: 0.3474, F1: 0.8397, AUC: 0.9253
Mejores resultados en la época:  25
f1-score 0.8483511094765233
AUC según el mejor F1-score 0.9269836290074216

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.5566, Test Loss: 0.4604, F1: 0.7746, AUC: 0.8823
Epoch [10/30] Train Loss: 0.3615, Test Loss: 0.3574, F1: 0.8394, AUC: 0.9207
Epoch [20/30] Train Loss: 0.3515, Test Loss: 0.3605, F1: 0.8471, AUC: 0.9239
Mejores resultados en la época:  29
f1-score 0.8497397065783246
AUC según el mejor F1-score 0.9256856535627366

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.5438, Test Loss: 0.4400, F1: 0.7928, AUC: 0.8832
Epoch [10/30] Train Loss: 0.3614, Test Loss: 0.3665, F1: 0.8311, AUC: 0.9194
Epoch [20/30] Train Loss: 0.3510, Test Loss: 0.3710, F1: 0.8194, AUC: 0.9224
Mejores resultados en la época:  29
f1-score 0.8506079210304562
AUC según el mejor F1-score 0.9253775007842098

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.5382, Test Loss: 0.4405, F1: 0.8024, AUC: 0.8881
Epoch [10/30] Train Loss: 0.3555, Test Loss: 0.3680, F1: 0.8186, AUC: 0.9258
Epoch [20/30] Train Loss: 0.3423, Test Loss: 0.3407, F1: 0.8395, AUC: 0.9300
Mejores resultados en la época:  29
f1-score 0.8549713431348118
AUC según el mejor F1-score 0.9333449497354348
Epoch [0/30] Train Loss: 0.5052, Test Loss: 0.4374, F1: 0.6699, AUC: 0.8948
Epoch [10/30] Train Loss: 0.3529, Test Loss: 0.3126, F1: 0.7520, AUC: 0.9242
Epoch [20/30] Train Loss: 0.3449, Test Loss: 0.3018, F1: 0.7608, AUC: 0.9286
Mejores resultados en la época:  24
f1-score 0.7609917964789381
AUC según el mejor F1-score 0.929446329658637
Confusion matrix Test saved: outputs_cv/5/lyrics_bert/cm_mlp_1.png

========================================
Entrenando red 5 con capas [323, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4804, Test Loss: 0.3796, F1: 0.8292, AUC: 0.9100
Epoch [10/30] Train Loss: 0.3583, Test Loss: 0.3399, F1: 0.8467, AUC: 0.9300
Epoch [20/30] Train Loss: 0.3360, Test Loss: 0.3214, F1: 0.8523, AUC: 0.9353
Mejores resultados en la época:  28
f1-score 0.8579566563467492
AUC según el mejor F1-score 0.9383225753598042

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4812, Test Loss: 0.4303, F1: 0.8182, AUC: 0.9048
Epoch [10/30] Train Loss: 0.3549, Test Loss: 0.4180, F1: 0.8277, AUC: 0.9236
Epoch [20/30] Train Loss: 0.3297, Test Loss: 0.3377, F1: 0.8426, AUC: 0.9321
Mejores resultados en la época:  28
f1-score 0.8553823849220732
AUC según el mejor F1-score 0.9341571175860827

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4729, Test Loss: 0.3926, F1: 0.8263, AUC: 0.9072
Epoch [10/30] Train Loss: 0.3601, Test Loss: 0.3448, F1: 0.8399, AUC: 0.9284
Epoch [20/30] Train Loss: 0.3335, Test Loss: 0.3354, F1: 0.8356, AUC: 0.9335
Mejores resultados en la época:  27
f1-score 0.8579819100199695
AUC según el mejor F1-score 0.9371276492360734

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4750, Test Loss: 0.5094, F1: 0.6950, AUC: 0.9028
Epoch [10/30] Train Loss: 0.3496, Test Loss: 0.3519, F1: 0.8358, AUC: 0.9249
Epoch [20/30] Train Loss: 0.3333, Test Loss: 0.3456, F1: 0.8425, AUC: 0.9285
Mejores resultados en la época:  27
f1-score 0.8531066682628996
AUC según el mejor F1-score 0.932859778580458

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4871, Test Loss: 0.4736, F1: 0.7084, AUC: 0.9057
Epoch [10/30] Train Loss: 0.3554, Test Loss: 0.3408, F1: 0.8364, AUC: 0.9289
Epoch [20/30] Train Loss: 0.3349, Test Loss: 0.3667, F1: 0.8406, AUC: 0.9296
Mejores resultados en la época:  29
f1-score 0.8568398727465536
AUC según el mejor F1-score 0.9350278018832306
Epoch [0/30] Train Loss: 0.4612, Test Loss: 0.8421, F1: 0.5006, AUC: 0.9101
Epoch [10/30] Train Loss: 0.3505, Test Loss: 0.2930, F1: 0.7542, AUC: 0.9291
Epoch [20/30] Train Loss: 0.3275, Test Loss: 0.3960, F1: 0.7208, AUC: 0.9329
Mejores resultados en la época:  23
f1-score 0.7677816224529027
AUC según el mejor F1-score 0.9327528266442561
Confusion matrix Test saved: outputs_cv/5/lyrics_bert/cm_mlp_5.png

========================================
Entrenando red 6 con capas [323, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4788, Test Loss: 0.3795, F1: 0.8318, AUC: 0.9104
Epoch [10/30] Train Loss: 0.3533, Test Loss: 0.3422, F1: 0.8476, AUC: 0.9313
Epoch [20/30] Train Loss: 0.3320, Test Loss: 0.3234, F1: 0.8454, AUC: 0.9357
Mejores resultados en la época:  19
f1-score 0.855377302436126
AUC según el mejor F1-score 0.936051323726038

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4763, Test Loss: 0.3824, F1: 0.8279, AUC: 0.9072
Epoch [10/30] Train Loss: 0.3545, Test Loss: 0.3458, F1: 0.8432, AUC: 0.9264
Epoch [20/30] Train Loss: 0.3321, Test Loss: 0.3437, F1: 0.8475, AUC: 0.9307
Mejores resultados en la época:  29
f1-score 0.8517181129877693
AUC según el mejor F1-score 0.9322213044025298

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4751, Test Loss: 0.3874, F1: 0.8261, AUC: 0.9073
Epoch [10/30] Train Loss: 0.3508, Test Loss: 0.3873, F1: 0.8178, AUC: 0.9269
Epoch [20/30] Train Loss: 0.3332, Test Loss: 0.3580, F1: 0.8447, AUC: 0.9305
Mejores resultados en la época:  29
f1-score 0.854322749435129
AUC según el mejor F1-score 0.9339483194744908

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.5244, Test Loss: 0.4146, F1: 0.7862, AUC: 0.9005
Epoch [10/30] Train Loss: 0.3471, Test Loss: 0.3491, F1: 0.8352, AUC: 0.9269
Epoch [20/30] Train Loss: 0.3306, Test Loss: 0.3408, F1: 0.8416, AUC: 0.9300
Mejores resultados en la época:  16
f1-score 0.8526480124894921
AUC según el mejor F1-score 0.9309160416467092

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4813, Test Loss: 0.4601, F1: 0.8124, AUC: 0.9083
Epoch [10/30] Train Loss: 0.3614, Test Loss: 0.3470, F1: 0.8404, AUC: 0.9308
Epoch [20/30] Train Loss: 0.3350, Test Loss: 0.3349, F1: 0.8331, AUC: 0.9348
Mejores resultados en la época:  29
f1-score 0.8573825503355704
AUC según el mejor F1-score 0.9360574882180686
Epoch [0/30] Train Loss: 0.4650, Test Loss: 0.3114, F1: 0.7333, AUC: 0.9104
Epoch [10/30] Train Loss: 0.3489, Test Loss: 0.3927, F1: 0.7211, AUC: 0.9270
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 323)
Shape of X_test after concatenation:  (21625, 323)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 323)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 323)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [323, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.5468, Test Loss: 0.4335, F1: 0.8086, AUC: 0.8910
Epoch [10/30] Train Loss: 0.3591, Test Loss: 0.3485, F1: 0.8475, AUC: 0.9241
Epoch [20/30] Train Loss: 0.3535, Test Loss: 0.3426, F1: 0.8493, AUC: 0.9272
Mejores resultados en la época:  29
f1-score 0.8497023809523809
AUC según el mejor F1-score 0.9283894658711915

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.5399, Test Loss: 0.4545, F1: 0.7999, AUC: 0.8847
Epoch [10/30] Train Loss: 0.3562, Test Loss: 0.3810, F1: 0.8150, AUC: 0.9201
Epoch [20/30] Train Loss: 0.3420, Test Loss: 0.3474, F1: 0.8397, AUC: 0.9253
Mejores resultados en la época:  25
f1-score 0.8483511094765233
AUC según el mejor F1-score 0.9269836290074216

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.5566, Test Loss: 0.4604, F1: 0.7746, AUC: 0.8823
Epoch [10/30] Train Loss: 0.3615, Test Loss: 0.3574, F1: 0.8394, AUC: 0.9207
Epoch [20/30] Train Loss: 0.3515, Test Loss: 0.3605, F1: 0.8471, AUC: 0.9239
Mejores resultados en la época:  29
f1-score 0.8497397065783246
AUC según el mejor F1-score 0.9256856535627366

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.5438, Test Loss: 0.4400, F1: 0.7928, AUC: 0.8832
Epoch [10/30] Train Loss: 0.3614, Test Loss: 0.3665, F1: 0.8311, AUC: 0.9194
Epoch [20/30] Train Loss: 0.3510, Test Loss: 0.3710, F1: 0.8194, AUC: 0.9224
Mejores resultados en la época:  29
f1-score 0.8506079210304562
AUC según el mejor F1-score 0.9253775007842098

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.5382, Test Loss: 0.4405, F1: 0.8024, AUC: 0.8881
Epoch [10/30] Train Loss: 0.3555, Test Loss: 0.3680, F1: 0.8186, AUC: 0.9258
Epoch [20/30] Train Loss: 0.3423, Test Loss: 0.3407, F1: 0.8395, AUC: 0.9300
Mejores resultados en la época:  29
f1-score 0.8549713431348118
AUC según el mejor F1-score 0.9333449497354348
Epoch [0/30] Train Loss: 0.5052, Test Loss: 0.4374, F1: 0.6699, AUC: 0.8948
Epoch [10/30] Train Loss: 0.3529, Test Loss: 0.3126, F1: 0.7520, AUC: 0.9242
Epoch [20/30] Train Loss: 0.3449, Test Loss: 0.3018, F1: 0.7608, AUC: 0.9286
Mejores resultados en la época:  24
f1-score 0.7609917964789381
AUC según el mejor F1-score 0.929446329658637
Confusion matrix Test saved: outputs_cv/5/lyrics_bert/cm_mlp_1.png

========================================
Entrenando red 5 con capas [323, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4804, Test Loss: 0.3796, F1: 0.8292, AUC: 0.9100
Epoch [10/30] Train Loss: 0.3583, Test Loss: 0.3399, F1: 0.8467, AUC: 0.9300
Epoch [20/30] Train Loss: 0.3360, Test Loss: 0.3214, F1: 0.8523, AUC: 0.9353
Mejores resultados en la época:  28
f1-score 0.8579566563467492
AUC según el mejor F1-score 0.9383225753598042

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4812, Test Loss: 0.4303, F1: 0.8182, AUC: 0.9048
Epoch [10/30] Train Loss: 0.3549, Test Loss: 0.4180, F1: 0.8277, AUC: 0.9236
Epoch [20/30] Train Loss: 0.3297, Test Loss: 0.3377, F1: 0.8426, AUC: 0.9321
Mejores resultados en la época:  28
f1-score 0.8553823849220732
AUC según el mejor F1-score 0.9341571175860827

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4729, Test Loss: 0.3926, F1: 0.8263, AUC: 0.9072
Epoch [10/30] Train Loss: 0.3601, Test Loss: 0.3448, F1: 0.8399, AUC: 0.9284
Epoch [20/30] Train Loss: 0.3335, Test Loss: 0.3354, F1: 0.8356, AUC: 0.9335
Mejores resultados en la época:  27
f1-score 0.8579819100199695
AUC según el mejor F1-score 0.9371276492360734

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4750, Test Loss: 0.5094, F1: 0.6950, AUC: 0.9028
Epoch [10/30] Train Loss: 0.3496, Test Loss: 0.3519, F1: 0.8358, AUC: 0.9249
Epoch [20/30] Train Loss: 0.3333, Test Loss: 0.3456, F1: 0.8425, AUC: 0.9285
Mejores resultados en la época:  27
f1-score 0.8531066682628996
AUC según el mejor F1-score 0.932859778580458

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4871, Test Loss: 0.4736, F1: 0.7084, AUC: 0.9057
Epoch [10/30] Train Loss: 0.3554, Test Loss: 0.3408, F1: 0.8364, AUC: 0.9289
Epoch [20/30] Train Loss: 0.3349, Test Loss: 0.3667, F1: 0.8406, AUC: 0.9296
Mejores resultados en la época:  29
f1-score 0.8568398727465536
AUC según el mejor F1-score 0.9350278018832306
Epoch [0/30] Train Loss: 0.4612, Test Loss: 0.8421, F1: 0.5006, AUC: 0.9101
Epoch [10/30] Train Loss: 0.3505, Test Loss: 0.2930, F1: 0.7542, AUC: 0.9291
Epoch [20/30] Train Loss: 0.3275, Test Loss: 0.3960, F1: 0.7208, AUC: 0.9329
Mejores resultados en la época:  23
f1-score 0.7677816224529027
AUC según el mejor F1-score 0.9327528266442561
Confusion matrix Test saved: outputs_cv/5/lyrics_bert/cm_mlp_5.png

========================================
Entrenando red 6 con capas [323, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4788, Test Loss: 0.3795, F1: 0.8318, AUC: 0.9104
Epoch [10/30] Train Loss: 0.3533, Test Loss: 0.3422, F1: 0.8476, AUC: 0.9313
Epoch [20/30] Train Loss: 0.3320, Test Loss: 0.3234, F1: 0.8454, AUC: 0.9357
Mejores resultados en la época:  19
f1-score 0.855377302436126
AUC según el mejor F1-score 0.936051323726038

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4763, Test Loss: 0.3824, F1: 0.8279, AUC: 0.9072
Epoch [10/30] Train Loss: 0.3545, Test Loss: 0.3458, F1: 0.8432, AUC: 0.9264
Epoch [20/30] Train Loss: 0.3321, Test Loss: 0.3437, F1: 0.8475, AUC: 0.9307
Mejores resultados en la época:  29
f1-score 0.8517181129877693
AUC según el mejor F1-score 0.9322213044025298

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4751, Test Loss: 0.3874, F1: 0.8261, AUC: 0.9073
Epoch [10/30] Train Loss: 0.3508, Test Loss: 0.3873, F1: 0.8178, AUC: 0.9269
Epoch [20/30] Train Loss: 0.3332, Test Loss: 0.3580, F1: 0.8447, AUC: 0.9305
Mejores resultados en la época:  29
f1-score 0.854322749435129
AUC según el mejor F1-score 0.9339483194744908

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.5244, Test Loss: 0.4146, F1: 0.7862, AUC: 0.9005
Epoch [10/30] Train Loss: 0.3471, Test Loss: 0.3491, F1: 0.8352, AUC: 0.9269
Epoch [20/30] Train Loss: 0.3306, Test Loss: 0.3408, F1: 0.8416, AUC: 0.9300
Mejores resultados en la época:  16
f1-score 0.8526480124894921
AUC según el mejor F1-score 0.9309160416467092

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4813, Test Loss: 0.4601, F1: 0.8124, AUC: 0.9083
Epoch [10/30] Train Loss: 0.3614, Test Loss: 0.3470, F1: 0.8404, AUC: 0.9308
Epoch [20/30] Train Loss: 0.3350, Test Loss: 0.3349, F1: 0.8331, AUC: 0.9348
Mejores resultados en la época:  29
f1-score 0.8573825503355704
AUC según el mejor F1-score 0.9360574882180686
Epoch [0/30] Train Loss: 0.4650, Test Loss: 0.3114, F1: 0.7333, AUC: 0.9104
Epoch [10/30] Train Loss: 0.3489, Test Loss: 0.3927, F1: 0.7211, AUC: 0.9270
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Epoch [20/30] Train Loss: 0.3253, Test Loss: 0.2945, F1: 0.7640, AUC: 0.9343
Mejores resultados en la época:  27
f1-score 0.7742672249714503
AUC según el mejor F1-score 0.9375302615131462
Confusion matrix Test saved: outputs_cv/5/lyrics_bert/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9343, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.9334, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.9342, 'f1_cv_std': 0.0028, 'params': 160801, 'accuracy_test': 0.9396, 'precision_test': 0.8415, 'recall_test': 0.9202, 'f1_score_test': 0.8791}, 'MLP_2744833': {'accuracy_cv_mean': 0.9466, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9445, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9491, 'recall_cv_std': 0.0082, 'f1_cv_mean': 0.9468, 'f1_cv_std': 0.0017, 'params': 2744833, 'accuracy_test': 0.9551, 'precision_test': 0.884, 'recall_test': 0.9345, 'f1_score_test': 0.9085}, 'MLP_5843969': {'accuracy_cv_mean': 0.9473, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9478, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9468, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.9472, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9572, 'precision_test': 0.9005, 'recall_test': 0.9225, 'f1_score_test': 0.9114}, 'Logistic Regression': {'accuracy_cv_mean': 0.928, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9345, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0048, 'f1_cv_mean': 0.9274, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9329, 'precision_test': 0.8194, 'recall_test': 0.9217, 'f1_score_test': 0.8676}, 'SVM': {'accuracy_cv_mean': 0.832, 'accuracy_cv_std': 0.0121, 'precision_cv_mean': 0.7958, 'precision_cv_std': 0.0282, 'recall_cv_mean': 0.8967, 'recall_cv_std': 0.028, 'f1_cv_mean': 0.8423, 'f1_cv_std': 0.0075, 'accuracy_test': 0.6783, 'precision_test': 0.4203, 'recall_test': 0.9178, 'f1_score_test': 0.5765}, 'Decision Tree': {'accuracy_cv_mean': 0.8289, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8831, 'precision_cv_std': 0.0196, 'recall_cv_mean': 0.7595, 'recall_cv_std': 0.0255, 'f1_cv_mean': 0.816, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8574, 'precision_test': 0.6715, 'recall_test': 0.788, 'f1_score_test': 0.7251}, 'Random Forest': {'accuracy_cv_mean': 0.8379, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.8738, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.7901, 'recall_cv_std': 0.0052, 'f1_cv_mean': 0.8298, 'f1_cv_std': 0.0038, 'accuracy_test': 0.866, 'precision_test': 0.6928, 'recall_test': 0.788, 'f1_score_test': 0.7373}, 'XGBoost': {'accuracy_cv_mean': 0.9067, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9261, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.8839, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.9045, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9206, 'precision_test': 0.7976, 'recall_test': 0.8938, 'f1_score_test': 0.843}, 'Naive Bayes': {'accuracy_cv_mean': 0.9038, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9303, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.873, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.003, 'accuracy_test': 0.9193, 'precision_test': 0.8014, 'recall_test': 0.8797, 'f1_score_test': 0.8387}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8516, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.857, 'precision_cv_std': 0.0177, 'recall_cv_mean': 0.8451, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0023, 'params': 10401, 'accuracy_test': 0.8801, 'precision_test': 0.7256, 'recall_test': 0.8, 'f1_score_test': 0.761}, 'MLP_338433': {'accuracy_cv_mean': 0.8548, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8482, 'precision_cv_std': 0.0164, 'recall_cv_mean': 0.8651, 'recall_cv_std': 0.0165, 'f1_cv_mean': 0.8563, 'f1_cv_std': 0.0018, 'params': 338433, 'accuracy_test': 0.8883, 'precision_test': 0.7616, 'recall_test': 0.774, 'f1_score_test': 0.7678}, 'MLP_1031169': {'accuracy_cv_mean': 0.8514, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8385, 'precision_cv_std': 0.0098, 'recall_cv_mean': 0.8709, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.8543, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8903, 'precision_test': 0.7607, 'recall_test': 0.7884, 'f1_score_test': 0.7743}}}
Saved on: outputs_cv/5/lyrics_bert

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8594, 'precision_cv_std': 0.0025, 'recall_cv_mean': 0.8341, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8466, 'f1_cv_std': 0.0021}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 47, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.87      0.90     16465
           1       0.66      0.83      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.85      0.82     21625
weighted avg       0.88      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/5/lyrics_bert/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/5/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.6642, 'accuracy_cv_std': 0.0289, 'precision_cv_mean': 0.6353, 'precision_cv_std': 0.0321, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0306, 'f1_cv_mean': 0.6989, 'f1_cv_std': 0.0209}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 47, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.88      0.49      0.63     16465
           1       0.33      0.79      0.47      5160

    accuracy                           0.57     21625
   macro avg       0.61      0.64      0.55     21625
weighted avg       0.75      0.57      0.59     21625

Confusion matrix Test saved as: outputs_cv/5/lyrics_bert/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/5/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7964, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8073, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.7927, 'f1_cv_std': 0.0038}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 47, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.84      0.88     16465
           1       0.61      0.76      0.67      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.80      0.78     21625
weighted avg       0.84      0.82      0.83     21625

Confusion matrix Test saved as: outputs_cv/5/lyrics_bert/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/5/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8535, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8848, 'precision_cv_std': 0.0055, 'recall_cv_mean': 0.8129, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0033}
Epoch [20/30] Train Loss: 0.3253, Test Loss: 0.2945, F1: 0.7640, AUC: 0.9343
Mejores resultados en la época:  27
f1-score 0.7742672249714503
AUC según el mejor F1-score 0.9375302615131462
Confusion matrix Test saved: outputs_cv/5/lyrics_bert/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9343, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.9334, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.9342, 'f1_cv_std': 0.0028, 'params': 160801, 'accuracy_test': 0.9396, 'precision_test': 0.8415, 'recall_test': 0.9202, 'f1_score_test': 0.8791}, 'MLP_2744833': {'accuracy_cv_mean': 0.9466, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9445, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9491, 'recall_cv_std': 0.0082, 'f1_cv_mean': 0.9468, 'f1_cv_std': 0.0017, 'params': 2744833, 'accuracy_test': 0.9551, 'precision_test': 0.884, 'recall_test': 0.9345, 'f1_score_test': 0.9085}, 'MLP_5843969': {'accuracy_cv_mean': 0.9473, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9478, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9468, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.9472, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9572, 'precision_test': 0.9005, 'recall_test': 0.9225, 'f1_score_test': 0.9114}, 'Logistic Regression': {'accuracy_cv_mean': 0.928, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9345, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0048, 'f1_cv_mean': 0.9274, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9329, 'precision_test': 0.8194, 'recall_test': 0.9217, 'f1_score_test': 0.8676}, 'SVM': {'accuracy_cv_mean': 0.832, 'accuracy_cv_std': 0.0121, 'precision_cv_mean': 0.7958, 'precision_cv_std': 0.0282, 'recall_cv_mean': 0.8967, 'recall_cv_std': 0.028, 'f1_cv_mean': 0.8423, 'f1_cv_std': 0.0075, 'accuracy_test': 0.6783, 'precision_test': 0.4203, 'recall_test': 0.9178, 'f1_score_test': 0.5765}, 'Decision Tree': {'accuracy_cv_mean': 0.8289, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8831, 'precision_cv_std': 0.0196, 'recall_cv_mean': 0.7595, 'recall_cv_std': 0.0255, 'f1_cv_mean': 0.816, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8574, 'precision_test': 0.6715, 'recall_test': 0.788, 'f1_score_test': 0.7251}, 'Random Forest': {'accuracy_cv_mean': 0.8379, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.8738, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.7901, 'recall_cv_std': 0.0052, 'f1_cv_mean': 0.8298, 'f1_cv_std': 0.0038, 'accuracy_test': 0.866, 'precision_test': 0.6928, 'recall_test': 0.788, 'f1_score_test': 0.7373}, 'XGBoost': {'accuracy_cv_mean': 0.9067, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9261, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.8839, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.9045, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9206, 'precision_test': 0.7976, 'recall_test': 0.8938, 'f1_score_test': 0.843}, 'Naive Bayes': {'accuracy_cv_mean': 0.9038, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9303, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.873, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.003, 'accuracy_test': 0.9193, 'precision_test': 0.8014, 'recall_test': 0.8797, 'f1_score_test': 0.8387}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8516, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.857, 'precision_cv_std': 0.0177, 'recall_cv_mean': 0.8451, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0023, 'params': 10401, 'accuracy_test': 0.8801, 'precision_test': 0.7256, 'recall_test': 0.8, 'f1_score_test': 0.761}, 'MLP_338433': {'accuracy_cv_mean': 0.8548, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8482, 'precision_cv_std': 0.0164, 'recall_cv_mean': 0.8651, 'recall_cv_std': 0.0165, 'f1_cv_mean': 0.8563, 'f1_cv_std': 0.0018, 'params': 338433, 'accuracy_test': 0.8883, 'precision_test': 0.7616, 'recall_test': 0.774, 'f1_score_test': 0.7678}, 'MLP_1031169': {'accuracy_cv_mean': 0.8514, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8385, 'precision_cv_std': 0.0098, 'recall_cv_mean': 0.8709, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.8543, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8903, 'precision_test': 0.7607, 'recall_test': 0.7884, 'f1_score_test': 0.7743}}}
Saved on: outputs_cv/5/lyrics_bert

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8594, 'precision_cv_std': 0.0025, 'recall_cv_mean': 0.8341, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8466, 'f1_cv_std': 0.0021}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 47, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.87      0.90     16465
           1       0.66      0.83      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.85      0.82     21625
weighted avg       0.88      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/5/lyrics_bert/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/5/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.6642, 'accuracy_cv_std': 0.0289, 'precision_cv_mean': 0.6353, 'precision_cv_std': 0.0321, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0306, 'f1_cv_mean': 0.6989, 'f1_cv_std': 0.0209}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 47, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.88      0.49      0.63     16465
           1       0.33      0.79      0.47      5160

    accuracy                           0.57     21625
   macro avg       0.61      0.64      0.55     21625
weighted avg       0.75      0.57      0.59     21625

Confusion matrix Test saved as: outputs_cv/5/lyrics_bert/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/5/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7964, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8073, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.7927, 'f1_cv_std': 0.0038}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 47, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.84      0.88     16465
           1       0.61      0.76      0.67      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.80      0.78     21625
weighted avg       0.84      0.82      0.83     21625

Confusion matrix Test saved as: outputs_cv/5/lyrics_bert/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/5/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8535, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8848, 'precision_cv_std': 0.0055, 'recall_cv_mean': 0.8129, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0033}
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:27:44] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:28:00] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:28:31] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:28:50] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:29:20] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:29:38] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:30:09] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:30:27] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:30:57] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:31:16] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:31:47] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:32:05] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 47, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.90      0.92     16465
           1       0.71      0.82      0.76      5160

    accuracy                           0.88     21625
   macro avg       0.82      0.86      0.84     21625
weighted avg       0.88      0.88      0.88     21625

Confusion matrix Test saved as: outputs_cv/5/lyrics_bert/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/5/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8762, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8923, 'precision_cv_std': 0.0032, 'recall_cv_mean': 0.8558, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.8736, 'f1_cv_std': 0.0027}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 47, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.90      0.93     16465
           1       0.73      0.87      0.80      5160

    accuracy                           0.89     21625
   macro avg       0.84      0.88      0.86     21625
weighted avg       0.90      0.89      0.90     21625

Confusion matrix Test saved as: outputs_cv/5/lyrics_bert/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/5/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7807, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.7651, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8102, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.787, 'f1_cv_std': 0.0027}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.75      0.83     16465
           1       0.50      0.81      0.62      5160

    accuracy                           0.77     21625
   macro avg       0.72      0.78      0.73     21625
weighted avg       0.83      0.77      0.78     21625

Confusion matrix Test saved as: outputs_cv/5/lyrics_bert/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/5/lyrics_bert/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.8762, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8923, 'precision_cv_std': 0.0032, 'recall_cv_mean': 0.8558, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.8736, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8933, 'precision_test': 0.7335, 'recall_test': 0.8682, 'f1_score_test': 0.7952}
Random Forest: {'accuracy_cv_mean': 0.8535, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8848, 'precision_cv_std': 0.0055, 'recall_cv_mean': 0.8129, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0033, 'accuracy_test': 0.8767, 'precision_test': 0.7104, 'recall_test': 0.8159, 'f1_score_test': 0.7595}
Logistic Regression: {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8594, 'precision_cv_std': 0.0025, 'recall_cv_mean': 0.8341, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8466, 'f1_cv_std': 0.0021, 'accuracy_test': 0.8592, 'precision_test': 0.6632, 'recall_test': 0.8331, 'f1_score_test': 0.7385}
Decision Tree: {'accuracy_cv_mean': 0.7964, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8073, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.7927, 'f1_cv_std': 0.0038, 'accuracy_test': 0.8246, 'precision_test': 0.6052, 'recall_test': 0.762, 'f1_score_test': 0.6746}
Naive Bayes: {'accuracy_cv_mean': 0.7807, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.7651, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8102, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.787, 'f1_cv_std': 0.0027, 'accuracy_test': 0.765, 'precision_test': 0.5047, 'recall_test': 0.8136, 'f1_score_test': 0.623}
SVM: {'accuracy_cv_mean': 0.6642, 'accuracy_cv_std': 0.0289, 'precision_cv_mean': 0.6353, 'precision_cv_std': 0.0321, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0306, 'f1_cv_mean': 0.6989, 'f1_cv_std': 0.0209, 'accuracy_test': 0.5661, 'precision_test': 0.3298, 'recall_test': 0.7932, 'f1_score_test': 0.4659}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9343, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.9334, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.9342, 'f1_cv_std': 0.0028, 'params': 160801, 'accuracy_test': 0.9396, 'precision_test': 0.8415, 'recall_test': 0.9202, 'f1_score_test': 0.8791}, 'MLP_2744833': {'accuracy_cv_mean': 0.9466, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9445, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9491, 'recall_cv_std': 0.0082, 'f1_cv_mean': 0.9468, 'f1_cv_std': 0.0017, 'params': 2744833, 'accuracy_test': 0.9551, 'precision_test': 0.884, 'recall_test': 0.9345, 'f1_score_test': 0.9085}, 'MLP_5843969': {'accuracy_cv_mean': 0.9473, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9478, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9468, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.9472, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9572, 'precision_test': 0.9005, 'recall_test': 0.9225, 'f1_score_test': 0.9114}, 'Logistic Regression': {'accuracy_cv_mean': 0.928, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9345, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0048, 'f1_cv_mean': 0.9274, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9329, 'precision_test': 0.8194, 'recall_test': 0.9217, 'f1_score_test': 0.8676}, 'SVM': {'accuracy_cv_mean': 0.832, 'accuracy_cv_std': 0.0121, 'precision_cv_mean': 0.7958, 'precision_cv_std': 0.0282, 'recall_cv_mean': 0.8967, 'recall_cv_std': 0.028, 'f1_cv_mean': 0.8423, 'f1_cv_std': 0.0075, 'accuracy_test': 0.6783, 'precision_test': 0.4203, 'recall_test': 0.9178, 'f1_score_test': 0.5765}, 'Decision Tree': {'accuracy_cv_mean': 0.8289, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8831, 'precision_cv_std': 0.0196, 'recall_cv_mean': 0.7595, 'recall_cv_std': 0.0255, 'f1_cv_mean': 0.816, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8574, 'precision_test': 0.6715, 'recall_test': 0.788, 'f1_score_test': 0.7251}, 'Random Forest': {'accuracy_cv_mean': 0.8379, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.8738, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.7901, 'recall_cv_std': 0.0052, 'f1_cv_mean': 0.8298, 'f1_cv_std': 0.0038, 'accuracy_test': 0.866, 'precision_test': 0.6928, 'recall_test': 0.788, 'f1_score_test': 0.7373}, 'XGBoost': {'accuracy_cv_mean': 0.9067, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9261, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.8839, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.9045, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9206, 'precision_test': 0.7976, 'recall_test': 0.8938, 'f1_score_test': 0.843}, 'Naive Bayes': {'accuracy_cv_mean': 0.9038, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9303, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.873, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.003, 'accuracy_test': 0.9193, 'precision_test': 0.8014, 'recall_test': 0.8797, 'f1_score_test': 0.8387}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8516, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.857, 'precision_cv_std': 0.0177, 'recall_cv_mean': 0.8451, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0023, 'params': 10401, 'accuracy_test': 0.8801, 'precision_test': 0.7256, 'recall_test': 0.8, 'f1_score_test': 0.761}, 'MLP_338433': {'accuracy_cv_mean': 0.8548, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8482, 'precision_cv_std': 0.0164, 'recall_cv_mean': 0.8651, 'recall_cv_std': 0.0165, 'f1_cv_mean': 0.8563, 'f1_cv_std': 0.0018, 'params': 338433, 'accuracy_test': 0.8883, 'precision_test': 0.7616, 'recall_test': 0.774, 'f1_score_test': 0.7678}, 'MLP_1031169': {'accuracy_cv_mean': 0.8514, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8385, 'precision_cv_std': 0.0098, 'recall_cv_mean': 0.8709, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.8543, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8903, 'precision_test': 0.7607, 'recall_test': 0.7884, 'f1_score_test': 0.7743}, 'Logistic Regression': {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8594, 'precision_cv_std': 0.0025, 'recall_cv_mean': 0.8341, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8466, 'f1_cv_std': 0.0021, 'accuracy_test': 0.8592, 'precision_test': 0.6632, 'recall_test': 0.8331, 'f1_score_test': 0.7385}, 'SVM': {'accuracy_cv_mean': 0.6642, 'accuracy_cv_std': 0.0289, 'precision_cv_mean': 0.6353, 'precision_cv_std': 0.0321, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0306, 'f1_cv_mean': 0.6989, 'f1_cv_std': 0.0209, 'accuracy_test': 0.5661, 'precision_test': 0.3298, 'recall_test': 0.7932, 'f1_score_test': 0.4659}, 'Decision Tree': {'accuracy_cv_mean': 0.7964, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8073, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.7927, 'f1_cv_std': 0.0038, 'accuracy_test': 0.8246, 'precision_test': 0.6052, 'recall_test': 0.762, 'f1_score_test': 0.6746}, 'Random Forest': {'accuracy_cv_mean': 0.8535, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8848, 'precision_cv_std': 0.0055, 'recall_cv_mean': /home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_4.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 47, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.90      0.92     16465
           1       0.71      0.82      0.76      5160

    accuracy                           0.88     21625
   macro avg       0.82      0.86      0.84     21625
weighted avg       0.88      0.88      0.88     21625

Confusion matrix Test saved as: outputs_cv/5/lyrics_bert/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/5/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8762, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8923, 'precision_cv_std': 0.0032, 'recall_cv_mean': 0.8558, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.8736, 'f1_cv_std': 0.0027}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 47, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.90      0.93     16465
           1       0.73      0.87      0.80      5160

    accuracy                           0.89     21625
   macro avg       0.84      0.88      0.86     21625
weighted avg       0.90      0.89      0.90     21625

Confusion matrix Test saved as: outputs_cv/5/lyrics_bert/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/5/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7807, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.7651, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8102, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.787, 'f1_cv_std': 0.0027}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.75      0.83     16465
           1       0.50      0.81      0.62      5160

    accuracy                           0.77     21625
   macro avg       0.72      0.78      0.73     21625
weighted avg       0.83      0.77      0.78     21625

Confusion matrix Test saved as: outputs_cv/5/lyrics_bert/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/5/lyrics_bert/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.8762, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8923, 'precision_cv_std': 0.0032, 'recall_cv_mean': 0.8558, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.8736, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8933, 'precision_test': 0.7335, 'recall_test': 0.8682, 'f1_score_test': 0.7952}
Random Forest: {'accuracy_cv_mean': 0.8535, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8848, 'precision_cv_std': 0.0055, 'recall_cv_mean': 0.8129, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0033, 'accuracy_test': 0.8767, 'precision_test': 0.7104, 'recall_test': 0.8159, 'f1_score_test': 0.7595}
Logistic Regression: {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8594, 'precision_cv_std': 0.0025, 'recall_cv_mean': 0.8341, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8466, 'f1_cv_std': 0.0021, 'accuracy_test': 0.8592, 'precision_test': 0.6632, 'recall_test': 0.8331, 'f1_score_test': 0.7385}
Decision Tree: {'accuracy_cv_mean': 0.7964, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8073, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.7927, 'f1_cv_std': 0.0038, 'accuracy_test': 0.8246, 'precision_test': 0.6052, 'recall_test': 0.762, 'f1_score_test': 0.6746}
Naive Bayes: {'accuracy_cv_mean': 0.7807, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.7651, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8102, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.787, 'f1_cv_std': 0.0027, 'accuracy_test': 0.765, 'precision_test': 0.5047, 'recall_test': 0.8136, 'f1_score_test': 0.623}
SVM: {'accuracy_cv_mean': 0.6642, 'accuracy_cv_std': 0.0289, 'precision_cv_mean': 0.6353, 'precision_cv_std': 0.0321, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0306, 'f1_cv_mean': 0.6989, 'f1_cv_std': 0.0209, 'accuracy_test': 0.5661, 'precision_test': 0.3298, 'recall_test': 0.7932, 'f1_score_test': 0.4659}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9343, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.9334, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.9342, 'f1_cv_std': 0.0028, 'params': 160801, 'accuracy_test': 0.9396, 'precision_test': 0.8415, 'recall_test': 0.9202, 'f1_score_test': 0.8791}, 'MLP_2744833': {'accuracy_cv_mean': 0.9466, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9445, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9491, 'recall_cv_std': 0.0082, 'f1_cv_mean': 0.9468, 'f1_cv_std': 0.0017, 'params': 2744833, 'accuracy_test': 0.9551, 'precision_test': 0.884, 'recall_test': 0.9345, 'f1_score_test': 0.9085}, 'MLP_5843969': {'accuracy_cv_mean': 0.9473, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9478, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9468, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.9472, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9572, 'precision_test': 0.9005, 'recall_test': 0.9225, 'f1_score_test': 0.9114}, 'Logistic Regression': {'accuracy_cv_mean': 0.928, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9345, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0048, 'f1_cv_mean': 0.9274, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9329, 'precision_test': 0.8194, 'recall_test': 0.9217, 'f1_score_test': 0.8676}, 'SVM': {'accuracy_cv_mean': 0.832, 'accuracy_cv_std': 0.0121, 'precision_cv_mean': 0.7958, 'precision_cv_std': 0.0282, 'recall_cv_mean': 0.8967, 'recall_cv_std': 0.028, 'f1_cv_mean': 0.8423, 'f1_cv_std': 0.0075, 'accuracy_test': 0.6783, 'precision_test': 0.4203, 'recall_test': 0.9178, 'f1_score_test': 0.5765}, 'Decision Tree': {'accuracy_cv_mean': 0.8289, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8831, 'precision_cv_std': 0.0196, 'recall_cv_mean': 0.7595, 'recall_cv_std': 0.0255, 'f1_cv_mean': 0.816, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8574, 'precision_test': 0.6715, 'recall_test': 0.788, 'f1_score_test': 0.7251}, 'Random Forest': {'accuracy_cv_mean': 0.8379, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.8738, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.7901, 'recall_cv_std': 0.0052, 'f1_cv_mean': 0.8298, 'f1_cv_std': 0.0038, 'accuracy_test': 0.866, 'precision_test': 0.6928, 'recall_test': 0.788, 'f1_score_test': 0.7373}, 'XGBoost': {'accuracy_cv_mean': 0.9067, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9261, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.8839, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.9045, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9206, 'precision_test': 0.7976, 'recall_test': 0.8938, 'f1_score_test': 0.843}, 'Naive Bayes': {'accuracy_cv_mean': 0.9038, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9303, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.873, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.003, 'accuracy_test': 0.9193, 'precision_test': 0.8014, 'recall_test': 0.8797, 'f1_score_test': 0.8387}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8516, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.857, 'precision_cv_std': 0.0177, 'recall_cv_mean': 0.8451, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0023, 'params': 10401, 'accuracy_test': 0.8801, 'precision_test': 0.7256, 'recall_test': 0.8, 'f1_score_test': 0.761}, 'MLP_338433': {'accuracy_cv_mean': 0.8548, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8482, 'precision_cv_std': 0.0164, 'recall_cv_mean': 0.8651, 'recall_cv_std': 0.0165, 'f1_cv_mean': 0.8563, 'f1_cv_std': 0.0018, 'params': 338433, 'accuracy_test': 0.8883, 'precision_test': 0.7616, 'recall_test': 0.774, 'f1_score_test': 0.7678}, 'MLP_1031169': {'accuracy_cv_mean': 0.8514, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8385, 'precision_cv_std': 0.0098, 'recall_cv_mean': 0.8709, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.8543, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8903, 'precision_test': 0.7607, 'recall_test': 0.7884, 'f1_score_test': 0.7743}, 'Logistic Regression': {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8594, 'precision_cv_std': 0.0025, 'recall_cv_mean': 0.8341, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8466, 'f1_cv_std': 0.0021, 'accuracy_test': 0.8592, 'precision_test': 0.6632, 'recall_test': 0.8331, 'f1_score_test': 0.7385}, 'SVM': {'accuracy_cv_mean': 0.6642, 'accuracy_cv_std': 0.0289, 'precision_cv_mean': 0.6353, 'precision_cv_std': 0.0321, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0306, 'f1_cv_mean': 0.6989, 'f1_cv_std': 0.0209, 'accuracy_test': 0.5661, 'precision_test': 0.3298, 'recall_test': 0.7932, 'f1_score_test': 0.4659}, 'Decision Tree': {'accuracy_cv_mean': 0.7964, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8073, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.7927, 'f1_cv_std': 0.0038, 'accuracy_test': 0.8246, 'precision_test': 0.6052, 'recall_test': 0.762, 'f1_score_test': 0.6746}, 'Random Forest': {'accuracy_cv_mean': 0.8535, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8848, 'precision_cv_std': 0.0055, 'recall_cv_mean': /home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_4.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
0.8129, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0033, 'accuracy_test': 0.8767, 'precision_test': 0.7104, 'recall_test': 0.8159, 'f1_score_test': 0.7595}, 'XGBoost': {'accuracy_cv_mean': 0.8762, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8923, 'precision_cv_std': 0.0032, 'recall_cv_mean': 0.8558, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.8736, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8933, 'precision_test': 0.7335, 'recall_test': 0.8682, 'f1_score_test': 0.7952}, 'Naive Bayes': {'accuracy_cv_mean': 0.7807, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.7651, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8102, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.787, 'f1_cv_std': 0.0027, 'accuracy_test': 0.765, 'precision_test': 0.5047, 'recall_test': 0.8136, 'f1_score_test': 0.623}}}
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 1559)
Shape of X_test after concatenation:  (21625, 1559)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1559)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1559)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [1559, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4113, Test Loss: 0.4111, F1: 0.8285, AUC: 0.9371
Epoch [10/30] Train Loss: 0.2577, Test Loss: 0.2652, F1: 0.8869, AUC: 0.9567
Epoch [20/30] Train Loss: 0.2463, Test Loss: 0.2997, F1: 0.8650, AUC: 0.9596
Mejores resultados en la época:  27
f1-score 0.8936681485041451
AUC según el mejor F1-score 0.9605507716258037

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4055, Test Loss: 0.3239, F1: 0.8600, AUC: 0.9370
Epoch [10/30] Train Loss: 0.2567, Test Loss: 0.2562, F1: 0.8916, AUC: 0.9596
Epoch [20/30] Train Loss: 0.2495, Test Loss: 0.2569, F1: 0.8843, AUC: 0.9623
Mejores resultados en la época:  29
f1-score 0.8976727004063539
AUC según el mejor F1-score 0.9632890901989063

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4423, Test Loss: 0.3321, F1: 0.8468, AUC: 0.9328
Epoch [10/30] Train Loss: 0.2587, Test Loss: 0.2704, F1: 0.8845, AUC: 0.9577
Epoch [20/30] Train Loss: 0.2484, Test Loss: 0.2559, F1: 0.8938, AUC: 0.9602
Mejores resultados en la época:  26
f1-score 0.8968282901932191
AUC según el mejor F1-score 0.9610281082867616

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4779, Test Loss: 0.3557, F1: 0.8516, AUC: 0.9283
Epoch [10/30] Train Loss: 0.2672, Test Loss: 0.2730, F1: 0.8815, AUC: 0.9577
Epoch [20/30] Train Loss: 0.2514, Test Loss: 0.2551, F1: 0.8891, AUC: 0.9613
Mejores resultados en la época:  19
f1-score 0.8969580377539978
AUC según el mejor F1-score 0.961060517052573

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4561, Test Loss: 0.3382, F1: 0.8580, AUC: 0.9352
Epoch [10/30] Train Loss: 0.2632, Test Loss: 0.2900, F1: 0.8674, AUC: 0.9595
Epoch [20/30] Train Loss: 0.2507, Test Loss: 0.2501, F1: 0.8984, AUC: 0.9624
Mejores resultados en la época:  26
f1-score 0.9001659947830211
AUC según el mejor F1-score 0.9632645811380153
Epoch [0/30] Train Loss: 0.4105, Test Loss: 0.3306, F1: 0.7483, AUC: 0.9390
Epoch [10/30] Train Loss: 0.2592, Test Loss: 0.2514, F1: 0.8059, AUC: 0.9601
Epoch [20/30] Train Loss: 0.2473, Test Loss: 0.2734, F1: 0.7971, AUC: 0.9626
Mejores resultados en la época:  24
f1-score 0.8304735758407688
AUC según el mejor F1-score 0.9630954667758953
Confusion matrix Test saved: outputs_cv/5/gpt/cm_mlp_1.png

========================================
Entrenando red 5 con capas [1559, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3789, Test Loss: 0.2964, F1: 0.8728, AUC: 0.9456
Epoch [10/30] Train Loss: 0.2630, Test Loss: 0.2854, F1: 0.8700, AUC: 0.9593
Epoch [20/30] Train Loss: 0.2439, Test Loss: 0.2619, F1: 0.8821, AUC: 0.9623
Mejores resultados en la época:  24
f1-score 0.8950499762018087
AUC según el mejor F1-score 0.9632568726150772

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3760, Test Loss: 0.3477, F1: 0.8329, AUC: 0.9466
Epoch [10/30] Train Loss: 0.2619, Test Loss: 0.2688, F1: 0.8772, AUC: 0.9619
Epoch [20/30] Train Loss: 0.2419, Test Loss: 0.2434, F1: 0.8941, AUC: 0.9648
Mejores resultados en la época:  22
f1-score 0.8983071197022452
AUC según el mejor F1-score 0.9645304354643651

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3817, Test Loss: 0.3264, F1: 0.8675, AUC: 0.9453
Epoch [10/30] Train Loss: 0.2669, Test Loss: 0.2644, F1: 0.8919, AUC: 0.9591
Epoch [20/30] Train Loss: 0.2474, Test Loss: 0.2488, F1: 0.8951, AUC: 0.9627
Mejores resultados en la época:  29
f1-score 0.8986859879651234
AUC según el mejor F1-score 0.9641791523007932

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3822, Test Loss: 0.2958, F1: 0.8725, AUC: 0.9482
Epoch [10/30] Train Loss: 0.2625, Test Loss: 0.3152, F1: 0.8430, AUC: 0.9617
Epoch [20/30] Train Loss: 0.2396, Test Loss: 0.2486, F1: 0.8924, AUC: 0.9652
Mejores resultados en la época:  22
f1-score 0.898511832154184
AUC según el mejor F1-score 0.9652952209687387

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3877, Test Loss: 0.3569, F1: 0.8655, AUC: 0.9489
Epoch [10/30] Train Loss: 0.2642, Test Loss: 0.2530, F1: 0.8906, AUC: 0.9628
Epoch [20/30] Train Loss: 0.2454, Test Loss: 0.2392, F1: 0.9014, AUC: 0.9654
Mejores resultados en la época:  28
f1-score 0.9026885557934808
AUC según el mejor F1-score 0.9665966219338333
Epoch [0/30] Train Loss: 0.3775, Test Loss: 0.4661, F1: 0.6957, AUC: 0.9478
Epoch [10/30] Train Loss: 0.2595, Test Loss: 0.2197, F1: 0.8228, AUC: 0.9624
Epoch [20/30] Train Loss: 0.2454, Test Loss: 0.2020, F1: 0.8280, AUC: 0.9649
Mejores resultados en la época:  28
f1-score 0.8331358609794629
AUC según el mejor F1-score 0.966509009009009
Confusion matrix Test saved: outputs_cv/5/gpt/cm_mlp_5.png

========================================
Entrenando red 6 con capas [1559, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3929, Test Loss: 0.3069, F1: 0.8608, AUC: 0.9451
Epoch [10/30] Train Loss: 0.2690, Test Loss: 0.3256, F1: 0.8702, AUC: 0.9582
Epoch [20/30] Train Loss: 0.2461, Test Loss: 0.2601, F1: 0.8908, AUC: 0.9625
Mejores resultados en la época:  27
f1-score 0.8953700328107911
AUC según el mejor F1-score 0.9640226417432847

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3996, Test Loss: 0.3589, F1: 0.8116, AUC: 0.9436
Epoch [10/30] Train Loss: 0.2772, Test Loss: 0.2561, F1: 0.8908, AUC: 0.9600
Epoch [20/30] Train Loss: 0.2503, Test Loss: 0.2540, F1: 0.8942, AUC: 0.9639
Mejores resultados en la época:  22
f1-score 0.8983774551665243
AUC según el mejor F1-score 0.9648561910341926

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3956, Test Loss: 0.3105, F1: 0.8626, AUC: 0.9446
Epoch [10/30] Train Loss: 0.2617, Test Loss: 0.2555, F1: 0.8938, AUC: 0.9601
Epoch [20/30] Train Loss: 0.2410, Test Loss: 0.2717, F1: 0.8825, AUC: 0.9626
Mejores resultados en la época:  23
f1-score 0.8987419079027726
0.8129, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0033, 'accuracy_test': 0.8767, 'precision_test': 0.7104, 'recall_test': 0.8159, 'f1_score_test': 0.7595}, 'XGBoost': {'accuracy_cv_mean': 0.8762, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8923, 'precision_cv_std': 0.0032, 'recall_cv_mean': 0.8558, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.8736, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8933, 'precision_test': 0.7335, 'recall_test': 0.8682, 'f1_score_test': 0.7952}, 'Naive Bayes': {'accuracy_cv_mean': 0.7807, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.7651, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8102, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.787, 'f1_cv_std': 0.0027, 'accuracy_test': 0.765, 'precision_test': 0.5047, 'recall_test': 0.8136, 'f1_score_test': 0.623}}}
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 1559)
Shape of X_test after concatenation:  (21625, 1559)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1559)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1559)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [1559, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4113, Test Loss: 0.4111, F1: 0.8285, AUC: 0.9371
Epoch [10/30] Train Loss: 0.2577, Test Loss: 0.2652, F1: 0.8869, AUC: 0.9567
Epoch [20/30] Train Loss: 0.2463, Test Loss: 0.2997, F1: 0.8650, AUC: 0.9596
Mejores resultados en la época:  27
f1-score 0.8936681485041451
AUC según el mejor F1-score 0.9605507716258037

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4055, Test Loss: 0.3239, F1: 0.8600, AUC: 0.9370
Epoch [10/30] Train Loss: 0.2567, Test Loss: 0.2562, F1: 0.8916, AUC: 0.9596
Epoch [20/30] Train Loss: 0.2495, Test Loss: 0.2569, F1: 0.8843, AUC: 0.9623
Mejores resultados en la época:  29
f1-score 0.8976727004063539
AUC según el mejor F1-score 0.9632890901989063

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4423, Test Loss: 0.3321, F1: 0.8468, AUC: 0.9328
Epoch [10/30] Train Loss: 0.2587, Test Loss: 0.2704, F1: 0.8845, AUC: 0.9577
Epoch [20/30] Train Loss: 0.2484, Test Loss: 0.2559, F1: 0.8938, AUC: 0.9602
Mejores resultados en la época:  26
f1-score 0.8968282901932191
AUC según el mejor F1-score 0.9610281082867616

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4779, Test Loss: 0.3557, F1: 0.8516, AUC: 0.9283
Epoch [10/30] Train Loss: 0.2672, Test Loss: 0.2730, F1: 0.8815, AUC: 0.9577
Epoch [20/30] Train Loss: 0.2514, Test Loss: 0.2551, F1: 0.8891, AUC: 0.9613
Mejores resultados en la época:  19
f1-score 0.8969580377539978
AUC según el mejor F1-score 0.961060517052573

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4561, Test Loss: 0.3382, F1: 0.8580, AUC: 0.9352
Epoch [10/30] Train Loss: 0.2632, Test Loss: 0.2900, F1: 0.8674, AUC: 0.9595
Epoch [20/30] Train Loss: 0.2507, Test Loss: 0.2501, F1: 0.8984, AUC: 0.9624
Mejores resultados en la época:  26
f1-score 0.9001659947830211
AUC según el mejor F1-score 0.9632645811380153
Epoch [0/30] Train Loss: 0.4105, Test Loss: 0.3306, F1: 0.7483, AUC: 0.9390
Epoch [10/30] Train Loss: 0.2592, Test Loss: 0.2514, F1: 0.8059, AUC: 0.9601
Epoch [20/30] Train Loss: 0.2473, Test Loss: 0.2734, F1: 0.7971, AUC: 0.9626
Mejores resultados en la época:  24
f1-score 0.8304735758407688
AUC según el mejor F1-score 0.9630954667758953
Confusion matrix Test saved: outputs_cv/5/gpt/cm_mlp_1.png

========================================
Entrenando red 5 con capas [1559, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3789, Test Loss: 0.2964, F1: 0.8728, AUC: 0.9456
Epoch [10/30] Train Loss: 0.2630, Test Loss: 0.2854, F1: 0.8700, AUC: 0.9593
Epoch [20/30] Train Loss: 0.2439, Test Loss: 0.2619, F1: 0.8821, AUC: 0.9623
Mejores resultados en la época:  24
f1-score 0.8950499762018087
AUC según el mejor F1-score 0.9632568726150772

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3760, Test Loss: 0.3477, F1: 0.8329, AUC: 0.9466
Epoch [10/30] Train Loss: 0.2619, Test Loss: 0.2688, F1: 0.8772, AUC: 0.9619
Epoch [20/30] Train Loss: 0.2419, Test Loss: 0.2434, F1: 0.8941, AUC: 0.9648
Mejores resultados en la época:  22
f1-score 0.8983071197022452
AUC según el mejor F1-score 0.9645304354643651

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3817, Test Loss: 0.3264, F1: 0.8675, AUC: 0.9453
Epoch [10/30] Train Loss: 0.2669, Test Loss: 0.2644, F1: 0.8919, AUC: 0.9591
Epoch [20/30] Train Loss: 0.2474, Test Loss: 0.2488, F1: 0.8951, AUC: 0.9627
Mejores resultados en la época:  29
f1-score 0.8986859879651234
AUC según el mejor F1-score 0.9641791523007932

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3822, Test Loss: 0.2958, F1: 0.8725, AUC: 0.9482
Epoch [10/30] Train Loss: 0.2625, Test Loss: 0.3152, F1: 0.8430, AUC: 0.9617
Epoch [20/30] Train Loss: 0.2396, Test Loss: 0.2486, F1: 0.8924, AUC: 0.9652
Mejores resultados en la época:  22
f1-score 0.898511832154184
AUC según el mejor F1-score 0.9652952209687387

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3877, Test Loss: 0.3569, F1: 0.8655, AUC: 0.9489
Epoch [10/30] Train Loss: 0.2642, Test Loss: 0.2530, F1: 0.8906, AUC: 0.9628
Epoch [20/30] Train Loss: 0.2454, Test Loss: 0.2392, F1: 0.9014, AUC: 0.9654
Mejores resultados en la época:  28
f1-score 0.9026885557934808
AUC según el mejor F1-score 0.9665966219338333
Epoch [0/30] Train Loss: 0.3775, Test Loss: 0.4661, F1: 0.6957, AUC: 0.9478
Epoch [10/30] Train Loss: 0.2595, Test Loss: 0.2197, F1: 0.8228, AUC: 0.9624
Epoch [20/30] Train Loss: 0.2454, Test Loss: 0.2020, F1: 0.8280, AUC: 0.9649
Mejores resultados en la época:  28
f1-score 0.8331358609794629
AUC según el mejor F1-score 0.966509009009009
Confusion matrix Test saved: outputs_cv/5/gpt/cm_mlp_5.png

========================================
Entrenando red 6 con capas [1559, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3929, Test Loss: 0.3069, F1: 0.8608, AUC: 0.9451
Epoch [10/30] Train Loss: 0.2690, Test Loss: 0.3256, F1: 0.8702, AUC: 0.9582
Epoch [20/30] Train Loss: 0.2461, Test Loss: 0.2601, F1: 0.8908, AUC: 0.9625
Mejores resultados en la época:  27
f1-score 0.8953700328107911
AUC según el mejor F1-score 0.9640226417432847

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3996, Test Loss: 0.3589, F1: 0.8116, AUC: 0.9436
Epoch [10/30] Train Loss: 0.2772, Test Loss: 0.2561, F1: 0.8908, AUC: 0.9600
Epoch [20/30] Train Loss: 0.2503, Test Loss: 0.2540, F1: 0.8942, AUC: 0.9639
Mejores resultados en la época:  22
f1-score 0.8983774551665243
AUC según el mejor F1-score 0.9648561910341926

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3956, Test Loss: 0.3105, F1: 0.8626, AUC: 0.9446
Epoch [10/30] Train Loss: 0.2617, Test Loss: 0.2555, F1: 0.8938, AUC: 0.9601
Epoch [20/30] Train Loss: 0.2410, Test Loss: 0.2717, F1: 0.8825, AUC: 0.9626
Mejores resultados en la época:  23
f1-score 0.8987419079027726
AUC según el mejor F1-score 0.9639609647294332

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3955, Test Loss: 0.3259, F1: 0.8581, AUC: 0.9481
Epoch [10/30] Train Loss: 0.2611, Test Loss: 0.2493, F1: 0.8936, AUC: 0.9623
Epoch [20/30] Train Loss: 0.2469, Test Loss: 0.2433, F1: 0.8940, AUC: 0.9646
Mejores resultados en la época:  22
f1-score 0.9011641720123544
AUC según el mejor F1-score 0.9649427080691907

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4010, Test Loss: 0.3816, F1: 0.8013, AUC: 0.9490
Epoch [10/30] Train Loss: 0.2604, Test Loss: 0.2724, F1: 0.8761, AUC: 0.9628
Epoch [20/30] Train Loss: 0.2461, Test Loss: 0.2454, F1: 0.9004, AUC: 0.9657
Mejores resultados en la época:  19
f1-score 0.9022356495468278
AUC según el mejor F1-score 0.9655673758365688
Epoch [0/30] Train Loss: 0.3854, Test Loss: 0.3834, F1: 0.7262, AUC: 0.9491
Epoch [10/30] Train Loss: 0.2628, Test Loss: 0.2458, F1: 0.8130, AUC: 0.9617
Epoch [20/30] Train Loss: 0.2472, Test Loss: 0.2065, F1: 0.8057, AUC: 0.9643
Mejores resultados en la época:  29
f1-score 0.8293051359516617
AUC según el mejor F1-score 0.965717848760702
Confusion matrix Test saved: outputs_cv/5/gpt/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9343, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.9334, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.9342, 'f1_cv_std': 0.0028, 'params': 160801, 'accuracy_test': 0.9396, 'precision_test': 0.8415, 'recall_test': 0.9202, 'f1_score_test': 0.8791}, 'MLP_2744833': {'accuracy_cv_mean': 0.9466, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9445, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9491, 'recall_cv_std': 0.0082, 'f1_cv_mean': 0.9468, 'f1_cv_std': 0.0017, 'params': 2744833, 'accuracy_test': 0.9551, 'precision_test': 0.884, 'recall_test': 0.9345, 'f1_score_test': 0.9085}, 'MLP_5843969': {'accuracy_cv_mean': 0.9473, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9478, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9468, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.9472, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9572, 'precision_test': 0.9005, 'recall_test': 0.9225, 'f1_score_test': 0.9114}, 'Logistic Regression': {'accuracy_cv_mean': 0.928, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9345, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0048, 'f1_cv_mean': 0.9274, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9329, 'precision_test': 0.8194, 'recall_test': 0.9217, 'f1_score_test': 0.8676}, 'SVM': {'accuracy_cv_mean': 0.832, 'accuracy_cv_std': 0.0121, 'precision_cv_mean': 0.7958, 'precision_cv_std': 0.0282, 'recall_cv_mean': 0.8967, 'recall_cv_std': 0.028, 'f1_cv_mean': 0.8423, 'f1_cv_std': 0.0075, 'accuracy_test': 0.6783, 'precision_test': 0.4203, 'recall_test': 0.9178, 'f1_score_test': 0.5765}, 'Decision Tree': {'accuracy_cv_mean': 0.8289, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8831, 'precision_cv_std': 0.0196, 'recall_cv_mean': 0.7595, 'recall_cv_std': 0.0255, 'f1_cv_mean': 0.816, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8574, 'precision_test': 0.6715, 'recall_test': 0.788, 'f1_score_test': 0.7251}, 'Random Forest': {'accuracy_cv_mean': 0.8379, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.8738, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.7901, 'recall_cv_std': 0.0052, 'f1_cv_mean': 0.8298, 'f1_cv_std': 0.0038, 'accuracy_test': 0.866, 'precision_test': 0.6928, 'recall_test': 0.788, 'f1_score_test': 0.7373}, 'XGBoost': {'accuracy_cv_mean': 0.9067, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9261, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.8839, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.9045, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9206, 'precision_test': 0.7976, 'recall_test': 0.8938, 'f1_score_test': 0.843}, 'Naive Bayes': {'accuracy_cv_mean': 0.9038, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9303, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.873, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.003, 'accuracy_test': 0.9193, 'precision_test': 0.8014, 'recall_test': 0.8797, 'f1_score_test': 0.8387}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8516, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.857, 'precision_cv_std': 0.0177, 'recall_cv_mean': 0.8451, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0023, 'params': 10401, 'accuracy_test': 0.8801, 'precision_test': 0.7256, 'recall_test': 0.8, 'f1_score_test': 0.761}, 'MLP_338433': {'accuracy_cv_mean': 0.8548, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8482, 'precision_cv_std': 0.0164, 'recall_cv_mean': 0.8651, 'recall_cv_std': 0.0165, 'f1_cv_mean': 0.8563, 'f1_cv_std': 0.0018, 'params': 338433, 'accuracy_test': 0.8883, 'precision_test': 0.7616, 'recall_test': 0.774, 'f1_score_test': 0.7678}, 'MLP_1031169': {'accuracy_cv_mean': 0.8514, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8385, 'precision_cv_std': 0.0098, 'recall_cv_mean': 0.8709, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.8543, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8903, 'precision_test': 0.7607, 'recall_test': 0.7884, 'f1_score_test': 0.7743}, 'Logistic Regression': {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8594, 'precision_cv_std': 0.0025, 'recall_cv_mean': 0.8341, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8466, 'f1_cv_std': 0.0021, 'accuracy_test': 0.8592, 'precision_test': 0.6632, 'recall_test': 0.8331, 'f1_score_test': 0.7385}, 'SVM': {'accuracy_cv_mean': 0.6642, 'accuracy_cv_std': 0.0289, 'precision_cv_mean': 0.6353, 'precision_cv_std': 0.0321, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0306, 'f1_cv_mean': 0.6989, 'f1_cv_std': 0.0209, 'accuracy_test': 0.5661, 'precision_test': 0.3298, 'recall_test': 0.7932, 'f1_score_test': 0.4659}, 'Decision Tree': {'accuracy_cv_mean': 0.7964, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8073, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.7927, 'f1_cv_std': 0.0038, 'accuracy_test': 0.8246, 'precision_test': 0.6052, 'recall_test': 0.762, 'f1_score_test': 0.6746}, 'Random Forest': {'accuracy_cv_mean': 0.8535, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8848, 'precision_cv_std': 0.0055, 'recall_cv_mean': 0.8129, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0033, 'accuracy_test': 0.8767, 'precision_test': 0.7104, 'recall_test': 0.8159, 'f1_score_test': 0.7595}, 'XGBoost': {'accuracy_cv_mean': 0.8762, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8923, 'precision_cv_std': 0.0032, 'recall_cv_mean': 0.8558, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.8736, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8933, 'precision_test': 0.7335, 'recall_test': 0.8682, 'f1_score_test': 0.7952}, 'Naive Bayes': {'accuracy_cv_mean': 0.7807, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.7651, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8102, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.787, 'f1_cv_std': 0.0027, 'accuracy_test': 0.765, 'precision_test': 0.5047, 'recall_test': 0.8136, 'f1_score_test': 0.623}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8967, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8942, 'precision_cv_std': 0.0111, 'recall_cv_mean': 0.9002, 'recall_cv_std': 0.012, 'f1_cv_mean': 0.8971, 'f1_cv_std': 0.0021, 'params': 49953, 'accuracy_test': 0.92, 'precision_test': 0.8404, 'recall_test': 0.8207, 'f1_score_test': 0.8305}, 'MLP_971265': {'accuracy_cv_mean': 0.8982, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8946, 'precision_cv_std': 0.0117, 'recall_cv_mean': 0.903, 'recall_cv_std': 0.012, 'f1_cv_mean': 0.8986, 'f1_cv_std': 0.0024, 'params': 971265, 'accuracy_test': 0.9218, 'precision_test': 0.8492, 'recall_test': 0.8176, 'f1_score_test': 0.8331}, 'MLP_2296833': {'accuracy_cv_mean': 0.8991, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.8987, 'precision_cv_std': 0.008, 'recall_cv_mean': 0.8998, 'recall_cv_std': 0.0108, 'f1_cv_mean': 0.8992, 'f1_cv_std': 0.0024, 'params': 2296833, 'accuracy_test': 0.9164, 'precision_test': 0.8085, 'recall_test': 0.8512, 'f1_score_test': 0.8293}}}
Saved on: outputs_cv/5/gpt

==============================
Model: Logistic Regression

AUC según el mejor F1-score 0.9639609647294332

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3955, Test Loss: 0.3259, F1: 0.8581, AUC: 0.9481
Epoch [10/30] Train Loss: 0.2611, Test Loss: 0.2493, F1: 0.8936, AUC: 0.9623
Epoch [20/30] Train Loss: 0.2469, Test Loss: 0.2433, F1: 0.8940, AUC: 0.9646
Mejores resultados en la época:  22
f1-score 0.9011641720123544
AUC según el mejor F1-score 0.9649427080691907

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4010, Test Loss: 0.3816, F1: 0.8013, AUC: 0.9490
Epoch [10/30] Train Loss: 0.2604, Test Loss: 0.2724, F1: 0.8761, AUC: 0.9628
Epoch [20/30] Train Loss: 0.2461, Test Loss: 0.2454, F1: 0.9004, AUC: 0.9657
Mejores resultados en la época:  19
f1-score 0.9022356495468278
AUC según el mejor F1-score 0.9655673758365688
Epoch [0/30] Train Loss: 0.3854, Test Loss: 0.3834, F1: 0.7262, AUC: 0.9491
Epoch [10/30] Train Loss: 0.2628, Test Loss: 0.2458, F1: 0.8130, AUC: 0.9617
Epoch [20/30] Train Loss: 0.2472, Test Loss: 0.2065, F1: 0.8057, AUC: 0.9643
Mejores resultados en la época:  29
f1-score 0.8293051359516617
AUC según el mejor F1-score 0.965717848760702
Confusion matrix Test saved: outputs_cv/5/gpt/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9343, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.9334, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.9342, 'f1_cv_std': 0.0028, 'params': 160801, 'accuracy_test': 0.9396, 'precision_test': 0.8415, 'recall_test': 0.9202, 'f1_score_test': 0.8791}, 'MLP_2744833': {'accuracy_cv_mean': 0.9466, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9445, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9491, 'recall_cv_std': 0.0082, 'f1_cv_mean': 0.9468, 'f1_cv_std': 0.0017, 'params': 2744833, 'accuracy_test': 0.9551, 'precision_test': 0.884, 'recall_test': 0.9345, 'f1_score_test': 0.9085}, 'MLP_5843969': {'accuracy_cv_mean': 0.9473, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9478, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9468, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.9472, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9572, 'precision_test': 0.9005, 'recall_test': 0.9225, 'f1_score_test': 0.9114}, 'Logistic Regression': {'accuracy_cv_mean': 0.928, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9345, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0048, 'f1_cv_mean': 0.9274, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9329, 'precision_test': 0.8194, 'recall_test': 0.9217, 'f1_score_test': 0.8676}, 'SVM': {'accuracy_cv_mean': 0.832, 'accuracy_cv_std': 0.0121, 'precision_cv_mean': 0.7958, 'precision_cv_std': 0.0282, 'recall_cv_mean': 0.8967, 'recall_cv_std': 0.028, 'f1_cv_mean': 0.8423, 'f1_cv_std': 0.0075, 'accuracy_test': 0.6783, 'precision_test': 0.4203, 'recall_test': 0.9178, 'f1_score_test': 0.5765}, 'Decision Tree': {'accuracy_cv_mean': 0.8289, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8831, 'precision_cv_std': 0.0196, 'recall_cv_mean': 0.7595, 'recall_cv_std': 0.0255, 'f1_cv_mean': 0.816, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8574, 'precision_test': 0.6715, 'recall_test': 0.788, 'f1_score_test': 0.7251}, 'Random Forest': {'accuracy_cv_mean': 0.8379, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.8738, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.7901, 'recall_cv_std': 0.0052, 'f1_cv_mean': 0.8298, 'f1_cv_std': 0.0038, 'accuracy_test': 0.866, 'precision_test': 0.6928, 'recall_test': 0.788, 'f1_score_test': 0.7373}, 'XGBoost': {'accuracy_cv_mean': 0.9067, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9261, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.8839, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.9045, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9206, 'precision_test': 0.7976, 'recall_test': 0.8938, 'f1_score_test': 0.843}, 'Naive Bayes': {'accuracy_cv_mean': 0.9038, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9303, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.873, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.003, 'accuracy_test': 0.9193, 'precision_test': 0.8014, 'recall_test': 0.8797, 'f1_score_test': 0.8387}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8516, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.857, 'precision_cv_std': 0.0177, 'recall_cv_mean': 0.8451, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0023, 'params': 10401, 'accuracy_test': 0.8801, 'precision_test': 0.7256, 'recall_test': 0.8, 'f1_score_test': 0.761}, 'MLP_338433': {'accuracy_cv_mean': 0.8548, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8482, 'precision_cv_std': 0.0164, 'recall_cv_mean': 0.8651, 'recall_cv_std': 0.0165, 'f1_cv_mean': 0.8563, 'f1_cv_std': 0.0018, 'params': 338433, 'accuracy_test': 0.8883, 'precision_test': 0.7616, 'recall_test': 0.774, 'f1_score_test': 0.7678}, 'MLP_1031169': {'accuracy_cv_mean': 0.8514, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8385, 'precision_cv_std': 0.0098, 'recall_cv_mean': 0.8709, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.8543, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8903, 'precision_test': 0.7607, 'recall_test': 0.7884, 'f1_score_test': 0.7743}, 'Logistic Regression': {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8594, 'precision_cv_std': 0.0025, 'recall_cv_mean': 0.8341, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8466, 'f1_cv_std': 0.0021, 'accuracy_test': 0.8592, 'precision_test': 0.6632, 'recall_test': 0.8331, 'f1_score_test': 0.7385}, 'SVM': {'accuracy_cv_mean': 0.6642, 'accuracy_cv_std': 0.0289, 'precision_cv_mean': 0.6353, 'precision_cv_std': 0.0321, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0306, 'f1_cv_mean': 0.6989, 'f1_cv_std': 0.0209, 'accuracy_test': 0.5661, 'precision_test': 0.3298, 'recall_test': 0.7932, 'f1_score_test': 0.4659}, 'Decision Tree': {'accuracy_cv_mean': 0.7964, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8073, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.7927, 'f1_cv_std': 0.0038, 'accuracy_test': 0.8246, 'precision_test': 0.6052, 'recall_test': 0.762, 'f1_score_test': 0.6746}, 'Random Forest': {'accuracy_cv_mean': 0.8535, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8848, 'precision_cv_std': 0.0055, 'recall_cv_mean': 0.8129, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0033, 'accuracy_test': 0.8767, 'precision_test': 0.7104, 'recall_test': 0.8159, 'f1_score_test': 0.7595}, 'XGBoost': {'accuracy_cv_mean': 0.8762, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8923, 'precision_cv_std': 0.0032, 'recall_cv_mean': 0.8558, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.8736, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8933, 'precision_test': 0.7335, 'recall_test': 0.8682, 'f1_score_test': 0.7952}, 'Naive Bayes': {'accuracy_cv_mean': 0.7807, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.7651, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8102, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.787, 'f1_cv_std': 0.0027, 'accuracy_test': 0.765, 'precision_test': 0.5047, 'recall_test': 0.8136, 'f1_score_test': 0.623}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8967, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8942, 'precision_cv_std': 0.0111, 'recall_cv_mean': 0.9002, 'recall_cv_std': 0.012, 'f1_cv_mean': 0.8971, 'f1_cv_std': 0.0021, 'params': 49953, 'accuracy_test': 0.92, 'precision_test': 0.8404, 'recall_test': 0.8207, 'f1_score_test': 0.8305}, 'MLP_971265': {'accuracy_cv_mean': 0.8982, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8946, 'precision_cv_std': 0.0117, 'recall_cv_mean': 0.903, 'recall_cv_std': 0.012, 'f1_cv_mean': 0.8986, 'f1_cv_std': 0.0024, 'params': 971265, 'accuracy_test': 0.9218, 'precision_test': 0.8492, 'recall_test': 0.8176, 'f1_score_test': 0.8331}, 'MLP_2296833': {'accuracy_cv_mean': 0.8991, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.8987, 'precision_cv_std': 0.008, 'recall_cv_mean': 0.8998, 'recall_cv_std': 0.0108, 'f1_cv_mean': 0.8992, 'f1_cv_std': 0.0024, 'params': 2296833, 'accuracy_test': 0.9164, 'precision_test': 0.8085, 'recall_test': 0.8512, 'f1_score_test': 0.8293}}}
Saved on: outputs_cv/5/gpt

==============================
Model: Logistic Regression

/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:59:01] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:59:02] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:03:48] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:03:49] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:08:31] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:08:34] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:13:16] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:13:21] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:17:55] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:18:00] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:22:32] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:22:37] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9018, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.907, 'precision_cv_std': 0.005, 'recall_cv_mean': 0.8955, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.9012, 'f1_cv_std': 0.0036}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 47, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.91      0.94     16465
           1       0.75      0.90      0.82      5160

    accuracy                           0.91     21625
   macro avg       0.86      0.90      0.88     21625
weighted avg       0.92      0.91      0.91     21625

Confusion matrix Test saved as: outputs_cv/5/gpt/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/5/gpt/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8367, 'accuracy_cv_std': 0.0059, 'precision_cv_mean': 0.8347, 'precision_cv_std': 0.0156, 'recall_cv_mean': 0.8411, 'recall_cv_std': 0.0358, 'f1_cv_mean': 0.8371, 'f1_cv_std': 0.0104}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 47, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.78      0.86     16465
           1       0.56      0.87      0.68      5160

    accuracy                           0.80     21625
   macro avg       0.75      0.83      0.77     21625
weighted avg       0.86      0.80      0.82     21625

Confusion matrix Test saved as: outputs_cv/5/gpt/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/5/gpt/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7976, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8128, 'precision_cv_std': 0.0078, 'recall_cv_mean': 0.7735, 'recall_cv_std': 0.0143, 'f1_cv_mean': 0.7925, 'f1_cv_std': 0.0058}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 47, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.83      0.88     16465
           1       0.59      0.79      0.68      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.81      0.78     21625
weighted avg       0.85      0.82      0.83     21625

Confusion matrix Test saved as: outputs_cv/5/gpt/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/5/gpt/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.873, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9029, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.836, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.8681, 'f1_cv_std': 0.0023}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 47, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.91      0.93     16465
           1       0.74      0.85      0.79      5160

    accuracy                           0.89     21625
   macro avg       0.85      0.88      0.86     21625
weighted avg       0.90      0.89      0.90     21625

Confusion matrix Test saved as: outputs_cv/5/gpt/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/5/gpt/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9075, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9205, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8921, 'recall_cv_std': 0.0042, 'f1_cv_mean': 0.9061, 'f1_cv_std': 0.0029}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 47, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.92      0.94     16465
           1       0.78      0.90      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.87      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/5/gpt/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/5/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8335, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.8628, 'precision_cv_std': 0.0088, 'recall_cv_mean': 0.7933, 'recall_cv_std': 0.0027, 'f1_cv_mean': 0.8266, 'f1_cv_std': 0.0026}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.87      0.90     16465
           1       0.66      0.79      0.72      5160

    accuracy                           0.85     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.85      0.86     21625

Confusion matrix Test saved as: outputs_cv/5/gpt/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/5/gpt/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.9075, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9205, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8921, 'recall_cv_std': 0.0042, 'f1_cv_mean': 0.9061, 'f1_cv_std': 0.0029, 'accuracy_test': 0.9157, 'precision_test': 0.7807, 'recall_test': 0.8994, 'f1_score_test': 0.8358}
Logistic Regression: {'accuracy_cv_mean': 0.9018, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.907, 'precision_cv_std': 0.005, 'recall_cv_mean': 0.8955, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.9012, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9056, 'precision_test': 0.7542, 'recall_test': 0.8967, 'f1_score_test': 0.8193}
Random Forest: {'accuracy_cv_mean': 0.873, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9029, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.836, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.8681, 'f1_cv_std': 0.0023, 'accuracy_test': 0.8929, 'precision_test': 0.742, 'recall_test': 0.8452, 'f1_score_test': 0.7903}
Naive Bayes: {'accuracy_cv_mean': 0.8335, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.8628, 'precision_cv_std': 0.0088, 'recall_cv_mean': 0.7933, 'recall_cv_std': 0.0027, 'f1_cv_mean': 0.8266, 'f1_cv_std': 0.0026, 'accuracy_test': 0.8534, 'precision_test': 0.6618, 'recall_test': 0.7888, 'f1_score_test': 0.7197}
SVM: {'accuracy_cv_mean': 0.8367, 'accuracy_cv_std': 0.0059, 'precision_cv_mean': 0.8347, 'precision_cv_std': 0.0156, 'recall_cv_mean': 0.8411, 'recall_cv_std': 0.0358, 'f1_cv_mean': 0.8371, 'f1_cv_std': 0.0104, 'accuracy_test': 0.803, 'precision_test': 0.5554, 'recall_test': 0.874, 'f1_score_test': 0.6792}
Decision Tree: {'accuracy_cv_mean': 0.7976, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8128, 'precision_cv_std': 0.0078, 'recall_cv_mean': 0.7735, 'recall_cv_std': 0.0143, 'f1_cv_mean': 0.7925, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8204, 'precision_test': 0.5928, 'recall_test': 0.7897, 'f1_score_test': 0.6772}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9343, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.9334, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.9342, 'f1_cv_std': 0.0028, 'params': 160801, 'accuracy_test': 0.9396, 'precision_test': 0.8415, 'recall_test': 0.9202, 'f1_score_test': 0.8791}, 'MLP_2744833': {'accuracy_cv_mean': 0.9466, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9445, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9491, 'recall_cv_std': 0.0082, 'f1_cv_mean': 0.9468, 'f1_cv_std': 0.0017, 'params': 2744833, 'accuracy_test': 0.9551, 'precision_test': 0.884, 'recall_test': 0.9345, 'f1_score_test': 0.9085}, 'MLP_5843969': {'accuracy_cv_mean': 0.9473, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9478, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9468, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.9472, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9572, 'precision_test': 0.9005, 'recall_test': 0.9225, 'f1_score_test': 0.9114}, 'Logistic Regression': {'accuracy_cv_mean': 0.928, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9345, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0048, 'f1_cv_mean': 0.9274, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9329, 'precision_test': 0.8194, 'recall_test': 0.9217, 'f1_score_test': 0.8676}, 'SVM': {'accuracy_cv_mean': 0.832, 'accuracy_cv_std': 0.0121, 'precision_cv_mean': 0.7958, 'precision_cv_std': 0.0282, 'recall_cv_mean': 0.8967, 'recall_cv_std': 0.028, 'f1_cv_mean': 0.8423, 'f1_cv_std': 0.0075, 'accuracy_test': 0.6783, 'precision_test': 0.4203, 'recall_test': 0.9178, 'f1_score_test': 0.5765}, 'Decision Tree': {'accuracy_cv_mean': 0.8289, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8831, 'precision_cv_std': 0.0196, 'recall_cv_mean': 0.7595, 'recall_cv_std': 0.0255, 'f1_cv_mean': 0.816, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8574, 'precision_test': 0.6715, 'recall_test': 0.788, 'f1_score_test': 0.7251}, 'Random Forest': {'accuracy_cv_mean': 0.8379, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.8738, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.7901, 'recall_cv_std': 0.0052, 'f1_cv_mean': 0.8298, 'f1_cv_std': 0.0038, 'accuracy_test': 0.866, 'precision_test': 0.6928, 'recall_test': 0.788, 'f1_score_test': 0.7373}, 'XGBoost': {'accuracy_cv_mean': 0.9067, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9261, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.8839, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.9045, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9206, 'precision_test': 0.7976, 'recall_test': 0.8938, 'f1_score_test': 0.843}, 'Naive Bayes': {'accuracy_cv_mean': 0.9038, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9303, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.873, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.003, 'accuracy_test': 0.9193, 'precision_test': 0.8014, 'recall_test': 0.8797, 'f1_score_test': 0.8387}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8516, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.857, 'precision_cv_std': 0.0177, 'recall_cv_mean': 0.8451, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0023, 'params': 10401, 'accuracy_test': 0.8801, 'precision_test': 0.7256, 'recall_test': 0.8, 'f1_score_test': 0.761}, 'MLP_338433': {'accuracy_cv_mean': 0.8548, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8482, 'precision_cv_std': 0.0164, 'recall_cv_mean': 0.8651, 'recall_cv_std': 0.0165, 'f1_cv_mean': 0.8563, 'f1_cv_std': 0.0018, 'params': 338433, 'accuracy_test': 0.8883, 'precision_test': 0.7616, 'recall_test': 0.774, 'f1_score_test': 0.7678}, 'MLP_1031169': {'accuracy_cv_mean': 0.8514, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8385, 'precision_cv_std': 0.0098, 'recall_cv_mean': 0.8709, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.8543, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8903, 'precision_test': 0.7607, 'recall_test': 0.7884, 'f1_score_test': 0.7743}, 'Logistic Regression': {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8594, 'precision_cv_std': 0.0025, 'recall_cv_mean': 0.8341, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8466, 'f1_cv_std': 0.0021, 'accuracy_test': 0.8592, 'precision_test': 0.6632, 'recall_test': 0.8331, 'f1_score_test': 0.7385}, 'SVM': {'accuracy_cv_mean': 0.6642, 'accuracy_cv_std': 0.0289, 'precision_cv_mean': 0.6353, 'precision_cv_std': 0.0321, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0306, 'f1_cv_mean': 0.6989, 'f1_cv_std': 0.0209, 'accuracy_test': 0.5661, 'precision_test': 0.3298, 'recall_test': 0.7932, 'f1_score_test': 0.4659}, 'Decision Tree': {'accuracy_cv_mean': 0.7964, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8073, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.7927, 'f1_cv_std': 0.0038, 'accuracy_test': 0.8246, 'precision_test': 0.6052, 'recall_test': 0.762, 'f1_score_test': 0.6746}, 'Random Forest': {'accuracy_cv_mean': 0.8535, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8848, 'precision_cv_std': 0.0055, 'recall_cv_mean': 0.8129, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0033, 'accuracy_test': 0.8767, 'precision_test': 0.7104, 'recall_test': 0.8159, 'f1_score_test': 0.7595}, 'XGBoost': {'accuracy_cv_mean': 0.8762, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8923, 'precision_cv_std': 0.0032, 'recall_cv_mean': 0.8558, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.8736, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8933, 'precision_test': 0.7335, 'recall_test': 0.8682, 'f1_score_test': 0.7952}, 'Naive Bayes': {'accuracy_cv_mean': 0.7807, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.7651, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8102, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.787, 'f1_cv_std': 0.0027, 'accuracy_test': 0.765, 'precision_test': 0.5047, 'recall_test': 0.8136, 'f1_score_test': 0.623}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8967, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8942, 'precision_cv_std': 0.0111, 'recall_cv_mean': 0.9002, 'recall_cv_std': 0.012, 'f1_cv_mean': 0.8971, 'f1_cv_std': 0.0021, 'params': 49953, 'accuracy_test': 0.92, 'precision_test': 0.8404, 'recall_test': 0.8207, 'f1_score_test': 0.8305}, 'MLP_971265': {'accuracy_cv_mean': 0.8982, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8946, 'precision_cv_std': 0.0117, 'recall_cv_mean': 0.903, 'recall_cv_std': 0.012, 'f1_cv_mean': 0.8986, 'f1_cv_std': 0.0024, 'params': 971265, 'accuracy_test': 0.9218, 'precision_test': 0.8492, 'recall_test': 0.8176, 'f1_score_test': 0.8331}, 'MLP_2296833': {'accuracy_cv_mean': 0.8991, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.8987, 'precision_cv_std': 0.008, 'recall_cv_mean': 0.8998, 'recall_cv_std': 0.0108, 'f1_cv_mean': 0.8992, 'f1_cv_std': 0.0024, 'params': 2296833, 'accuracy_test': 0.9164, 'precision_test': 0.8085, 'recall_test': 0.8512, 'f1_score_test': 0.8293}, 'Logistic Regression': {'accuracy_cv_mean': 0.9018, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.907, 'precision_cv_std': 0.005, 'recall_cv_mean': 0.8955, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.9012, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9056, 'precision_test': 0.7542, 'recall_test': 0.8967, 'f1_score_test': 0.8193}, 'SVM': {'accuracy_cv_mean': 0.8367, 'accuracy_cv_std': 0.0059, 'precision_cv_mean': 0.8347, 'precision_cv_std': 0.0156, 'recall_cv_mean': 0.8411, 'recall_cv_std': 0.0358, 'f1_cv_mean': 0.8371, 'f1_cv_std': 0.0104, 'accuracy_test': 0.803, 'precision_test': 0.5554, 'recall_test': 0.874, 'f1_score_test': 0.6792}, 'Decision Tree': {'accuracy_cv_mean': 0.7976, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8128, 'precision_cv_std': 0.0078, 'recall_cv_mean': 0.7735, 'recall_cv_std': 0.0143, 'f1_cv_mean': 0.7925, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8204, 'precision_test': 0.5928, 'recall_test': 0.7897, 'f1_score_test': 0.6772}, 'Random Forest': {'accuracy_cv_mean': 0.873, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9029, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.836, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.8681, 'f1_cv_std': 0.0023, 'accuracy_test': 0.8929, 'precision_test': 0.742, 'recall_test': 0.8452, 'f1_score_test': 0.7903}, 'XGBoost': {'accuracy_cv_mean': 0.9075, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9205, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8921, 'recall_cv_std': 0.0042, 'f1_cv_mean': 0.9061, 'f1_cv_std': 0.0029, 'accuracy_test': 0.9157, 'precision_test': 0.7807, 'recall_test': 0.8994, 'f1_score_test': 0.8358}, 'Naive Bayes': {'accuracy_cv_mean': 0.8335, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.8628, 'precision_cv_std': 0.0088, 'recall_cv_mean': 0.7933, 'recall_cv_std': 0.0027, 'f1_cv_mean': 0.8266, 'f1_cv_std': 0.0026, 'accuracy_test': 0.8534, 'precision_test': 0.6618, 'recall_test': 0.7888, 'f1_score_test': 0.7197}}}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
MLP_5843969: {'accuracy_cv_mean': 0.9473, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9478, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9468, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.9472, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9572, 'precision_test': 0.9005, 'recall_test': 0.9225, 'f1_score_test': 0.9114}
MLP_2744833: {'accuracy_cv_mean': 0.9466, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9445, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9491, 'recall_cv_std': 0.0082, 'f1_cv_mean': 0.9468, 'f1_cv_std': 0.0017, 'params': 2744833, 'accuracy_test': 0.9551, 'precision_test': 0.884, 'recall_test': 0.9345, 'f1_score_test': 0.9085}
MLP_160801: {'accuracy_cv_mean': 0.9343, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.9334, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.9342, 'f1_cv_std': 0.0028, 'params': 160801, 'accuracy_test': 0.9396, 'precision_test': 0.8415, 'recall_test': 0.9202, 'f1_score_test': 0.8791}
Logistic Regression: {'accuracy_cv_mean': 0.928, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9345, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0048, 'f1_cv_mean': 0.9274, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9329, 'precision_test': 0.8194, 'recall_test': 0.9217, 'f1_score_test': 0.8676}
XGBoost: {'accuracy_cv_mean': 0.9067, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9261, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.8839, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.9045, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9206, 'precision_test': 0.7976, 'recall_test': 0.8938, 'f1_score_test': 0.843}
Naive Bayes: {'accuracy_cv_mean': 0.9038, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9303, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.873, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.003, 'accuracy_test': 0.9193, 'precision_test': 0.8014, 'recall_test': 0.8797, 'f1_score_test': 0.8387}
Random Forest: {'accuracy_cv_mean': 0.8379, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.8738, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.7901, 'recall_cv_std': 0.0052, 'f1_cv_mean': 0.8298, 'f1_cv_std': 0.0038, 'accuracy_test': 0.866, 'precision_test': 0.6928, 'recall_test': 0.788, 'f1_score_test': 0.7373}
Decision Tree: {'accuracy_cv_mean': 0.8289, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8831, 'precision_cv_std': 0.0196, 'recall_cv_mean': 0.7595, 'recall_cv_std': 0.0255, 'f1_cv_mean': 0.816, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8574, 'precision_test': 0.6715, 'recall_test': 0.788, 'f1_score_test': 0.7251}
SVM: {'accuracy_cv_mean': 0.832, 'accuracy_cv_std': 0.0121, 'precision_cv_mean': 0.7958, 'precision_cv_std': 0.0282, 'recall_cv_mean': 0.8967, 'recall_cv_std': 0.028, 'f1_cv_mean': 0.8423, 'f1_cv_std': 0.0075, 'accuracy_test': 0.6783, 'precision_test': 0.4203, 'recall_test': 0.9178, 'f1_score_test': 0.5765}


EMBEDDINGS TYPE: LYRICS_BERT
XGBoost: {'accuracy_cv_mean': 0.8762, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8923, 'precision_cv_std': 0.0032, 'recall_cv_mean': 0.8558, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.8736, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8933, 'precision_test': 0.7335, 'recall_test': 0.8682, 'f1_score_test': 0.7952}
MLP_1031169: {'accuracy_cv_mean': 0.8514, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8385, 'precision_cv_std': 0.0098, 'recall_cv_mean': 0.8709, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.8543, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8903, 'precision_test': 0.7607, 'recall_test': 0.7884, 'f1_score_test': 0.7743}
MLP_338433: {'accuracy_cv_mean': 0.8548, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8482, 'precision_cv_std': 0.0164, 'recall_cv_mean': 0.8651, 'recall_cv_std': 0.0165, 'f1_cv_mean': 0.8563, 'f1_cv_std': 0.0018, 'params': 338433, 'accuracy_test': 0.8883, 'precision_test': 0.7616, 'recall_test': 0.774, 'f1_score_test': 0.7678}
MLP_10401: {'accuracy_cv_mean': 0.8516, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.857, 'precision_cv_std': 0.0177, 'recall_cv_mean': 0.8451, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0023, 'params': 10401, 'accuracy_test': 0.8801, 'precision_test': 0.7256, 'recall_test': 0.8, 'f1_score_test': 0.761}
Random Forest: {'accuracy_cv_mean': 0.8535, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8848, 'precision_cv_std': 0.0055, 'recall_cv_mean': 0.8129, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0033, 'accuracy_test': 0.8767, 'precision_test': 0.7104, 'recall_test': 0.8159, 'f1_score_test': 0.7595}
Logistic Regression: {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8594, 'precision_cv_std': 0.0025, 'recall_cv_mean': 0.8341, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8466, 'f1_cv_std': 0.0021, 'accuracy_test': 0.8592, 'precision_test': 0.6632, 'recall_test': 0.8331, 'f1_score_test': 0.7385}
Decision Tree: {'accuracy_cv_mean': 0.7964, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8073, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.7927, 'f1_cv_std': 0.0038, 'accuracy_test': 0.8246, 'precision_test': 0.6052, 'recall_test': 0.762, 'f1_score_test': 0.6746}
Naive Bayes: {'accuracy_cv_mean': 0.7807, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.7651, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8102, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.787, 'f1_cv_std': 0.0027, 'accuracy_test': 0.765, 'precision_test': 0.5047, 'recall_test': 0.8136, 'f1_score_test': 0.623}
SVM: {'accuracy_cv_mean': 0.6642, 'accuracy_cv_std': 0.0289, 'precision_cv_mean': 0.6353, 'precision_cv_std': 0.0321, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0306, 'f1_cv_mean': 0.6989, 'f1_cv_std': 0.0209, 'accuracy_test': 0.5661, 'precision_test': 0.3298, 'recall_test': 0.7932, 'f1_score_test': 0.4659}


EMBEDDINGS TYPE: GPT
XGBoost: {'accuracy_cv_mean': 0.9075, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9205, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8921, 'recall_cv_std': 0.0042, 'f1_cv_mean': 0.9061, 'f1_cv_std': 0.0029, 'accuracy_test': 0.9157, 'precision_test': 0.7807, 'recall_test': 0.8994, 'f1_score_test': 0.8358}
MLP_971265: {'accuracy_cv_mean': 0.8982, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8946, 'precision_cv_std': 0.0117, 'recall_cv_mean': 0.903, 'recall_cv_std': 0.012, 'f1_cv_mean': 0.8986, 'f1_cv_std': 0.0024, 'params': 971265, 'accuracy_test': 0.9218, 'precision_test': 0.8492, 'recall_test': 0.8176, 'f1_score_test': 0.8331}
MLP_49953: {'accuracy_cv_mean': 0.8967, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8942, 'precision_cv_std': 0.0111, 'recall_cv_mean': 0.9002, 'recall_cv_std': 0.012, 'f1_cv_mean': 0.8971, 'f1_cv_std': 0.0021, 'params': 49953, 'accuracy_test': 0.92, 'precision_test': 0.8404, 'recall_test': 0.8207, 'f1_score_test': 0.8305}
MLP_2296833: {'accuracy_cv_mean': 0.8991, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.8987, 'precision_cv_std': 0.008, 'recall_cv_mean': 0.8998, 'recall_cv_std': 0.0108, 'f1_cv_mean': 0.8992, 'f1_cv_std': 0.0024, 'params': 2296833, 'accuracy_test': 0.9164, 'precision_test': 0.8085, 'recall_test': 0.8512, 'f1_score_test': 0.8293}
Logistic Regression: {'accuracy_cv_mean': 0.9018, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.907, 'precision_cv_std': 0.005, 'recall_cv_mean': 0.8955, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.9012, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9056, 'precision_test': 0.7542, 'recall_test': 0.8967, 'f1_score_test': 0.8193}
Random Forest: {'accuracy_cv_mean': 0.873, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9029, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.836, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.8681, 'f1_cv_std': 0.0023, 'accuracy_test': 0.8929, 'precision_test': 0.742, 'recall_test': 0.8452, 'f1_score_test': 0.7903}
Naive Bayes: {'accuracy_cv_mean': 0.8335, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.8628, 'precision_cv_std': 0.0088, 'recall_cv_mean': 0.7933, 'recall_cv_std': 0.0027, 'f1_cv_mean': 0.8266, 'f1_cv_std': 0.0026, 'accuracy_test': 0.8534, 'precision_test': 0.6618, 'recall_test': 0.7888, 'f1_score_test': 0.7197}
SVM: {'accuracy_cv_mean': 0.8367, 'accuracy_cv_std': 0.0059, 'precision_cv_mean': 0.8347, 'precision_cv_std': 0.0156, 'recall_cv_mean': 0.8411, 'recall_cv_std': 0.0358, 'f1_cv_mean': 0.8371, 'f1_cv_std': 0.0104, 'accuracy_test': 0.803, 'precision_test': 0.5554, 'recall_test': 0.874, 'f1_score_test': 0.6792}
Decision Tree: {'accuracy_cv_mean': 0.7976, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8128, 'precision_cv_std': 0.0078, 'recall_cv_mean': 0.7735, 'recall_cv_std': 0.0143, 'f1_cv_mean': 0.7925, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8204, 'precision_test': 0.5928, 'recall_test': 0.7897, 'f1_score_test': 0.6772}
Diccionario global guardado en: outputs_cv/5/gpt/global_metrics.json

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
====================================

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9018, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.907, 'precision_cv_std': 0.005, 'recall_cv_mean': 0.8955, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.9012, 'f1_cv_std': 0.0036}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 47, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.91      0.94     16465
           1       0.75      0.90      0.82      5160

    accuracy                           0.91     21625
   macro avg       0.86      0.90      0.88     21625
weighted avg       0.92      0.91      0.91     21625

Confusion matrix Test saved as: outputs_cv/5/gpt/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/5/gpt/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8367, 'accuracy_cv_std': 0.0059, 'precision_cv_mean': 0.8347, 'precision_cv_std': 0.0156, 'recall_cv_mean': 0.8411, 'recall_cv_std': 0.0358, 'f1_cv_mean': 0.8371, 'f1_cv_std': 0.0104}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 47, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.78      0.86     16465
           1       0.56      0.87      0.68      5160

    accuracy                           0.80     21625
   macro avg       0.75      0.83      0.77     21625
weighted avg       0.86      0.80      0.82     21625

Confusion matrix Test saved as: outputs_cv/5/gpt/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/5/gpt/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7976, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8128, 'precision_cv_std': 0.0078, 'recall_cv_mean': 0.7735, 'recall_cv_std': 0.0143, 'f1_cv_mean': 0.7925, 'f1_cv_std': 0.0058}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 47, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.83      0.88     16465
           1       0.59      0.79      0.68      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.81      0.78     21625
weighted avg       0.85      0.82      0.83     21625

Confusion matrix Test saved as: outputs_cv/5/gpt/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/5/gpt/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.873, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9029, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.836, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.8681, 'f1_cv_std': 0.0023}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 47, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.91      0.93     16465
           1       0.74      0.85      0.79      5160

    accuracy                           0.89     21625
   macro avg       0.85      0.88      0.86     21625
weighted avg       0.90      0.89      0.90     21625

Confusion matrix Test saved as: outputs_cv/5/gpt/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/5/gpt/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9075, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9205, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8921, 'recall_cv_std': 0.0042, 'f1_cv_mean': 0.9061, 'f1_cv_std': 0.0029}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 47, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.92      0.94     16465
           1       0.78      0.90      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.87      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/5/gpt/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/5/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8335, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.8628, 'precision_cv_std': 0.0088, 'recall_cv_mean': 0.7933, 'recall_cv_std': 0.0027, 'f1_cv_mean': 0.8266, 'f1_cv_std': 0.0026}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.87      0.90     16465
           1       0.66      0.79      0.72      5160

    accuracy                           0.85     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.85      0.86     21625

Confusion matrix Test saved as: outputs_cv/5/gpt/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/5/gpt/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.9075, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9205, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8921, 'recall_cv_std': 0.0042, 'f1_cv_mean': 0.9061, 'f1_cv_std': 0.0029, 'accuracy_test': 0.9157, 'precision_test': 0.7807, 'recall_test': 0.8994, 'f1_score_test': 0.8358}
Logistic Regression: {'accuracy_cv_mean': 0.9018, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.907, 'precision_cv_std': 0.005, 'recall_cv_mean': 0.8955, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.9012, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9056, 'precision_test': 0.7542, 'recall_test': 0.8967, 'f1_score_test': 0.8193}
Random Forest: {'accuracy_cv_mean': 0.873, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9029, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.836, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.8681, 'f1_cv_std': 0.0023, 'accuracy_test': 0.8929, 'precision_test': 0.742, 'recall_test': 0.8452, 'f1_score_test': 0.7903}
Naive Bayes: {'accuracy_cv_mean': 0.8335, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.8628, 'precision_cv_std': 0.0088, 'recall_cv_mean': 0.7933, 'recall_cv_std': 0.0027, 'f1_cv_mean': 0.8266, 'f1_cv_std': 0.0026, 'accuracy_test': 0.8534, 'precision_test': 0.6618, 'recall_test': 0.7888, 'f1_score_test': 0.7197}
SVM: {'accuracy_cv_mean': 0.8367, 'accuracy_cv_std': 0.0059, 'precision_cv_mean': 0.8347, 'precision_cv_std': 0.0156, 'recall_cv_mean': 0.8411, 'recall_cv_std': 0.0358, 'f1_cv_mean': 0.8371, 'f1_cv_std': 0.0104, 'accuracy_test': 0.803, 'precision_test': 0.5554, 'recall_test': 0.874, 'f1_score_test': 0.6792}
Decision Tree: {'accuracy_cv_mean': 0.7976, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8128, 'precision_cv_std': 0.0078, 'recall_cv_mean': 0.7735, 'recall_cv_std': 0.0143, 'f1_cv_mean': 0.7925, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8204, 'precision_test': 0.5928, 'recall_test': 0.7897, 'f1_score_test': 0.6772}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9343, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.9334, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.9342, 'f1_cv_std': 0.0028, 'params': 160801, 'accuracy_test': 0.9396, 'precision_test': 0.8415, 'recall_test': 0.9202, 'f1_score_test': 0.8791}, 'MLP_2744833': {'accuracy_cv_mean': 0.9466, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9445, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9491, 'recall_cv_std': 0.0082, 'f1_cv_mean': 0.9468, 'f1_cv_std': 0.0017, 'params': 2744833, 'accuracy_test': 0.9551, 'precision_test': 0.884, 'recall_test': 0.9345, 'f1_score_test': 0.9085}, 'MLP_5843969': {'accuracy_cv_mean': 0.9473, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9478, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9468, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.9472, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9572, 'precision_test': 0.9005, 'recall_test': 0.9225, 'f1_score_test': 0.9114}, 'Logistic Regression': {'accuracy_cv_mean': 0.928, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9345, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0048, 'f1_cv_mean': 0.9274, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9329, 'precision_test': 0.8194, 'recall_test': 0.9217, 'f1_score_test': 0.8676}, 'SVM': {'accuracy_cv_mean': 0.832, 'accuracy_cv_std': 0.0121, 'precision_cv_mean': 0.7958, 'precision_cv_std': 0.0282, 'recall_cv_mean': 0.8967, 'recall_cv_std': 0.028, 'f1_cv_mean': 0.8423, 'f1_cv_std': 0.0075, 'accuracy_test': 0.6783, 'precision_test': 0.4203, 'recall_test': 0.9178, 'f1_score_test': 0.5765}, 'Decision Tree': {'accuracy_cv_mean': 0.8289, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8831, 'precision_cv_std': 0.0196, 'recall_cv_mean': 0.7595, 'recall_cv_std': 0.0255, 'f1_cv_mean': 0.816, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8574, 'precision_test': 0.6715, 'recall_test': 0.788, 'f1_score_test': 0.7251}, 'Random Forest': {'accuracy_cv_mean': 0.8379, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.8738, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.7901, 'recall_cv_std': 0.0052, 'f1_cv_mean': 0.8298, 'f1_cv_std': 0.0038, 'accuracy_test': 0.866, 'precision_test': 0.6928, 'recall_test': 0.788, 'f1_score_test': 0.7373}, 'XGBoost': {'accuracy_cv_mean': 0.9067, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9261, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.8839, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.9045, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9206, 'precision_test': 0.7976, 'recall_test': 0.8938, 'f1_score_test': 0.843}, 'Naive Bayes': {'accuracy_cv_mean': 0.9038, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9303, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.873, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.003, 'accuracy_test': 0.9193, 'precision_test': 0.8014, 'recall_test': 0.8797, 'f1_score_test': 0.8387}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8516, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.857, 'precision_cv_std': 0.0177, 'recall_cv_mean': 0.8451, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0023, 'params': 10401, 'accuracy_test': 0.8801, 'precision_test': 0.7256, 'recall_test': 0.8, 'f1_score_test': 0.761}, 'MLP_338433': {'accuracy_cv_mean': 0.8548, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8482, 'precision_cv_std': 0.0164, 'recall_cv_mean': 0.8651, 'recall_cv_std': 0.0165, 'f1_cv_mean': 0.8563, 'f1_cv_std': 0.0018, 'params': 338433, 'accuracy_test': 0.8883, 'precision_test': 0.7616, 'recall_test': 0.774, 'f1_score_test': 0.7678}, 'MLP_1031169': {'accuracy_cv_mean': 0.8514, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8385, 'precision_cv_std': 0.0098, 'recall_cv_mean': 0.8709, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.8543, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8903, 'precision_test': 0.7607, 'recall_test': 0.7884, 'f1_score_test': 0.7743}, 'Logistic Regression': {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8594, 'precision_cv_std': 0.0025, 'recall_cv_mean': 0.8341, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8466, 'f1_cv_std': 0.0021, 'accuracy_test': 0.8592, 'precision_test': 0.6632, 'recall_test': 0.8331, 'f1_score_test': 0.7385}, 'SVM': {'accuracy_cv_mean': 0.6642, 'accuracy_cv_std': 0.0289, 'precision_cv_mean': 0.6353, 'precision_cv_std': 0.0321, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0306, 'f1_cv_mean': 0.6989, 'f1_cv_std': 0.0209, 'accuracy_test': 0.5661, 'precision_test': 0.3298, 'recall_test': 0.7932, 'f1_score_test': 0.4659}, 'Decision Tree': {'accuracy_cv_mean': 0.7964, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8073, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.7927, 'f1_cv_std': 0.0038, 'accuracy_test': 0.8246, 'precision_test': 0.6052, 'recall_test': 0.762, 'f1_score_test': 0.6746}, 'Random Forest': {'accuracy_cv_mean': 0.8535, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8848, 'precision_cv_std': 0.0055, 'recall_cv_mean': 0.8129, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0033, 'accuracy_test': 0.8767, 'precision_test': 0.7104, 'recall_test': 0.8159, 'f1_score_test': 0.7595}, 'XGBoost': {'accuracy_cv_mean': 0.8762, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8923, 'precision_cv_std': 0.0032, 'recall_cv_mean': 0.8558, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.8736, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8933, 'precision_test': 0.7335, 'recall_test': 0.8682, 'f1_score_test': 0.7952}, 'Naive Bayes': {'accuracy_cv_mean': 0.7807, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.7651, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8102, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.787, 'f1_cv_std': 0.0027, 'accuracy_test': 0.765, 'precision_test': 0.5047, 'recall_test': 0.8136, 'f1_score_test': 0.623}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8967, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8942, 'precision_cv_std': 0.0111, 'recall_cv_mean': 0.9002, 'recall_cv_std': 0.012, 'f1_cv_mean': 0.8971, 'f1_cv_std': 0.0021, 'params': 49953, 'accuracy_test': 0.92, 'precision_test': 0.8404, 'recall_test': 0.8207, 'f1_score_test': 0.8305}, 'MLP_971265': {'accuracy_cv_mean': 0.8982, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8946, 'precision_cv_std': 0.0117, 'recall_cv_mean': 0.903, 'recall_cv_std': 0.012, 'f1_cv_mean': 0.8986, 'f1_cv_std': 0.0024, 'params': 971265, 'accuracy_test': 0.9218, 'precision_test': 0.8492, 'recall_test': 0.8176, 'f1_score_test': 0.8331}, 'MLP_2296833': {'accuracy_cv_mean': 0.8991, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.8987, 'precision_cv_std': 0.008, 'recall_cv_mean': 0.8998, 'recall_cv_std': 0.0108, 'f1_cv_mean': 0.8992, 'f1_cv_std': 0.0024, 'params': 2296833, 'accuracy_test': 0.9164, 'precision_test': 0.8085, 'recall_test': 0.8512, 'f1_score_test': 0.8293}, 'Logistic Regression': {'accuracy_cv_mean': 0.9018, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.907, 'precision_cv_std': 0.005, 'recall_cv_mean': 0.8955, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.9012, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9056, 'precision_test': 0.7542, 'recall_test': 0.8967, 'f1_score_test': 0.8193}, 'SVM': {'accuracy_cv_mean': 0.8367, 'accuracy_cv_std': 0.0059, 'precision_cv_mean': 0.8347, 'precision_cv_std': 0.0156, 'recall_cv_mean': 0.8411, 'recall_cv_std': 0.0358, 'f1_cv_mean': 0.8371, 'f1_cv_std': 0.0104, 'accuracy_test': 0.803, 'precision_test': 0.5554, 'recall_test': 0.874, 'f1_score_test': 0.6792}, 'Decision Tree': {'accuracy_cv_mean': 0.7976, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8128, 'precision_cv_std': 0.0078, 'recall_cv_mean': 0.7735, 'recall_cv_std': 0.0143, 'f1_cv_mean': 0.7925, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8204, 'precision_test': 0.5928, 'recall_test': 0.7897, 'f1_score_test': 0.6772}, 'Random Forest': {'accuracy_cv_mean': 0.873, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9029, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.836, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.8681, 'f1_cv_std': 0.0023, 'accuracy_test': 0.8929, 'precision_test': 0.742, 'recall_test': 0.8452, 'f1_score_test': 0.7903}, 'XGBoost': {'accuracy_cv_mean': 0.9075, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9205, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8921, 'recall_cv_std': 0.0042, 'f1_cv_mean': 0.9061, 'f1_cv_std': 0.0029, 'accuracy_test': 0.9157, 'precision_test': 0.7807, 'recall_test': 0.8994, 'f1_score_test': 0.8358}, 'Naive Bayes': {'accuracy_cv_mean': 0.8335, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.8628, 'precision_cv_std': 0.0088, 'recall_cv_mean': 0.7933, 'recall_cv_std': 0.0027, 'f1_cv_mean': 0.8266, 'f1_cv_std': 0.0026, 'accuracy_test': 0.8534, 'precision_test': 0.6618, 'recall_test': 0.7888, 'f1_score_test': 0.7197}}}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
MLP_5843969: {'accuracy_cv_mean': 0.9473, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9478, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9468, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.9472, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9572, 'precision_test': 0.9005, 'recall_test': 0.9225, 'f1_score_test': 0.9114}
MLP_2744833: {'accuracy_cv_mean': 0.9466, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9445, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.9491, 'recall_cv_std': 0.0082, 'f1_cv_mean': 0.9468, 'f1_cv_std': 0.0017, 'params': 2744833, 'accuracy_test': 0.9551, 'precision_test': 0.884, 'recall_test': 0.9345, 'f1_score_test': 0.9085}
MLP_160801: {'accuracy_cv_mean': 0.9343, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0075, 'recall_cv_mean': 0.9334, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.9342, 'f1_cv_std': 0.0028, 'params': 160801, 'accuracy_test': 0.9396, 'precision_test': 0.8415, 'recall_test': 0.9202, 'f1_score_test': 0.8791}
Logistic Regression: {'accuracy_cv_mean': 0.928, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9345, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.9205, 'recall_cv_std': 0.0048, 'f1_cv_mean': 0.9274, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9329, 'precision_test': 0.8194, 'recall_test': 0.9217, 'f1_score_test': 0.8676}
XGBoost: {'accuracy_cv_mean': 0.9067, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9261, 'precision_cv_std': 0.0038, 'recall_cv_mean': 0.8839, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.9045, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9206, 'precision_test': 0.7976, 'recall_test': 0.8938, 'f1_score_test': 0.843}
Naive Bayes: {'accuracy_cv_mean': 0.9038, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9303, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.873, 'recall_cv_std': 0.0047, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.003, 'accuracy_test': 0.9193, 'precision_test': 0.8014, 'recall_test': 0.8797, 'f1_score_test': 0.8387}
Random Forest: {'accuracy_cv_mean': 0.8379, 'accuracy_cv_std': 0.0042, 'precision_cv_mean': 0.8738, 'precision_cv_std': 0.0091, 'recall_cv_mean': 0.7901, 'recall_cv_std': 0.0052, 'f1_cv_mean': 0.8298, 'f1_cv_std': 0.0038, 'accuracy_test': 0.866, 'precision_test': 0.6928, 'recall_test': 0.788, 'f1_score_test': 0.7373}
Decision Tree: {'accuracy_cv_mean': 0.8289, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.8831, 'precision_cv_std': 0.0196, 'recall_cv_mean': 0.7595, 'recall_cv_std': 0.0255, 'f1_cv_mean': 0.816, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8574, 'precision_test': 0.6715, 'recall_test': 0.788, 'f1_score_test': 0.7251}
SVM: {'accuracy_cv_mean': 0.832, 'accuracy_cv_std': 0.0121, 'precision_cv_mean': 0.7958, 'precision_cv_std': 0.0282, 'recall_cv_mean': 0.8967, 'recall_cv_std': 0.028, 'f1_cv_mean': 0.8423, 'f1_cv_std': 0.0075, 'accuracy_test': 0.6783, 'precision_test': 0.4203, 'recall_test': 0.9178, 'f1_score_test': 0.5765}


EMBEDDINGS TYPE: LYRICS_BERT
XGBoost: {'accuracy_cv_mean': 0.8762, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8923, 'precision_cv_std': 0.0032, 'recall_cv_mean': 0.8558, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.8736, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8933, 'precision_test': 0.7335, 'recall_test': 0.8682, 'f1_score_test': 0.7952}
MLP_1031169: {'accuracy_cv_mean': 0.8514, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8385, 'precision_cv_std': 0.0098, 'recall_cv_mean': 0.8709, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.8543, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8903, 'precision_test': 0.7607, 'recall_test': 0.7884, 'f1_score_test': 0.7743}
MLP_338433: {'accuracy_cv_mean': 0.8548, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8482, 'precision_cv_std': 0.0164, 'recall_cv_mean': 0.8651, 'recall_cv_std': 0.0165, 'f1_cv_mean': 0.8563, 'f1_cv_std': 0.0018, 'params': 338433, 'accuracy_test': 0.8883, 'precision_test': 0.7616, 'recall_test': 0.774, 'f1_score_test': 0.7678}
MLP_10401: {'accuracy_cv_mean': 0.8516, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.857, 'precision_cv_std': 0.0177, 'recall_cv_mean': 0.8451, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.8507, 'f1_cv_std': 0.0023, 'params': 10401, 'accuracy_test': 0.8801, 'precision_test': 0.7256, 'recall_test': 0.8, 'f1_score_test': 0.761}
Random Forest: {'accuracy_cv_mean': 0.8535, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8848, 'precision_cv_std': 0.0055, 'recall_cv_mean': 0.8129, 'recall_cv_std': 0.003, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0033, 'accuracy_test': 0.8767, 'precision_test': 0.7104, 'recall_test': 0.8159, 'f1_score_test': 0.7595}
Logistic Regression: {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8594, 'precision_cv_std': 0.0025, 'recall_cv_mean': 0.8341, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8466, 'f1_cv_std': 0.0021, 'accuracy_test': 0.8592, 'precision_test': 0.6632, 'recall_test': 0.8331, 'f1_score_test': 0.7385}
Decision Tree: {'accuracy_cv_mean': 0.7964, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8073, 'precision_cv_std': 0.0077, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.7927, 'f1_cv_std': 0.0038, 'accuracy_test': 0.8246, 'precision_test': 0.6052, 'recall_test': 0.762, 'f1_score_test': 0.6746}
Naive Bayes: {'accuracy_cv_mean': 0.7807, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.7651, 'precision_cv_std': 0.0041, 'recall_cv_mean': 0.8102, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.787, 'f1_cv_std': 0.0027, 'accuracy_test': 0.765, 'precision_test': 0.5047, 'recall_test': 0.8136, 'f1_score_test': 0.623}
SVM: {'accuracy_cv_mean': 0.6642, 'accuracy_cv_std': 0.0289, 'precision_cv_mean': 0.6353, 'precision_cv_std': 0.0321, 'recall_cv_mean': 0.7787, 'recall_cv_std': 0.0306, 'f1_cv_mean': 0.6989, 'f1_cv_std': 0.0209, 'accuracy_test': 0.5661, 'precision_test': 0.3298, 'recall_test': 0.7932, 'f1_score_test': 0.4659}


EMBEDDINGS TYPE: GPT
XGBoost: {'accuracy_cv_mean': 0.9075, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9205, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8921, 'recall_cv_std': 0.0042, 'f1_cv_mean': 0.9061, 'f1_cv_std': 0.0029, 'accuracy_test': 0.9157, 'precision_test': 0.7807, 'recall_test': 0.8994, 'f1_score_test': 0.8358}
MLP_971265: {'accuracy_cv_mean': 0.8982, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8946, 'precision_cv_std': 0.0117, 'recall_cv_mean': 0.903, 'recall_cv_std': 0.012, 'f1_cv_mean': 0.8986, 'f1_cv_std': 0.0024, 'params': 971265, 'accuracy_test': 0.9218, 'precision_test': 0.8492, 'recall_test': 0.8176, 'f1_score_test': 0.8331}
MLP_49953: {'accuracy_cv_mean': 0.8967, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8942, 'precision_cv_std': 0.0111, 'recall_cv_mean': 0.9002, 'recall_cv_std': 0.012, 'f1_cv_mean': 0.8971, 'f1_cv_std': 0.0021, 'params': 49953, 'accuracy_test': 0.92, 'precision_test': 0.8404, 'recall_test': 0.8207, 'f1_score_test': 0.8305}
MLP_2296833: {'accuracy_cv_mean': 0.8991, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.8987, 'precision_cv_std': 0.008, 'recall_cv_mean': 0.8998, 'recall_cv_std': 0.0108, 'f1_cv_mean': 0.8992, 'f1_cv_std': 0.0024, 'params': 2296833, 'accuracy_test': 0.9164, 'precision_test': 0.8085, 'recall_test': 0.8512, 'f1_score_test': 0.8293}
Logistic Regression: {'accuracy_cv_mean': 0.9018, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.907, 'precision_cv_std': 0.005, 'recall_cv_mean': 0.8955, 'recall_cv_std': 0.0051, 'f1_cv_mean': 0.9012, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9056, 'precision_test': 0.7542, 'recall_test': 0.8967, 'f1_score_test': 0.8193}
Random Forest: {'accuracy_cv_mean': 0.873, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9029, 'precision_cv_std': 0.0064, 'recall_cv_mean': 0.836, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.8681, 'f1_cv_std': 0.0023, 'accuracy_test': 0.8929, 'precision_test': 0.742, 'recall_test': 0.8452, 'f1_score_test': 0.7903}
Naive Bayes: {'accuracy_cv_mean': 0.8335, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.8628, 'precision_cv_std': 0.0088, 'recall_cv_mean': 0.7933, 'recall_cv_std': 0.0027, 'f1_cv_mean': 0.8266, 'f1_cv_std': 0.0026, 'accuracy_test': 0.8534, 'precision_test': 0.6618, 'recall_test': 0.7888, 'f1_score_test': 0.7197}
SVM: {'accuracy_cv_mean': 0.8367, 'accuracy_cv_std': 0.0059, 'precision_cv_mean': 0.8347, 'precision_cv_std': 0.0156, 'recall_cv_mean': 0.8411, 'recall_cv_std': 0.0358, 'f1_cv_mean': 0.8371, 'f1_cv_std': 0.0104, 'accuracy_test': 0.803, 'precision_test': 0.5554, 'recall_test': 0.874, 'f1_score_test': 0.6792}
Decision Tree: {'accuracy_cv_mean': 0.7976, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8128, 'precision_cv_std': 0.0078, 'recall_cv_mean': 0.7735, 'recall_cv_std': 0.0143, 'f1_cv_mean': 0.7925, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8204, 'precision_test': 0.5928, 'recall_test': 0.7897, 'f1_score_test': 0.6772}
Diccionario global guardado en: outputs_cv/5/gpt/global_metrics.json

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
====================================

