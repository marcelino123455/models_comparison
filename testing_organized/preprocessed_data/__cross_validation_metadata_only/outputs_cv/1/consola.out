2025-10-23 01:43:10.246595: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-23 01:43:10.246594: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-23 01:43:10.345812: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-23 01:43:10.345815: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-23 01:43:15.959167: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-23 01:43:15.959167: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 15 ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']

CAT_LOW 3 ['emotion', 'Key', 'Time signature']
CAT_HIGH 12 ['Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
After removing some columns that ignorated, for TF-IDF embbedings you are selecteing this columns:
--> ['text']
After removing some columns that ignorated, for numeric cols you are selecteing this columns:
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
--> PaTH:  ../../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy
Running experiment with TFIDF embeddings
Contaning the categorical cols
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 5023)
Shape of X_test after concatenation:  (21625, 5023)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [5023, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3185, Test Loss: 0.2015, F1: 0.9226, AUC: 0.9764
Epoch [10/30] Train Loss: 0.0822, Test Loss: 0.2090, F1: 0.9290, AUC: 0.9815
Epoch [20/30] Train Loss: 0.0634, Test Loss: 0.2840, F1: 0.9242, AUC: 0.9790
Mejores resultados en la época:  3
f1-score 0.933074684772066
AUC según el mejor F1-score 0.9832824189877412

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3197, Test Loss: 0.2053, F1: 0.9161, AUC: 0.9756
Epoch [10/30] Train Loss: 0.0796, Test Loss: 0.2161, F1: 0.9268, AUC: 0.9806
Epoch [20/30] Train Loss: 0.0579, Test Loss: 0.2946, F1: 0.9261, AUC: 0.9781
Mejores resultados en la época:  4
f1-score 0.9325063845311929
AUC según el mejor F1-score 0.9822682693066072

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3159, Test Loss: 0.1956, F1: 0.9209, AUC: 0.9784
Epoch [10/30] Train Loss: 0.0815, Test Loss: 0.2125, F1: 0.9250, AUC: 0.9812
Epoch [20/30] Train Loss: 0.0576, Test Loss: 0.2784, F1: 0.9288, AUC: 0.9794
Mejores resultados en la época:  3
f1-score 0.9337539432176656
AUC según el mejor F1-score 0.983512226015564

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3304, Test Loss: 0.1999, F1: 0.9226, AUC: 0.9778
Epoch [10/30] Train Loss: 0.0748, Test Loss: 0.2104, F1: 0.9306, AUC: 0.9813
Epoch [20/30] Train Loss: 0.0441, Test Loss: 0.2844, F1: 0.9262, AUC: 0.9790
Mejores resultados en la época:  3
f1-score 0.9370696944075372
AUC según el mejor F1-score 0.984349730363291

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3191, Test Loss: 0.2042, F1: 0.9226, AUC: 0.9772
Epoch [10/30] Train Loss: 0.0801, Test Loss: 0.2195, F1: 0.9300, AUC: 0.9799
Epoch [20/30] Train Loss: 0.0637, Test Loss: 0.2893, F1: 0.9278, AUC: 0.9775
Mejores resultados en la época:  4
f1-score 0.9354174239670423
AUC según el mejor F1-score 0.982658190860715
Epoch [0/30] Train Loss: 0.2941, Test Loss: 0.1833, F1: 0.8647, AUC: 0.9784
Epoch [10/30] Train Loss: 0.0899, Test Loss: 0.2104, F1: 0.8623, AUC: 0.9804
Epoch [20/30] Train Loss: 0.0763, Test Loss: 0.2524, F1: 0.8564, AUC: 0.9793
Mejores resultados en la época:  1
f1-score 0.8736038719285183
AUC según el mejor F1-score 0.9820795050341695
Confusion matrix Test saved: outputs_cv/1/tfidf/cm_mlp_1.png

========================================
Entrenando red 5 con capas [5023, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2342, Test Loss: 0.1789, F1: 0.9294, AUC: 0.9808
Epoch [10/30] Train Loss: 0.0024, Test Loss: 0.4327, F1: 0.9436, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0021, Test Loss: 0.4149, F1: 0.9424, AUC: 0.9858
Mejores resultados en la época:  17
f1-score 0.9474582469828111
AUC según el mejor F1-score 0.9863167402800312

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2344, Test Loss: 0.1829, F1: 0.9273, AUC: 0.9801
Epoch [10/30] Train Loss: 0.0037, Test Loss: 0.3710, F1: 0.9351, AUC: 0.9852
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5189, F1: 0.9415, AUC: 0.9856
Mejores resultados en la época:  29
f1-score 0.9429951690821256
AUC según el mejor F1-score 0.9840037642344209

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2370, Test Loss: 0.1791, F1: 0.9273, AUC: 0.9811
Epoch [10/30] Train Loss: 0.0025, Test Loss: 0.4973, F1: 0.9413, AUC: 0.9853
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6385, F1: 0.9380, AUC: 0.9826
Mejores resultados en la época:  27
f1-score 0.9446736229070614
AUC según el mejor F1-score 0.9860539234327115

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2363, Test Loss: 0.1691, F1: 0.9309, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0031, Test Loss: 0.4165, F1: 0.9431, AUC: 0.9854
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6696, F1: 0.9453, AUC: 0.9817
Mejores resultados en la época:  26
f1-score 0.9457984721716988
AUC según el mejor F1-score 0.9817186358317225

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2342, Test Loss: 0.1803, F1: 0.9277, AUC: 0.9810
Epoch [10/30] Train Loss: 0.0029, Test Loss: 0.5693, F1: 0.9357, AUC: 0.9837
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6169, F1: 0.9459, AUC: 0.9840
Mejores resultados en la época:  27
f1-score 0.9465484180249281
AUC según el mejor F1-score 0.9826807310244693
Epoch [0/30] Train Loss: 0.2248, Test Loss: 0.1702, F1: 0.8647, AUC: 0.9812
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 15 ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']

CAT_LOW 3 ['emotion', 'Key', 'Time signature']
CAT_HIGH 12 ['Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
After removing some columns that ignorated, for TF-IDF embbedings you are selecteing this columns:
--> ['text']
After removing some columns that ignorated, for numeric cols you are selecteing this columns:
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
--> PaTH:  ../../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy
Running experiment with TFIDF embeddings
Contaning the categorical cols
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 5023)
Shape of X_test after concatenation:  (21625, 5023)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [5023, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3185, Test Loss: 0.2015, F1: 0.9226, AUC: 0.9764
Epoch [10/30] Train Loss: 0.0822, Test Loss: 0.2090, F1: 0.9290, AUC: 0.9815
Epoch [20/30] Train Loss: 0.0634, Test Loss: 0.2840, F1: 0.9242, AUC: 0.9790
Mejores resultados en la época:  3
f1-score 0.933074684772066
AUC según el mejor F1-score 0.9832824189877412

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3197, Test Loss: 0.2053, F1: 0.9161, AUC: 0.9756
Epoch [10/30] Train Loss: 0.0796, Test Loss: 0.2161, F1: 0.9268, AUC: 0.9806
Epoch [20/30] Train Loss: 0.0579, Test Loss: 0.2946, F1: 0.9261, AUC: 0.9781
Mejores resultados en la época:  4
f1-score 0.9325063845311929
AUC según el mejor F1-score 0.9822682693066072

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3159, Test Loss: 0.1956, F1: 0.9209, AUC: 0.9784
Epoch [10/30] Train Loss: 0.0815, Test Loss: 0.2125, F1: 0.9250, AUC: 0.9812
Epoch [20/30] Train Loss: 0.0576, Test Loss: 0.2784, F1: 0.9288, AUC: 0.9794
Mejores resultados en la época:  3
f1-score 0.9337539432176656
AUC según el mejor F1-score 0.983512226015564

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3304, Test Loss: 0.1999, F1: 0.9226, AUC: 0.9778
Epoch [10/30] Train Loss: 0.0748, Test Loss: 0.2104, F1: 0.9306, AUC: 0.9813
Epoch [20/30] Train Loss: 0.0441, Test Loss: 0.2844, F1: 0.9262, AUC: 0.9790
Mejores resultados en la época:  3
f1-score 0.9370696944075372
AUC según el mejor F1-score 0.984349730363291

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3191, Test Loss: 0.2042, F1: 0.9226, AUC: 0.9772
Epoch [10/30] Train Loss: 0.0801, Test Loss: 0.2195, F1: 0.9300, AUC: 0.9799
Epoch [20/30] Train Loss: 0.0637, Test Loss: 0.2893, F1: 0.9278, AUC: 0.9775
Mejores resultados en la época:  4
f1-score 0.9354174239670423
AUC según el mejor F1-score 0.982658190860715
Epoch [0/30] Train Loss: 0.2941, Test Loss: 0.1833, F1: 0.8647, AUC: 0.9784
Epoch [10/30] Train Loss: 0.0899, Test Loss: 0.2104, F1: 0.8623, AUC: 0.9804
Epoch [20/30] Train Loss: 0.0763, Test Loss: 0.2524, F1: 0.8564, AUC: 0.9793
Mejores resultados en la época:  1
f1-score 0.8736038719285183
AUC según el mejor F1-score 0.9820795050341695
Confusion matrix Test saved: outputs_cv/1/tfidf/cm_mlp_1.png

========================================
Entrenando red 5 con capas [5023, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2342, Test Loss: 0.1789, F1: 0.9294, AUC: 0.9808
Epoch [10/30] Train Loss: 0.0024, Test Loss: 0.4327, F1: 0.9436, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0021, Test Loss: 0.4149, F1: 0.9424, AUC: 0.9858
Mejores resultados en la época:  17
f1-score 0.9474582469828111
AUC según el mejor F1-score 0.9863167402800312

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2344, Test Loss: 0.1829, F1: 0.9273, AUC: 0.9801
Epoch [10/30] Train Loss: 0.0037, Test Loss: 0.3710, F1: 0.9351, AUC: 0.9852
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5189, F1: 0.9415, AUC: 0.9856
Mejores resultados en la época:  29
f1-score 0.9429951690821256
AUC según el mejor F1-score 0.9840037642344209

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2370, Test Loss: 0.1791, F1: 0.9273, AUC: 0.9811
Epoch [10/30] Train Loss: 0.0025, Test Loss: 0.4973, F1: 0.9413, AUC: 0.9853
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6385, F1: 0.9380, AUC: 0.9826
Mejores resultados en la época:  27
f1-score 0.9446736229070614
AUC según el mejor F1-score 0.9860539234327115

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2363, Test Loss: 0.1691, F1: 0.9309, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0031, Test Loss: 0.4165, F1: 0.9431, AUC: 0.9854
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6696, F1: 0.9453, AUC: 0.9817
Mejores resultados en la época:  26
f1-score 0.9457984721716988
AUC según el mejor F1-score 0.9817186358317225

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2342, Test Loss: 0.1803, F1: 0.9277, AUC: 0.9810
Epoch [10/30] Train Loss: 0.0029, Test Loss: 0.5693, F1: 0.9357, AUC: 0.9837
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6169, F1: 0.9459, AUC: 0.9840
Mejores resultados en la época:  27
f1-score 0.9465484180249281
AUC según el mejor F1-score 0.9826807310244693
Epoch [0/30] Train Loss: 0.2248, Test Loss: 0.1702, F1: 0.8647, AUC: 0.9812
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.3379, F1: 0.8943, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0012, Test Loss: 0.3522, F1: 0.9060, AUC: 0.9869
Mejores resultados en la época:  26
f1-score 0.9090564248458985
AUC según el mejor F1-score 0.9872180064830967
Confusion matrix Test saved: outputs_cv/1/tfidf/cm_mlp_5.png

========================================
Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2362, Test Loss: 0.1737, F1: 0.9289, AUC: 0.9815
Epoch [10/30] Train Loss: 0.0035, Test Loss: 0.4715, F1: 0.9457, AUC: 0.9847
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6707, F1: 0.9474, AUC: 0.9841
Mejores resultados en la época:  17
f1-score 0.9483009708737864
AUC según el mejor F1-score 0.9855855947847185

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2390, Test Loss: 0.1785, F1: 0.9298, AUC: 0.9810
Epoch [10/30] Train Loss: 0.0009, Test Loss: 0.4095, F1: 0.9427, AUC: 0.9839
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.9199, F1: 0.9430, AUC: 0.9825
Mejores resultados en la época:  9
f1-score 0.9456046624575036
AUC según el mejor F1-score 0.9843426063638003

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2375, Test Loss: 0.1916, F1: 0.9242, AUC: 0.9814
Epoch [10/30] Train Loss: 0.0040, Test Loss: 0.4471, F1: 0.9429, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0013, Test Loss: 0.4280, F1: 0.9426, AUC: 0.9853
Mejores resultados en la época:  16
f1-score 0.9456947162426614
AUC según el mejor F1-score 0.9865226041854456

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2413, Test Loss: 0.1741, F1: 0.9309, AUC: 0.9816
Epoch [10/30] Train Loss: 0.0056, Test Loss: 0.4864, F1: 0.9421, AUC: 0.9850
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.9570, F1: 0.9430, AUC: 0.9763
Mejores resultados en la época:  12
f1-score 0.944167376231602
AUC según el mejor F1-score 0.9855542497130824

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2347, Test Loss: 0.1895, F1: 0.9278, AUC: 0.9805
Epoch [10/30] Train Loss: 0.0058, Test Loss: 0.3829, F1: 0.9406, AUC: 0.9838
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6605, F1: 0.9457, AUC: 0.9810
Mejores resultados en la época:  26
f1-score 0.9463462005341102
AUC según el mejor F1-score 0.9783800501706479
Epoch [0/30] Train Loss: 0.2211, Test Loss: 0.1676, F1: 0.8675, AUC: 0.9820
Epoch [10/30] Train Loss: 0.0035, Test Loss: 0.3623, F1: 0.9006, AUC: 0.9867
Epoch [20/30] Train Loss: 0.0024, Test Loss: 0.3812, F1: 0.8812, AUC: 0.9871
Mejores resultados en la época:  18
f1-score 0.9076014478948371
AUC según el mejor F1-score 0.9877024614109797
Confusion matrix Test saved: outputs_cv/1/tfidf/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9344, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0009, 'recall_cv_mean': 0.9336, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9344, 'f1_cv_std': 0.0017, 'params': 160801, 'accuracy_test': 0.9372, 'precision_test': 0.8404, 'recall_test': 0.9095, 'f1_score_test': 0.8736}, 'MLP_2744833': {'accuracy_cv_mean': 0.9454, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9447, 'precision_cv_std': 0.0058, 'recall_cv_mean': 0.9464, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.9455, 'f1_cv_std': 0.0015, 'params': 2744833, 'accuracy_test': 0.9557, 'precision_test': 0.8901, 'recall_test': 0.9289, 'f1_score_test': 0.9091}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0013, 'precision_cv_mean': 0.9499, 'precision_cv_std': 0.0027, 'recall_cv_mean': 0.9422, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.946, 'f1_cv_std': 0.0013, 'params': 5843969, 'accuracy_test': 0.9551, 'precision_test': 0.8925, 'recall_test': 0.9233, 'f1_score_test': 0.9076}}}
Saved on: outputs_cv/1/tfidf

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9278, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9348, 'precision_cv_std': 0.0019, 'recall_cv_mean': 0.9198, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.9272, 'f1_cv_std': 0.0035}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 43, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.93     21625

Confusion matrix Test saved as: outputs_cv/1/tfidf/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/1/tfidf/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8164, 'accuracy_cv_std': 0.0186, 'precision_cv_mean': 0.7725, 'precision_cv_std': 0.0325, 'recall_cv_mean': 0.9014, 'recall_cv_std': 0.0238, 'f1_cv_mean': 0.8311, 'f1_cv_std': 0.0121}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 43, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.71      0.81     16465
           1       0.49      0.90      0.63      5160

    accuracy                           0.75     21625
   macro avg       0.72      0.80      0.72     21625
weighted avg       0.84      0.75      0.77     21625

Confusion matrix Test saved as: outputs_cv/1/tfidf/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/1/tfidf/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.829, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7388, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.812, 'f1_cv_std': 0.0056}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 43, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.91      0.92      0.92     16465
           1       0.75      0.72      0.73      5160

    accuracy                           0.87     21625
   macro avg       0.83      0.82      0.83     21625
weighted avg       0.87      0.87      0.87     21625

Confusion matrix Test saved as: outputs_cv/1/tfidf/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/1/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8321, 'accuracy_cv_std': 0.009, 'precision_cv_mean': 0.8668, 'precision_cv_std': 0.0097, 'recall_cv_mean': 0.7849, 'recall_cv_std': 0.0135, 'f1_cv_mean': 0.8238, 'f1_cv_std': 0.01}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 43, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.78      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/1/tfidf/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/1/tfidf/random_forest_model.pkl

Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.3379, F1: 0.8943, AUC: 0.9865
Epoch [20/30] Train Loss: 0.0012, Test Loss: 0.3522, F1: 0.9060, AUC: 0.9869
Mejores resultados en la época:  26
f1-score 0.9090564248458985
AUC según el mejor F1-score 0.9872180064830967
Confusion matrix Test saved: outputs_cv/1/tfidf/cm_mlp_5.png

========================================
Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2362, Test Loss: 0.1737, F1: 0.9289, AUC: 0.9815
Epoch [10/30] Train Loss: 0.0035, Test Loss: 0.4715, F1: 0.9457, AUC: 0.9847
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6707, F1: 0.9474, AUC: 0.9841
Mejores resultados en la época:  17
f1-score 0.9483009708737864
AUC según el mejor F1-score 0.9855855947847185

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2390, Test Loss: 0.1785, F1: 0.9298, AUC: 0.9810
Epoch [10/30] Train Loss: 0.0009, Test Loss: 0.4095, F1: 0.9427, AUC: 0.9839
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.9199, F1: 0.9430, AUC: 0.9825
Mejores resultados en la época:  9
f1-score 0.9456046624575036
AUC según el mejor F1-score 0.9843426063638003

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2375, Test Loss: 0.1916, F1: 0.9242, AUC: 0.9814
Epoch [10/30] Train Loss: 0.0040, Test Loss: 0.4471, F1: 0.9429, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0013, Test Loss: 0.4280, F1: 0.9426, AUC: 0.9853
Mejores resultados en la época:  16
f1-score 0.9456947162426614
AUC según el mejor F1-score 0.9865226041854456

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2413, Test Loss: 0.1741, F1: 0.9309, AUC: 0.9816
Epoch [10/30] Train Loss: 0.0056, Test Loss: 0.4864, F1: 0.9421, AUC: 0.9850
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.9570, F1: 0.9430, AUC: 0.9763
Mejores resultados en la época:  12
f1-score 0.944167376231602
AUC según el mejor F1-score 0.9855542497130824

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2347, Test Loss: 0.1895, F1: 0.9278, AUC: 0.9805
Epoch [10/30] Train Loss: 0.0058, Test Loss: 0.3829, F1: 0.9406, AUC: 0.9838
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6605, F1: 0.9457, AUC: 0.9810
Mejores resultados en la época:  26
f1-score 0.9463462005341102
AUC según el mejor F1-score 0.9783800501706479
Epoch [0/30] Train Loss: 0.2211, Test Loss: 0.1676, F1: 0.8675, AUC: 0.9820
Epoch [10/30] Train Loss: 0.0035, Test Loss: 0.3623, F1: 0.9006, AUC: 0.9867
Epoch [20/30] Train Loss: 0.0024, Test Loss: 0.3812, F1: 0.8812, AUC: 0.9871
Mejores resultados en la época:  18
f1-score 0.9076014478948371
AUC según el mejor F1-score 0.9877024614109797
Confusion matrix Test saved: outputs_cv/1/tfidf/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9344, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0009, 'recall_cv_mean': 0.9336, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9344, 'f1_cv_std': 0.0017, 'params': 160801, 'accuracy_test': 0.9372, 'precision_test': 0.8404, 'recall_test': 0.9095, 'f1_score_test': 0.8736}, 'MLP_2744833': {'accuracy_cv_mean': 0.9454, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9447, 'precision_cv_std': 0.0058, 'recall_cv_mean': 0.9464, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.9455, 'f1_cv_std': 0.0015, 'params': 2744833, 'accuracy_test': 0.9557, 'precision_test': 0.8901, 'recall_test': 0.9289, 'f1_score_test': 0.9091}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0013, 'precision_cv_mean': 0.9499, 'precision_cv_std': 0.0027, 'recall_cv_mean': 0.9422, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.946, 'f1_cv_std': 0.0013, 'params': 5843969, 'accuracy_test': 0.9551, 'precision_test': 0.8925, 'recall_test': 0.9233, 'f1_score_test': 0.9076}}}
Saved on: outputs_cv/1/tfidf

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9278, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9348, 'precision_cv_std': 0.0019, 'recall_cv_mean': 0.9198, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.9272, 'f1_cv_std': 0.0035}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 43, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.93     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.93      0.93     21625

Confusion matrix Test saved as: outputs_cv/1/tfidf/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/1/tfidf/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8164, 'accuracy_cv_std': 0.0186, 'precision_cv_mean': 0.7725, 'precision_cv_std': 0.0325, 'recall_cv_mean': 0.9014, 'recall_cv_std': 0.0238, 'f1_cv_mean': 0.8311, 'f1_cv_std': 0.0121}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 43, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.71      0.81     16465
           1       0.49      0.90      0.63      5160

    accuracy                           0.75     21625
   macro avg       0.72      0.80      0.72     21625
weighted avg       0.84      0.75      0.77     21625

Confusion matrix Test saved as: outputs_cv/1/tfidf/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/1/tfidf/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.829, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7388, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.812, 'f1_cv_std': 0.0056}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 43, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.91      0.92      0.92     16465
           1       0.75      0.72      0.73      5160

    accuracy                           0.87     21625
   macro avg       0.83      0.82      0.83     21625
weighted avg       0.87      0.87      0.87     21625

Confusion matrix Test saved as: outputs_cv/1/tfidf/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/1/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8321, 'accuracy_cv_std': 0.009, 'precision_cv_mean': 0.8668, 'precision_cv_std': 0.0097, 'recall_cv_mean': 0.7849, 'recall_cv_std': 0.0135, 'f1_cv_mean': 0.8238, 'f1_cv_std': 0.01}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 43, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.78      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/1/tfidf/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/1/tfidf/random_forest_model.pkl

/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:13:01] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:13:07] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:17:04] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:17:11] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:21:04] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:21:11] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:25:05] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:25:13] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:29:07] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:29:14] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:33:15] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:33:22] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.9273, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.885, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.9057, 'f1_cv_std': 0.0039}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 43, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/1/tfidf/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/1/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9033, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.9277, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0066, 'f1_cv_mean': 0.9004, 'f1_cv_std': 0.0035}
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.94      0.95     16465
           1       0.81      0.87      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.90      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/1/tfidf/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/1/tfidf/naive_bayes_model.pkl


Resumen Final:
Logistic Regression: {'accuracy_cv_mean': 0.9278, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9348, 'precision_cv_std': 0.0019, 'recall_cv_mean': 0.9198, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.9272, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9337, 'precision_test': 0.8257, 'recall_test': 0.9155, 'f1_score_test': 0.8683}
XGBoost: {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.9273, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.885, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.9057, 'f1_cv_std': 0.0039, 'accuracy_test': 0.9204, 'precision_test': 0.8022, 'recall_test': 0.8843, 'f1_score_test': 0.8413}
Naive Bayes: {'accuracy_cv_mean': 0.9033, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.9277, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0066, 'f1_cv_mean': 0.9004, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9193, 'precision_test': 0.8091, 'recall_test': 0.8659, 'f1_score_test': 0.8365}
Decision Tree: {'accuracy_cv_mean': 0.829, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7388, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.812, 'f1_cv_std': 0.0056, 'accuracy_test': 0.875, 'precision_test': 0.7471, 'recall_test': 0.7196, 'f1_score_test': 0.7331}
Random Forest: {'accuracy_cv_mean': 0.8321, 'accuracy_cv_std': 0.009, 'precision_cv_mean': 0.8668, 'precision_cv_std': 0.0097, 'recall_cv_mean': 0.7849, 'recall_cv_std': 0.0135, 'f1_cv_mean': 0.8238, 'f1_cv_std': 0.01, 'accuracy_test': 0.8567, 'precision_test': 0.6735, 'recall_test': 0.7756, 'f1_score_test': 0.721}
SVM: {'accuracy_cv_mean': 0.8164, 'accuracy_cv_std': 0.0186, 'precision_cv_mean': 0.7725, 'precision_cv_std': 0.0325, 'recall_cv_mean': 0.9014, 'recall_cv_std': 0.0238, 'f1_cv_mean': 0.8311, 'f1_cv_std': 0.0121, 'accuracy_test': 0.753, 'precision_test': 0.4904, 'recall_test': 0.8963, 'f1_score_test': 0.6339}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9344, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0009, 'recall_cv_mean': 0.9336, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9344, 'f1_cv_std': 0.0017, 'params': 160801, 'accuracy_test': 0.9372, 'precision_test': 0.8404, 'recall_test': 0.9095, 'f1_score_test': 0.8736}, 'MLP_2744833': {'accuracy_cv_mean': 0.9454, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9447, 'precision_cv_std': 0.0058, 'recall_cv_mean': 0.9464, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.9455, 'f1_cv_std': 0.0015, 'params': 2744833, 'accuracy_test': 0.9557, 'precision_test': 0.8901, 'recall_test': 0.9289, 'f1_score_test': 0.9091}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0013, 'precision_cv_mean': 0.9499, 'precision_cv_std': 0.0027, 'recall_cv_mean': 0.9422, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.946, 'f1_cv_std': 0.0013, 'params': 5843969, 'accuracy_test': 0.9551, 'precision_test': 0.8925, 'recall_test': 0.9233, 'f1_score_test': 0.9076}, 'Logistic Regression': {'accuracy_cv_mean': 0.9278, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9348, 'precision_cv_std': 0.0019, 'recall_cv_mean': 0.9198, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.9272, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9337, 'precision_test': 0.8257, 'recall_test': 0.9155, 'f1_score_test': 0.8683}, 'SVM': {'accuracy_cv_mean': 0.8164, 'accuracy_cv_std': 0.0186, 'precision_cv_mean': 0.7725, 'precision_cv_std': 0.0325, 'recall_cv_mean': 0.9014, 'recall_cv_std': 0.0238, 'f1_cv_mean': 0.8311, 'f1_cv_std': 0.0121, 'accuracy_test': 0.753, 'precision_test': 0.4904, 'recall_test': 0.8963, 'f1_score_test': 0.6339}, 'Decision Tree': {'accuracy_cv_mean': 0.829, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7388, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.812, 'f1_cv_std': 0.0056, 'accuracy_test': 0.875, 'precision_test': 0.7471, 'recall_test': 0.7196, 'f1_score_test': 0.7331}, 'Random Forest': {'accuracy_cv_mean': 0.8321, 'accuracy_cv_std': 0.009, 'precision_cv_mean': 0.8668, 'precision_cv_std': 0.0097, 'recall_cv_mean': 0.7849, 'recall_cv_std': 0.0135, 'f1_cv_mean': 0.8238, 'f1_cv_std': 0.01, 'accuracy_test': 0.8567, 'precision_test': 0.6735, 'recall_test': 0.7756, 'f1_score_test': 0.721}, 'XGBoost': {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.9273, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.885, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.9057, 'f1_cv_std': 0.0039, 'accuracy_test': 0.9204, 'precision_test': 0.8022, 'recall_test': 0.8843, 'f1_score_test': 0.8413}, 'Naive Bayes': {'accuracy_cv_mean': 0.9033, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.9277, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0066, 'f1_cv_mean': 0.9004, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9193, 'precision_test': 0.8091, 'recall_test': 0.8659, 'f1_score_test': 0.8365}}}
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.9273, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.885, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.9057, 'f1_cv_std': 0.0039}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 43, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.88      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/1/tfidf/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/1/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9033, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.9277, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0066, 'f1_cv_mean': 0.9004, 'f1_cv_std': 0.0035}
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.94      0.95     16465
           1       0.81      0.87      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.90      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/1/tfidf/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/1/tfidf/naive_bayes_model.pkl


Resumen Final:
Logistic Regression: {'accuracy_cv_mean': 0.9278, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9348, 'precision_cv_std': 0.0019, 'recall_cv_mean': 0.9198, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.9272, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9337, 'precision_test': 0.8257, 'recall_test': 0.9155, 'f1_score_test': 0.8683}
XGBoost: {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.9273, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.885, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.9057, 'f1_cv_std': 0.0039, 'accuracy_test': 0.9204, 'precision_test': 0.8022, 'recall_test': 0.8843, 'f1_score_test': 0.8413}
Naive Bayes: {'accuracy_cv_mean': 0.9033, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.9277, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0066, 'f1_cv_mean': 0.9004, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9193, 'precision_test': 0.8091, 'recall_test': 0.8659, 'f1_score_test': 0.8365}
Decision Tree: {'accuracy_cv_mean': 0.829, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7388, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.812, 'f1_cv_std': 0.0056, 'accuracy_test': 0.875, 'precision_test': 0.7471, 'recall_test': 0.7196, 'f1_score_test': 0.7331}
Random Forest: {'accuracy_cv_mean': 0.8321, 'accuracy_cv_std': 0.009, 'precision_cv_mean': 0.8668, 'precision_cv_std': 0.0097, 'recall_cv_mean': 0.7849, 'recall_cv_std': 0.0135, 'f1_cv_mean': 0.8238, 'f1_cv_std': 0.01, 'accuracy_test': 0.8567, 'precision_test': 0.6735, 'recall_test': 0.7756, 'f1_score_test': 0.721}
SVM: {'accuracy_cv_mean': 0.8164, 'accuracy_cv_std': 0.0186, 'precision_cv_mean': 0.7725, 'precision_cv_std': 0.0325, 'recall_cv_mean': 0.9014, 'recall_cv_std': 0.0238, 'f1_cv_mean': 0.8311, 'f1_cv_std': 0.0121, 'accuracy_test': 0.753, 'precision_test': 0.4904, 'recall_test': 0.8963, 'f1_score_test': 0.6339}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9344, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0009, 'recall_cv_mean': 0.9336, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9344, 'f1_cv_std': 0.0017, 'params': 160801, 'accuracy_test': 0.9372, 'precision_test': 0.8404, 'recall_test': 0.9095, 'f1_score_test': 0.8736}, 'MLP_2744833': {'accuracy_cv_mean': 0.9454, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9447, 'precision_cv_std': 0.0058, 'recall_cv_mean': 0.9464, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.9455, 'f1_cv_std': 0.0015, 'params': 2744833, 'accuracy_test': 0.9557, 'precision_test': 0.8901, 'recall_test': 0.9289, 'f1_score_test': 0.9091}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0013, 'precision_cv_mean': 0.9499, 'precision_cv_std': 0.0027, 'recall_cv_mean': 0.9422, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.946, 'f1_cv_std': 0.0013, 'params': 5843969, 'accuracy_test': 0.9551, 'precision_test': 0.8925, 'recall_test': 0.9233, 'f1_score_test': 0.9076}, 'Logistic Regression': {'accuracy_cv_mean': 0.9278, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9348, 'precision_cv_std': 0.0019, 'recall_cv_mean': 0.9198, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.9272, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9337, 'precision_test': 0.8257, 'recall_test': 0.9155, 'f1_score_test': 0.8683}, 'SVM': {'accuracy_cv_mean': 0.8164, 'accuracy_cv_std': 0.0186, 'precision_cv_mean': 0.7725, 'precision_cv_std': 0.0325, 'recall_cv_mean': 0.9014, 'recall_cv_std': 0.0238, 'f1_cv_mean': 0.8311, 'f1_cv_std': 0.0121, 'accuracy_test': 0.753, 'precision_test': 0.4904, 'recall_test': 0.8963, 'f1_score_test': 0.6339}, 'Decision Tree': {'accuracy_cv_mean': 0.829, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7388, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.812, 'f1_cv_std': 0.0056, 'accuracy_test': 0.875, 'precision_test': 0.7471, 'recall_test': 0.7196, 'f1_score_test': 0.7331}, 'Random Forest': {'accuracy_cv_mean': 0.8321, 'accuracy_cv_std': 0.009, 'precision_cv_mean': 0.8668, 'precision_cv_std': 0.0097, 'recall_cv_mean': 0.7849, 'recall_cv_std': 0.0135, 'f1_cv_mean': 0.8238, 'f1_cv_std': 0.01, 'accuracy_test': 0.8567, 'precision_test': 0.6735, 'recall_test': 0.7756, 'f1_score_test': 0.721}, 'XGBoost': {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.9273, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.885, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.9057, 'f1_cv_std': 0.0039, 'accuracy_test': 0.9204, 'precision_test': 0.8022, 'recall_test': 0.8843, 'f1_score_test': 0.8413}, 'Naive Bayes': {'accuracy_cv_mean': 0.9033, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.9277, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0066, 'f1_cv_mean': 0.9004, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9193, 'precision_test': 0.8091, 'recall_test': 0.8659, 'f1_score_test': 0.8365}}}
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 323)
Shape of X_test after concatenation:  (21625, 323)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 323)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 323)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [323, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.5656, Test Loss: 0.4513, F1: 0.8014, AUC: 0.8828
Epoch [10/30] Train Loss: 0.3628, Test Loss: 0.3566, F1: 0.8414, AUC: 0.9211
Epoch [20/30] Train Loss: 0.3471, Test Loss: 0.3442, F1: 0.8425, AUC: 0.9269
Mejores resultados en la época:  29
f1-score 0.853482413230841
AUC según el mejor F1-score 0.9311852655433116

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.5506, Test Loss: 0.4546, F1: 0.7995, AUC: 0.8754
Epoch [10/30] Train Loss: 0.3629, Test Loss: 0.3687, F1: 0.8423, AUC: 0.9176
Epoch [20/30] Train Loss: 0.3498, Test Loss: 0.3511, F1: 0.8403, AUC: 0.9236
Mejores resultados en la época:  27
f1-score 0.8476155449307343
AUC según el mejor F1-score 0.9267021212667509

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.5701, Test Loss: 0.4629, F1: 0.7969, AUC: 0.8787
Epoch [10/30] Train Loss: 0.3663, Test Loss: 0.3630, F1: 0.8440, AUC: 0.9196
Epoch [20/30] Train Loss: 0.3566, Test Loss: 0.3552, F1: 0.8419, AUC: 0.9229
Mejores resultados en la época:  22
f1-score 0.8471700399951521
AUC según el mejor F1-score 0.9233797196119524

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.5642, Test Loss: 0.4859, F1: 0.7387, AUC: 0.8795
Epoch [10/30] Train Loss: 0.3620, Test Loss: 0.3533, F1: 0.8377, AUC: 0.9224
Epoch [20/30] Train Loss: 0.3469, Test Loss: 0.3900, F1: 0.8036, AUC: 0.9274
Mejores resultados en la época:  26
f1-score 0.8542999289267946
AUC según el mejor F1-score 0.9306132755929472

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.5732, Test Loss: 0.4645, F1: 0.7844, AUC: 0.8778
Epoch [10/30] Train Loss: 0.3616, Test Loss: 0.3638, F1: 0.8395, AUC: 0.9175
Epoch [20/30] Train Loss: 0.3488, Test Loss: 0.3564, F1: 0.8467, AUC: 0.9237
Mejores resultados en la época:  29
f1-score 0.8494018296973962
AUC según el mejor F1-score 0.9270913163079962
Epoch [0/30] Train Loss: 0.5258, Test Loss: 0.4545, F1: 0.6523, AUC: 0.8882
Epoch [10/30] Train Loss: 0.3600, Test Loss: 0.2910, F1: 0.7512, AUC: 0.9210
Epoch [20/30] Train Loss: 0.3491, Test Loss: 0.4169, F1: 0.6914, AUC: 0.9253
Mejores resultados en la época:  18
f1-score 0.7564414195430238
AUC según el mejor F1-score 0.9250357288304766
Confusion matrix Test saved: outputs_cv/1/lyrics_bert/cm_mlp_1.png

========================================
Entrenando red 5 con capas [323, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4821, Test Loss: 0.3919, F1: 0.8228, AUC: 0.9042
Epoch [10/30] Train Loss: 0.3539, Test Loss: 0.3435, F1: 0.8374, AUC: 0.9289
Epoch [20/30] Train Loss: 0.3315, Test Loss: 0.3413, F1: 0.8400, AUC: 0.9328
Mejores resultados en la época:  24
f1-score 0.8527265982440955
AUC según el mejor F1-score 0.9361278478231478

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4677, Test Loss: 0.4198, F1: 0.8234, AUC: 0.9039
Epoch [10/30] Train Loss: 0.3525, Test Loss: 0.3529, F1: 0.8391, AUC: 0.9253
Epoch [20/30] Train Loss: 0.3373, Test Loss: 0.3452, F1: 0.8379, AUC: 0.9299
Mejores resultados en la época:  22
f1-score 0.8520082016644555
AUC según el mejor F1-score 0.9294089851496304

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4810, Test Loss: 0.3915, F1: 0.8258, AUC: 0.9067
Epoch [10/30] Train Loss: 0.3530, Test Loss: 0.3439, F1: 0.8467, AUC: 0.9283
Epoch [20/30] Train Loss: 0.3410, Test Loss: 0.3348, F1: 0.8464, AUC: 0.9314
Mejores resultados en la época:  27
f1-score 0.8594483921474166
AUC según el mejor F1-score 0.9355633358966559

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4940, Test Loss: 0.3951, F1: 0.8058, AUC: 0.9042
Epoch [10/30] Train Loss: 0.3573, Test Loss: 0.3385, F1: 0.8498, AUC: 0.9303
Epoch [20/30] Train Loss: 0.3326, Test Loss: 0.3360, F1: 0.8227, AUC: 0.9341
Mejores resultados en la época:  25
f1-score 0.857248611967921
AUC según el mejor F1-score 0.9358906675269497

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4886, Test Loss: 0.3968, F1: 0.8261, AUC: 0.9025
Epoch [10/30] Train Loss: 0.3601, Test Loss: 0.3668, F1: 0.8425, AUC: 0.9250
Epoch [20/30] Train Loss: 0.3375, Test Loss: 0.3408, F1: 0.8389, AUC: 0.9310
Mejores resultados en la época:  28
f1-score 0.8549431748747404
AUC según el mejor F1-score 0.934834566937712
Epoch [0/30] Train Loss: 0.4756, Test Loss: 0.3869, F1: 0.7096, AUC: 0.9051
Epoch [10/30] Train Loss: 0.3515, Test Loss: 0.3011, F1: 0.7544, AUC: 0.9281
Epoch [20/30] Train Loss: 0.3272, Test Loss: 0.5291, F1: 0.6177, AUC: 0.9286
Mejores resultados en la época:  29
f1-score 0.7765781538760821
AUC según el mejor F1-score 0.9357548252459411
Confusion matrix Test saved: outputs_cv/1/lyrics_bert/cm_mlp_5.png

========================================
Entrenando red 6 con capas [323, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4956, Test Loss: 0.3889, F1: 0.8267, AUC: 0.9064
Epoch [10/30] Train Loss: 0.3539, Test Loss: 0.3608, F1: 0.8101, AUC: 0.9288
Epoch [20/30] Train Loss: 0.3326, Test Loss: 0.3352, F1: 0.8498, AUC: 0.9330
Mejores resultados en la época:  28
f1-score 0.8608726331882864
AUC según el mejor F1-score 0.9381041530519499

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.5024, Test Loss: 0.4307, F1: 0.7874, AUC: 0.8962
Epoch [10/30] Train Loss: 0.3557, Test Loss: 0.3647, F1: 0.8264, AUC: 0.9247
Epoch [20/30] Train Loss: 0.3336, Test Loss: 0.3436, F1: 0.8476, AUC: 0.9277
Mejores resultados en la época:  27
f1-score 0.8571776748720449
AUC según el mejor F1-score 0.9334996500078871

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4747, Test Loss: 0.3943, F1: 0.8178, AUC: 0.9070
Epoch [10/30] Train Loss: 0.3528, Test Loss: 0.3567, F1: 0.8511, AUC: 0.9290
Epoch [20/30] Train Loss: 0.3322, Test Loss: 0.3658, F1: 0.8545, AUC: 0.9330
Mejores resultados en la época:  26
f1-score 0.8589206653105181
AUC según el mejor F1-score 0.9369703171008353

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4891, Test Loss: 0.4706, F1: 0.8042, AUC: 0.9028
Epoch [10/30] Train Loss: 0.3552, Test Loss: 0.3494, F1: 0.8482, AUC: 0.9282
Epoch [20/30] Train Loss: 0.3373, Test Loss: 0.3438, F1: 0.8280, AUC: 0.9328
Mejores resultados en la época:  26
f1-score 0.8557504873294347
AUC según el mejor F1-score 0.9356561676462246

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4821, Test Loss: 0.5150, F1: 0.7956, AUC: 0.9044
Epoch [10/30] Train Loss: 0.3525, Test Loss: 0.3437, F1: 0.8467, AUC: 0.9277
Epoch [20/30] Train Loss: 0.3317, Test Loss: 0.3436, F1: 0.8304, AUC: 0.9318
Mejores resultados en la época:  25
f1-score 0.8557474687313877
AUC según el mejor F1-score 0.9334329092025854
Epoch [0/30] Train Loss: 0.4626, Test Loss: 0.3925, F1: 0.7106, AUC: 0.9089
Epoch [10/30] Train Loss: 0.3542, Test Loss: 0.3632, F1: 0.7424, AUC: 0.9281
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 323)
Shape of X_test after concatenation:  (21625, 323)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 323)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 323)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [323, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.5656, Test Loss: 0.4513, F1: 0.8014, AUC: 0.8828
Epoch [10/30] Train Loss: 0.3628, Test Loss: 0.3566, F1: 0.8414, AUC: 0.9211
Epoch [20/30] Train Loss: 0.3471, Test Loss: 0.3442, F1: 0.8425, AUC: 0.9269
Mejores resultados en la época:  29
f1-score 0.853482413230841
AUC según el mejor F1-score 0.9311852655433116

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.5506, Test Loss: 0.4546, F1: 0.7995, AUC: 0.8754
Epoch [10/30] Train Loss: 0.3629, Test Loss: 0.3687, F1: 0.8423, AUC: 0.9176
Epoch [20/30] Train Loss: 0.3498, Test Loss: 0.3511, F1: 0.8403, AUC: 0.9236
Mejores resultados en la época:  27
f1-score 0.8476155449307343
AUC según el mejor F1-score 0.9267021212667509

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.5701, Test Loss: 0.4629, F1: 0.7969, AUC: 0.8787
Epoch [10/30] Train Loss: 0.3663, Test Loss: 0.3630, F1: 0.8440, AUC: 0.9196
Epoch [20/30] Train Loss: 0.3566, Test Loss: 0.3552, F1: 0.8419, AUC: 0.9229
Mejores resultados en la época:  22
f1-score 0.8471700399951521
AUC según el mejor F1-score 0.9233797196119524

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.5642, Test Loss: 0.4859, F1: 0.7387, AUC: 0.8795
Epoch [10/30] Train Loss: 0.3620, Test Loss: 0.3533, F1: 0.8377, AUC: 0.9224
Epoch [20/30] Train Loss: 0.3469, Test Loss: 0.3900, F1: 0.8036, AUC: 0.9274
Mejores resultados en la época:  26
f1-score 0.8542999289267946
AUC según el mejor F1-score 0.9306132755929472

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.5732, Test Loss: 0.4645, F1: 0.7844, AUC: 0.8778
Epoch [10/30] Train Loss: 0.3616, Test Loss: 0.3638, F1: 0.8395, AUC: 0.9175
Epoch [20/30] Train Loss: 0.3488, Test Loss: 0.3564, F1: 0.8467, AUC: 0.9237
Mejores resultados en la época:  29
f1-score 0.8494018296973962
AUC según el mejor F1-score 0.9270913163079962
Epoch [0/30] Train Loss: 0.5258, Test Loss: 0.4545, F1: 0.6523, AUC: 0.8882
Epoch [10/30] Train Loss: 0.3600, Test Loss: 0.2910, F1: 0.7512, AUC: 0.9210
Epoch [20/30] Train Loss: 0.3491, Test Loss: 0.4169, F1: 0.6914, AUC: 0.9253
Mejores resultados en la época:  18
f1-score 0.7564414195430238
AUC según el mejor F1-score 0.9250357288304766
Confusion matrix Test saved: outputs_cv/1/lyrics_bert/cm_mlp_1.png

========================================
Entrenando red 5 con capas [323, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4821, Test Loss: 0.3919, F1: 0.8228, AUC: 0.9042
Epoch [10/30] Train Loss: 0.3539, Test Loss: 0.3435, F1: 0.8374, AUC: 0.9289
Epoch [20/30] Train Loss: 0.3315, Test Loss: 0.3413, F1: 0.8400, AUC: 0.9328
Mejores resultados en la época:  24
f1-score 0.8527265982440955
AUC según el mejor F1-score 0.9361278478231478

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4677, Test Loss: 0.4198, F1: 0.8234, AUC: 0.9039
Epoch [10/30] Train Loss: 0.3525, Test Loss: 0.3529, F1: 0.8391, AUC: 0.9253
Epoch [20/30] Train Loss: 0.3373, Test Loss: 0.3452, F1: 0.8379, AUC: 0.9299
Mejores resultados en la época:  22
f1-score 0.8520082016644555
AUC según el mejor F1-score 0.9294089851496304

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4810, Test Loss: 0.3915, F1: 0.8258, AUC: 0.9067
Epoch [10/30] Train Loss: 0.3530, Test Loss: 0.3439, F1: 0.8467, AUC: 0.9283
Epoch [20/30] Train Loss: 0.3410, Test Loss: 0.3348, F1: 0.8464, AUC: 0.9314
Mejores resultados en la época:  27
f1-score 0.8594483921474166
AUC según el mejor F1-score 0.9355633358966559

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4940, Test Loss: 0.3951, F1: 0.8058, AUC: 0.9042
Epoch [10/30] Train Loss: 0.3573, Test Loss: 0.3385, F1: 0.8498, AUC: 0.9303
Epoch [20/30] Train Loss: 0.3326, Test Loss: 0.3360, F1: 0.8227, AUC: 0.9341
Mejores resultados en la época:  25
f1-score 0.857248611967921
AUC según el mejor F1-score 0.9358906675269497

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4886, Test Loss: 0.3968, F1: 0.8261, AUC: 0.9025
Epoch [10/30] Train Loss: 0.3601, Test Loss: 0.3668, F1: 0.8425, AUC: 0.9250
Epoch [20/30] Train Loss: 0.3375, Test Loss: 0.3408, F1: 0.8389, AUC: 0.9310
Mejores resultados en la época:  28
f1-score 0.8549431748747404
AUC según el mejor F1-score 0.934834566937712
Epoch [0/30] Train Loss: 0.4756, Test Loss: 0.3869, F1: 0.7096, AUC: 0.9051
Epoch [10/30] Train Loss: 0.3515, Test Loss: 0.3011, F1: 0.7544, AUC: 0.9281
Epoch [20/30] Train Loss: 0.3272, Test Loss: 0.5291, F1: 0.6177, AUC: 0.9286
Mejores resultados en la época:  29
f1-score 0.7765781538760821
AUC según el mejor F1-score 0.9357548252459411
Confusion matrix Test saved: outputs_cv/1/lyrics_bert/cm_mlp_5.png

========================================
Entrenando red 6 con capas [323, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4956, Test Loss: 0.3889, F1: 0.8267, AUC: 0.9064
Epoch [10/30] Train Loss: 0.3539, Test Loss: 0.3608, F1: 0.8101, AUC: 0.9288
Epoch [20/30] Train Loss: 0.3326, Test Loss: 0.3352, F1: 0.8498, AUC: 0.9330
Mejores resultados en la época:  28
f1-score 0.8608726331882864
AUC según el mejor F1-score 0.9381041530519499

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.5024, Test Loss: 0.4307, F1: 0.7874, AUC: 0.8962
Epoch [10/30] Train Loss: 0.3557, Test Loss: 0.3647, F1: 0.8264, AUC: 0.9247
Epoch [20/30] Train Loss: 0.3336, Test Loss: 0.3436, F1: 0.8476, AUC: 0.9277
Mejores resultados en la época:  27
f1-score 0.8571776748720449
AUC según el mejor F1-score 0.9334996500078871

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4747, Test Loss: 0.3943, F1: 0.8178, AUC: 0.9070
Epoch [10/30] Train Loss: 0.3528, Test Loss: 0.3567, F1: 0.8511, AUC: 0.9290
Epoch [20/30] Train Loss: 0.3322, Test Loss: 0.3658, F1: 0.8545, AUC: 0.9330
Mejores resultados en la época:  26
f1-score 0.8589206653105181
AUC según el mejor F1-score 0.9369703171008353

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4891, Test Loss: 0.4706, F1: 0.8042, AUC: 0.9028
Epoch [10/30] Train Loss: 0.3552, Test Loss: 0.3494, F1: 0.8482, AUC: 0.9282
Epoch [20/30] Train Loss: 0.3373, Test Loss: 0.3438, F1: 0.8280, AUC: 0.9328
Mejores resultados en la época:  26
f1-score 0.8557504873294347
AUC según el mejor F1-score 0.9356561676462246

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4821, Test Loss: 0.5150, F1: 0.7956, AUC: 0.9044
Epoch [10/30] Train Loss: 0.3525, Test Loss: 0.3437, F1: 0.8467, AUC: 0.9277
Epoch [20/30] Train Loss: 0.3317, Test Loss: 0.3436, F1: 0.8304, AUC: 0.9318
Mejores resultados en la época:  25
f1-score 0.8557474687313877
AUC según el mejor F1-score 0.9334329092025854
Epoch [0/30] Train Loss: 0.4626, Test Loss: 0.3925, F1: 0.7106, AUC: 0.9089
Epoch [10/30] Train Loss: 0.3542, Test Loss: 0.3632, F1: 0.7424, AUC: 0.9281
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Epoch [20/30] Train Loss: 0.3269, Test Loss: 0.3442, F1: 0.7505, AUC: 0.9326
Mejores resultados en la época:  28
f1-score 0.7717931235998832
AUC según el mejor F1-score 0.9362691179551644
Confusion matrix Test saved: outputs_cv/1/lyrics_bert/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9344, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0009, 'recall_cv_mean': 0.9336, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9344, 'f1_cv_std': 0.0017, 'params': 160801, 'accuracy_test': 0.9372, 'precision_test': 0.8404, 'recall_test': 0.9095, 'f1_score_test': 0.8736}, 'MLP_2744833': {'accuracy_cv_mean': 0.9454, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9447, 'precision_cv_std': 0.0058, 'recall_cv_mean': 0.9464, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.9455, 'f1_cv_std': 0.0015, 'params': 2744833, 'accuracy_test': 0.9557, 'precision_test': 0.8901, 'recall_test': 0.9289, 'f1_score_test': 0.9091}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0013, 'precision_cv_mean': 0.9499, 'precision_cv_std': 0.0027, 'recall_cv_mean': 0.9422, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.946, 'f1_cv_std': 0.0013, 'params': 5843969, 'accuracy_test': 0.9551, 'precision_test': 0.8925, 'recall_test': 0.9233, 'f1_score_test': 0.9076}, 'Logistic Regression': {'accuracy_cv_mean': 0.9278, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9348, 'precision_cv_std': 0.0019, 'recall_cv_mean': 0.9198, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.9272, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9337, 'precision_test': 0.8257, 'recall_test': 0.9155, 'f1_score_test': 0.8683}, 'SVM': {'accuracy_cv_mean': 0.8164, 'accuracy_cv_std': 0.0186, 'precision_cv_mean': 0.7725, 'precision_cv_std': 0.0325, 'recall_cv_mean': 0.9014, 'recall_cv_std': 0.0238, 'f1_cv_mean': 0.8311, 'f1_cv_std': 0.0121, 'accuracy_test': 0.753, 'precision_test': 0.4904, 'recall_test': 0.8963, 'f1_score_test': 0.6339}, 'Decision Tree': {'accuracy_cv_mean': 0.829, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7388, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.812, 'f1_cv_std': 0.0056, 'accuracy_test': 0.875, 'precision_test': 0.7471, 'recall_test': 0.7196, 'f1_score_test': 0.7331}, 'Random Forest': {'accuracy_cv_mean': 0.8321, 'accuracy_cv_std': 0.009, 'precision_cv_mean': 0.8668, 'precision_cv_std': 0.0097, 'recall_cv_mean': 0.7849, 'recall_cv_std': 0.0135, 'f1_cv_mean': 0.8238, 'f1_cv_std': 0.01, 'accuracy_test': 0.8567, 'precision_test': 0.6735, 'recall_test': 0.7756, 'f1_score_test': 0.721}, 'XGBoost': {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.9273, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.885, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.9057, 'f1_cv_std': 0.0039, 'accuracy_test': 0.9204, 'precision_test': 0.8022, 'recall_test': 0.8843, 'f1_score_test': 0.8413}, 'Naive Bayes': {'accuracy_cv_mean': 0.9033, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.9277, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0066, 'f1_cv_mean': 0.9004, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9193, 'precision_test': 0.8091, 'recall_test': 0.8659, 'f1_score_test': 0.8365}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.848, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8373, 'precision_cv_std': 0.0139, 'recall_cv_mean': 0.8645, 'recall_cv_std': 0.0191, 'f1_cv_mean': 0.8504, 'f1_cv_std': 0.003, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.759, 'recall_test': 0.7539, 'f1_score_test': 0.7564}, 'MLP_338433': {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.862, 'precision_cv_std': 0.0095, 'recall_cv_mean': 0.8489, 'recall_cv_std': 0.0102, 'f1_cv_mean': 0.8553, 'f1_cv_std': 0.0028, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7795, 'recall_test': 0.7736, 'f1_score_test': 0.7766}, 'MLP_1031169': {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.85, 'precision_cv_std': 0.0102, 'recall_cv_mean': 0.8658, 'recall_cv_std': 0.0132, 'f1_cv_mean': 0.8577, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8917, 'precision_test': 0.7758, 'recall_test': 0.7678, 'f1_score_test': 0.7718}}}
Saved on: outputs_cv/1/lyrics_bert

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8604, 'precision_cv_std': 0.004, 'recall_cv_mean': 0.8326, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8462, 'f1_cv_std': 0.006}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 43, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.87      0.90     16465
           1       0.66      0.83      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.85      0.82     21625
weighted avg       0.87      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/1/lyrics_bert/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/1/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.6564, 'accuracy_cv_std': 0.0276, 'precision_cv_mean': 0.634, 'precision_cv_std': 0.0178, 'recall_cv_mean': 0.7365, 'recall_cv_std': 0.0655, 'f1_cv_mean': 0.6806, 'f1_cv_std': 0.037}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 43, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.83      0.53      0.65     16465
           1       0.31      0.66      0.42      5160

    accuracy                           0.56     21625
   macro avg       0.57      0.60      0.54     21625
weighted avg       0.71      0.56      0.60     21625

Confusion matrix Test saved as: outputs_cv/1/lyrics_bert/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/1/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7959, 'accuracy_cv_std': 0.004, 'precision_cv_mean': 0.8116, 'precision_cv_std': 0.015, 'recall_cv_mean': 0.7717, 'recall_cv_std': 0.0236, 'f1_cv_mean': 0.7907, 'f1_cv_std': 0.0067}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 43, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.82      0.86     16465
           1       0.57      0.77      0.65      5160

    accuracy                           0.81     21625
   macro avg       0.74      0.79      0.76     21625
weighted avg       0.83      0.81      0.81     21625

Confusion matrix Test saved as: outputs_cv/1/lyrics_bert/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/1/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.855, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8866, 'precision_cv_std': 0.0042, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0127, 'f1_cv_mean': 0.8488, 'f1_cv_std': 0.0058}
Epoch [20/30] Train Loss: 0.3269, Test Loss: 0.3442, F1: 0.7505, AUC: 0.9326
Mejores resultados en la época:  28
f1-score 0.7717931235998832
AUC según el mejor F1-score 0.9362691179551644
Confusion matrix Test saved: outputs_cv/1/lyrics_bert/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9344, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0009, 'recall_cv_mean': 0.9336, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9344, 'f1_cv_std': 0.0017, 'params': 160801, 'accuracy_test': 0.9372, 'precision_test': 0.8404, 'recall_test': 0.9095, 'f1_score_test': 0.8736}, 'MLP_2744833': {'accuracy_cv_mean': 0.9454, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9447, 'precision_cv_std': 0.0058, 'recall_cv_mean': 0.9464, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.9455, 'f1_cv_std': 0.0015, 'params': 2744833, 'accuracy_test': 0.9557, 'precision_test': 0.8901, 'recall_test': 0.9289, 'f1_score_test': 0.9091}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0013, 'precision_cv_mean': 0.9499, 'precision_cv_std': 0.0027, 'recall_cv_mean': 0.9422, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.946, 'f1_cv_std': 0.0013, 'params': 5843969, 'accuracy_test': 0.9551, 'precision_test': 0.8925, 'recall_test': 0.9233, 'f1_score_test': 0.9076}, 'Logistic Regression': {'accuracy_cv_mean': 0.9278, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9348, 'precision_cv_std': 0.0019, 'recall_cv_mean': 0.9198, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.9272, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9337, 'precision_test': 0.8257, 'recall_test': 0.9155, 'f1_score_test': 0.8683}, 'SVM': {'accuracy_cv_mean': 0.8164, 'accuracy_cv_std': 0.0186, 'precision_cv_mean': 0.7725, 'precision_cv_std': 0.0325, 'recall_cv_mean': 0.9014, 'recall_cv_std': 0.0238, 'f1_cv_mean': 0.8311, 'f1_cv_std': 0.0121, 'accuracy_test': 0.753, 'precision_test': 0.4904, 'recall_test': 0.8963, 'f1_score_test': 0.6339}, 'Decision Tree': {'accuracy_cv_mean': 0.829, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7388, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.812, 'f1_cv_std': 0.0056, 'accuracy_test': 0.875, 'precision_test': 0.7471, 'recall_test': 0.7196, 'f1_score_test': 0.7331}, 'Random Forest': {'accuracy_cv_mean': 0.8321, 'accuracy_cv_std': 0.009, 'precision_cv_mean': 0.8668, 'precision_cv_std': 0.0097, 'recall_cv_mean': 0.7849, 'recall_cv_std': 0.0135, 'f1_cv_mean': 0.8238, 'f1_cv_std': 0.01, 'accuracy_test': 0.8567, 'precision_test': 0.6735, 'recall_test': 0.7756, 'f1_score_test': 0.721}, 'XGBoost': {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.9273, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.885, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.9057, 'f1_cv_std': 0.0039, 'accuracy_test': 0.9204, 'precision_test': 0.8022, 'recall_test': 0.8843, 'f1_score_test': 0.8413}, 'Naive Bayes': {'accuracy_cv_mean': 0.9033, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.9277, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0066, 'f1_cv_mean': 0.9004, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9193, 'precision_test': 0.8091, 'recall_test': 0.8659, 'f1_score_test': 0.8365}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.848, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8373, 'precision_cv_std': 0.0139, 'recall_cv_mean': 0.8645, 'recall_cv_std': 0.0191, 'f1_cv_mean': 0.8504, 'f1_cv_std': 0.003, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.759, 'recall_test': 0.7539, 'f1_score_test': 0.7564}, 'MLP_338433': {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.862, 'precision_cv_std': 0.0095, 'recall_cv_mean': 0.8489, 'recall_cv_std': 0.0102, 'f1_cv_mean': 0.8553, 'f1_cv_std': 0.0028, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7795, 'recall_test': 0.7736, 'f1_score_test': 0.7766}, 'MLP_1031169': {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.85, 'precision_cv_std': 0.0102, 'recall_cv_mean': 0.8658, 'recall_cv_std': 0.0132, 'f1_cv_mean': 0.8577, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8917, 'precision_test': 0.7758, 'recall_test': 0.7678, 'f1_score_test': 0.7718}}}
Saved on: outputs_cv/1/lyrics_bert

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8604, 'precision_cv_std': 0.004, 'recall_cv_mean': 0.8326, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8462, 'f1_cv_std': 0.006}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 43, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.87      0.90     16465
           1       0.66      0.83      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.85      0.82     21625
weighted avg       0.87      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/1/lyrics_bert/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/1/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.6564, 'accuracy_cv_std': 0.0276, 'precision_cv_mean': 0.634, 'precision_cv_std': 0.0178, 'recall_cv_mean': 0.7365, 'recall_cv_std': 0.0655, 'f1_cv_mean': 0.6806, 'f1_cv_std': 0.037}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 43, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.83      0.53      0.65     16465
           1       0.31      0.66      0.42      5160

    accuracy                           0.56     21625
   macro avg       0.57      0.60      0.54     21625
weighted avg       0.71      0.56      0.60     21625

Confusion matrix Test saved as: outputs_cv/1/lyrics_bert/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/1/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7959, 'accuracy_cv_std': 0.004, 'precision_cv_mean': 0.8116, 'precision_cv_std': 0.015, 'recall_cv_mean': 0.7717, 'recall_cv_std': 0.0236, 'f1_cv_mean': 0.7907, 'f1_cv_std': 0.0067}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 43, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.82      0.86     16465
           1       0.57      0.77      0.65      5160

    accuracy                           0.81     21625
   macro avg       0.74      0.79      0.76     21625
weighted avg       0.83      0.81      0.81     21625

Confusion matrix Test saved as: outputs_cv/1/lyrics_bert/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/1/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.855, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8866, 'precision_cv_std': 0.0042, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0127, 'f1_cv_mean': 0.8488, 'f1_cv_std': 0.0058}
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:21:27] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:21:53] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:22:13] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:22:42] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:23:01] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:23:31] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:23:50] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:24:21] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:24:38] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:25:09] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:25:27] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:25:58] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 43, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.90      0.92     16465
           1       0.72      0.80      0.76      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.85      0.84     21625
weighted avg       0.88      0.88      0.88     21625

Confusion matrix Test saved as: outputs_cv/1/lyrics_bert/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/1/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8771, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8931, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8745, 'f1_cv_std': 0.0054}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 43, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.90      0.93     16465
           1       0.73      0.86      0.79      5160

    accuracy                           0.89     21625
   macro avg       0.84      0.88      0.86     21625
weighted avg       0.90      0.89      0.89     21625

Confusion matrix Test saved as: outputs_cv/1/lyrics_bert/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/1/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7792, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.7615, 'precision_cv_std': 0.0066, 'recall_cv_mean': 0.813, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.7864, 'f1_cv_std': 0.0053}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.75      0.83     16465
           1       0.50      0.80      0.62      5160

    accuracy                           0.76     21625
   macro avg       0.71      0.78      0.72     21625
weighted avg       0.82      0.76      0.78     21625

Confusion matrix Test saved as: outputs_cv/1/lyrics_bert/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/1/lyrics_bert/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.8771, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8931, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8745, 'f1_cv_std': 0.0054, 'accuracy_test': 0.8921, 'precision_test': 0.7345, 'recall_test': 0.8579, 'f1_score_test': 0.7915}
Random Forest: {'accuracy_cv_mean': 0.855, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8866, 'precision_cv_std': 0.0042, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0127, 'f1_cv_mean': 0.8488, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8768, 'precision_test': 0.7161, 'recall_test': 0.8014, 'f1_score_test': 0.7564}
Logistic Regression: {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8604, 'precision_cv_std': 0.004, 'recall_cv_mean': 0.8326, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8462, 'f1_cv_std': 0.006, 'accuracy_test': 0.8564, 'precision_test': 0.6585, 'recall_test': 0.8273, 'f1_score_test': 0.7333}
Decision Tree: {'accuracy_cv_mean': 0.7959, 'accuracy_cv_std': 0.004, 'precision_cv_mean': 0.8116, 'precision_cv_std': 0.015, 'recall_cv_mean': 0.7717, 'recall_cv_std': 0.0236, 'f1_cv_mean': 0.7907, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8053, 'precision_test': 0.568, 'recall_test': 0.7684, 'f1_score_test': 0.6532}
Naive Bayes: {'accuracy_cv_mean': 0.7792, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.7615, 'precision_cv_std': 0.0066, 'recall_cv_mean': 0.813, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.7864, 'f1_cv_std': 0.0053, 'accuracy_test': 0.7625, 'precision_test': 0.5014, 'recall_test': 0.8014, 'f1_score_test': 0.6168}
SVM: {'accuracy_cv_mean': 0.6564, 'accuracy_cv_std': 0.0276, 'precision_cv_mean': 0.634, 'precision_cv_std': 0.0178, 'recall_cv_mean': 0.7365, 'recall_cv_std': 0.0655, 'f1_cv_mean': 0.6806, 'f1_cv_std': 0.037, 'accuracy_test': 0.5639, 'precision_test': 0.3076, 'recall_test': 0.6616, 'f1_score_test': 0.42}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9344, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0009, 'recall_cv_mean': 0.9336, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9344, 'f1_cv_std': 0.0017, 'params': 160801, 'accuracy_test': 0.9372, 'precision_test': 0.8404, 'recall_test': 0.9095, 'f1_score_test': 0.8736}, 'MLP_2744833': {'accuracy_cv_mean': 0.9454, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9447, 'precision_cv_std': 0.0058, 'recall_cv_mean': 0.9464, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.9455, 'f1_cv_std': 0.0015, 'params': 2744833, 'accuracy_test': 0.9557, 'precision_test': 0.8901, 'recall_test': 0.9289, 'f1_score_test': 0.9091}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0013, 'precision_cv_mean': 0.9499, 'precision_cv_std': 0.0027, 'recall_cv_mean': 0.9422, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.946, 'f1_cv_std': 0.0013, 'params': 5843969, 'accuracy_test': 0.9551, 'precision_test': 0.8925, 'recall_test': 0.9233, 'f1_score_test': 0.9076}, 'Logistic Regression': {'accuracy_cv_mean': 0.9278, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9348, 'precision_cv_std': 0.0019, 'recall_cv_mean': 0.9198, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.9272, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9337, 'precision_test': 0.8257, 'recall_test': 0.9155, 'f1_score_test': 0.8683}, 'SVM': {'accuracy_cv_mean': 0.8164, 'accuracy_cv_std': 0.0186, 'precision_cv_mean': 0.7725, 'precision_cv_std': 0.0325, 'recall_cv_mean': 0.9014, 'recall_cv_std': 0.0238, 'f1_cv_mean': 0.8311, 'f1_cv_std': 0.0121, 'accuracy_test': 0.753, 'precision_test': 0.4904, 'recall_test': 0.8963, 'f1_score_test': 0.6339}, 'Decision Tree': {'accuracy_cv_mean': 0.829, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7388, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.812, 'f1_cv_std': 0.0056, 'accuracy_test': 0.875, 'precision_test': 0.7471, 'recall_test': 0.7196, 'f1_score_test': 0.7331}, 'Random Forest': {'accuracy_cv_mean': 0.8321, 'accuracy_cv_std': 0.009, 'precision_cv_mean': 0.8668, 'precision_cv_std': 0.0097, 'recall_cv_mean': 0.7849, 'recall_cv_std': 0.0135, 'f1_cv_mean': 0.8238, 'f1_cv_std': 0.01, 'accuracy_test': 0.8567, 'precision_test': 0.6735, 'recall_test': 0.7756, 'f1_score_test': 0.721}, 'XGBoost': {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.9273, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.885, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.9057, 'f1_cv_std': 0.0039, 'accuracy_test': 0.9204, 'precision_test': 0.8022, 'recall_test': 0.8843, 'f1_score_test': 0.8413}, 'Naive Bayes': {'accuracy_cv_mean': 0.9033, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.9277, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0066, 'f1_cv_mean': 0.9004, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9193, 'precision_test': 0.8091, 'recall_test': 0.8659, 'f1_score_test': 0.8365}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.848, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8373, 'precision_cv_std': 0.0139, 'recall_cv_mean': 0.8645, 'recall_cv_std': 0.0191, 'f1_cv_mean': 0.8504, 'f1_cv_std': 0.003, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.759, 'recall_test': 0.7539, 'f1_score_test': 0.7564}, 'MLP_338433': {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.862, 'precision_cv_std': 0.0095, 'recall_cv_mean': 0.8489, 'recall_cv_std': 0.0102, 'f1_cv_mean': 0.8553, 'f1_cv_std': 0.0028, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7795, 'recall_test': 0.7736, 'f1_score_test': 0.7766}, 'MLP_1031169': {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.85, 'precision_cv_std': 0.0102, 'recall_cv_mean': 0.8658, 'recall_cv_std': 0.0132, 'f1_cv_mean': 0.8577, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8917, 'precision_test': 0.7758, 'recall_test': 0.7678, 'f1_score_test': 0.7718}, 'Logistic Regression': {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8604, 'precision_cv_std': 0.004, 'recall_cv_mean': 0.8326, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8462, 'f1_cv_std': 0.006, 'accuracy_test': 0.8564, 'precision_test': 0.6585, 'recall_test': 0.8273, 'f1_score_test': 0.7333}, 'SVM': {'accuracy_cv_mean': 0.6564, 'accuracy_cv_std': 0.0276, 'precision_cv_mean': 0.634, 'precision_cv_std': 0.0178, 'recall_cv_mean': 0.7365, 'recall_cv_std': 0.0655, 'f1_cv_mean': 0.6806, 'f1_cv_std': 0.037, 'accuracy_test': 0.5639, 'precision_test': 0.3076, 'recall_test': 0.6616, 'f1_score_test': 0.42}, 'Decision Tree': {'accuracy_cv_mean': 0.7959, 'accuracy_cv_std': 0.004, 'precision_cv_mean': 0.8116, 'precision_cv_std': 0.015, 'recall_cv_mean': 0.7717, 'recall_cv_std': 0.0236, 'f1_cv_mean': 0.7907, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8053, 'precision_test': 0.568, 'recall_test': 0.7684, 'f1_score_test': 0.6532}, 'Random Forest': {'accuracy_cv_mean': 0.855, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8866, 'precision_cv_std': 0.0042, 'recall_cv_mean': 0.8143/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 43, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.90      0.92     16465
           1       0.72      0.80      0.76      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.85      0.84     21625
weighted avg       0.88      0.88      0.88     21625

Confusion matrix Test saved as: outputs_cv/1/lyrics_bert/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/1/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8771, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8931, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8745, 'f1_cv_std': 0.0054}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 43, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.90      0.93     16465
           1       0.73      0.86      0.79      5160

    accuracy                           0.89     21625
   macro avg       0.84      0.88      0.86     21625
weighted avg       0.90      0.89      0.89     21625

Confusion matrix Test saved as: outputs_cv/1/lyrics_bert/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/1/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7792, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.7615, 'precision_cv_std': 0.0066, 'recall_cv_mean': 0.813, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.7864, 'f1_cv_std': 0.0053}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.75      0.83     16465
           1       0.50      0.80      0.62      5160

    accuracy                           0.76     21625
   macro avg       0.71      0.78      0.72     21625
weighted avg       0.82      0.76      0.78     21625

Confusion matrix Test saved as: outputs_cv/1/lyrics_bert/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/1/lyrics_bert/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.8771, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8931, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8745, 'f1_cv_std': 0.0054, 'accuracy_test': 0.8921, 'precision_test': 0.7345, 'recall_test': 0.8579, 'f1_score_test': 0.7915}
Random Forest: {'accuracy_cv_mean': 0.855, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8866, 'precision_cv_std': 0.0042, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0127, 'f1_cv_mean': 0.8488, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8768, 'precision_test': 0.7161, 'recall_test': 0.8014, 'f1_score_test': 0.7564}
Logistic Regression: {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8604, 'precision_cv_std': 0.004, 'recall_cv_mean': 0.8326, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8462, 'f1_cv_std': 0.006, 'accuracy_test': 0.8564, 'precision_test': 0.6585, 'recall_test': 0.8273, 'f1_score_test': 0.7333}
Decision Tree: {'accuracy_cv_mean': 0.7959, 'accuracy_cv_std': 0.004, 'precision_cv_mean': 0.8116, 'precision_cv_std': 0.015, 'recall_cv_mean': 0.7717, 'recall_cv_std': 0.0236, 'f1_cv_mean': 0.7907, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8053, 'precision_test': 0.568, 'recall_test': 0.7684, 'f1_score_test': 0.6532}
Naive Bayes: {'accuracy_cv_mean': 0.7792, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.7615, 'precision_cv_std': 0.0066, 'recall_cv_mean': 0.813, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.7864, 'f1_cv_std': 0.0053, 'accuracy_test': 0.7625, 'precision_test': 0.5014, 'recall_test': 0.8014, 'f1_score_test': 0.6168}
SVM: {'accuracy_cv_mean': 0.6564, 'accuracy_cv_std': 0.0276, 'precision_cv_mean': 0.634, 'precision_cv_std': 0.0178, 'recall_cv_mean': 0.7365, 'recall_cv_std': 0.0655, 'f1_cv_mean': 0.6806, 'f1_cv_std': 0.037, 'accuracy_test': 0.5639, 'precision_test': 0.3076, 'recall_test': 0.6616, 'f1_score_test': 0.42}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9344, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0009, 'recall_cv_mean': 0.9336, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9344, 'f1_cv_std': 0.0017, 'params': 160801, 'accuracy_test': 0.9372, 'precision_test': 0.8404, 'recall_test': 0.9095, 'f1_score_test': 0.8736}, 'MLP_2744833': {'accuracy_cv_mean': 0.9454, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9447, 'precision_cv_std': 0.0058, 'recall_cv_mean': 0.9464, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.9455, 'f1_cv_std': 0.0015, 'params': 2744833, 'accuracy_test': 0.9557, 'precision_test': 0.8901, 'recall_test': 0.9289, 'f1_score_test': 0.9091}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0013, 'precision_cv_mean': 0.9499, 'precision_cv_std': 0.0027, 'recall_cv_mean': 0.9422, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.946, 'f1_cv_std': 0.0013, 'params': 5843969, 'accuracy_test': 0.9551, 'precision_test': 0.8925, 'recall_test': 0.9233, 'f1_score_test': 0.9076}, 'Logistic Regression': {'accuracy_cv_mean': 0.9278, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9348, 'precision_cv_std': 0.0019, 'recall_cv_mean': 0.9198, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.9272, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9337, 'precision_test': 0.8257, 'recall_test': 0.9155, 'f1_score_test': 0.8683}, 'SVM': {'accuracy_cv_mean': 0.8164, 'accuracy_cv_std': 0.0186, 'precision_cv_mean': 0.7725, 'precision_cv_std': 0.0325, 'recall_cv_mean': 0.9014, 'recall_cv_std': 0.0238, 'f1_cv_mean': 0.8311, 'f1_cv_std': 0.0121, 'accuracy_test': 0.753, 'precision_test': 0.4904, 'recall_test': 0.8963, 'f1_score_test': 0.6339}, 'Decision Tree': {'accuracy_cv_mean': 0.829, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7388, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.812, 'f1_cv_std': 0.0056, 'accuracy_test': 0.875, 'precision_test': 0.7471, 'recall_test': 0.7196, 'f1_score_test': 0.7331}, 'Random Forest': {'accuracy_cv_mean': 0.8321, 'accuracy_cv_std': 0.009, 'precision_cv_mean': 0.8668, 'precision_cv_std': 0.0097, 'recall_cv_mean': 0.7849, 'recall_cv_std': 0.0135, 'f1_cv_mean': 0.8238, 'f1_cv_std': 0.01, 'accuracy_test': 0.8567, 'precision_test': 0.6735, 'recall_test': 0.7756, 'f1_score_test': 0.721}, 'XGBoost': {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.9273, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.885, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.9057, 'f1_cv_std': 0.0039, 'accuracy_test': 0.9204, 'precision_test': 0.8022, 'recall_test': 0.8843, 'f1_score_test': 0.8413}, 'Naive Bayes': {'accuracy_cv_mean': 0.9033, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.9277, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0066, 'f1_cv_mean': 0.9004, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9193, 'precision_test': 0.8091, 'recall_test': 0.8659, 'f1_score_test': 0.8365}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.848, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8373, 'precision_cv_std': 0.0139, 'recall_cv_mean': 0.8645, 'recall_cv_std': 0.0191, 'f1_cv_mean': 0.8504, 'f1_cv_std': 0.003, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.759, 'recall_test': 0.7539, 'f1_score_test': 0.7564}, 'MLP_338433': {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.862, 'precision_cv_std': 0.0095, 'recall_cv_mean': 0.8489, 'recall_cv_std': 0.0102, 'f1_cv_mean': 0.8553, 'f1_cv_std': 0.0028, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7795, 'recall_test': 0.7736, 'f1_score_test': 0.7766}, 'MLP_1031169': {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.85, 'precision_cv_std': 0.0102, 'recall_cv_mean': 0.8658, 'recall_cv_std': 0.0132, 'f1_cv_mean': 0.8577, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8917, 'precision_test': 0.7758, 'recall_test': 0.7678, 'f1_score_test': 0.7718}, 'Logistic Regression': {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8604, 'precision_cv_std': 0.004, 'recall_cv_mean': 0.8326, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8462, 'f1_cv_std': 0.006, 'accuracy_test': 0.8564, 'precision_test': 0.6585, 'recall_test': 0.8273, 'f1_score_test': 0.7333}, 'SVM': {'accuracy_cv_mean': 0.6564, 'accuracy_cv_std': 0.0276, 'precision_cv_mean': 0.634, 'precision_cv_std': 0.0178, 'recall_cv_mean': 0.7365, 'recall_cv_std': 0.0655, 'f1_cv_mean': 0.6806, 'f1_cv_std': 0.037, 'accuracy_test': 0.5639, 'precision_test': 0.3076, 'recall_test': 0.6616, 'f1_score_test': 0.42}, 'Decision Tree': {'accuracy_cv_mean': 0.7959, 'accuracy_cv_std': 0.004, 'precision_cv_mean': 0.8116, 'precision_cv_std': 0.015, 'recall_cv_mean': 0.7717, 'recall_cv_std': 0.0236, 'f1_cv_mean': 0.7907, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8053, 'precision_test': 0.568, 'recall_test': 0.7684, 'f1_score_test': 0.6532}, 'Random Forest': {'accuracy_cv_mean': 0.855, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8866, 'precision_cv_std': 0.0042, 'recall_cv_mean': 0.8143/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
, 'recall_cv_std': 0.0127, 'f1_cv_mean': 0.8488, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8768, 'precision_test': 0.7161, 'recall_test': 0.8014, 'f1_score_test': 0.7564}, 'XGBoost': {'accuracy_cv_mean': 0.8771, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8931, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8745, 'f1_cv_std': 0.0054, 'accuracy_test': 0.8921, 'precision_test': 0.7345, 'recall_test': 0.8579, 'f1_score_test': 0.7915}, 'Naive Bayes': {'accuracy_cv_mean': 0.7792, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.7615, 'precision_cv_std': 0.0066, 'recall_cv_mean': 0.813, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.7864, 'f1_cv_std': 0.0053, 'accuracy_test': 0.7625, 'precision_test': 0.5014, 'recall_test': 0.8014, 'f1_score_test': 0.6168}}}
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 1559)
Shape of X_test after concatenation:  (21625, 1559)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1559)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1559)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [1559, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4151, Test Loss: 0.3259, F1: 0.8560, AUC: 0.9352
Epoch [10/30] Train Loss: 0.2594, Test Loss: 0.2659, F1: 0.8859, AUC: 0.9570
Epoch [20/30] Train Loss: 0.2463, Test Loss: 0.2567, F1: 0.8908, AUC: 0.9596
Mejores resultados en la época:  24
f1-score 0.8933202838267678
AUC según el mejor F1-score 0.9602891871450784

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.5763, Test Loss: 0.4644, F1: 0.8397, AUC: 0.9128
Epoch [10/30] Train Loss: 0.2795, Test Loss: 0.2822, F1: 0.8831, AUC: 0.9521
Epoch [20/30] Train Loss: 0.2579, Test Loss: 0.2665, F1: 0.8887, AUC: 0.9571
Mejores resultados en la época:  28
f1-score 0.8904813089693061
AUC según el mejor F1-score 0.959173954061129

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.5701, Test Loss: 0.4557, F1: 0.8442, AUC: 0.9136
Epoch [10/30] Train Loss: 0.2799, Test Loss: 0.2740, F1: 0.8852, AUC: 0.9555
Epoch [20/30] Train Loss: 0.2612, Test Loss: 0.2686, F1: 0.8828, AUC: 0.9593
Mejores resultados en la época:  22
f1-score 0.8965109402720284
AUC según el mejor F1-score 0.9601535681355537

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.5586, Test Loss: 0.4737, F1: 0.8529, AUC: 0.9172
Epoch [10/30] Train Loss: 0.2746, Test Loss: 0.2671, F1: 0.8891, AUC: 0.9585
Epoch [20/30] Train Loss: 0.2536, Test Loss: 0.2523, F1: 0.8970, AUC: 0.9623
Mejores resultados en la época:  29
f1-score 0.8985649454188642
AUC según el mejor F1-score 0.9636931377410622

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4470, Test Loss: 0.3373, F1: 0.8542, AUC: 0.9311
Epoch [10/30] Train Loss: 0.2614, Test Loss: 0.2707, F1: 0.8821, AUC: 0.9557
Epoch [20/30] Train Loss: 0.2478, Test Loss: 0.2597, F1: 0.8871, AUC: 0.9588
Mejores resultados en la época:  19
f1-score 0.8922966335210255
AUC según el mejor F1-score 0.9586100960210976
Epoch [0/30] Train Loss: 0.4134, Test Loss: 0.3072, F1: 0.7638, AUC: 0.9388
Epoch [10/30] Train Loss: 0.2633, Test Loss: 0.2795, F1: 0.7877, AUC: 0.9585
Epoch [20/30] Train Loss: 0.2503, Test Loss: 0.2125, F1: 0.8257, AUC: 0.9612
Mejores resultados en la época:  22
f1-score 0.8317346123101519
AUC según el mejor F1-score 0.9615079261388381
Confusion matrix Test saved: outputs_cv/1/gpt/cm_mlp_1.png

========================================
Entrenando red 5 con capas [1559, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3776, Test Loss: 0.3184, F1: 0.8540, AUC: 0.9459
Epoch [10/30] Train Loss: 0.2633, Test Loss: 0.2949, F1: 0.8785, AUC: 0.9584
Epoch [20/30] Train Loss: 0.2425, Test Loss: 0.2624, F1: 0.8910, AUC: 0.9622
Mejores resultados en la época:  26
f1-score 0.897841726618705
AUC según el mejor F1-score 0.963639229022069

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3837, Test Loss: 0.3338, F1: 0.8401, AUC: 0.9455
Epoch [10/30] Train Loss: 0.2627, Test Loss: 0.2764, F1: 0.8751, AUC: 0.9600
Epoch [20/30] Train Loss: 0.2391, Test Loss: 0.2547, F1: 0.8888, AUC: 0.9623
Mejores resultados en la época:  27
f1-score 0.894137194398291
AUC según el mejor F1-score 0.9632321079149391

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3869, Test Loss: 0.2901, F1: 0.8769, AUC: 0.9489
Epoch [10/30] Train Loss: 0.2668, Test Loss: 0.3270, F1: 0.8279, AUC: 0.9627
Epoch [20/30] Train Loss: 0.2473, Test Loss: 0.2560, F1: 0.8985, AUC: 0.9645
Mejores resultados en la época:  23
f1-score 0.9007396802672393
AUC según el mejor F1-score 0.9657868918916381

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3988, Test Loss: 0.3098, F1: 0.8709, AUC: 0.9461
Epoch [10/30] Train Loss: 0.2668, Test Loss: 0.2504, F1: 0.8866, AUC: 0.9626
Epoch [20/30] Train Loss: 0.2491, Test Loss: 0.2469, F1: 0.8964, AUC: 0.9657
Mejores resultados en la época:  19
f1-score 0.8996157540826129
AUC según el mejor F1-score 0.9646391202386252

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3898, Test Loss: 0.3060, F1: 0.8658, AUC: 0.9444
Epoch [10/30] Train Loss: 0.2607, Test Loss: 0.2599, F1: 0.8897, AUC: 0.9592
Epoch [20/30] Train Loss: 0.2449, Test Loss: 0.2779, F1: 0.8709, AUC: 0.9619
Mejores resultados en la época:  17
f1-score 0.8926077534312545
AUC según el mejor F1-score 0.9614110342084552
Epoch [0/30] Train Loss: 0.3762, Test Loss: 0.2685, F1: 0.7978, AUC: 0.9470
Epoch [10/30] Train Loss: 0.2629, Test Loss: 0.2182, F1: 0.8228, AUC: 0.9609
Epoch [20/30] Train Loss: 0.2439, Test Loss: 0.2522, F1: 0.8000, AUC: 0.9630
Mejores resultados en la época:  23
f1-score 0.8313371293932849
AUC según el mejor F1-score 0.9648222327370486
Confusion matrix Test saved: outputs_cv/1/gpt/cm_mlp_5.png

========================================
Entrenando red 6 con capas [1559, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4110, Test Loss: 0.3209, F1: 0.8688, AUC: 0.9424
Epoch [10/30] Train Loss: 0.2621, Test Loss: 0.2790, F1: 0.8723, AUC: 0.9589
Epoch [20/30] Train Loss: 0.2425, Test Loss: 0.2549, F1: 0.8917, AUC: 0.9615
Mejores resultados en la época:  29
f1-score 0.8949869327631267
AUC según el mejor F1-score 0.9630938187777929

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3892, Test Loss: 0.3131, F1: 0.8741, AUC: 0.9461
Epoch [10/30] Train Loss: 0.2625, Test Loss: 0.3148, F1: 0.8760, AUC: 0.9592
Epoch [20/30] Train Loss: 0.2470, Test Loss: 0.2805, F1: 0.8918, AUC: 0.9617
Mejores resultados en la época:  28
f1-score 0.8949453715932285
AUC según el mejor F1-score 0.962981966838306

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4008, Test Loss: 0.2947, F1: 0.8763, AUC: 0.9482
Epoch [10/30] Train Loss: 0.2616, Test Loss: 0.2487, F1: 0.8924, AUC: 0.9630
Epoch [20/30] Train Loss: 0.2456, Test Loss: 0.2450, F1: 0.8882, AUC: 0.9659
Mejores resultados en la época:  22
f1-score 0.8995428437463369
AUC según el mejor F1-score 0.9659116836803678

, 'recall_cv_std': 0.0127, 'f1_cv_mean': 0.8488, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8768, 'precision_test': 0.7161, 'recall_test': 0.8014, 'f1_score_test': 0.7564}, 'XGBoost': {'accuracy_cv_mean': 0.8771, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8931, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8745, 'f1_cv_std': 0.0054, 'accuracy_test': 0.8921, 'precision_test': 0.7345, 'recall_test': 0.8579, 'f1_score_test': 0.7915}, 'Naive Bayes': {'accuracy_cv_mean': 0.7792, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.7615, 'precision_cv_std': 0.0066, 'recall_cv_mean': 0.813, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.7864, 'f1_cv_std': 0.0053, 'accuracy_test': 0.7625, 'precision_test': 0.5014, 'recall_test': 0.8014, 'f1_score_test': 0.6168}}}
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 1559)
Shape of X_test after concatenation:  (21625, 1559)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1559)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1559)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [1559, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4151, Test Loss: 0.3259, F1: 0.8560, AUC: 0.9352
Epoch [10/30] Train Loss: 0.2594, Test Loss: 0.2659, F1: 0.8859, AUC: 0.9570
Epoch [20/30] Train Loss: 0.2463, Test Loss: 0.2567, F1: 0.8908, AUC: 0.9596
Mejores resultados en la época:  24
f1-score 0.8933202838267678
AUC según el mejor F1-score 0.9602891871450784

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.5763, Test Loss: 0.4644, F1: 0.8397, AUC: 0.9128
Epoch [10/30] Train Loss: 0.2795, Test Loss: 0.2822, F1: 0.8831, AUC: 0.9521
Epoch [20/30] Train Loss: 0.2579, Test Loss: 0.2665, F1: 0.8887, AUC: 0.9571
Mejores resultados en la época:  28
f1-score 0.8904813089693061
AUC según el mejor F1-score 0.959173954061129

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.5701, Test Loss: 0.4557, F1: 0.8442, AUC: 0.9136
Epoch [10/30] Train Loss: 0.2799, Test Loss: 0.2740, F1: 0.8852, AUC: 0.9555
Epoch [20/30] Train Loss: 0.2612, Test Loss: 0.2686, F1: 0.8828, AUC: 0.9593
Mejores resultados en la época:  22
f1-score 0.8965109402720284
AUC según el mejor F1-score 0.9601535681355537

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.5586, Test Loss: 0.4737, F1: 0.8529, AUC: 0.9172
Epoch [10/30] Train Loss: 0.2746, Test Loss: 0.2671, F1: 0.8891, AUC: 0.9585
Epoch [20/30] Train Loss: 0.2536, Test Loss: 0.2523, F1: 0.8970, AUC: 0.9623
Mejores resultados en la época:  29
f1-score 0.8985649454188642
AUC según el mejor F1-score 0.9636931377410622

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4470, Test Loss: 0.3373, F1: 0.8542, AUC: 0.9311
Epoch [10/30] Train Loss: 0.2614, Test Loss: 0.2707, F1: 0.8821, AUC: 0.9557
Epoch [20/30] Train Loss: 0.2478, Test Loss: 0.2597, F1: 0.8871, AUC: 0.9588
Mejores resultados en la época:  19
f1-score 0.8922966335210255
AUC según el mejor F1-score 0.9586100960210976
Epoch [0/30] Train Loss: 0.4134, Test Loss: 0.3072, F1: 0.7638, AUC: 0.9388
Epoch [10/30] Train Loss: 0.2633, Test Loss: 0.2795, F1: 0.7877, AUC: 0.9585
Epoch [20/30] Train Loss: 0.2503, Test Loss: 0.2125, F1: 0.8257, AUC: 0.9612
Mejores resultados en la época:  22
f1-score 0.8317346123101519
AUC según el mejor F1-score 0.9615079261388381
Confusion matrix Test saved: outputs_cv/1/gpt/cm_mlp_1.png

========================================
Entrenando red 5 con capas [1559, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3776, Test Loss: 0.3184, F1: 0.8540, AUC: 0.9459
Epoch [10/30] Train Loss: 0.2633, Test Loss: 0.2949, F1: 0.8785, AUC: 0.9584
Epoch [20/30] Train Loss: 0.2425, Test Loss: 0.2624, F1: 0.8910, AUC: 0.9622
Mejores resultados en la época:  26
f1-score 0.897841726618705
AUC según el mejor F1-score 0.963639229022069

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3837, Test Loss: 0.3338, F1: 0.8401, AUC: 0.9455
Epoch [10/30] Train Loss: 0.2627, Test Loss: 0.2764, F1: 0.8751, AUC: 0.9600
Epoch [20/30] Train Loss: 0.2391, Test Loss: 0.2547, F1: 0.8888, AUC: 0.9623
Mejores resultados en la época:  27
f1-score 0.894137194398291
AUC según el mejor F1-score 0.9632321079149391

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3869, Test Loss: 0.2901, F1: 0.8769, AUC: 0.9489
Epoch [10/30] Train Loss: 0.2668, Test Loss: 0.3270, F1: 0.8279, AUC: 0.9627
Epoch [20/30] Train Loss: 0.2473, Test Loss: 0.2560, F1: 0.8985, AUC: 0.9645
Mejores resultados en la época:  23
f1-score 0.9007396802672393
AUC según el mejor F1-score 0.9657868918916381

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3988, Test Loss: 0.3098, F1: 0.8709, AUC: 0.9461
Epoch [10/30] Train Loss: 0.2668, Test Loss: 0.2504, F1: 0.8866, AUC: 0.9626
Epoch [20/30] Train Loss: 0.2491, Test Loss: 0.2469, F1: 0.8964, AUC: 0.9657
Mejores resultados en la época:  19
f1-score 0.8996157540826129
AUC según el mejor F1-score 0.9646391202386252

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3898, Test Loss: 0.3060, F1: 0.8658, AUC: 0.9444
Epoch [10/30] Train Loss: 0.2607, Test Loss: 0.2599, F1: 0.8897, AUC: 0.9592
Epoch [20/30] Train Loss: 0.2449, Test Loss: 0.2779, F1: 0.8709, AUC: 0.9619
Mejores resultados en la época:  17
f1-score 0.8926077534312545
AUC según el mejor F1-score 0.9614110342084552
Epoch [0/30] Train Loss: 0.3762, Test Loss: 0.2685, F1: 0.7978, AUC: 0.9470
Epoch [10/30] Train Loss: 0.2629, Test Loss: 0.2182, F1: 0.8228, AUC: 0.9609
Epoch [20/30] Train Loss: 0.2439, Test Loss: 0.2522, F1: 0.8000, AUC: 0.9630
Mejores resultados en la época:  23
f1-score 0.8313371293932849
AUC según el mejor F1-score 0.9648222327370486
Confusion matrix Test saved: outputs_cv/1/gpt/cm_mlp_5.png

========================================
Entrenando red 6 con capas [1559, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4110, Test Loss: 0.3209, F1: 0.8688, AUC: 0.9424
Epoch [10/30] Train Loss: 0.2621, Test Loss: 0.2790, F1: 0.8723, AUC: 0.9589
Epoch [20/30] Train Loss: 0.2425, Test Loss: 0.2549, F1: 0.8917, AUC: 0.9615
Mejores resultados en la época:  29
f1-score 0.8949869327631267
AUC según el mejor F1-score 0.9630938187777929

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3892, Test Loss: 0.3131, F1: 0.8741, AUC: 0.9461
Epoch [10/30] Train Loss: 0.2625, Test Loss: 0.3148, F1: 0.8760, AUC: 0.9592
Epoch [20/30] Train Loss: 0.2470, Test Loss: 0.2805, F1: 0.8918, AUC: 0.9617
Mejores resultados en la época:  28
f1-score 0.8949453715932285
AUC según el mejor F1-score 0.962981966838306

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4008, Test Loss: 0.2947, F1: 0.8763, AUC: 0.9482
Epoch [10/30] Train Loss: 0.2616, Test Loss: 0.2487, F1: 0.8924, AUC: 0.9630
Epoch [20/30] Train Loss: 0.2456, Test Loss: 0.2450, F1: 0.8882, AUC: 0.9659
Mejores resultados en la época:  22
f1-score 0.8995428437463369
AUC según el mejor F1-score 0.9659116836803678

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4112, Test Loss: 0.4346, F1: 0.7571, AUC: 0.9462
Epoch [10/30] Train Loss: 0.2682, Test Loss: 0.2580, F1: 0.8948, AUC: 0.9624
Epoch [20/30] Train Loss: 0.2488, Test Loss: 0.2383, F1: 0.9003, AUC: 0.9660
Mejores resultados en la época:  20
f1-score 0.9002773423369107
AUC según el mejor F1-score 0.9659736857675771

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4028, Test Loss: 0.3055, F1: 0.8650, AUC: 0.9425
Epoch [10/30] Train Loss: 0.2586, Test Loss: 0.2587, F1: 0.8835, AUC: 0.9600
Epoch [20/30] Train Loss: 0.2484, Test Loss: 0.2642, F1: 0.8824, AUC: 0.9619
Mejores resultados en la época:  23
f1-score 0.8934534534534535
AUC según el mejor F1-score 0.963382153919265
Epoch [0/30] Train Loss: 0.3839, Test Loss: 0.2799, F1: 0.7918, AUC: 0.9468
Epoch [10/30] Train Loss: 0.2589, Test Loss: 0.2519, F1: 0.8005, AUC: 0.9609
Epoch [20/30] Train Loss: 0.2426, Test Loss: 0.2221, F1: 0.8139, AUC: 0.9637
Mejores resultados en la época:  24
f1-score 0.8343509946818988
AUC según el mejor F1-score 0.9646294995021152
Confusion matrix Test saved: outputs_cv/1/gpt/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9344, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0009, 'recall_cv_mean': 0.9336, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9344, 'f1_cv_std': 0.0017, 'params': 160801, 'accuracy_test': 0.9372, 'precision_test': 0.8404, 'recall_test': 0.9095, 'f1_score_test': 0.8736}, 'MLP_2744833': {'accuracy_cv_mean': 0.9454, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9447, 'precision_cv_std': 0.0058, 'recall_cv_mean': 0.9464, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.9455, 'f1_cv_std': 0.0015, 'params': 2744833, 'accuracy_test': 0.9557, 'precision_test': 0.8901, 'recall_test': 0.9289, 'f1_score_test': 0.9091}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0013, 'precision_cv_mean': 0.9499, 'precision_cv_std': 0.0027, 'recall_cv_mean': 0.9422, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.946, 'f1_cv_std': 0.0013, 'params': 5843969, 'accuracy_test': 0.9551, 'precision_test': 0.8925, 'recall_test': 0.9233, 'f1_score_test': 0.9076}, 'Logistic Regression': {'accuracy_cv_mean': 0.9278, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9348, 'precision_cv_std': 0.0019, 'recall_cv_mean': 0.9198, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.9272, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9337, 'precision_test': 0.8257, 'recall_test': 0.9155, 'f1_score_test': 0.8683}, 'SVM': {'accuracy_cv_mean': 0.8164, 'accuracy_cv_std': 0.0186, 'precision_cv_mean': 0.7725, 'precision_cv_std': 0.0325, 'recall_cv_mean': 0.9014, 'recall_cv_std': 0.0238, 'f1_cv_mean': 0.8311, 'f1_cv_std': 0.0121, 'accuracy_test': 0.753, 'precision_test': 0.4904, 'recall_test': 0.8963, 'f1_score_test': 0.6339}, 'Decision Tree': {'accuracy_cv_mean': 0.829, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7388, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.812, 'f1_cv_std': 0.0056, 'accuracy_test': 0.875, 'precision_test': 0.7471, 'recall_test': 0.7196, 'f1_score_test': 0.7331}, 'Random Forest': {'accuracy_cv_mean': 0.8321, 'accuracy_cv_std': 0.009, 'precision_cv_mean': 0.8668, 'precision_cv_std': 0.0097, 'recall_cv_mean': 0.7849, 'recall_cv_std': 0.0135, 'f1_cv_mean': 0.8238, 'f1_cv_std': 0.01, 'accuracy_test': 0.8567, 'precision_test': 0.6735, 'recall_test': 0.7756, 'f1_score_test': 0.721}, 'XGBoost': {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.9273, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.885, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.9057, 'f1_cv_std': 0.0039, 'accuracy_test': 0.9204, 'precision_test': 0.8022, 'recall_test': 0.8843, 'f1_score_test': 0.8413}, 'Naive Bayes': {'accuracy_cv_mean': 0.9033, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.9277, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0066, 'f1_cv_mean': 0.9004, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9193, 'precision_test': 0.8091, 'recall_test': 0.8659, 'f1_score_test': 0.8365}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.848, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8373, 'precision_cv_std': 0.0139, 'recall_cv_mean': 0.8645, 'recall_cv_std': 0.0191, 'f1_cv_mean': 0.8504, 'f1_cv_std': 0.003, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.759, 'recall_test': 0.7539, 'f1_score_test': 0.7564}, 'MLP_338433': {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.862, 'precision_cv_std': 0.0095, 'recall_cv_mean': 0.8489, 'recall_cv_std': 0.0102, 'f1_cv_mean': 0.8553, 'f1_cv_std': 0.0028, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7795, 'recall_test': 0.7736, 'f1_score_test': 0.7766}, 'MLP_1031169': {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.85, 'precision_cv_std': 0.0102, 'recall_cv_mean': 0.8658, 'recall_cv_std': 0.0132, 'f1_cv_mean': 0.8577, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8917, 'precision_test': 0.7758, 'recall_test': 0.7678, 'f1_score_test': 0.7718}, 'Logistic Regression': {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8604, 'precision_cv_std': 0.004, 'recall_cv_mean': 0.8326, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8462, 'f1_cv_std': 0.006, 'accuracy_test': 0.8564, 'precision_test': 0.6585, 'recall_test': 0.8273, 'f1_score_test': 0.7333}, 'SVM': {'accuracy_cv_mean': 0.6564, 'accuracy_cv_std': 0.0276, 'precision_cv_mean': 0.634, 'precision_cv_std': 0.0178, 'recall_cv_mean': 0.7365, 'recall_cv_std': 0.0655, 'f1_cv_mean': 0.6806, 'f1_cv_std': 0.037, 'accuracy_test': 0.5639, 'precision_test': 0.3076, 'recall_test': 0.6616, 'f1_score_test': 0.42}, 'Decision Tree': {'accuracy_cv_mean': 0.7959, 'accuracy_cv_std': 0.004, 'precision_cv_mean': 0.8116, 'precision_cv_std': 0.015, 'recall_cv_mean': 0.7717, 'recall_cv_std': 0.0236, 'f1_cv_mean': 0.7907, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8053, 'precision_test': 0.568, 'recall_test': 0.7684, 'f1_score_test': 0.6532}, 'Random Forest': {'accuracy_cv_mean': 0.855, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8866, 'precision_cv_std': 0.0042, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0127, 'f1_cv_mean': 0.8488, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8768, 'precision_test': 0.7161, 'recall_test': 0.8014, 'f1_score_test': 0.7564}, 'XGBoost': {'accuracy_cv_mean': 0.8771, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8931, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8745, 'f1_cv_std': 0.0054, 'accuracy_test': 0.8921, 'precision_test': 0.7345, 'recall_test': 0.8579, 'f1_score_test': 0.7915}, 'Naive Bayes': {'accuracy_cv_mean': 0.7792, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.7615, 'precision_cv_std': 0.0066, 'recall_cv_mean': 0.813, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.7864, 'f1_cv_std': 0.0053, 'accuracy_test': 0.7625, 'precision_test': 0.5014, 'recall_test': 0.8014, 'f1_score_test': 0.6168}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8936, 'accuracy_cv_std': 0.0037, 'precision_cv_mean': 0.8898, 'precision_cv_std': 0.0137, 'recall_cv_mean': 0.8991, 'recall_cv_std': 0.0121, 'f1_cv_mean': 0.8942, 'f1_cv_std': 0.0029, 'params': 49953, 'accuracy_test': 0.9221, 'precision_test': 0.8585, 'recall_test': 0.8066, 'f1_score_test': 0.8317}, 'MLP_971265': {'accuracy_cv_mean': 0.8957, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8863, 'precision_cv_std': 0.0052, 'recall_cv_mean': 0.9079, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.897, 'f1_cv_std': 0.0031, 'params': 971265, 'accuracy_test': 0.9206, 'precision_test': 0.8424, 'recall_test': 0.8205, 'f1_score_test': 0.8313}, 'MLP_2296833': {'accuracy_cv_mean': 0.8951, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8838, 'precision_cv_std': 0.0084, 'recall_cv_mean': 0.9101, 'recall_cv_std': 0.0105, 'f1_cv_mean': 0.8966, 'f1_cv_std': 0.0027, 'params': 2296833, 'accuracy_test': 0.9222, 'precision_test': 0.8482, 'recall_test': 0.8209, 'f1_score_test': 0.8344}}}
Saved on: outputs_cv/1/gpt

==============================
Model: Logistic Regression

Promedio CV (validación interna):
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4112, Test Loss: 0.4346, F1: 0.7571, AUC: 0.9462
Epoch [10/30] Train Loss: 0.2682, Test Loss: 0.2580, F1: 0.8948, AUC: 0.9624
Epoch [20/30] Train Loss: 0.2488, Test Loss: 0.2383, F1: 0.9003, AUC: 0.9660
Mejores resultados en la época:  20
f1-score 0.9002773423369107
AUC según el mejor F1-score 0.9659736857675771

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4028, Test Loss: 0.3055, F1: 0.8650, AUC: 0.9425
Epoch [10/30] Train Loss: 0.2586, Test Loss: 0.2587, F1: 0.8835, AUC: 0.9600
Epoch [20/30] Train Loss: 0.2484, Test Loss: 0.2642, F1: 0.8824, AUC: 0.9619
Mejores resultados en la época:  23
f1-score 0.8934534534534535
AUC según el mejor F1-score 0.963382153919265
Epoch [0/30] Train Loss: 0.3839, Test Loss: 0.2799, F1: 0.7918, AUC: 0.9468
Epoch [10/30] Train Loss: 0.2589, Test Loss: 0.2519, F1: 0.8005, AUC: 0.9609
Epoch [20/30] Train Loss: 0.2426, Test Loss: 0.2221, F1: 0.8139, AUC: 0.9637
Mejores resultados en la época:  24
f1-score 0.8343509946818988
AUC según el mejor F1-score 0.9646294995021152
Confusion matrix Test saved: outputs_cv/1/gpt/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9344, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0009, 'recall_cv_mean': 0.9336, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9344, 'f1_cv_std': 0.0017, 'params': 160801, 'accuracy_test': 0.9372, 'precision_test': 0.8404, 'recall_test': 0.9095, 'f1_score_test': 0.8736}, 'MLP_2744833': {'accuracy_cv_mean': 0.9454, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9447, 'precision_cv_std': 0.0058, 'recall_cv_mean': 0.9464, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.9455, 'f1_cv_std': 0.0015, 'params': 2744833, 'accuracy_test': 0.9557, 'precision_test': 0.8901, 'recall_test': 0.9289, 'f1_score_test': 0.9091}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0013, 'precision_cv_mean': 0.9499, 'precision_cv_std': 0.0027, 'recall_cv_mean': 0.9422, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.946, 'f1_cv_std': 0.0013, 'params': 5843969, 'accuracy_test': 0.9551, 'precision_test': 0.8925, 'recall_test': 0.9233, 'f1_score_test': 0.9076}, 'Logistic Regression': {'accuracy_cv_mean': 0.9278, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9348, 'precision_cv_std': 0.0019, 'recall_cv_mean': 0.9198, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.9272, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9337, 'precision_test': 0.8257, 'recall_test': 0.9155, 'f1_score_test': 0.8683}, 'SVM': {'accuracy_cv_mean': 0.8164, 'accuracy_cv_std': 0.0186, 'precision_cv_mean': 0.7725, 'precision_cv_std': 0.0325, 'recall_cv_mean': 0.9014, 'recall_cv_std': 0.0238, 'f1_cv_mean': 0.8311, 'f1_cv_std': 0.0121, 'accuracy_test': 0.753, 'precision_test': 0.4904, 'recall_test': 0.8963, 'f1_score_test': 0.6339}, 'Decision Tree': {'accuracy_cv_mean': 0.829, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7388, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.812, 'f1_cv_std': 0.0056, 'accuracy_test': 0.875, 'precision_test': 0.7471, 'recall_test': 0.7196, 'f1_score_test': 0.7331}, 'Random Forest': {'accuracy_cv_mean': 0.8321, 'accuracy_cv_std': 0.009, 'precision_cv_mean': 0.8668, 'precision_cv_std': 0.0097, 'recall_cv_mean': 0.7849, 'recall_cv_std': 0.0135, 'f1_cv_mean': 0.8238, 'f1_cv_std': 0.01, 'accuracy_test': 0.8567, 'precision_test': 0.6735, 'recall_test': 0.7756, 'f1_score_test': 0.721}, 'XGBoost': {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.9273, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.885, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.9057, 'f1_cv_std': 0.0039, 'accuracy_test': 0.9204, 'precision_test': 0.8022, 'recall_test': 0.8843, 'f1_score_test': 0.8413}, 'Naive Bayes': {'accuracy_cv_mean': 0.9033, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.9277, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0066, 'f1_cv_mean': 0.9004, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9193, 'precision_test': 0.8091, 'recall_test': 0.8659, 'f1_score_test': 0.8365}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.848, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8373, 'precision_cv_std': 0.0139, 'recall_cv_mean': 0.8645, 'recall_cv_std': 0.0191, 'f1_cv_mean': 0.8504, 'f1_cv_std': 0.003, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.759, 'recall_test': 0.7539, 'f1_score_test': 0.7564}, 'MLP_338433': {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.862, 'precision_cv_std': 0.0095, 'recall_cv_mean': 0.8489, 'recall_cv_std': 0.0102, 'f1_cv_mean': 0.8553, 'f1_cv_std': 0.0028, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7795, 'recall_test': 0.7736, 'f1_score_test': 0.7766}, 'MLP_1031169': {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.85, 'precision_cv_std': 0.0102, 'recall_cv_mean': 0.8658, 'recall_cv_std': 0.0132, 'f1_cv_mean': 0.8577, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8917, 'precision_test': 0.7758, 'recall_test': 0.7678, 'f1_score_test': 0.7718}, 'Logistic Regression': {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8604, 'precision_cv_std': 0.004, 'recall_cv_mean': 0.8326, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8462, 'f1_cv_std': 0.006, 'accuracy_test': 0.8564, 'precision_test': 0.6585, 'recall_test': 0.8273, 'f1_score_test': 0.7333}, 'SVM': {'accuracy_cv_mean': 0.6564, 'accuracy_cv_std': 0.0276, 'precision_cv_mean': 0.634, 'precision_cv_std': 0.0178, 'recall_cv_mean': 0.7365, 'recall_cv_std': 0.0655, 'f1_cv_mean': 0.6806, 'f1_cv_std': 0.037, 'accuracy_test': 0.5639, 'precision_test': 0.3076, 'recall_test': 0.6616, 'f1_score_test': 0.42}, 'Decision Tree': {'accuracy_cv_mean': 0.7959, 'accuracy_cv_std': 0.004, 'precision_cv_mean': 0.8116, 'precision_cv_std': 0.015, 'recall_cv_mean': 0.7717, 'recall_cv_std': 0.0236, 'f1_cv_mean': 0.7907, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8053, 'precision_test': 0.568, 'recall_test': 0.7684, 'f1_score_test': 0.6532}, 'Random Forest': {'accuracy_cv_mean': 0.855, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8866, 'precision_cv_std': 0.0042, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0127, 'f1_cv_mean': 0.8488, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8768, 'precision_test': 0.7161, 'recall_test': 0.8014, 'f1_score_test': 0.7564}, 'XGBoost': {'accuracy_cv_mean': 0.8771, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8931, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8745, 'f1_cv_std': 0.0054, 'accuracy_test': 0.8921, 'precision_test': 0.7345, 'recall_test': 0.8579, 'f1_score_test': 0.7915}, 'Naive Bayes': {'accuracy_cv_mean': 0.7792, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.7615, 'precision_cv_std': 0.0066, 'recall_cv_mean': 0.813, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.7864, 'f1_cv_std': 0.0053, 'accuracy_test': 0.7625, 'precision_test': 0.5014, 'recall_test': 0.8014, 'f1_score_test': 0.6168}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8936, 'accuracy_cv_std': 0.0037, 'precision_cv_mean': 0.8898, 'precision_cv_std': 0.0137, 'recall_cv_mean': 0.8991, 'recall_cv_std': 0.0121, 'f1_cv_mean': 0.8942, 'f1_cv_std': 0.0029, 'params': 49953, 'accuracy_test': 0.9221, 'precision_test': 0.8585, 'recall_test': 0.8066, 'f1_score_test': 0.8317}, 'MLP_971265': {'accuracy_cv_mean': 0.8957, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8863, 'precision_cv_std': 0.0052, 'recall_cv_mean': 0.9079, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.897, 'f1_cv_std': 0.0031, 'params': 971265, 'accuracy_test': 0.9206, 'precision_test': 0.8424, 'recall_test': 0.8205, 'f1_score_test': 0.8313}, 'MLP_2296833': {'accuracy_cv_mean': 0.8951, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8838, 'precision_cv_std': 0.0084, 'recall_cv_mean': 0.9101, 'recall_cv_std': 0.0105, 'f1_cv_mean': 0.8966, 'f1_cv_std': 0.0027, 'params': 2296833, 'accuracy_test': 0.9222, 'precision_test': 0.8482, 'recall_test': 0.8209, 'f1_score_test': 0.8344}}}
Saved on: outputs_cv/1/gpt

==============================
Model: Logistic Regression

Promedio CV (validación interna):
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:46:50] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:47:37] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:51:25] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:52:13] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:55:58] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:56:47] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:00:33] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:01:24] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:05:11] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:06:02] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:09:57] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:10:49] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
{'accuracy_cv_mean': 0.8997, 'accuracy_cv_std': 0.0056, 'precision_cv_mean': 0.9043, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.8939, 'recall_cv_std': 0.0086, 'f1_cv_mean': 0.8991, 'f1_cv_std': 0.0059}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 43, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.91      0.94     16465
           1       0.76      0.89      0.82      5160

    accuracy                           0.91     21625
   macro avg       0.86      0.90      0.88     21625
weighted avg       0.91      0.91      0.91     21625

Confusion matrix Test saved as: outputs_cv/1/gpt/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/1/gpt/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8458, 'accuracy_cv_std': 0.0054, 'precision_cv_mean': 0.8523, 'precision_cv_std': 0.0199, 'recall_cv_mean': 0.8381, 'recall_cv_std': 0.0265, 'f1_cv_mean': 0.8446, 'f1_cv_std': 0.0066}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 43, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.80      0.87     16465
           1       0.58      0.86      0.69      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.83      0.78     21625
weighted avg       0.86      0.82      0.83     21625

Confusion matrix Test saved as: outputs_cv/1/gpt/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/1/gpt/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8089, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8186, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.7936, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.8059, 'f1_cv_std': 0.0036}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 43, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.84      0.88     16465
           1       0.60      0.78      0.68      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.81      0.78     21625
weighted avg       0.85      0.82      0.83     21625

Confusion matrix Test saved as: outputs_cv/1/gpt/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/1/gpt/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8732, 'accuracy_cv_std': 0.005, 'precision_cv_mean': 0.9017, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8377, 'recall_cv_std': 0.0093, 'f1_cv_mean': 0.8685, 'f1_cv_std': 0.0058}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 43, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.91      0.93     16465
           1       0.75      0.83      0.79      5160

    accuracy                           0.90     21625
   macro avg       0.85      0.87      0.86     21625
weighted avg       0.90      0.90      0.90     21625

Confusion matrix Test saved as: outputs_cv/1/gpt/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/1/gpt/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9082, 'accuracy_cv_std': 0.0054, 'precision_cv_mean': 0.9218, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8921, 'recall_cv_std': 0.0101, 'f1_cv_mean': 0.9067, 'f1_cv_std': 0.0059}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 43, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.79      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/1/gpt/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/1/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8322, 'accuracy_cv_std': 0.0067, 'precision_cv_mean': 0.8618, 'precision_cv_std': 0.0059, 'recall_cv_mean': 0.7913, 'recall_cv_std': 0.0098, 'f1_cv_mean': 0.825, 'f1_cv_std': 0.0074}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.79      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/1/gpt/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/1/gpt/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.9082, 'accuracy_cv_std': 0.0054, 'precision_cv_mean': 0.9218, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8921, 'recall_cv_std': 0.0101, 'f1_cv_mean': 0.9067, 'f1_cv_std': 0.0059, 'accuracy_test': 0.9192, 'precision_test': 0.7944, 'recall_test': 0.8921, 'f1_score_test': 0.8404}
Logistic Regression: {'accuracy_cv_mean': 0.8997, 'accuracy_cv_std': 0.0056, 'precision_cv_mean': 0.9043, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.8939, 'recall_cv_std': 0.0086, 'f1_cv_mean': 0.8991, 'f1_cv_std': 0.0059, 'accuracy_test': 0.9064, 'precision_test': 0.7599, 'recall_test': 0.8882, 'f1_score_test': 0.8191}
Random Forest: {'accuracy_cv_mean': 0.8732, 'accuracy_cv_std': 0.005, 'precision_cv_mean': 0.9017, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8377, 'recall_cv_std': 0.0093, 'f1_cv_mean': 0.8685, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8952, 'precision_test': 0.7529, 'recall_test': 0.8345, 'f1_score_test': 0.7916}
Naive Bayes: {'accuracy_cv_mean': 0.8322, 'accuracy_cv_std': 0.0067, 'precision_cv_mean': 0.8618, 'precision_cv_std': 0.0059, 'recall_cv_mean': 0.7913, 'recall_cv_std': 0.0098, 'f1_cv_mean': 0.825, 'f1_cv_std': 0.0074, 'accuracy_test': 0.8555, 'precision_test': 0.6666, 'recall_test': 0.7891, 'f1_score_test': 0.7227}
SVM: {'accuracy_cv_mean': 0.8458, 'accuracy_cv_std': 0.0054, 'precision_cv_mean': 0.8523, 'precision_cv_std': 0.0199, 'recall_cv_mean': 0.8381, 'recall_cv_std': 0.0265, 'f1_cv_mean': 0.8446, 'f1_cv_std': 0.0066, 'accuracy_test': 0.8164, 'precision_test': 0.5776, 'recall_test': 0.8579, 'f1_score_test': 0.6904}
Decision Tree: {'accuracy_cv_mean': 0.8089, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8186, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.7936, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.8059, 'f1_cv_std': 0.0036, 'accuracy_test': 0.8223, 'precision_test': 0.5985, 'recall_test': 0.776, 'f1_score_test': 0.6758}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9344, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0009, 'recall_cv_mean': 0.9336, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9344, 'f1_cv_std': 0.0017, 'params': 160801, 'accuracy_test': 0.9372, 'precision_test': 0.8404, 'recall_test': 0.9095, 'f1_score_test': 0.8736}, 'MLP_2744833': {'accuracy_cv_mean': 0.9454, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9447, 'precision_cv_std': 0.0058, 'recall_cv_mean': 0.9464, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.9455, 'f1_cv_std': 0.0015, 'params': 2744833, 'accuracy_test': 0.9557, 'precision_test': 0.8901, 'recall_test': 0.9289, 'f1_score_test': 0.9091}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0013, 'precision_cv_mean': 0.9499, 'precision_cv_std': 0.0027, 'recall_cv_mean': 0.9422, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.946, 'f1_cv_std': 0.0013, 'params': 5843969, 'accuracy_test': 0.9551, 'precision_test': 0.8925, 'recall_test': 0.9233, 'f1_score_test': 0.9076}, 'Logistic Regression': {'accuracy_cv_mean': 0.9278, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9348, 'precision_cv_std': 0.0019, 'recall_cv_mean': 0.9198, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.9272, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9337, 'precision_test': 0.8257, 'recall_test': 0.9155, 'f1_score_test': 0.8683}, 'SVM': {'accuracy_cv_mean': 0.8164, 'accuracy_cv_std': 0.0186, 'precision_cv_mean': 0.7725, 'precision_cv_std': 0.0325, 'recall_cv_mean': 0.9014, 'recall_cv_std': 0.0238, 'f1_cv_mean': 0.8311, 'f1_cv_std': 0.0121, 'accuracy_test': 0.753, 'precision_test': 0.4904, 'recall_test': 0.8963, 'f1_score_test': 0.6339}, 'Decision Tree': {'accuracy_cv_mean': 0.829, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7388, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.812, 'f1_cv_std': 0.0056, 'accuracy_test': 0.875, 'precision_test': 0.7471, 'recall_test': 0.7196, 'f1_score_test': 0.7331}, 'Random Forest': {'accuracy_cv_mean': 0.8321, 'accuracy_cv_std': 0.009, 'precision_cv_mean': 0.8668, 'precision_cv_std': 0.0097, 'recall_cv_mean': 0.7849, 'recall_cv_std': 0.0135, 'f1_cv_mean': 0.8238, 'f1_cv_std': 0.01, 'accuracy_test': 0.8567, 'precision_test': 0.6735, 'recall_test': 0.7756, 'f1_score_test': 0.721}, 'XGBoost': {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.9273, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.885, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.9057, 'f1_cv_std': 0.0039, 'accuracy_test': 0.9204, 'precision_test': 0.8022, 'recall_test': 0.8843, 'f1_score_test': 0.8413}, 'Naive Bayes': {'accuracy_cv_mean': 0.9033, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.9277, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0066, 'f1_cv_mean': 0.9004, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9193, 'precision_test': 0.8091, 'recall_test': 0.8659, 'f1_score_test': 0.8365}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.848, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8373, 'precision_cv_std': 0.0139, 'recall_cv_mean': 0.8645, 'recall_cv_std': 0.0191, 'f1_cv_mean': 0.8504, 'f1_cv_std': 0.003, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.759, 'recall_test': 0.7539, 'f1_score_test': 0.7564}, 'MLP_338433': {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.862, 'precision_cv_std': 0.0095, 'recall_cv_mean': 0.8489, 'recall_cv_std': 0.0102, 'f1_cv_mean': 0.8553, 'f1_cv_std': 0.0028, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7795, 'recall_test': 0.7736, 'f1_score_test': 0.7766}, 'MLP_1031169': {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.85, 'precision_cv_std': 0.0102, 'recall_cv_mean': 0.8658, 'recall_cv_std': 0.0132, 'f1_cv_mean': 0.8577, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8917, 'precision_test': 0.7758, 'recall_test': 0.7678, 'f1_score_test': 0.7718}, 'Logistic Regression': {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8604, 'precision_cv_std': 0.004, 'recall_cv_mean': 0.8326, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8462, 'f1_cv_std': 0.006, 'accuracy_test': 0.8564, 'precision_test': 0.6585, 'recall_test': 0.8273, 'f1_score_test': 0.7333}, 'SVM': {'accuracy_cv_mean': 0.6564, 'accuracy_cv_std': 0.0276, 'precision_cv_mean': 0.634, 'precision_cv_std': 0.0178, 'recall_cv_mean': 0.7365, 'recall_cv_std': 0.0655, 'f1_cv_mean': 0.6806, 'f1_cv_std': 0.037, 'accuracy_test': 0.5639, 'precision_test': 0.3076, 'recall_test': 0.6616, 'f1_score_test': 0.42}, 'Decision Tree': {'accuracy_cv_mean': 0.7959, 'accuracy_cv_std': 0.004, 'precision_cv_mean': 0.8116, 'precision_cv_std': 0.015, 'recall_cv_mean': 0.7717, 'recall_cv_std': 0.0236, 'f1_cv_mean': 0.7907, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8053, 'precision_test': 0.568, 'recall_test': 0.7684, 'f1_score_test': 0.6532}, 'Random Forest': {'accuracy_cv_mean': 0.855, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8866, 'precision_cv_std': 0.0042, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0127, 'f1_cv_mean': 0.8488, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8768, 'precision_test': 0.7161, 'recall_test': 0.8014, 'f1_score_test': 0.7564}, 'XGBoost': {'accuracy_cv_mean': 0.8771, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8931, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8745, 'f1_cv_std': 0.0054, 'accuracy_test': 0.8921, 'precision_test': 0.7345, 'recall_test': 0.8579, 'f1_score_test': 0.7915}, 'Naive Bayes': {'accuracy_cv_mean': 0.7792, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.7615, 'precision_cv_std': 0.0066, 'recall_cv_mean': 0.813, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.7864, 'f1_cv_std': 0.0053, 'accuracy_test': 0.7625, 'precision_test': 0.5014, 'recall_test': 0.8014, 'f1_score_test': 0.6168}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8936, 'accuracy_cv_std': 0.0037, 'precision_cv_mean': 0.8898, 'precision_cv_std': 0.0137, 'recall_cv_mean': 0.8991, 'recall_cv_std': 0.0121, 'f1_cv_mean': 0.8942, 'f1_cv_std': 0.0029, 'params': 49953, 'accuracy_test': 0.9221, 'precision_test': 0.8585, 'recall_test': 0.8066, 'f1_score_test': 0.8317}, 'MLP_971265': {'accuracy_cv_mean': 0.8957, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8863, 'precision_cv_std': 0.0052, 'recall_cv_mean': 0.9079, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.897, 'f1_cv_std': 0.0031, 'params': 971265, 'accuracy_test': 0.9206, 'precision_test': 0.8424, 'recall_test': 0.8205, 'f1_score_test': 0.8313}, 'MLP_2296833': {'accuracy_cv_mean': 0.8951, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8838, 'precision_cv_std': 0.0084, 'recall_cv_mean': 0.9101, 'recall_cv_std': 0.0105, 'f1_cv_mean': 0.8966, 'f1_cv_std': 0.0027, 'params': 2296833, 'accuracy_test': 0.9222, 'precision_test': 0.8482, 'recall_test': 0.8209, 'f1_score_test': 0.8344}, 'Logistic Regression': {'accuracy_cv_mean': 0.8997, 'accuracy_cv_std': 0.0056, 'precision_cv_mean': 0.9043, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.8939, 'recall_cv_std': 0.0086, 'f1_cv_mean': 0.8991, 'f1_cv_std': 0.0059, 'accuracy_test': 0.9064, 'precision_test': 0.7599, 'recall_test': 0.8882, 'f1_score_test': 0.8191}, 'SVM': {'accuracy_cv_mean': 0.8458, 'accuracy_cv_std': 0.0054, 'precision_cv_mean': 0.8523, 'precision_cv_std': 0.0199, 'recall_cv_mean': 0.8381, 'recall_cv_std': 0.0265, 'f1_cv_mean': 0.8446, 'f1_cv_std': 0.0066, 'accuracy_test': 0.8164, 'precision_test': 0.5776, 'recall_test': 0.8579, 'f1_score_test': 0.6904}, 'Decision Tree': {'accuracy_cv_mean': 0.8089, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8186, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.7936, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.8059, 'f1_cv_std': 0.0036, 'accuracy_test': 0.8223, 'precision_test': 0.5985, 'recall_test': 0.776, 'f1_score_test': 0.6758}, 'Random Forest': {'accuracy_cv_mean': 0.8732, 'accuracy_cv_std': 0.005, 'precision_cv_mean': 0.9017, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8377, 'recall_cv_std': 0.0093, 'f1_cv_mean': 0.8685, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8952, 'precision_test': 0.7529, 'recall_test': 0.8345, 'f1_score_test': 0.7916}, 'XGBoost': {'accuracy_cv_mean': 0.9082, 'accuracy_cv_std': 0.0054, 'precision_cv_mean': 0.9218, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8921, 'recall_cv_std': 0.0101, 'f1_cv_mean': 0.9067, 'f1_cv_std': 0.0059, 'accuracy_test': 0.9192, 'precision_test': 0.7944, 'recall_test': 0.8921, 'f1_score_test': 0.8404}, 'Naive Bayes': {'accuracy_cv_mean': 0.8322, 'accuracy_cv_std': 0.0067, 'precision_cv_mean': 0.8618, 'precision_cv_std': 0.0059, 'recall_cv_mean': 0.7913, 'recall_cv_std': 0.0098, 'f1_cv_mean': 0.825, 'f1_cv_std': 0.0074, 'accuracy_test': 0.8555, 'precision_test': 0.6666, 'recall_test': 0.7891, 'f1_score_test': 0.7227}}}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
MLP_2744833: {'accuracy_cv_mean': 0.9454, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9447, 'precision_cv_std': 0.0058, 'recall_cv_mean': 0.9464, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.9455, 'f1_cv_std': 0.0015, 'params': 2744833, 'accuracy_test': 0.9557, 'precision_test': 0.8901, 'recall_test': 0.9289, 'f1_score_test': 0.9091}
MLP_5843969: {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0013, 'precision_cv_mean': 0.9499, 'precision_cv_std': 0.0027, 'recall_cv_mean': 0.9422, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.946, 'f1_cv_std': 0.0013, 'params': 5843969, 'accuracy_test': 0.9551, 'precision_test': 0.8925, 'recall_test': 0.9233, 'f1_score_test': 0.9076}
MLP_160801: {'accuracy_cv_mean': 0.9344, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0009, 'recall_cv_mean': 0.9336, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9344, 'f1_cv_std': 0.0017, 'params': 160801, 'accuracy_test': 0.9372, 'precision_test': 0.8404, 'recall_test': 0.9095, 'f1_score_test': 0.8736}
Logistic Regression: {'accuracy_cv_mean': 0.9278, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9348, 'precision_cv_std': 0.0019, 'recall_cv_mean': 0.9198, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.9272, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9337, 'precision_test': 0.8257, 'recall_test': 0.9155, 'f1_score_test': 0.8683}
XGBoost: {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.9273, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.885, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.9057, 'f1_cv_std': 0.0039, 'accuracy_test': 0.9204, 'precision_test': 0.8022, 'recall_test': 0.8843, 'f1_score_test': 0.8413}
Naive Bayes: {'accuracy_cv_mean': 0.9033, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.9277, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0066, 'f1_cv_mean': 0.9004, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9193, 'precision_test': 0.8091, 'recall_test': 0.8659, 'f1_score_test': 0.8365}
Decision Tree: {'accuracy_cv_mean': 0.829, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7388, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.812, 'f1_cv_std': 0.0056, 'accuracy_test': 0.875, 'precision_test': 0.7471, 'recall_test': 0.7196, 'f1_score_test': 0.7331}
Random Forest: {'accuracy_cv_mean': 0.8321, 'accuracy_cv_std': 0.009, 'precision_cv_mean': 0.8668, 'precision_cv_std': 0.0097, 'recall_cv_mean': 0.7849, 'recall_cv_std': 0.0135, 'f1_cv_mean': 0.8238, 'f1_cv_std': 0.01, 'accuracy_test': 0.8567, 'precision_test': 0.6735, 'recall_test': 0.7756, 'f1_score_test': 0.721}
SVM: {'accuracy_cv_mean': 0.8164, 'accuracy_cv_std': 0.0186, 'precision_cv_mean': 0.7725, 'precision_cv_std': 0.0325, 'recall_cv_mean': 0.9014, 'recall_cv_std': 0.0238, 'f1_cv_mean': 0.8311, 'f1_cv_std': 0.0121, 'accuracy_test': 0.753, 'precision_test': 0.4904, 'recall_test': 0.8963, 'f1_score_test': 0.6339}


EMBEDDINGS TYPE: LYRICS_BERT
XGBoost: {'accuracy_cv_mean': 0.8771, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8931, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8745, 'f1_cv_std': 0.0054, 'accuracy_test': 0.8921, 'precision_test': 0.7345, 'recall_test': 0.8579, 'f1_score_test': 0.7915}
MLP_338433: {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.862, 'precision_cv_std': 0.0095, 'recall_cv_mean': 0.8489, 'recall_cv_std': 0.0102, 'f1_cv_mean': 0.8553, 'f1_cv_std': 0.0028, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7795, 'recall_test': 0.7736, 'f1_score_test': 0.7766}
MLP_1031169: {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.85, 'precision_cv_std': 0.0102, 'recall_cv_mean': 0.8658, 'recall_cv_std': 0.0132, 'f1_cv_mean': 0.8577, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8917, 'precision_test': 0.7758, 'recall_test': 0.7678, 'f1_score_test': 0.7718}
MLP_10401: {'accuracy_cv_mean': 0.848, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8373, 'precision_cv_std': 0.0139, 'recall_cv_mean': 0.8645, 'recall_cv_std': 0.0191, 'f1_cv_mean': 0.8504, 'f1_cv_std': 0.003, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.759, 'recall_test': 0.7539, 'f1_score_test': 0.7564}
Random Forest: {'accuracy_cv_mean': 0.855, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8866, 'precision_cv_std': 0.0042, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0127, 'f1_cv_mean': 0.8488, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8768, 'precision_test': 0.7161, 'recall_test': 0.8014, 'f1_score_test': 0.7564}
Logistic Regression: {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8604, 'precision_cv_std': 0.004, 'recall_cv_mean': 0.8326, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8462, 'f1_cv_std': 0.006, 'accuracy_test': 0.8564, 'precision_test': 0.6585, 'recall_test': 0.8273, 'f1_score_test': 0.7333}
Decision Tree: {'accuracy_cv_mean': 0.7959, 'accuracy_cv_std': 0.004, 'precision_cv_mean': 0.8116, 'precision_cv_std': 0.015, 'recall_cv_mean': 0.7717, 'recall_cv_std': 0.0236, 'f1_cv_mean': 0.7907, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8053, 'precision_test': 0.568, 'recall_test': 0.7684, 'f1_score_test': 0.6532}
Naive Bayes: {'accuracy_cv_mean': 0.7792, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.7615, 'precision_cv_std': 0.0066, 'recall_cv_mean': 0.813, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.7864, 'f1_cv_std': 0.0053, 'accuracy_test': 0.7625, 'precision_test': 0.5014, 'recall_test': 0.8014, 'f1_score_test': 0.6168}
SVM: {'accuracy_cv_mean': 0.6564, 'accuracy_cv_std': 0.0276, 'precision_cv_mean': 0.634, 'precision_cv_std': 0.0178, 'recall_cv_mean': 0.7365, 'recall_cv_std': 0.0655, 'f1_cv_mean': 0.6806, 'f1_cv_std': 0.037, 'accuracy_test': 0.5639, 'precision_test': 0.3076, 'recall_test': 0.6616, 'f1_score_test': 0.42}


EMBEDDINGS TYPE: GPT
XGBoost: {'accuracy_cv_mean': 0.9082, 'accuracy_cv_std': 0.0054, 'precision_cv_mean': 0.9218, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8921, 'recall_cv_std': 0.0101, 'f1_cv_mean': 0.9067, 'f1_cv_std': 0.0059, 'accuracy_test': 0.9192, 'precision_test': 0.7944, 'recall_test': 0.8921, 'f1_score_test': 0.8404}
MLP_2296833: {'accuracy_cv_mean': 0.8951, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8838, 'precision_cv_std': 0.0084, 'recall_cv_mean': 0.9101, 'recall_cv_std': 0.0105, 'f1_cv_mean': 0.8966, 'f1_cv_std': 0.0027, 'params': 2296833, 'accuracy_test': 0.9222, 'precision_test': 0.8482, 'recall_test': 0.8209, 'f1_score_test': 0.8344}
MLP_49953: {'accuracy_cv_mean': 0.8936, 'accuracy_cv_std': 0.0037, 'precision_cv_mean': 0.8898, 'precision_cv_std': 0.0137, 'recall_cv_mean': 0.8991, 'recall_cv_std': 0.0121, 'f1_cv_mean': 0.8942, 'f1_cv_std': 0.0029, 'params': 49953, 'accuracy_test': 0.9221, 'precision_test': 0.8585, 'recall_test': 0.8066, 'f1_score_test': 0.8317}
MLP_971265: {'accuracy_cv_mean': 0.8957, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8863, 'precision_cv_std': 0.0052, 'recall_cv_mean': 0.9079, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.897, 'f1_cv_std': 0.0031, 'params': 971265, 'accuracy_test': 0.9206, 'precision_test': 0.8424, 'recall_test': 0.8205, 'f1_score_test': 0.8313}
Logistic Regression: {'accuracy_cv_mean': 0.8997, 'accuracy_cv_std': 0.0056, 'precision_cv_mean': 0.9043, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.8939, 'recall_cv_std': 0.0086, 'f1_cv_mean': 0.8991, 'f1_cv_std': 0.0059, 'accuracy_test': 0.9064, 'precision_test': 0.7599, 'recall_test': 0.8882, 'f1_score_test': 0.8191}
Random Forest: {'accuracy_cv_mean': 0.8732, 'accuracy_cv_std': 0.005, 'precision_cv_mean': 0.9017, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8377, 'recall_cv_std': 0.0093, 'f1_cv_mean': 0.8685, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8952, 'precision_test': 0.7529, 'recall_test': 0.8345, 'f1_score_test': 0.7916}
Naive Bayes: {'accuracy_cv_mean': 0.8322, 'accuracy_cv_std': 0.0067, 'precision_cv_mean': 0.8618, 'precision_cv_std': 0.0059, 'recall_cv_mean': 0.7913, 'recall_cv_std': 0.0098, 'f1_cv_mean': 0.825, 'f1_cv_std': 0.0074, 'accuracy_test': 0.8555, 'precision_test': 0.6666, 'recall_test': 0.7891, 'f1_score_test': 0.7227}
SVM: {'accuracy_cv_mean': 0.8458, 'accuracy_cv_std': 0.0054, 'precision_cv_mean': 0.8523, 'precision_cv_std': 0.0199, 'recall_cv_mean': 0.8381, 'recall_cv_std': 0.0265, 'f1_cv_mean': 0.8446, 'f1_cv_std': 0.0066, 'accuracy_test': 0.8164, 'precision_test': 0.5776, 'recall_test': 0.8579, 'f1_score_test': 0.6904}
Decision Tree: {'accuracy_cv_mean': 0.8089, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8186, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.7936, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.8059, 'f1_cv_std': 0.0036, 'accuracy_test': 0.8223, 'precision_test': 0.5985, 'recall_test': 0.776, 'f1_score_test': 0.6758}
Diccionario global guardado en: outputs_cv/1/gpt/global_metrics.json

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
====================================

{'accuracy_cv_mean': 0.8997, 'accuracy_cv_std': 0.0056, 'precision_cv_mean': 0.9043, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.8939, 'recall_cv_std': 0.0086, 'f1_cv_mean': 0.8991, 'f1_cv_std': 0.0059}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 43, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.91      0.94     16465
           1       0.76      0.89      0.82      5160

    accuracy                           0.91     21625
   macro avg       0.86      0.90      0.88     21625
weighted avg       0.91      0.91      0.91     21625

Confusion matrix Test saved as: outputs_cv/1/gpt/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/1/gpt/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8458, 'accuracy_cv_std': 0.0054, 'precision_cv_mean': 0.8523, 'precision_cv_std': 0.0199, 'recall_cv_mean': 0.8381, 'recall_cv_std': 0.0265, 'f1_cv_mean': 0.8446, 'f1_cv_std': 0.0066}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 43, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.80      0.87     16465
           1       0.58      0.86      0.69      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.83      0.78     21625
weighted avg       0.86      0.82      0.83     21625

Confusion matrix Test saved as: outputs_cv/1/gpt/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/1/gpt/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8089, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8186, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.7936, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.8059, 'f1_cv_std': 0.0036}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 43, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.84      0.88     16465
           1       0.60      0.78      0.68      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.81      0.78     21625
weighted avg       0.85      0.82      0.83     21625

Confusion matrix Test saved as: outputs_cv/1/gpt/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/1/gpt/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8732, 'accuracy_cv_std': 0.005, 'precision_cv_mean': 0.9017, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8377, 'recall_cv_std': 0.0093, 'f1_cv_mean': 0.8685, 'f1_cv_std': 0.0058}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 43, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.91      0.93     16465
           1       0.75      0.83      0.79      5160

    accuracy                           0.90     21625
   macro avg       0.85      0.87      0.86     21625
weighted avg       0.90      0.90      0.90     21625

Confusion matrix Test saved as: outputs_cv/1/gpt/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/1/gpt/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9082, 'accuracy_cv_std': 0.0054, 'precision_cv_mean': 0.9218, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8921, 'recall_cv_std': 0.0101, 'f1_cv_mean': 0.9067, 'f1_cv_std': 0.0059}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 43, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.79      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/1/gpt/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/1/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8322, 'accuracy_cv_std': 0.0067, 'precision_cv_mean': 0.8618, 'precision_cv_std': 0.0059, 'recall_cv_mean': 0.7913, 'recall_cv_std': 0.0098, 'f1_cv_mean': 0.825, 'f1_cv_std': 0.0074}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.79      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/1/gpt/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/1/gpt/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.9082, 'accuracy_cv_std': 0.0054, 'precision_cv_mean': 0.9218, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8921, 'recall_cv_std': 0.0101, 'f1_cv_mean': 0.9067, 'f1_cv_std': 0.0059, 'accuracy_test': 0.9192, 'precision_test': 0.7944, 'recall_test': 0.8921, 'f1_score_test': 0.8404}
Logistic Regression: {'accuracy_cv_mean': 0.8997, 'accuracy_cv_std': 0.0056, 'precision_cv_mean': 0.9043, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.8939, 'recall_cv_std': 0.0086, 'f1_cv_mean': 0.8991, 'f1_cv_std': 0.0059, 'accuracy_test': 0.9064, 'precision_test': 0.7599, 'recall_test': 0.8882, 'f1_score_test': 0.8191}
Random Forest: {'accuracy_cv_mean': 0.8732, 'accuracy_cv_std': 0.005, 'precision_cv_mean': 0.9017, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8377, 'recall_cv_std': 0.0093, 'f1_cv_mean': 0.8685, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8952, 'precision_test': 0.7529, 'recall_test': 0.8345, 'f1_score_test': 0.7916}
Naive Bayes: {'accuracy_cv_mean': 0.8322, 'accuracy_cv_std': 0.0067, 'precision_cv_mean': 0.8618, 'precision_cv_std': 0.0059, 'recall_cv_mean': 0.7913, 'recall_cv_std': 0.0098, 'f1_cv_mean': 0.825, 'f1_cv_std': 0.0074, 'accuracy_test': 0.8555, 'precision_test': 0.6666, 'recall_test': 0.7891, 'f1_score_test': 0.7227}
SVM: {'accuracy_cv_mean': 0.8458, 'accuracy_cv_std': 0.0054, 'precision_cv_mean': 0.8523, 'precision_cv_std': 0.0199, 'recall_cv_mean': 0.8381, 'recall_cv_std': 0.0265, 'f1_cv_mean': 0.8446, 'f1_cv_std': 0.0066, 'accuracy_test': 0.8164, 'precision_test': 0.5776, 'recall_test': 0.8579, 'f1_score_test': 0.6904}
Decision Tree: {'accuracy_cv_mean': 0.8089, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8186, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.7936, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.8059, 'f1_cv_std': 0.0036, 'accuracy_test': 0.8223, 'precision_test': 0.5985, 'recall_test': 0.776, 'f1_score_test': 0.6758}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9344, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0009, 'recall_cv_mean': 0.9336, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9344, 'f1_cv_std': 0.0017, 'params': 160801, 'accuracy_test': 0.9372, 'precision_test': 0.8404, 'recall_test': 0.9095, 'f1_score_test': 0.8736}, 'MLP_2744833': {'accuracy_cv_mean': 0.9454, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9447, 'precision_cv_std': 0.0058, 'recall_cv_mean': 0.9464, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.9455, 'f1_cv_std': 0.0015, 'params': 2744833, 'accuracy_test': 0.9557, 'precision_test': 0.8901, 'recall_test': 0.9289, 'f1_score_test': 0.9091}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0013, 'precision_cv_mean': 0.9499, 'precision_cv_std': 0.0027, 'recall_cv_mean': 0.9422, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.946, 'f1_cv_std': 0.0013, 'params': 5843969, 'accuracy_test': 0.9551, 'precision_test': 0.8925, 'recall_test': 0.9233, 'f1_score_test': 0.9076}, 'Logistic Regression': {'accuracy_cv_mean': 0.9278, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9348, 'precision_cv_std': 0.0019, 'recall_cv_mean': 0.9198, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.9272, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9337, 'precision_test': 0.8257, 'recall_test': 0.9155, 'f1_score_test': 0.8683}, 'SVM': {'accuracy_cv_mean': 0.8164, 'accuracy_cv_std': 0.0186, 'precision_cv_mean': 0.7725, 'precision_cv_std': 0.0325, 'recall_cv_mean': 0.9014, 'recall_cv_std': 0.0238, 'f1_cv_mean': 0.8311, 'f1_cv_std': 0.0121, 'accuracy_test': 0.753, 'precision_test': 0.4904, 'recall_test': 0.8963, 'f1_score_test': 0.6339}, 'Decision Tree': {'accuracy_cv_mean': 0.829, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7388, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.812, 'f1_cv_std': 0.0056, 'accuracy_test': 0.875, 'precision_test': 0.7471, 'recall_test': 0.7196, 'f1_score_test': 0.7331}, 'Random Forest': {'accuracy_cv_mean': 0.8321, 'accuracy_cv_std': 0.009, 'precision_cv_mean': 0.8668, 'precision_cv_std': 0.0097, 'recall_cv_mean': 0.7849, 'recall_cv_std': 0.0135, 'f1_cv_mean': 0.8238, 'f1_cv_std': 0.01, 'accuracy_test': 0.8567, 'precision_test': 0.6735, 'recall_test': 0.7756, 'f1_score_test': 0.721}, 'XGBoost': {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.9273, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.885, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.9057, 'f1_cv_std': 0.0039, 'accuracy_test': 0.9204, 'precision_test': 0.8022, 'recall_test': 0.8843, 'f1_score_test': 0.8413}, 'Naive Bayes': {'accuracy_cv_mean': 0.9033, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.9277, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0066, 'f1_cv_mean': 0.9004, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9193, 'precision_test': 0.8091, 'recall_test': 0.8659, 'f1_score_test': 0.8365}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.848, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8373, 'precision_cv_std': 0.0139, 'recall_cv_mean': 0.8645, 'recall_cv_std': 0.0191, 'f1_cv_mean': 0.8504, 'f1_cv_std': 0.003, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.759, 'recall_test': 0.7539, 'f1_score_test': 0.7564}, 'MLP_338433': {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.862, 'precision_cv_std': 0.0095, 'recall_cv_mean': 0.8489, 'recall_cv_std': 0.0102, 'f1_cv_mean': 0.8553, 'f1_cv_std': 0.0028, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7795, 'recall_test': 0.7736, 'f1_score_test': 0.7766}, 'MLP_1031169': {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.85, 'precision_cv_std': 0.0102, 'recall_cv_mean': 0.8658, 'recall_cv_std': 0.0132, 'f1_cv_mean': 0.8577, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8917, 'precision_test': 0.7758, 'recall_test': 0.7678, 'f1_score_test': 0.7718}, 'Logistic Regression': {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8604, 'precision_cv_std': 0.004, 'recall_cv_mean': 0.8326, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8462, 'f1_cv_std': 0.006, 'accuracy_test': 0.8564, 'precision_test': 0.6585, 'recall_test': 0.8273, 'f1_score_test': 0.7333}, 'SVM': {'accuracy_cv_mean': 0.6564, 'accuracy_cv_std': 0.0276, 'precision_cv_mean': 0.634, 'precision_cv_std': 0.0178, 'recall_cv_mean': 0.7365, 'recall_cv_std': 0.0655, 'f1_cv_mean': 0.6806, 'f1_cv_std': 0.037, 'accuracy_test': 0.5639, 'precision_test': 0.3076, 'recall_test': 0.6616, 'f1_score_test': 0.42}, 'Decision Tree': {'accuracy_cv_mean': 0.7959, 'accuracy_cv_std': 0.004, 'precision_cv_mean': 0.8116, 'precision_cv_std': 0.015, 'recall_cv_mean': 0.7717, 'recall_cv_std': 0.0236, 'f1_cv_mean': 0.7907, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8053, 'precision_test': 0.568, 'recall_test': 0.7684, 'f1_score_test': 0.6532}, 'Random Forest': {'accuracy_cv_mean': 0.855, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8866, 'precision_cv_std': 0.0042, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0127, 'f1_cv_mean': 0.8488, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8768, 'precision_test': 0.7161, 'recall_test': 0.8014, 'f1_score_test': 0.7564}, 'XGBoost': {'accuracy_cv_mean': 0.8771, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8931, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8745, 'f1_cv_std': 0.0054, 'accuracy_test': 0.8921, 'precision_test': 0.7345, 'recall_test': 0.8579, 'f1_score_test': 0.7915}, 'Naive Bayes': {'accuracy_cv_mean': 0.7792, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.7615, 'precision_cv_std': 0.0066, 'recall_cv_mean': 0.813, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.7864, 'f1_cv_std': 0.0053, 'accuracy_test': 0.7625, 'precision_test': 0.5014, 'recall_test': 0.8014, 'f1_score_test': 0.6168}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8936, 'accuracy_cv_std': 0.0037, 'precision_cv_mean': 0.8898, 'precision_cv_std': 0.0137, 'recall_cv_mean': 0.8991, 'recall_cv_std': 0.0121, 'f1_cv_mean': 0.8942, 'f1_cv_std': 0.0029, 'params': 49953, 'accuracy_test': 0.9221, 'precision_test': 0.8585, 'recall_test': 0.8066, 'f1_score_test': 0.8317}, 'MLP_971265': {'accuracy_cv_mean': 0.8957, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8863, 'precision_cv_std': 0.0052, 'recall_cv_mean': 0.9079, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.897, 'f1_cv_std': 0.0031, 'params': 971265, 'accuracy_test': 0.9206, 'precision_test': 0.8424, 'recall_test': 0.8205, 'f1_score_test': 0.8313}, 'MLP_2296833': {'accuracy_cv_mean': 0.8951, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8838, 'precision_cv_std': 0.0084, 'recall_cv_mean': 0.9101, 'recall_cv_std': 0.0105, 'f1_cv_mean': 0.8966, 'f1_cv_std': 0.0027, 'params': 2296833, 'accuracy_test': 0.9222, 'precision_test': 0.8482, 'recall_test': 0.8209, 'f1_score_test': 0.8344}, 'Logistic Regression': {'accuracy_cv_mean': 0.8997, 'accuracy_cv_std': 0.0056, 'precision_cv_mean': 0.9043, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.8939, 'recall_cv_std': 0.0086, 'f1_cv_mean': 0.8991, 'f1_cv_std': 0.0059, 'accuracy_test': 0.9064, 'precision_test': 0.7599, 'recall_test': 0.8882, 'f1_score_test': 0.8191}, 'SVM': {'accuracy_cv_mean': 0.8458, 'accuracy_cv_std': 0.0054, 'precision_cv_mean': 0.8523, 'precision_cv_std': 0.0199, 'recall_cv_mean': 0.8381, 'recall_cv_std': 0.0265, 'f1_cv_mean': 0.8446, 'f1_cv_std': 0.0066, 'accuracy_test': 0.8164, 'precision_test': 0.5776, 'recall_test': 0.8579, 'f1_score_test': 0.6904}, 'Decision Tree': {'accuracy_cv_mean': 0.8089, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8186, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.7936, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.8059, 'f1_cv_std': 0.0036, 'accuracy_test': 0.8223, 'precision_test': 0.5985, 'recall_test': 0.776, 'f1_score_test': 0.6758}, 'Random Forest': {'accuracy_cv_mean': 0.8732, 'accuracy_cv_std': 0.005, 'precision_cv_mean': 0.9017, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8377, 'recall_cv_std': 0.0093, 'f1_cv_mean': 0.8685, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8952, 'precision_test': 0.7529, 'recall_test': 0.8345, 'f1_score_test': 0.7916}, 'XGBoost': {'accuracy_cv_mean': 0.9082, 'accuracy_cv_std': 0.0054, 'precision_cv_mean': 0.9218, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8921, 'recall_cv_std': 0.0101, 'f1_cv_mean': 0.9067, 'f1_cv_std': 0.0059, 'accuracy_test': 0.9192, 'precision_test': 0.7944, 'recall_test': 0.8921, 'f1_score_test': 0.8404}, 'Naive Bayes': {'accuracy_cv_mean': 0.8322, 'accuracy_cv_std': 0.0067, 'precision_cv_mean': 0.8618, 'precision_cv_std': 0.0059, 'recall_cv_mean': 0.7913, 'recall_cv_std': 0.0098, 'f1_cv_mean': 0.825, 'f1_cv_std': 0.0074, 'accuracy_test': 0.8555, 'precision_test': 0.6666, 'recall_test': 0.7891, 'f1_score_test': 0.7227}}}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
MLP_2744833: {'accuracy_cv_mean': 0.9454, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9447, 'precision_cv_std': 0.0058, 'recall_cv_mean': 0.9464, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.9455, 'f1_cv_std': 0.0015, 'params': 2744833, 'accuracy_test': 0.9557, 'precision_test': 0.8901, 'recall_test': 0.9289, 'f1_score_test': 0.9091}
MLP_5843969: {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0013, 'precision_cv_mean': 0.9499, 'precision_cv_std': 0.0027, 'recall_cv_mean': 0.9422, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.946, 'f1_cv_std': 0.0013, 'params': 5843969, 'accuracy_test': 0.9551, 'precision_test': 0.8925, 'recall_test': 0.9233, 'f1_score_test': 0.9076}
MLP_160801: {'accuracy_cv_mean': 0.9344, 'accuracy_cv_std': 0.0015, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0009, 'recall_cv_mean': 0.9336, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9344, 'f1_cv_std': 0.0017, 'params': 160801, 'accuracy_test': 0.9372, 'precision_test': 0.8404, 'recall_test': 0.9095, 'f1_score_test': 0.8736}
Logistic Regression: {'accuracy_cv_mean': 0.9278, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9348, 'precision_cv_std': 0.0019, 'recall_cv_mean': 0.9198, 'recall_cv_std': 0.0062, 'f1_cv_mean': 0.9272, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9337, 'precision_test': 0.8257, 'recall_test': 0.9155, 'f1_score_test': 0.8683}
XGBoost: {'accuracy_cv_mean': 0.9078, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.9273, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.885, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.9057, 'f1_cv_std': 0.0039, 'accuracy_test': 0.9204, 'precision_test': 0.8022, 'recall_test': 0.8843, 'f1_score_test': 0.8413}
Naive Bayes: {'accuracy_cv_mean': 0.9033, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.9277, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0066, 'f1_cv_mean': 0.9004, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9193, 'precision_test': 0.8091, 'recall_test': 0.8659, 'f1_score_test': 0.8365}
Decision Tree: {'accuracy_cv_mean': 0.829, 'accuracy_cv_std': 0.0046, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0076, 'recall_cv_mean': 0.7388, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.812, 'f1_cv_std': 0.0056, 'accuracy_test': 0.875, 'precision_test': 0.7471, 'recall_test': 0.7196, 'f1_score_test': 0.7331}
Random Forest: {'accuracy_cv_mean': 0.8321, 'accuracy_cv_std': 0.009, 'precision_cv_mean': 0.8668, 'precision_cv_std': 0.0097, 'recall_cv_mean': 0.7849, 'recall_cv_std': 0.0135, 'f1_cv_mean': 0.8238, 'f1_cv_std': 0.01, 'accuracy_test': 0.8567, 'precision_test': 0.6735, 'recall_test': 0.7756, 'f1_score_test': 0.721}
SVM: {'accuracy_cv_mean': 0.8164, 'accuracy_cv_std': 0.0186, 'precision_cv_mean': 0.7725, 'precision_cv_std': 0.0325, 'recall_cv_mean': 0.9014, 'recall_cv_std': 0.0238, 'f1_cv_mean': 0.8311, 'f1_cv_std': 0.0121, 'accuracy_test': 0.753, 'precision_test': 0.4904, 'recall_test': 0.8963, 'f1_score_test': 0.6339}


EMBEDDINGS TYPE: LYRICS_BERT
XGBoost: {'accuracy_cv_mean': 0.8771, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8931, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8568, 'recall_cv_std': 0.0112, 'f1_cv_mean': 0.8745, 'f1_cv_std': 0.0054, 'accuracy_test': 0.8921, 'precision_test': 0.7345, 'recall_test': 0.8579, 'f1_score_test': 0.7915}
MLP_338433: {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.862, 'precision_cv_std': 0.0095, 'recall_cv_mean': 0.8489, 'recall_cv_std': 0.0102, 'f1_cv_mean': 0.8553, 'f1_cv_std': 0.0028, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7795, 'recall_test': 0.7736, 'f1_score_test': 0.7766}
MLP_1031169: {'accuracy_cv_mean': 0.8564, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.85, 'precision_cv_std': 0.0102, 'recall_cv_mean': 0.8658, 'recall_cv_std': 0.0132, 'f1_cv_mean': 0.8577, 'f1_cv_std': 0.002, 'params': 1031169, 'accuracy_test': 0.8917, 'precision_test': 0.7758, 'recall_test': 0.7678, 'f1_score_test': 0.7718}
MLP_10401: {'accuracy_cv_mean': 0.848, 'accuracy_cv_std': 0.0022, 'precision_cv_mean': 0.8373, 'precision_cv_std': 0.0139, 'recall_cv_mean': 0.8645, 'recall_cv_std': 0.0191, 'f1_cv_mean': 0.8504, 'f1_cv_std': 0.003, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.759, 'recall_test': 0.7539, 'f1_score_test': 0.7564}
Random Forest: {'accuracy_cv_mean': 0.855, 'accuracy_cv_std': 0.0044, 'precision_cv_mean': 0.8866, 'precision_cv_std': 0.0042, 'recall_cv_mean': 0.8143, 'recall_cv_std': 0.0127, 'f1_cv_mean': 0.8488, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8768, 'precision_test': 0.7161, 'recall_test': 0.8014, 'f1_score_test': 0.7564}
Logistic Regression: {'accuracy_cv_mean': 0.8488, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.8604, 'precision_cv_std': 0.004, 'recall_cv_mean': 0.8326, 'recall_cv_std': 0.0103, 'f1_cv_mean': 0.8462, 'f1_cv_std': 0.006, 'accuracy_test': 0.8564, 'precision_test': 0.6585, 'recall_test': 0.8273, 'f1_score_test': 0.7333}
Decision Tree: {'accuracy_cv_mean': 0.7959, 'accuracy_cv_std': 0.004, 'precision_cv_mean': 0.8116, 'precision_cv_std': 0.015, 'recall_cv_mean': 0.7717, 'recall_cv_std': 0.0236, 'f1_cv_mean': 0.7907, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8053, 'precision_test': 0.568, 'recall_test': 0.7684, 'f1_score_test': 0.6532}
Naive Bayes: {'accuracy_cv_mean': 0.7792, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.7615, 'precision_cv_std': 0.0066, 'recall_cv_mean': 0.813, 'recall_cv_std': 0.0094, 'f1_cv_mean': 0.7864, 'f1_cv_std': 0.0053, 'accuracy_test': 0.7625, 'precision_test': 0.5014, 'recall_test': 0.8014, 'f1_score_test': 0.6168}
SVM: {'accuracy_cv_mean': 0.6564, 'accuracy_cv_std': 0.0276, 'precision_cv_mean': 0.634, 'precision_cv_std': 0.0178, 'recall_cv_mean': 0.7365, 'recall_cv_std': 0.0655, 'f1_cv_mean': 0.6806, 'f1_cv_std': 0.037, 'accuracy_test': 0.5639, 'precision_test': 0.3076, 'recall_test': 0.6616, 'f1_score_test': 0.42}


EMBEDDINGS TYPE: GPT
XGBoost: {'accuracy_cv_mean': 0.9082, 'accuracy_cv_std': 0.0054, 'precision_cv_mean': 0.9218, 'precision_cv_std': 0.0026, 'recall_cv_mean': 0.8921, 'recall_cv_std': 0.0101, 'f1_cv_mean': 0.9067, 'f1_cv_std': 0.0059, 'accuracy_test': 0.9192, 'precision_test': 0.7944, 'recall_test': 0.8921, 'f1_score_test': 0.8404}
MLP_2296833: {'accuracy_cv_mean': 0.8951, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8838, 'precision_cv_std': 0.0084, 'recall_cv_mean': 0.9101, 'recall_cv_std': 0.0105, 'f1_cv_mean': 0.8966, 'f1_cv_std': 0.0027, 'params': 2296833, 'accuracy_test': 0.9222, 'precision_test': 0.8482, 'recall_test': 0.8209, 'f1_score_test': 0.8344}
MLP_49953: {'accuracy_cv_mean': 0.8936, 'accuracy_cv_std': 0.0037, 'precision_cv_mean': 0.8898, 'precision_cv_std': 0.0137, 'recall_cv_mean': 0.8991, 'recall_cv_std': 0.0121, 'f1_cv_mean': 0.8942, 'f1_cv_std': 0.0029, 'params': 49953, 'accuracy_test': 0.9221, 'precision_test': 0.8585, 'recall_test': 0.8066, 'f1_score_test': 0.8317}
MLP_971265: {'accuracy_cv_mean': 0.8957, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8863, 'precision_cv_std': 0.0052, 'recall_cv_mean': 0.9079, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.897, 'f1_cv_std': 0.0031, 'params': 971265, 'accuracy_test': 0.9206, 'precision_test': 0.8424, 'recall_test': 0.8205, 'f1_score_test': 0.8313}
Logistic Regression: {'accuracy_cv_mean': 0.8997, 'accuracy_cv_std': 0.0056, 'precision_cv_mean': 0.9043, 'precision_cv_std': 0.0054, 'recall_cv_mean': 0.8939, 'recall_cv_std': 0.0086, 'f1_cv_mean': 0.8991, 'f1_cv_std': 0.0059, 'accuracy_test': 0.9064, 'precision_test': 0.7599, 'recall_test': 0.8882, 'f1_score_test': 0.8191}
Random Forest: {'accuracy_cv_mean': 0.8732, 'accuracy_cv_std': 0.005, 'precision_cv_mean': 0.9017, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8377, 'recall_cv_std': 0.0093, 'f1_cv_mean': 0.8685, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8952, 'precision_test': 0.7529, 'recall_test': 0.8345, 'f1_score_test': 0.7916}
Naive Bayes: {'accuracy_cv_mean': 0.8322, 'accuracy_cv_std': 0.0067, 'precision_cv_mean': 0.8618, 'precision_cv_std': 0.0059, 'recall_cv_mean': 0.7913, 'recall_cv_std': 0.0098, 'f1_cv_mean': 0.825, 'f1_cv_std': 0.0074, 'accuracy_test': 0.8555, 'precision_test': 0.6666, 'recall_test': 0.7891, 'f1_score_test': 0.7227}
SVM: {'accuracy_cv_mean': 0.8458, 'accuracy_cv_std': 0.0054, 'precision_cv_mean': 0.8523, 'precision_cv_std': 0.0199, 'recall_cv_mean': 0.8381, 'recall_cv_std': 0.0265, 'f1_cv_mean': 0.8446, 'f1_cv_std': 0.0066, 'accuracy_test': 0.8164, 'precision_test': 0.5776, 'recall_test': 0.8579, 'f1_score_test': 0.6904}
Decision Tree: {'accuracy_cv_mean': 0.8089, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8186, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.7936, 'recall_cv_std': 0.0057, 'f1_cv_mean': 0.8059, 'f1_cv_std': 0.0036, 'accuracy_test': 0.8223, 'precision_test': 0.5985, 'recall_test': 0.776, 'f1_score_test': 0.6758}
Diccionario global guardado en: outputs_cv/1/gpt/global_metrics.json

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
====================================

