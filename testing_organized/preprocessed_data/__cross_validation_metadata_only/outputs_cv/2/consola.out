2025-10-23 01:43:48.327860: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-23 01:43:48.327860: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-23 01:43:48.379194: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-23 01:43:48.379194: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-23 01:43:50.752811: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-23 01:43:50.752811: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_1.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_1.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 15 ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']

CAT_LOW 3 ['emotion', 'Key', 'Time signature']
CAT_HIGH 12 ['Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
After removing some columns that ignorated, for TF-IDF embbedings you are selecteing this columns:
--> ['text']
After removing some columns that ignorated, for numeric cols you are selecteing this columns:
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
--> PaTH:  ../../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy
Running experiment with TFIDF embeddings
Contaning the categorical cols
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 5023)
Shape of X_test after concatenation:  (21625, 5023)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [5023, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3173, Test Loss: 0.1981, F1: 0.9221, AUC: 0.9775
Epoch [10/30] Train Loss: 0.0755, Test Loss: 0.2109, F1: 0.9286, AUC: 0.9813
Epoch [20/30] Train Loss: 0.0395, Test Loss: 0.2962, F1: 0.9290, AUC: 0.9788
Mejores resultados en la época:  3
f1-score 0.9338474515001808
AUC según el mejor F1-score 0.9830943363717626

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3234, Test Loss: 0.2018, F1: 0.9202, AUC: 0.9770
Epoch [10/30] Train Loss: 0.0815, Test Loss: 0.2027, F1: 0.9314, AUC: 0.9817
Epoch [20/30] Train Loss: 0.0501, Test Loss: 0.2812, F1: 0.9276, AUC: 0.9794
Mejores resultados en la época:  3
f1-score 0.9346673095467696
AUC según el mejor F1-score 0.9838491315688661

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3231, Test Loss: 0.1984, F1: 0.9223, AUC: 0.9773
Epoch [10/30] Train Loss: 0.0836, Test Loss: 0.2164, F1: 0.9260, AUC: 0.9808
Epoch [20/30] Train Loss: 0.0681, Test Loss: 0.2787, F1: 0.9235, AUC: 0.9786
Mejores resultados en la época:  2
f1-score 0.9343768813967489
AUC según el mejor F1-score 0.982804436801424

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3215, Test Loss: 0.2003, F1: 0.9208, AUC: 0.9766
Epoch [10/30] Train Loss: 0.0787, Test Loss: 0.2132, F1: 0.9301, AUC: 0.9812
Epoch [20/30] Train Loss: 0.0498, Test Loss: 0.2788, F1: 0.9243, AUC: 0.9789
Mejores resultados en la época:  4
f1-score 0.9335753176043557
AUC según el mejor F1-score 0.9827491439433641

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3216, Test Loss: 0.2065, F1: 0.9183, AUC: 0.9754
Epoch [10/30] Train Loss: 0.0779, Test Loss: 0.2268, F1: 0.9247, AUC: 0.9796
Epoch [20/30] Train Loss: 0.0412, Test Loss: 0.3068, F1: 0.9256, AUC: 0.9777
Mejores resultados en la época:  2
f1-score 0.9297035692679976
AUC según el mejor F1-score 0.9810908570521599
Epoch [0/30] Train Loss: 0.3020, Test Loss: 0.1935, F1: 0.8586, AUC: 0.9792
Epoch [10/30] Train Loss: 0.0793, Test Loss: 0.1933, F1: 0.8664, AUC: 0.9837
Epoch [20/30] Train Loss: 0.0290, Test Loss: 0.2469, F1: 0.8688, AUC: 0.9834
Mejores resultados en la época:  5
f1-score 0.88333176647551
AUC según el mejor F1-score 0.983883466691149
Confusion matrix Test saved: outputs_cv/2/tfidf/cm_mlp_1.png

========================================
Entrenando red 5 con capas [5023, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2356, Test Loss: 0.1832, F1: 0.9273, AUC: 0.9812
Epoch [10/30] Train Loss: 0.0029, Test Loss: 0.3685, F1: 0.9380, AUC: 0.9845
Epoch [20/30] Train Loss: 0.0026, Test Loss: 0.3942, F1: 0.9391, AUC: 0.9852
Mejores resultados en la época:  18
f1-score 0.9419556840077071
AUC según el mejor F1-score 0.9846095310997687

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2374, Test Loss: 0.1757, F1: 0.9321, AUC: 0.9823
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.3728, F1: 0.9432, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5317, F1: 0.9465, AUC: 0.9866
Mejores resultados en la época:  29
f1-score 0.9467982667308619
AUC según el mejor F1-score 0.9855706890173368

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2382, Test Loss: 0.1810, F1: 0.9275, AUC: 0.9802
Epoch [10/30] Train Loss: 0.0028, Test Loss: 0.5413, F1: 0.9409, AUC: 0.9833
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6226, F1: 0.9426, AUC: 0.9835
Mejores resultados en la época:  17
f1-score 0.9439048081593007
AUC según el mejor F1-score 0.9845126729538489

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2354, Test Loss: 0.1766, F1: 0.9289, AUC: 0.9811
Epoch [10/30] Train Loss: 0.0039, Test Loss: 0.3042, F1: 0.9410, AUC: 0.9861
Epoch [20/30] Train Loss: 0.0007, Test Loss: 0.3933, F1: 0.9417, AUC: 0.9866
Mejores resultados en la época:  17
f1-score 0.94417593040116
AUC según el mejor F1-score 0.9856801870082254

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2346, Test Loss: 0.1901, F1: 0.9277, AUC: 0.9789
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.4082, F1: 0.9342, AUC: 0.9838
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6575, F1: 0.9420, AUC: 0.9815
Mejores resultados en la época:  15
f1-score 0.9429538312787044
AUC según el mejor F1-score 0.9857841417738733
Epoch [0/30] Train Loss: 0.2254, Test Loss: 0.1483, F1: 0.8812, AUC: 0.9828
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 15 ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']

CAT_LOW 3 ['emotion', 'Key', 'Time signature']
CAT_HIGH 12 ['Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
After removing some columns that ignorated, for TF-IDF embbedings you are selecteing this columns:
--> ['text']
After removing some columns that ignorated, for numeric cols you are selecteing this columns:
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
--> PaTH:  ../../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy
Running experiment with TFIDF embeddings
Contaning the categorical cols
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 5023)
Shape of X_test after concatenation:  (21625, 5023)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [5023, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3173, Test Loss: 0.1981, F1: 0.9221, AUC: 0.9775
Epoch [10/30] Train Loss: 0.0755, Test Loss: 0.2109, F1: 0.9286, AUC: 0.9813
Epoch [20/30] Train Loss: 0.0395, Test Loss: 0.2962, F1: 0.9290, AUC: 0.9788
Mejores resultados en la época:  3
f1-score 0.9338474515001808
AUC según el mejor F1-score 0.9830943363717626

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3234, Test Loss: 0.2018, F1: 0.9202, AUC: 0.9770
Epoch [10/30] Train Loss: 0.0815, Test Loss: 0.2027, F1: 0.9314, AUC: 0.9817
Epoch [20/30] Train Loss: 0.0501, Test Loss: 0.2812, F1: 0.9276, AUC: 0.9794
Mejores resultados en la época:  3
f1-score 0.9346673095467696
AUC según el mejor F1-score 0.9838491315688661

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3231, Test Loss: 0.1984, F1: 0.9223, AUC: 0.9773
Epoch [10/30] Train Loss: 0.0836, Test Loss: 0.2164, F1: 0.9260, AUC: 0.9808
Epoch [20/30] Train Loss: 0.0681, Test Loss: 0.2787, F1: 0.9235, AUC: 0.9786
Mejores resultados en la época:  2
f1-score 0.9343768813967489
AUC según el mejor F1-score 0.982804436801424

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3215, Test Loss: 0.2003, F1: 0.9208, AUC: 0.9766
Epoch [10/30] Train Loss: 0.0787, Test Loss: 0.2132, F1: 0.9301, AUC: 0.9812
Epoch [20/30] Train Loss: 0.0498, Test Loss: 0.2788, F1: 0.9243, AUC: 0.9789
Mejores resultados en la época:  4
f1-score 0.9335753176043557
AUC según el mejor F1-score 0.9827491439433641

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3216, Test Loss: 0.2065, F1: 0.9183, AUC: 0.9754
Epoch [10/30] Train Loss: 0.0779, Test Loss: 0.2268, F1: 0.9247, AUC: 0.9796
Epoch [20/30] Train Loss: 0.0412, Test Loss: 0.3068, F1: 0.9256, AUC: 0.9777
Mejores resultados en la época:  2
f1-score 0.9297035692679976
AUC según el mejor F1-score 0.9810908570521599
Epoch [0/30] Train Loss: 0.3020, Test Loss: 0.1935, F1: 0.8586, AUC: 0.9792
Epoch [10/30] Train Loss: 0.0793, Test Loss: 0.1933, F1: 0.8664, AUC: 0.9837
Epoch [20/30] Train Loss: 0.0290, Test Loss: 0.2469, F1: 0.8688, AUC: 0.9834
Mejores resultados en la época:  5
f1-score 0.88333176647551
AUC según el mejor F1-score 0.983883466691149
Confusion matrix Test saved: outputs_cv/2/tfidf/cm_mlp_1.png

========================================
Entrenando red 5 con capas [5023, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2356, Test Loss: 0.1832, F1: 0.9273, AUC: 0.9812
Epoch [10/30] Train Loss: 0.0029, Test Loss: 0.3685, F1: 0.9380, AUC: 0.9845
Epoch [20/30] Train Loss: 0.0026, Test Loss: 0.3942, F1: 0.9391, AUC: 0.9852
Mejores resultados en la época:  18
f1-score 0.9419556840077071
AUC según el mejor F1-score 0.9846095310997687

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2374, Test Loss: 0.1757, F1: 0.9321, AUC: 0.9823
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.3728, F1: 0.9432, AUC: 0.9859
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5317, F1: 0.9465, AUC: 0.9866
Mejores resultados en la época:  29
f1-score 0.9467982667308619
AUC según el mejor F1-score 0.9855706890173368

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2382, Test Loss: 0.1810, F1: 0.9275, AUC: 0.9802
Epoch [10/30] Train Loss: 0.0028, Test Loss: 0.5413, F1: 0.9409, AUC: 0.9833
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6226, F1: 0.9426, AUC: 0.9835
Mejores resultados en la época:  17
f1-score 0.9439048081593007
AUC según el mejor F1-score 0.9845126729538489

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2354, Test Loss: 0.1766, F1: 0.9289, AUC: 0.9811
Epoch [10/30] Train Loss: 0.0039, Test Loss: 0.3042, F1: 0.9410, AUC: 0.9861
Epoch [20/30] Train Loss: 0.0007, Test Loss: 0.3933, F1: 0.9417, AUC: 0.9866
Mejores resultados en la época:  17
f1-score 0.94417593040116
AUC según el mejor F1-score 0.9856801870082254

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2346, Test Loss: 0.1901, F1: 0.9277, AUC: 0.9789
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.4082, F1: 0.9342, AUC: 0.9838
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6575, F1: 0.9420, AUC: 0.9815
Mejores resultados en la época:  15
f1-score 0.9429538312787044
AUC según el mejor F1-score 0.9857841417738733
Epoch [0/30] Train Loss: 0.2254, Test Loss: 0.1483, F1: 0.8812, AUC: 0.9828
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.3124, F1: 0.8994, AUC: 0.9878
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5492, F1: 0.8963, AUC: 0.9870
Mejores resultados en la época:  8
f1-score 0.9105799447671651
AUC según el mejor F1-score 0.9879028335887494
Confusion matrix Test saved: outputs_cv/2/tfidf/cm_mlp_5.png

========================================
Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2474, Test Loss: 0.1911, F1: 0.9274, AUC: 0.9810
Epoch [10/30] Train Loss: 0.0055, Test Loss: 0.3711, F1: 0.9380, AUC: 0.9849
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7046, F1: 0.9447, AUC: 0.9837
Mejores resultados en la época:  9
f1-score 0.9457215067998556
AUC según el mejor F1-score 0.9860560360611592

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2368, Test Loss: 0.1723, F1: 0.9301, AUC: 0.9823
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.5108, F1: 0.9428, AUC: 0.9840
Epoch [20/30] Train Loss: 0.0016, Test Loss: 0.6102, F1: 0.9451, AUC: 0.9820
Mejores resultados en la época:  26
f1-score 0.9490043710539097
AUC según el mejor F1-score 0.9834065359090499

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2435, Test Loss: 0.1812, F1: 0.9252, AUC: 0.9809
Epoch [10/30] Train Loss: 0.0026, Test Loss: 0.4961, F1: 0.9432, AUC: 0.9830
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8445, F1: 0.9446, AUC: 0.9806
Mejores resultados en la época:  23
f1-score 0.9449142304904566
AUC según el mejor F1-score 0.979334180497341

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2423, Test Loss: 0.1706, F1: 0.9300, AUC: 0.9823
Epoch [10/30] Train Loss: 0.0043, Test Loss: 0.3813, F1: 0.9432, AUC: 0.9861
Epoch [20/30] Train Loss: 0.0035, Test Loss: 0.5136, F1: 0.9419, AUC: 0.9861
Mejores resultados en la época:  5
f1-score 0.94550277308898
AUC según el mejor F1-score 0.9858460685258544

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2418, Test Loss: 0.1861, F1: 0.9230, AUC: 0.9792
Epoch [10/30] Train Loss: 0.0025, Test Loss: 0.6511, F1: 0.9424, AUC: 0.9830
Epoch [20/30] Train Loss: 0.0000, Test Loss: 1.2523, F1: 0.9437, AUC: 0.9752
Mejores resultados en la época:  9
f1-score 0.9457024592159727
AUC según el mejor F1-score 0.9859218480868097
Epoch [0/30] Train Loss: 0.2237, Test Loss: 0.1511, F1: 0.8786, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0023, Test Loss: 0.6380, F1: 0.8934, AUC: 0.9831
Epoch [20/30] Train Loss: 0.0042, Test Loss: 0.2282, F1: 0.9019, AUC: 0.9880
Mejores resultados en la época:  21
f1-score 0.91190833959429
AUC según el mejor F1-score 0.9880232911249374
Confusion matrix Test saved: outputs_cv/2/tfidf/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.933, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0014, 'recall_cv_mean': 0.9367, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9332, 'f1_cv_std': 0.0018, 'params': 160801, 'accuracy_test': 0.9426, 'precision_test': 0.8578, 'recall_test': 0.9105, 'f1_score_test': 0.8833}, 'MLP_2744833': {'accuracy_cv_mean': 0.9438, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9412, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.9467, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.944, 'f1_cv_std': 0.0016, 'params': 2744833, 'accuracy_test': 0.9566, 'precision_test': 0.8952, 'recall_test': 0.9266, 'f1_score_test': 0.9106}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.945, 'precision_cv_std': 0.0049, 'recall_cv_mean': 0.9474, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9462, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9566, 'precision_test': 0.8847, 'recall_test': 0.9409, 'f1_score_test': 0.9119}}}
Saved on: outputs_cv/2/tfidf

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9287, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.937, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9191, 'recall_cv_std': 0.0045, 'f1_cv_mean': 0.928, 'f1_cv_std': 0.0036}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 44, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

Confusion matrix Test saved as: outputs_cv/2/tfidf/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/2/tfidf/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.818, 'accuracy_cv_std': 0.0219, 'precision_cv_mean': 0.7609, 'precision_cv_std': 0.03, 'recall_cv_mean': 0.9306, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.8368, 'f1_cv_std': 0.0159}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 44, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.98      0.56      0.72     16465
           1       0.41      0.96      0.57      5160

    accuracy                           0.66     21625
   macro avg       0.69      0.76      0.64     21625
weighted avg       0.84      0.66      0.68     21625

Confusion matrix Test saved as: outputs_cv/2/tfidf/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/2/tfidf/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8291, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0147, 'recall_cv_mean': 0.7558, 'recall_cv_std': 0.0138, 'f1_cv_mean': 0.8155, 'f1_cv_std': 0.0057}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 44, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.70      0.77      0.73      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.83      0.82     21625
weighted avg       0.87      0.87      0.87     21625

Confusion matrix Test saved as: outputs_cv/2/tfidf/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/2/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8347, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8736, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.7827, 'recall_cv_std': 0.0068, 'f1_cv_mean': 0.8257, 'f1_cv_std': 0.0048}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 44, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.78      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/2/tfidf/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/2/tfidf/random_forest_model.pkl

==============================
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.3124, F1: 0.8994, AUC: 0.9878
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5492, F1: 0.8963, AUC: 0.9870
Mejores resultados en la época:  8
f1-score 0.9105799447671651
AUC según el mejor F1-score 0.9879028335887494
Confusion matrix Test saved: outputs_cv/2/tfidf/cm_mlp_5.png

========================================
Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2474, Test Loss: 0.1911, F1: 0.9274, AUC: 0.9810
Epoch [10/30] Train Loss: 0.0055, Test Loss: 0.3711, F1: 0.9380, AUC: 0.9849
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7046, F1: 0.9447, AUC: 0.9837
Mejores resultados en la época:  9
f1-score 0.9457215067998556
AUC según el mejor F1-score 0.9860560360611592

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2368, Test Loss: 0.1723, F1: 0.9301, AUC: 0.9823
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.5108, F1: 0.9428, AUC: 0.9840
Epoch [20/30] Train Loss: 0.0016, Test Loss: 0.6102, F1: 0.9451, AUC: 0.9820
Mejores resultados en la época:  26
f1-score 0.9490043710539097
AUC según el mejor F1-score 0.9834065359090499

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2435, Test Loss: 0.1812, F1: 0.9252, AUC: 0.9809
Epoch [10/30] Train Loss: 0.0026, Test Loss: 0.4961, F1: 0.9432, AUC: 0.9830
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8445, F1: 0.9446, AUC: 0.9806
Mejores resultados en la época:  23
f1-score 0.9449142304904566
AUC según el mejor F1-score 0.979334180497341

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2423, Test Loss: 0.1706, F1: 0.9300, AUC: 0.9823
Epoch [10/30] Train Loss: 0.0043, Test Loss: 0.3813, F1: 0.9432, AUC: 0.9861
Epoch [20/30] Train Loss: 0.0035, Test Loss: 0.5136, F1: 0.9419, AUC: 0.9861
Mejores resultados en la época:  5
f1-score 0.94550277308898
AUC según el mejor F1-score 0.9858460685258544

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2418, Test Loss: 0.1861, F1: 0.9230, AUC: 0.9792
Epoch [10/30] Train Loss: 0.0025, Test Loss: 0.6511, F1: 0.9424, AUC: 0.9830
Epoch [20/30] Train Loss: 0.0000, Test Loss: 1.2523, F1: 0.9437, AUC: 0.9752
Mejores resultados en la época:  9
f1-score 0.9457024592159727
AUC según el mejor F1-score 0.9859218480868097
Epoch [0/30] Train Loss: 0.2237, Test Loss: 0.1511, F1: 0.8786, AUC: 0.9830
Epoch [10/30] Train Loss: 0.0023, Test Loss: 0.6380, F1: 0.8934, AUC: 0.9831
Epoch [20/30] Train Loss: 0.0042, Test Loss: 0.2282, F1: 0.9019, AUC: 0.9880
Mejores resultados en la época:  21
f1-score 0.91190833959429
AUC según el mejor F1-score 0.9880232911249374
Confusion matrix Test saved: outputs_cv/2/tfidf/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.933, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0014, 'recall_cv_mean': 0.9367, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9332, 'f1_cv_std': 0.0018, 'params': 160801, 'accuracy_test': 0.9426, 'precision_test': 0.8578, 'recall_test': 0.9105, 'f1_score_test': 0.8833}, 'MLP_2744833': {'accuracy_cv_mean': 0.9438, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9412, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.9467, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.944, 'f1_cv_std': 0.0016, 'params': 2744833, 'accuracy_test': 0.9566, 'precision_test': 0.8952, 'recall_test': 0.9266, 'f1_score_test': 0.9106}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.945, 'precision_cv_std': 0.0049, 'recall_cv_mean': 0.9474, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9462, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9566, 'precision_test': 0.8847, 'recall_test': 0.9409, 'f1_score_test': 0.9119}}}
Saved on: outputs_cv/2/tfidf

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9287, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.937, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9191, 'recall_cv_std': 0.0045, 'f1_cv_mean': 0.928, 'f1_cv_std': 0.0036}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 44, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

Confusion matrix Test saved as: outputs_cv/2/tfidf/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/2/tfidf/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.818, 'accuracy_cv_std': 0.0219, 'precision_cv_mean': 0.7609, 'precision_cv_std': 0.03, 'recall_cv_mean': 0.9306, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.8368, 'f1_cv_std': 0.0159}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 44, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.98      0.56      0.72     16465
           1       0.41      0.96      0.57      5160

    accuracy                           0.66     21625
   macro avg       0.69      0.76      0.64     21625
weighted avg       0.84      0.66      0.68     21625

Confusion matrix Test saved as: outputs_cv/2/tfidf/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/2/tfidf/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8291, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0147, 'recall_cv_mean': 0.7558, 'recall_cv_std': 0.0138, 'f1_cv_mean': 0.8155, 'f1_cv_std': 0.0057}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 44, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.90      0.91     16465
           1       0.70      0.77      0.73      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.83      0.82     21625
weighted avg       0.87      0.87      0.87     21625

Confusion matrix Test saved as: outputs_cv/2/tfidf/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/2/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8347, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8736, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.7827, 'recall_cv_std': 0.0068, 'f1_cv_mean': 0.8257, 'f1_cv_std': 0.0048}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 44, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.78      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/2/tfidf/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/2/tfidf/random_forest_model.pkl

==============================
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:13:28] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:13:37] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:17:28] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:17:37] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:21:22] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:21:32] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:25:21] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:25:30] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:29:18] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:29:26] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:33:20] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:33:30] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9063, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.9269, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8821, 'recall_cv_std': 0.0074, 'f1_cv_mean': 0.9039, 'f1_cv_std': 0.0052}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 44, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.94      0.95     16465
           1       0.81      0.88      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.89      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/2/tfidf/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/2/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8737, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9009, 'f1_cv_std': 0.0035}
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.87      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.90      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/2/tfidf/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/2/tfidf/naive_bayes_model.pkl


Resumen Final:
Logistic Regression: {'accuracy_cv_mean': 0.9287, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.937, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9191, 'recall_cv_std': 0.0045, 'f1_cv_mean': 0.928, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9353, 'precision_test': 0.8262, 'recall_test': 0.9229, 'f1_score_test': 0.8718}
XGBoost: {'accuracy_cv_mean': 0.9063, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.9269, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8821, 'recall_cv_std': 0.0074, 'f1_cv_mean': 0.9039, 'f1_cv_std': 0.0052, 'accuracy_test': 0.9232, 'precision_test': 0.8108, 'recall_test': 0.8845, 'f1_score_test': 0.846}
Naive Bayes: {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8737, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9009, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9183, 'precision_test': 0.8043, 'recall_test': 0.869, 'f1_score_test': 0.8354}
Decision Tree: {'accuracy_cv_mean': 0.8291, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0147, 'recall_cv_mean': 0.7558, 'recall_cv_std': 0.0138, 'f1_cv_mean': 0.8155, 'f1_cv_std': 0.0057, 'accuracy_test': 0.8674, 'precision_test': 0.7033, 'recall_test': 0.7684, 'f1_score_test': 0.7344}
Random Forest: {'accuracy_cv_mean': 0.8347, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8736, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.7827, 'recall_cv_std': 0.0068, 'f1_cv_mean': 0.8257, 'f1_cv_std': 0.0048, 'accuracy_test': 0.8568, 'precision_test': 0.6727, 'recall_test': 0.7789, 'f1_score_test': 0.7219}
SVM: {'accuracy_cv_mean': 0.818, 'accuracy_cv_std': 0.0219, 'precision_cv_mean': 0.7609, 'precision_cv_std': 0.03, 'recall_cv_mean': 0.9306, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.8368, 'f1_cv_std': 0.0159, 'accuracy_test': 0.6576, 'precision_test': 0.4073, 'recall_test': 0.9558, 'f1_score_test': 0.5712}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.933, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0014, 'recall_cv_mean': 0.9367, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9332, 'f1_cv_std': 0.0018, 'params': 160801, 'accuracy_test': 0.9426, 'precision_test': 0.8578, 'recall_test': 0.9105, 'f1_score_test': 0.8833}, 'MLP_2744833': {'accuracy_cv_mean': 0.9438, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9412, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.9467, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.944, 'f1_cv_std': 0.0016, 'params': 2744833, 'accuracy_test': 0.9566, 'precision_test': 0.8952, 'recall_test': 0.9266, 'f1_score_test': 0.9106}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.945, 'precision_cv_std': 0.0049, 'recall_cv_mean': 0.9474, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9462, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9566, 'precision_test': 0.8847, 'recall_test': 0.9409, 'f1_score_test': 0.9119}, 'Logistic Regression': {'accuracy_cv_mean': 0.9287, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.937, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9191, 'recall_cv_std': 0.0045, 'f1_cv_mean': 0.928, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9353, 'precision_test': 0.8262, 'recall_test': 0.9229, 'f1_score_test': 0.8718}, 'SVM': {'accuracy_cv_mean': 0.818, 'accuracy_cv_std': 0.0219, 'precision_cv_mean': 0.7609, 'precision_cv_std': 0.03, 'recall_cv_mean': 0.9306, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.8368, 'f1_cv_std': 0.0159, 'accuracy_test': 0.6576, 'precision_test': 0.4073, 'recall_test': 0.9558, 'f1_score_test': 0.5712}, 'Decision Tree': {'accuracy_cv_mean': 0.8291, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0147, 'recall_cv_mean': 0.7558, 'recall_cv_std': 0.0138, 'f1_cv_mean': 0.8155, 'f1_cv_std': 0.0057, 'accuracy_test': 0.8674, 'precision_test': 0.7033, 'recall_test': 0.7684, 'f1_score_test': 0.7344}, 'Random Forest': {'accuracy_cv_mean': 0.8347, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8736, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.7827, 'recall_cv_std': 0.0068, 'f1_cv_mean': 0.8257, 'f1_cv_std': 0.0048, 'accuracy_test': 0.8568, 'precision_test': 0.6727, 'recall_test': 0.7789, 'f1_score_test': 0.7219}, 'XGBoost': {'accuracy_cv_mean': 0.9063, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.9269, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8821, 'recall_cv_std': 0.0074, 'f1_cv_mean': 0.9039, 'f1_cv_std': 0.0052, 'accuracy_test': 0.9232, 'precision_test': 0.8108, 'recall_test': 0.8845, 'f1_score_test': 0.846}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8737, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9009, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9183, 'precision_test': 0.8043, 'recall_test': 0.869, 'f1_score_test': 0.8354}}}
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_1.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9063, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.9269, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8821, 'recall_cv_std': 0.0074, 'f1_cv_mean': 0.9039, 'f1_cv_std': 0.0052}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 44, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.94      0.95     16465
           1       0.81      0.88      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.89      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/2/tfidf/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/2/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8737, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9009, 'f1_cv_std': 0.0035}
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.87      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.90      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/2/tfidf/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/2/tfidf/naive_bayes_model.pkl


Resumen Final:
Logistic Regression: {'accuracy_cv_mean': 0.9287, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.937, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9191, 'recall_cv_std': 0.0045, 'f1_cv_mean': 0.928, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9353, 'precision_test': 0.8262, 'recall_test': 0.9229, 'f1_score_test': 0.8718}
XGBoost: {'accuracy_cv_mean': 0.9063, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.9269, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8821, 'recall_cv_std': 0.0074, 'f1_cv_mean': 0.9039, 'f1_cv_std': 0.0052, 'accuracy_test': 0.9232, 'precision_test': 0.8108, 'recall_test': 0.8845, 'f1_score_test': 0.846}
Naive Bayes: {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8737, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9009, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9183, 'precision_test': 0.8043, 'recall_test': 0.869, 'f1_score_test': 0.8354}
Decision Tree: {'accuracy_cv_mean': 0.8291, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0147, 'recall_cv_mean': 0.7558, 'recall_cv_std': 0.0138, 'f1_cv_mean': 0.8155, 'f1_cv_std': 0.0057, 'accuracy_test': 0.8674, 'precision_test': 0.7033, 'recall_test': 0.7684, 'f1_score_test': 0.7344}
Random Forest: {'accuracy_cv_mean': 0.8347, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8736, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.7827, 'recall_cv_std': 0.0068, 'f1_cv_mean': 0.8257, 'f1_cv_std': 0.0048, 'accuracy_test': 0.8568, 'precision_test': 0.6727, 'recall_test': 0.7789, 'f1_score_test': 0.7219}
SVM: {'accuracy_cv_mean': 0.818, 'accuracy_cv_std': 0.0219, 'precision_cv_mean': 0.7609, 'precision_cv_std': 0.03, 'recall_cv_mean': 0.9306, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.8368, 'f1_cv_std': 0.0159, 'accuracy_test': 0.6576, 'precision_test': 0.4073, 'recall_test': 0.9558, 'f1_score_test': 0.5712}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.933, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0014, 'recall_cv_mean': 0.9367, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9332, 'f1_cv_std': 0.0018, 'params': 160801, 'accuracy_test': 0.9426, 'precision_test': 0.8578, 'recall_test': 0.9105, 'f1_score_test': 0.8833}, 'MLP_2744833': {'accuracy_cv_mean': 0.9438, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9412, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.9467, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.944, 'f1_cv_std': 0.0016, 'params': 2744833, 'accuracy_test': 0.9566, 'precision_test': 0.8952, 'recall_test': 0.9266, 'f1_score_test': 0.9106}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.945, 'precision_cv_std': 0.0049, 'recall_cv_mean': 0.9474, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9462, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9566, 'precision_test': 0.8847, 'recall_test': 0.9409, 'f1_score_test': 0.9119}, 'Logistic Regression': {'accuracy_cv_mean': 0.9287, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.937, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9191, 'recall_cv_std': 0.0045, 'f1_cv_mean': 0.928, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9353, 'precision_test': 0.8262, 'recall_test': 0.9229, 'f1_score_test': 0.8718}, 'SVM': {'accuracy_cv_mean': 0.818, 'accuracy_cv_std': 0.0219, 'precision_cv_mean': 0.7609, 'precision_cv_std': 0.03, 'recall_cv_mean': 0.9306, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.8368, 'f1_cv_std': 0.0159, 'accuracy_test': 0.6576, 'precision_test': 0.4073, 'recall_test': 0.9558, 'f1_score_test': 0.5712}, 'Decision Tree': {'accuracy_cv_mean': 0.8291, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0147, 'recall_cv_mean': 0.7558, 'recall_cv_std': 0.0138, 'f1_cv_mean': 0.8155, 'f1_cv_std': 0.0057, 'accuracy_test': 0.8674, 'precision_test': 0.7033, 'recall_test': 0.7684, 'f1_score_test': 0.7344}, 'Random Forest': {'accuracy_cv_mean': 0.8347, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8736, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.7827, 'recall_cv_std': 0.0068, 'f1_cv_mean': 0.8257, 'f1_cv_std': 0.0048, 'accuracy_test': 0.8568, 'precision_test': 0.6727, 'recall_test': 0.7789, 'f1_score_test': 0.7219}, 'XGBoost': {'accuracy_cv_mean': 0.9063, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.9269, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8821, 'recall_cv_std': 0.0074, 'f1_cv_mean': 0.9039, 'f1_cv_std': 0.0052, 'accuracy_test': 0.9232, 'precision_test': 0.8108, 'recall_test': 0.8845, 'f1_score_test': 0.846}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8737, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9009, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9183, 'precision_test': 0.8043, 'recall_test': 0.869, 'f1_score_test': 0.8354}}}
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_1.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 323)
Shape of X_test after concatenation:  (21625, 323)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 323)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 323)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [323, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.5551, Test Loss: 0.4472, F1: 0.7995, AUC: 0.8793
Epoch [10/30] Train Loss: 0.3611, Test Loss: 0.3550, F1: 0.8416, AUC: 0.9208
Epoch [20/30] Train Loss: 0.3462, Test Loss: 0.3399, F1: 0.8454, AUC: 0.9280
Mejores resultados en la época:  29
f1-score 0.8505014749262537
AUC según el mejor F1-score 0.9306819611576829

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.5487, Test Loss: 0.4575, F1: 0.7883, AUC: 0.8791
Epoch [10/30] Train Loss: 0.3619, Test Loss: 0.3775, F1: 0.8115, AUC: 0.9179
Epoch [20/30] Train Loss: 0.3482, Test Loss: 0.3606, F1: 0.8325, AUC: 0.9191
Mejores resultados en la época:  28
f1-score 0.8451289573401785
AUC según el mejor F1-score 0.9242834257725648

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.5431, Test Loss: 0.4402, F1: 0.8083, AUC: 0.8881
Epoch [10/30] Train Loss: 0.3635, Test Loss: 0.3740, F1: 0.8204, AUC: 0.9216
Epoch [20/30] Train Loss: 0.3533, Test Loss: 0.3512, F1: 0.8476, AUC: 0.9242
Mejores resultados en la época:  25
f1-score 0.8495512517713746
AUC según el mejor F1-score 0.9252602523511207

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.5646, Test Loss: 0.4541, F1: 0.8030, AUC: 0.8851
Epoch [10/30] Train Loss: 0.3653, Test Loss: 0.3811, F1: 0.8380, AUC: 0.9212
Epoch [20/30] Train Loss: 0.3536, Test Loss: 0.3582, F1: 0.8454, AUC: 0.9257
Mejores resultados en la época:  27
f1-score 0.8524871355060034
AUC según el mejor F1-score 0.9283863191536921

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.5629, Test Loss: 0.4486, F1: 0.7945, AUC: 0.8810
Epoch [10/30] Train Loss: 0.3654, Test Loss: 0.3601, F1: 0.8387, AUC: 0.9190
Epoch [20/30] Train Loss: 0.3539, Test Loss: 0.3580, F1: 0.8375, AUC: 0.9225
Mejores resultados en la época:  19
f1-score 0.847928493779442
AUC según el mejor F1-score 0.9223447921890818
Epoch [0/30] Train Loss: 0.5174, Test Loss: 0.3702, F1: 0.7073, AUC: 0.8926
Epoch [10/30] Train Loss: 0.3600, Test Loss: 0.3517, F1: 0.7320, AUC: 0.9243
Epoch [20/30] Train Loss: 0.3516, Test Loss: 0.2787, F1: 0.7566, AUC: 0.9273
Mejores resultados en la época:  29
f1-score 0.76262674121103
AUC según el mejor F1-score 0.9305263160992192
Confusion matrix Test saved: outputs_cv/2/lyrics_bert/cm_mlp_1.png

========================================
Entrenando red 5 con capas [323, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4848, Test Loss: 0.4533, F1: 0.8067, AUC: 0.9017
Epoch [10/30] Train Loss: 0.3574, Test Loss: 0.3454, F1: 0.8280, AUC: 0.9275
Epoch [20/30] Train Loss: 0.3384, Test Loss: 0.3602, F1: 0.8464, AUC: 0.9300
Mejores resultados en la época:  27
f1-score 0.8540395411276543
AUC según el mejor F1-score 0.9350350614164564

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4892, Test Loss: 0.4095, F1: 0.8013, AUC: 0.8975
Epoch [10/30] Train Loss: 0.3514, Test Loss: 0.3605, F1: 0.8372, AUC: 0.9225
Epoch [20/30] Train Loss: 0.3361, Test Loss: 0.3467, F1: 0.8342, AUC: 0.9274
Mejores resultados en la época:  27
f1-score 0.8460303741202617
AUC según el mejor F1-score 0.9301737566477375

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4837, Test Loss: 0.4362, F1: 0.8250, AUC: 0.9082
Epoch [10/30] Train Loss: 0.3576, Test Loss: 0.3799, F1: 0.8001, AUC: 0.9262
Epoch [20/30] Train Loss: 0.3416, Test Loss: 0.3399, F1: 0.8535, AUC: 0.9307
Mejores resultados en la época:  26
f1-score 0.8558721000120206
AUC según el mejor F1-score 0.9343501296684394

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4683, Test Loss: 0.3800, F1: 0.8283, AUC: 0.9091
Epoch [10/30] Train Loss: 0.3563, Test Loss: 0.3461, F1: 0.8312, AUC: 0.9284
Epoch [20/30] Train Loss: 0.3408, Test Loss: 0.3346, F1: 0.8556, AUC: 0.9341
Mejores resultados en la época:  28
f1-score 0.860655737704918
AUC según el mejor F1-score 0.9379161712526507

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4780, Test Loss: 0.4066, F1: 0.7973, AUC: 0.9028
Epoch [10/30] Train Loss: 0.3557, Test Loss: 0.3687, F1: 0.8097, AUC: 0.9264
Epoch [20/30] Train Loss: 0.3361, Test Loss: 0.3316, F1: 0.8484, AUC: 0.9326
Mejores resultados en la época:  25
f1-score 0.8554692169754932
AUC según el mejor F1-score 0.9338871756799146
Epoch [0/30] Train Loss: 0.4630, Test Loss: 0.3231, F1: 0.7367, AUC: 0.9102
Epoch [10/30] Train Loss: 0.3517, Test Loss: 0.2831, F1: 0.7612, AUC: 0.9313
Epoch [20/30] Train Loss: 0.3353, Test Loss: 0.2717, F1: 0.7627, AUC: 0.9334
Mejores resultados en la época:  29
f1-score 0.7728213815170337
AUC según el mejor F1-score 0.937744275501004
Confusion matrix Test saved: outputs_cv/2/lyrics_bert/cm_mlp_5.png

========================================
Entrenando red 6 con capas [323, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4889, Test Loss: 0.4221, F1: 0.8169, AUC: 0.9019
Epoch [10/30] Train Loss: 0.3522, Test Loss: 0.3426, F1: 0.8372, AUC: 0.9275
Epoch [20/30] Train Loss: 0.3359, Test Loss: 0.3291, F1: 0.8303, AUC: 0.9340
Mejores resultados en la época:  27
f1-score 0.8548787699586043
AUC según el mejor F1-score 0.9347649090536928

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4690, Test Loss: 0.4360, F1: 0.7647, AUC: 0.9030
Epoch [10/30] Train Loss: 0.3530, Test Loss: 0.3602, F1: 0.8185, AUC: 0.9233
Epoch [20/30] Train Loss: 0.3344, Test Loss: 0.3547, F1: 0.8442, AUC: 0.9277
Mejores resultados en la época:  25
f1-score 0.8486407053637032
AUC según el mejor F1-score 0.9307445184333875

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4951, Test Loss: 0.3900, F1: 0.8129, AUC: 0.9077
Epoch [10/30] Train Loss: 0.3577, Test Loss: 0.3408, F1: 0.8477, AUC: 0.9278
Epoch [20/30] Train Loss: 0.3349, Test Loss: 0.3552, F1: 0.8540, AUC: 0.9337
Mejores resultados en la época:  29
f1-score 0.8592155416205582
AUC según el mejor F1-score 0.9375771109383451

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4824, Test Loss: 0.3903, F1: 0.8196, AUC: 0.9095
Epoch [10/30] Train Loss: 0.3570, Test Loss: 0.3376, F1: 0.8466, AUC: 0.9285
Epoch [20/30] Train Loss: 0.3427, Test Loss: 0.3629, F1: 0.8270, AUC: 0.9337
Mejores resultados en la época:  25
f1-score 0.8578940778765494
AUC según el mejor F1-score 0.9361942553575151

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.5927, Test Loss: 0.4953, F1: 0.7211, AUC: 0.8893
Epoch [10/30] Train Loss: 0.3548, Test Loss: 0.3496, F1: 0.8381, AUC: 0.9261
Epoch [20/30] Train Loss: 0.3394, Test Loss: 0.3410, F1: 0.8367, AUC: 0.9307
Mejores resultados en la época:  26
f1-score 0.8561211109786625
AUC según el mejor F1-score 0.9320260273149217
Epoch [0/30] Train Loss: 0.4750, Test Loss: 0.3078, F1: 0.7323, AUC: 0.9086
Epoch [10/30] Train Loss: 0.3515, Test Loss: 0.3479, F1: 0.7319, AUC: 0.9316
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 323)
Shape of X_test after concatenation:  (21625, 323)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 323)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 323)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [323, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.5551, Test Loss: 0.4472, F1: 0.7995, AUC: 0.8793
Epoch [10/30] Train Loss: 0.3611, Test Loss: 0.3550, F1: 0.8416, AUC: 0.9208
Epoch [20/30] Train Loss: 0.3462, Test Loss: 0.3399, F1: 0.8454, AUC: 0.9280
Mejores resultados en la época:  29
f1-score 0.8505014749262537
AUC según el mejor F1-score 0.9306819611576829

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.5487, Test Loss: 0.4575, F1: 0.7883, AUC: 0.8791
Epoch [10/30] Train Loss: 0.3619, Test Loss: 0.3775, F1: 0.8115, AUC: 0.9179
Epoch [20/30] Train Loss: 0.3482, Test Loss: 0.3606, F1: 0.8325, AUC: 0.9191
Mejores resultados en la época:  28
f1-score 0.8451289573401785
AUC según el mejor F1-score 0.9242834257725648

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.5431, Test Loss: 0.4402, F1: 0.8083, AUC: 0.8881
Epoch [10/30] Train Loss: 0.3635, Test Loss: 0.3740, F1: 0.8204, AUC: 0.9216
Epoch [20/30] Train Loss: 0.3533, Test Loss: 0.3512, F1: 0.8476, AUC: 0.9242
Mejores resultados en la época:  25
f1-score 0.8495512517713746
AUC según el mejor F1-score 0.9252602523511207

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.5646, Test Loss: 0.4541, F1: 0.8030, AUC: 0.8851
Epoch [10/30] Train Loss: 0.3653, Test Loss: 0.3811, F1: 0.8380, AUC: 0.9212
Epoch [20/30] Train Loss: 0.3536, Test Loss: 0.3582, F1: 0.8454, AUC: 0.9257
Mejores resultados en la época:  27
f1-score 0.8524871355060034
AUC según el mejor F1-score 0.9283863191536921

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.5629, Test Loss: 0.4486, F1: 0.7945, AUC: 0.8810
Epoch [10/30] Train Loss: 0.3654, Test Loss: 0.3601, F1: 0.8387, AUC: 0.9190
Epoch [20/30] Train Loss: 0.3539, Test Loss: 0.3580, F1: 0.8375, AUC: 0.9225
Mejores resultados en la época:  19
f1-score 0.847928493779442
AUC según el mejor F1-score 0.9223447921890818
Epoch [0/30] Train Loss: 0.5174, Test Loss: 0.3702, F1: 0.7073, AUC: 0.8926
Epoch [10/30] Train Loss: 0.3600, Test Loss: 0.3517, F1: 0.7320, AUC: 0.9243
Epoch [20/30] Train Loss: 0.3516, Test Loss: 0.2787, F1: 0.7566, AUC: 0.9273
Mejores resultados en la época:  29
f1-score 0.76262674121103
AUC según el mejor F1-score 0.9305263160992192
Confusion matrix Test saved: outputs_cv/2/lyrics_bert/cm_mlp_1.png

========================================
Entrenando red 5 con capas [323, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4848, Test Loss: 0.4533, F1: 0.8067, AUC: 0.9017
Epoch [10/30] Train Loss: 0.3574, Test Loss: 0.3454, F1: 0.8280, AUC: 0.9275
Epoch [20/30] Train Loss: 0.3384, Test Loss: 0.3602, F1: 0.8464, AUC: 0.9300
Mejores resultados en la época:  27
f1-score 0.8540395411276543
AUC según el mejor F1-score 0.9350350614164564

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4892, Test Loss: 0.4095, F1: 0.8013, AUC: 0.8975
Epoch [10/30] Train Loss: 0.3514, Test Loss: 0.3605, F1: 0.8372, AUC: 0.9225
Epoch [20/30] Train Loss: 0.3361, Test Loss: 0.3467, F1: 0.8342, AUC: 0.9274
Mejores resultados en la época:  27
f1-score 0.8460303741202617
AUC según el mejor F1-score 0.9301737566477375

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4837, Test Loss: 0.4362, F1: 0.8250, AUC: 0.9082
Epoch [10/30] Train Loss: 0.3576, Test Loss: 0.3799, F1: 0.8001, AUC: 0.9262
Epoch [20/30] Train Loss: 0.3416, Test Loss: 0.3399, F1: 0.8535, AUC: 0.9307
Mejores resultados en la época:  26
f1-score 0.8558721000120206
AUC según el mejor F1-score 0.9343501296684394

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4683, Test Loss: 0.3800, F1: 0.8283, AUC: 0.9091
Epoch [10/30] Train Loss: 0.3563, Test Loss: 0.3461, F1: 0.8312, AUC: 0.9284
Epoch [20/30] Train Loss: 0.3408, Test Loss: 0.3346, F1: 0.8556, AUC: 0.9341
Mejores resultados en la época:  28
f1-score 0.860655737704918
AUC según el mejor F1-score 0.9379161712526507

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4780, Test Loss: 0.4066, F1: 0.7973, AUC: 0.9028
Epoch [10/30] Train Loss: 0.3557, Test Loss: 0.3687, F1: 0.8097, AUC: 0.9264
Epoch [20/30] Train Loss: 0.3361, Test Loss: 0.3316, F1: 0.8484, AUC: 0.9326
Mejores resultados en la época:  25
f1-score 0.8554692169754932
AUC según el mejor F1-score 0.9338871756799146
Epoch [0/30] Train Loss: 0.4630, Test Loss: 0.3231, F1: 0.7367, AUC: 0.9102
Epoch [10/30] Train Loss: 0.3517, Test Loss: 0.2831, F1: 0.7612, AUC: 0.9313
Epoch [20/30] Train Loss: 0.3353, Test Loss: 0.2717, F1: 0.7627, AUC: 0.9334
Mejores resultados en la época:  29
f1-score 0.7728213815170337
AUC según el mejor F1-score 0.937744275501004
Confusion matrix Test saved: outputs_cv/2/lyrics_bert/cm_mlp_5.png

========================================
Entrenando red 6 con capas [323, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4889, Test Loss: 0.4221, F1: 0.8169, AUC: 0.9019
Epoch [10/30] Train Loss: 0.3522, Test Loss: 0.3426, F1: 0.8372, AUC: 0.9275
Epoch [20/30] Train Loss: 0.3359, Test Loss: 0.3291, F1: 0.8303, AUC: 0.9340
Mejores resultados en la época:  27
f1-score 0.8548787699586043
AUC según el mejor F1-score 0.9347649090536928

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4690, Test Loss: 0.4360, F1: 0.7647, AUC: 0.9030
Epoch [10/30] Train Loss: 0.3530, Test Loss: 0.3602, F1: 0.8185, AUC: 0.9233
Epoch [20/30] Train Loss: 0.3344, Test Loss: 0.3547, F1: 0.8442, AUC: 0.9277
Mejores resultados en la época:  25
f1-score 0.8486407053637032
AUC según el mejor F1-score 0.9307445184333875

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4951, Test Loss: 0.3900, F1: 0.8129, AUC: 0.9077
Epoch [10/30] Train Loss: 0.3577, Test Loss: 0.3408, F1: 0.8477, AUC: 0.9278
Epoch [20/30] Train Loss: 0.3349, Test Loss: 0.3552, F1: 0.8540, AUC: 0.9337
Mejores resultados en la época:  29
f1-score 0.8592155416205582
AUC según el mejor F1-score 0.9375771109383451

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4824, Test Loss: 0.3903, F1: 0.8196, AUC: 0.9095
Epoch [10/30] Train Loss: 0.3570, Test Loss: 0.3376, F1: 0.8466, AUC: 0.9285
Epoch [20/30] Train Loss: 0.3427, Test Loss: 0.3629, F1: 0.8270, AUC: 0.9337
Mejores resultados en la época:  25
f1-score 0.8578940778765494
AUC según el mejor F1-score 0.9361942553575151

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.5927, Test Loss: 0.4953, F1: 0.7211, AUC: 0.8893
Epoch [10/30] Train Loss: 0.3548, Test Loss: 0.3496, F1: 0.8381, AUC: 0.9261
Epoch [20/30] Train Loss: 0.3394, Test Loss: 0.3410, F1: 0.8367, AUC: 0.9307
Mejores resultados en la época:  26
f1-score 0.8561211109786625
AUC según el mejor F1-score 0.9320260273149217
Epoch [0/30] Train Loss: 0.4750, Test Loss: 0.3078, F1: 0.7323, AUC: 0.9086
Epoch [10/30] Train Loss: 0.3515, Test Loss: 0.3479, F1: 0.7319, AUC: 0.9316
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Epoch [20/30] Train Loss: 0.3271, Test Loss: 0.3069, F1: 0.7700, AUC: 0.9374
Mejores resultados en la época:  29
f1-score 0.7786181520850368
AUC según el mejor F1-score 0.9410449402891263
Confusion matrix Test saved: outputs_cv/2/lyrics_bert/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.933, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0014, 'recall_cv_mean': 0.9367, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9332, 'f1_cv_std': 0.0018, 'params': 160801, 'accuracy_test': 0.9426, 'precision_test': 0.8578, 'recall_test': 0.9105, 'f1_score_test': 0.8833}, 'MLP_2744833': {'accuracy_cv_mean': 0.9438, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9412, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.9467, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.944, 'f1_cv_std': 0.0016, 'params': 2744833, 'accuracy_test': 0.9566, 'precision_test': 0.8952, 'recall_test': 0.9266, 'f1_score_test': 0.9106}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.945, 'precision_cv_std': 0.0049, 'recall_cv_mean': 0.9474, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9462, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9566, 'precision_test': 0.8847, 'recall_test': 0.9409, 'f1_score_test': 0.9119}, 'Logistic Regression': {'accuracy_cv_mean': 0.9287, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.937, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9191, 'recall_cv_std': 0.0045, 'f1_cv_mean': 0.928, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9353, 'precision_test': 0.8262, 'recall_test': 0.9229, 'f1_score_test': 0.8718}, 'SVM': {'accuracy_cv_mean': 0.818, 'accuracy_cv_std': 0.0219, 'precision_cv_mean': 0.7609, 'precision_cv_std': 0.03, 'recall_cv_mean': 0.9306, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.8368, 'f1_cv_std': 0.0159, 'accuracy_test': 0.6576, 'precision_test': 0.4073, 'recall_test': 0.9558, 'f1_score_test': 0.5712}, 'Decision Tree': {'accuracy_cv_mean': 0.8291, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0147, 'recall_cv_mean': 0.7558, 'recall_cv_std': 0.0138, 'f1_cv_mean': 0.8155, 'f1_cv_std': 0.0057, 'accuracy_test': 0.8674, 'precision_test': 0.7033, 'recall_test': 0.7684, 'f1_score_test': 0.7344}, 'Random Forest': {'accuracy_cv_mean': 0.8347, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8736, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.7827, 'recall_cv_std': 0.0068, 'f1_cv_mean': 0.8257, 'f1_cv_std': 0.0048, 'accuracy_test': 0.8568, 'precision_test': 0.6727, 'recall_test': 0.7789, 'f1_score_test': 0.7219}, 'XGBoost': {'accuracy_cv_mean': 0.9063, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.9269, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8821, 'recall_cv_std': 0.0074, 'f1_cv_mean': 0.9039, 'f1_cv_std': 0.0052, 'accuracy_test': 0.9232, 'precision_test': 0.8108, 'recall_test': 0.8845, 'f1_score_test': 0.846}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8737, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9009, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9183, 'precision_test': 0.8043, 'recall_test': 0.869, 'f1_score_test': 0.8354}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8481, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.8437, 'precision_cv_std': 0.0132, 'recall_cv_mean': 0.855, 'recall_cv_std': 0.0146, 'f1_cv_mean': 0.8491, 'f1_cv_std': 0.0025, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.7462, 'recall_test': 0.7798, 'f1_score_test': 0.7626}, 'MLP_338433': {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8547, 'precision_cv_std': 0.0068, 'recall_cv_mean': 0.8544, 'recall_cv_std': 0.014, 'f1_cv_mean': 0.8544, 'f1_cv_std': 0.0047, 'params': 338433, 'accuracy_test': 0.8884, 'precision_test': 0.7512, 'recall_test': 0.7957, 'f1_score_test': 0.7728}, 'MLP_1031169': {'accuracy_cv_mean': 0.8559, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.0191, 'recall_cv_mean': 0.8523, 'recall_cv_std': 0.0175, 'f1_cv_mean': 0.8554, 'f1_cv_std': 0.0037, 'params': 1031169, 'accuracy_test': 0.8998, 'precision_test': 0.8237, 'recall_test': 0.7382, 'f1_score_test': 0.7786}}}
Saved on: outputs_cv/2/lyrics_bert

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8475, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8313, 'recall_cv_std': 0.0079, 'f1_cv_mean': 0.845, 'f1_cv_std': 0.0049}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 44, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.87      0.90     16465
           1       0.67      0.83      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.85      0.82     21625
weighted avg       0.88      0.86      0.87     21625

Confusion matrix Test saved as: outputs_cv/2/lyrics_bert/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/2/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.6815, 'accuracy_cv_std': 0.0227, 'precision_cv_mean': 0.6619, 'precision_cv_std': 0.0387, 'recall_cv_mean': 0.7619, 'recall_cv_std': 0.0757, 'f1_cv_mean': 0.7044, 'f1_cv_std': 0.0175}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 44, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.86      0.51      0.64     16465
           1       0.32      0.74      0.45      5160

    accuracy                           0.56     21625
   macro avg       0.59      0.62      0.54     21625
weighted avg       0.73      0.56      0.59     21625

Confusion matrix Test saved as: outputs_cv/2/lyrics_bert/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/2/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.792, 'accuracy_cv_std': 0.0047, 'precision_cv_mean': 0.8112, 'precision_cv_std': 0.0162, 'recall_cv_mean': 0.7622, 'recall_cv_std': 0.0246, 'f1_cv_mean': 0.7855, 'f1_cv_std': 0.0072}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 44, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.83      0.88     16465
           1       0.59      0.78      0.67      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.81      0.78     21625
weighted avg       0.85      0.82      0.83     21625

Confusion matrix Test saved as: outputs_cv/2/lyrics_bert/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/2/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8534, 'accuracy_cv_std': 0.0064, 'precision_cv_mean': 0.8862, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0104, 'f1_cv_mean': 0.8469, 'f1_cv_std': 0.0072}
Epoch [20/30] Train Loss: 0.3271, Test Loss: 0.3069, F1: 0.7700, AUC: 0.9374
Mejores resultados en la época:  29
f1-score 0.7786181520850368
AUC según el mejor F1-score 0.9410449402891263
Confusion matrix Test saved: outputs_cv/2/lyrics_bert/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.933, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0014, 'recall_cv_mean': 0.9367, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9332, 'f1_cv_std': 0.0018, 'params': 160801, 'accuracy_test': 0.9426, 'precision_test': 0.8578, 'recall_test': 0.9105, 'f1_score_test': 0.8833}, 'MLP_2744833': {'accuracy_cv_mean': 0.9438, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9412, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.9467, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.944, 'f1_cv_std': 0.0016, 'params': 2744833, 'accuracy_test': 0.9566, 'precision_test': 0.8952, 'recall_test': 0.9266, 'f1_score_test': 0.9106}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.945, 'precision_cv_std': 0.0049, 'recall_cv_mean': 0.9474, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9462, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9566, 'precision_test': 0.8847, 'recall_test': 0.9409, 'f1_score_test': 0.9119}, 'Logistic Regression': {'accuracy_cv_mean': 0.9287, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.937, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9191, 'recall_cv_std': 0.0045, 'f1_cv_mean': 0.928, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9353, 'precision_test': 0.8262, 'recall_test': 0.9229, 'f1_score_test': 0.8718}, 'SVM': {'accuracy_cv_mean': 0.818, 'accuracy_cv_std': 0.0219, 'precision_cv_mean': 0.7609, 'precision_cv_std': 0.03, 'recall_cv_mean': 0.9306, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.8368, 'f1_cv_std': 0.0159, 'accuracy_test': 0.6576, 'precision_test': 0.4073, 'recall_test': 0.9558, 'f1_score_test': 0.5712}, 'Decision Tree': {'accuracy_cv_mean': 0.8291, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0147, 'recall_cv_mean': 0.7558, 'recall_cv_std': 0.0138, 'f1_cv_mean': 0.8155, 'f1_cv_std': 0.0057, 'accuracy_test': 0.8674, 'precision_test': 0.7033, 'recall_test': 0.7684, 'f1_score_test': 0.7344}, 'Random Forest': {'accuracy_cv_mean': 0.8347, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8736, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.7827, 'recall_cv_std': 0.0068, 'f1_cv_mean': 0.8257, 'f1_cv_std': 0.0048, 'accuracy_test': 0.8568, 'precision_test': 0.6727, 'recall_test': 0.7789, 'f1_score_test': 0.7219}, 'XGBoost': {'accuracy_cv_mean': 0.9063, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.9269, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8821, 'recall_cv_std': 0.0074, 'f1_cv_mean': 0.9039, 'f1_cv_std': 0.0052, 'accuracy_test': 0.9232, 'precision_test': 0.8108, 'recall_test': 0.8845, 'f1_score_test': 0.846}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8737, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9009, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9183, 'precision_test': 0.8043, 'recall_test': 0.869, 'f1_score_test': 0.8354}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8481, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.8437, 'precision_cv_std': 0.0132, 'recall_cv_mean': 0.855, 'recall_cv_std': 0.0146, 'f1_cv_mean': 0.8491, 'f1_cv_std': 0.0025, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.7462, 'recall_test': 0.7798, 'f1_score_test': 0.7626}, 'MLP_338433': {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8547, 'precision_cv_std': 0.0068, 'recall_cv_mean': 0.8544, 'recall_cv_std': 0.014, 'f1_cv_mean': 0.8544, 'f1_cv_std': 0.0047, 'params': 338433, 'accuracy_test': 0.8884, 'precision_test': 0.7512, 'recall_test': 0.7957, 'f1_score_test': 0.7728}, 'MLP_1031169': {'accuracy_cv_mean': 0.8559, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.0191, 'recall_cv_mean': 0.8523, 'recall_cv_std': 0.0175, 'f1_cv_mean': 0.8554, 'f1_cv_std': 0.0037, 'params': 1031169, 'accuracy_test': 0.8998, 'precision_test': 0.8237, 'recall_test': 0.7382, 'f1_score_test': 0.7786}}}
Saved on: outputs_cv/2/lyrics_bert

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8475, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8313, 'recall_cv_std': 0.0079, 'f1_cv_mean': 0.845, 'f1_cv_std': 0.0049}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 44, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.87      0.90     16465
           1       0.67      0.83      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.85      0.82     21625
weighted avg       0.88      0.86      0.87     21625

Confusion matrix Test saved as: outputs_cv/2/lyrics_bert/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/2/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.6815, 'accuracy_cv_std': 0.0227, 'precision_cv_mean': 0.6619, 'precision_cv_std': 0.0387, 'recall_cv_mean': 0.7619, 'recall_cv_std': 0.0757, 'f1_cv_mean': 0.7044, 'f1_cv_std': 0.0175}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 44, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.86      0.51      0.64     16465
           1       0.32      0.74      0.45      5160

    accuracy                           0.56     21625
   macro avg       0.59      0.62      0.54     21625
weighted avg       0.73      0.56      0.59     21625

Confusion matrix Test saved as: outputs_cv/2/lyrics_bert/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/2/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.792, 'accuracy_cv_std': 0.0047, 'precision_cv_mean': 0.8112, 'precision_cv_std': 0.0162, 'recall_cv_mean': 0.7622, 'recall_cv_std': 0.0246, 'f1_cv_mean': 0.7855, 'f1_cv_std': 0.0072}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 44, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.83      0.88     16465
           1       0.59      0.78      0.67      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.81      0.78     21625
weighted avg       0.85      0.82      0.83     21625

Confusion matrix Test saved as: outputs_cv/2/lyrics_bert/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/2/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8534, 'accuracy_cv_std': 0.0064, 'precision_cv_mean': 0.8862, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0104, 'f1_cv_mean': 0.8469, 'f1_cv_std': 0.0072}
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:21:16] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:21:49] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:22:01] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:22:38] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:22:49] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:23:26] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:23:38] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:24:14] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:24:26] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:25:03] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:25:16] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:25:53] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 44, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.90      0.92     16465
           1       0.72      0.81      0.76      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.86      0.84     21625
weighted avg       0.89      0.88      0.88     21625

Confusion matrix Test saved as: outputs_cv/2/lyrics_bert/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/2/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8765, 'accuracy_cv_std': 0.0045, 'precision_cv_mean': 0.8911, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8579, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0049}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 44, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.90      0.93     16465
           1       0.74      0.87      0.80      5160

    accuracy                           0.89     21625
   macro avg       0.85      0.89      0.86     21625
weighted avg       0.90      0.89      0.90     21625

Confusion matrix Test saved as: outputs_cv/2/lyrics_bert/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/2/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7796, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.7631, 'precision_cv_std': 0.0021, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.7863, 'f1_cv_std': 0.0035}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.75      0.83     16465
           1       0.51      0.81      0.62      5160

    accuracy                           0.77     21625
   macro avg       0.72      0.78      0.73     21625
weighted avg       0.83      0.77      0.78     21625

Confusion matrix Test saved as: outputs_cv/2/lyrics_bert/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/2/lyrics_bert/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.8765, 'accuracy_cv_std': 0.0045, 'precision_cv_mean': 0.8911, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8579, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8948, 'precision_test': 0.7382, 'recall_test': 0.8667, 'f1_score_test': 0.7973}
Random Forest: {'accuracy_cv_mean': 0.8534, 'accuracy_cv_std': 0.0064, 'precision_cv_mean': 0.8862, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0104, 'f1_cv_mean': 0.8469, 'f1_cv_std': 0.0072, 'accuracy_test': 0.88, 'precision_test': 0.7214, 'recall_test': 0.8101, 'f1_score_test': 0.7632}
Logistic Regression: {'accuracy_cv_mean': 0.8475, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8313, 'recall_cv_std': 0.0079, 'f1_cv_mean': 0.845, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8608, 'precision_test': 0.6666, 'recall_test': 0.8333, 'f1_score_test': 0.7407}
Decision Tree: {'accuracy_cv_mean': 0.792, 'accuracy_cv_std': 0.0047, 'precision_cv_mean': 0.8112, 'precision_cv_std': 0.0162, 'recall_cv_mean': 0.7622, 'recall_cv_std': 0.0246, 'f1_cv_mean': 0.7855, 'f1_cv_std': 0.0072, 'accuracy_test': 0.8198, 'precision_test': 0.5927, 'recall_test': 0.7829, 'f1_score_test': 0.6747}
Naive Bayes: {'accuracy_cv_mean': 0.7796, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.7631, 'precision_cv_std': 0.0021, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.7863, 'f1_cv_std': 0.0035, 'accuracy_test': 0.7662, 'precision_test': 0.5062, 'recall_test': 0.8091, 'f1_score_test': 0.6228}
SVM: {'accuracy_cv_mean': 0.6815, 'accuracy_cv_std': 0.0227, 'precision_cv_mean': 0.6619, 'precision_cv_std': 0.0387, 'recall_cv_mean': 0.7619, 'recall_cv_std': 0.0757, 'f1_cv_mean': 0.7044, 'f1_cv_std': 0.0175, 'accuracy_test': 0.5634, 'precision_test': 0.3204, 'recall_test': 0.7399, 'f1_score_test': 0.4472}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.933, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0014, 'recall_cv_mean': 0.9367, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9332, 'f1_cv_std': 0.0018, 'params': 160801, 'accuracy_test': 0.9426, 'precision_test': 0.8578, 'recall_test': 0.9105, 'f1_score_test': 0.8833}, 'MLP_2744833': {'accuracy_cv_mean': 0.9438, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9412, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.9467, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.944, 'f1_cv_std': 0.0016, 'params': 2744833, 'accuracy_test': 0.9566, 'precision_test': 0.8952, 'recall_test': 0.9266, 'f1_score_test': 0.9106}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.945, 'precision_cv_std': 0.0049, 'recall_cv_mean': 0.9474, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9462, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9566, 'precision_test': 0.8847, 'recall_test': 0.9409, 'f1_score_test': 0.9119}, 'Logistic Regression': {'accuracy_cv_mean': 0.9287, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.937, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9191, 'recall_cv_std': 0.0045, 'f1_cv_mean': 0.928, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9353, 'precision_test': 0.8262, 'recall_test': 0.9229, 'f1_score_test': 0.8718}, 'SVM': {'accuracy_cv_mean': 0.818, 'accuracy_cv_std': 0.0219, 'precision_cv_mean': 0.7609, 'precision_cv_std': 0.03, 'recall_cv_mean': 0.9306, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.8368, 'f1_cv_std': 0.0159, 'accuracy_test': 0.6576, 'precision_test': 0.4073, 'recall_test': 0.9558, 'f1_score_test': 0.5712}, 'Decision Tree': {'accuracy_cv_mean': 0.8291, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0147, 'recall_cv_mean': 0.7558, 'recall_cv_std': 0.0138, 'f1_cv_mean': 0.8155, 'f1_cv_std': 0.0057, 'accuracy_test': 0.8674, 'precision_test': 0.7033, 'recall_test': 0.7684, 'f1_score_test': 0.7344}, 'Random Forest': {'accuracy_cv_mean': 0.8347, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8736, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.7827, 'recall_cv_std': 0.0068, 'f1_cv_mean': 0.8257, 'f1_cv_std': 0.0048, 'accuracy_test': 0.8568, 'precision_test': 0.6727, 'recall_test': 0.7789, 'f1_score_test': 0.7219}, 'XGBoost': {'accuracy_cv_mean': 0.9063, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.9269, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8821, 'recall_cv_std': 0.0074, 'f1_cv_mean': 0.9039, 'f1_cv_std': 0.0052, 'accuracy_test': 0.9232, 'precision_test': 0.8108, 'recall_test': 0.8845, 'f1_score_test': 0.846}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8737, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9009, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9183, 'precision_test': 0.8043, 'recall_test': 0.869, 'f1_score_test': 0.8354}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8481, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.8437, 'precision_cv_std': 0.0132, 'recall_cv_mean': 0.855, 'recall_cv_std': 0.0146, 'f1_cv_mean': 0.8491, 'f1_cv_std': 0.0025, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.7462, 'recall_test': 0.7798, 'f1_score_test': 0.7626}, 'MLP_338433': {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8547, 'precision_cv_std': 0.0068, 'recall_cv_mean': 0.8544, 'recall_cv_std': 0.014, 'f1_cv_mean': 0.8544, 'f1_cv_std': 0.0047, 'params': 338433, 'accuracy_test': 0.8884, 'precision_test': 0.7512, 'recall_test': 0.7957, 'f1_score_test': 0.7728}, 'MLP_1031169': {'accuracy_cv_mean': 0.8559, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.0191, 'recall_cv_mean': 0.8523, 'recall_cv_std': 0.0175, 'f1_cv_mean': 0.8554, 'f1_cv_std': 0.0037, 'params': 1031169, 'accuracy_test': 0.8998, 'precision_test': 0.8237, 'recall_test': 0.7382, 'f1_score_test': 0.7786}, 'Logistic Regression': {'accuracy_cv_mean': 0.8475, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8313, 'recall_cv_std': 0.0079, 'f1_cv_mean': 0.845, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8608, 'precision_test': 0.6666, 'recall_test': 0.8333, 'f1_score_test': 0.7407}, 'SVM': {'accuracy_cv_mean': 0.6815, 'accuracy_cv_std': 0.0227, 'precision_cv_mean': 0.6619, 'precision_cv_std': 0.0387, 'recall_cv_mean': 0.7619, 'recall_cv_std': 0.0757, 'f1_cv_mean': 0.7044, 'f1_cv_std': 0.0175, 'accuracy_test': 0.5634, 'precision_test': 0.3204, 'recall_test': 0.7399, 'f1_score_test': 0.4472}, 'Decision Tree': {'accuracy_cv_mean': 0.792, 'accuracy_cv_std': 0.0047, 'precision_cv_mean': 0.8112, 'precision_cv_std': 0.0162, 'recall_cv_mean': 0.7622, 'recall_cv_std': 0.0246, 'f1_cv_mean': 0.7855, 'f1_cv_std': 0.0072, 'accuracy_test': 0.8198, 'precision_test': 0.5927, 'recall_test': 0.7829, 'f1_score_test': 0.6747}, 'Random Forest': {'accuracy_cv_mean': 0.8534, 'accuracy_cv_std': 0.0064, 'precision_cv_mean': 0.8862, 'precision_cv_std': 0.0051, 'recall_cv_me/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_1.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 44, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.90      0.92     16465
           1       0.72      0.81      0.76      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.86      0.84     21625
weighted avg       0.89      0.88      0.88     21625

Confusion matrix Test saved as: outputs_cv/2/lyrics_bert/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/2/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8765, 'accuracy_cv_std': 0.0045, 'precision_cv_mean': 0.8911, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8579, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0049}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 44, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.90      0.93     16465
           1       0.74      0.87      0.80      5160

    accuracy                           0.89     21625
   macro avg       0.85      0.89      0.86     21625
weighted avg       0.90      0.89      0.90     21625

Confusion matrix Test saved as: outputs_cv/2/lyrics_bert/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/2/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7796, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.7631, 'precision_cv_std': 0.0021, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.7863, 'f1_cv_std': 0.0035}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.75      0.83     16465
           1       0.51      0.81      0.62      5160

    accuracy                           0.77     21625
   macro avg       0.72      0.78      0.73     21625
weighted avg       0.83      0.77      0.78     21625

Confusion matrix Test saved as: outputs_cv/2/lyrics_bert/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/2/lyrics_bert/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.8765, 'accuracy_cv_std': 0.0045, 'precision_cv_mean': 0.8911, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8579, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8948, 'precision_test': 0.7382, 'recall_test': 0.8667, 'f1_score_test': 0.7973}
Random Forest: {'accuracy_cv_mean': 0.8534, 'accuracy_cv_std': 0.0064, 'precision_cv_mean': 0.8862, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0104, 'f1_cv_mean': 0.8469, 'f1_cv_std': 0.0072, 'accuracy_test': 0.88, 'precision_test': 0.7214, 'recall_test': 0.8101, 'f1_score_test': 0.7632}
Logistic Regression: {'accuracy_cv_mean': 0.8475, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8313, 'recall_cv_std': 0.0079, 'f1_cv_mean': 0.845, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8608, 'precision_test': 0.6666, 'recall_test': 0.8333, 'f1_score_test': 0.7407}
Decision Tree: {'accuracy_cv_mean': 0.792, 'accuracy_cv_std': 0.0047, 'precision_cv_mean': 0.8112, 'precision_cv_std': 0.0162, 'recall_cv_mean': 0.7622, 'recall_cv_std': 0.0246, 'f1_cv_mean': 0.7855, 'f1_cv_std': 0.0072, 'accuracy_test': 0.8198, 'precision_test': 0.5927, 'recall_test': 0.7829, 'f1_score_test': 0.6747}
Naive Bayes: {'accuracy_cv_mean': 0.7796, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.7631, 'precision_cv_std': 0.0021, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.7863, 'f1_cv_std': 0.0035, 'accuracy_test': 0.7662, 'precision_test': 0.5062, 'recall_test': 0.8091, 'f1_score_test': 0.6228}
SVM: {'accuracy_cv_mean': 0.6815, 'accuracy_cv_std': 0.0227, 'precision_cv_mean': 0.6619, 'precision_cv_std': 0.0387, 'recall_cv_mean': 0.7619, 'recall_cv_std': 0.0757, 'f1_cv_mean': 0.7044, 'f1_cv_std': 0.0175, 'accuracy_test': 0.5634, 'precision_test': 0.3204, 'recall_test': 0.7399, 'f1_score_test': 0.4472}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.933, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0014, 'recall_cv_mean': 0.9367, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9332, 'f1_cv_std': 0.0018, 'params': 160801, 'accuracy_test': 0.9426, 'precision_test': 0.8578, 'recall_test': 0.9105, 'f1_score_test': 0.8833}, 'MLP_2744833': {'accuracy_cv_mean': 0.9438, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9412, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.9467, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.944, 'f1_cv_std': 0.0016, 'params': 2744833, 'accuracy_test': 0.9566, 'precision_test': 0.8952, 'recall_test': 0.9266, 'f1_score_test': 0.9106}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.945, 'precision_cv_std': 0.0049, 'recall_cv_mean': 0.9474, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9462, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9566, 'precision_test': 0.8847, 'recall_test': 0.9409, 'f1_score_test': 0.9119}, 'Logistic Regression': {'accuracy_cv_mean': 0.9287, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.937, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9191, 'recall_cv_std': 0.0045, 'f1_cv_mean': 0.928, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9353, 'precision_test': 0.8262, 'recall_test': 0.9229, 'f1_score_test': 0.8718}, 'SVM': {'accuracy_cv_mean': 0.818, 'accuracy_cv_std': 0.0219, 'precision_cv_mean': 0.7609, 'precision_cv_std': 0.03, 'recall_cv_mean': 0.9306, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.8368, 'f1_cv_std': 0.0159, 'accuracy_test': 0.6576, 'precision_test': 0.4073, 'recall_test': 0.9558, 'f1_score_test': 0.5712}, 'Decision Tree': {'accuracy_cv_mean': 0.8291, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0147, 'recall_cv_mean': 0.7558, 'recall_cv_std': 0.0138, 'f1_cv_mean': 0.8155, 'f1_cv_std': 0.0057, 'accuracy_test': 0.8674, 'precision_test': 0.7033, 'recall_test': 0.7684, 'f1_score_test': 0.7344}, 'Random Forest': {'accuracy_cv_mean': 0.8347, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8736, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.7827, 'recall_cv_std': 0.0068, 'f1_cv_mean': 0.8257, 'f1_cv_std': 0.0048, 'accuracy_test': 0.8568, 'precision_test': 0.6727, 'recall_test': 0.7789, 'f1_score_test': 0.7219}, 'XGBoost': {'accuracy_cv_mean': 0.9063, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.9269, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8821, 'recall_cv_std': 0.0074, 'f1_cv_mean': 0.9039, 'f1_cv_std': 0.0052, 'accuracy_test': 0.9232, 'precision_test': 0.8108, 'recall_test': 0.8845, 'f1_score_test': 0.846}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8737, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9009, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9183, 'precision_test': 0.8043, 'recall_test': 0.869, 'f1_score_test': 0.8354}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8481, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.8437, 'precision_cv_std': 0.0132, 'recall_cv_mean': 0.855, 'recall_cv_std': 0.0146, 'f1_cv_mean': 0.8491, 'f1_cv_std': 0.0025, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.7462, 'recall_test': 0.7798, 'f1_score_test': 0.7626}, 'MLP_338433': {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8547, 'precision_cv_std': 0.0068, 'recall_cv_mean': 0.8544, 'recall_cv_std': 0.014, 'f1_cv_mean': 0.8544, 'f1_cv_std': 0.0047, 'params': 338433, 'accuracy_test': 0.8884, 'precision_test': 0.7512, 'recall_test': 0.7957, 'f1_score_test': 0.7728}, 'MLP_1031169': {'accuracy_cv_mean': 0.8559, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.0191, 'recall_cv_mean': 0.8523, 'recall_cv_std': 0.0175, 'f1_cv_mean': 0.8554, 'f1_cv_std': 0.0037, 'params': 1031169, 'accuracy_test': 0.8998, 'precision_test': 0.8237, 'recall_test': 0.7382, 'f1_score_test': 0.7786}, 'Logistic Regression': {'accuracy_cv_mean': 0.8475, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8313, 'recall_cv_std': 0.0079, 'f1_cv_mean': 0.845, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8608, 'precision_test': 0.6666, 'recall_test': 0.8333, 'f1_score_test': 0.7407}, 'SVM': {'accuracy_cv_mean': 0.6815, 'accuracy_cv_std': 0.0227, 'precision_cv_mean': 0.6619, 'precision_cv_std': 0.0387, 'recall_cv_mean': 0.7619, 'recall_cv_std': 0.0757, 'f1_cv_mean': 0.7044, 'f1_cv_std': 0.0175, 'accuracy_test': 0.5634, 'precision_test': 0.3204, 'recall_test': 0.7399, 'f1_score_test': 0.4472}, 'Decision Tree': {'accuracy_cv_mean': 0.792, 'accuracy_cv_std': 0.0047, 'precision_cv_mean': 0.8112, 'precision_cv_std': 0.0162, 'recall_cv_mean': 0.7622, 'recall_cv_std': 0.0246, 'f1_cv_mean': 0.7855, 'f1_cv_std': 0.0072, 'accuracy_test': 0.8198, 'precision_test': 0.5927, 'recall_test': 0.7829, 'f1_score_test': 0.6747}, 'Random Forest': {'accuracy_cv_mean': 0.8534, 'accuracy_cv_std': 0.0064, 'precision_cv_mean': 0.8862, 'precision_cv_std': 0.0051, 'recall_cv_me/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_1.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
an': 0.811, 'recall_cv_std': 0.0104, 'f1_cv_mean': 0.8469, 'f1_cv_std': 0.0072, 'accuracy_test': 0.88, 'precision_test': 0.7214, 'recall_test': 0.8101, 'f1_score_test': 0.7632}, 'XGBoost': {'accuracy_cv_mean': 0.8765, 'accuracy_cv_std': 0.0045, 'precision_cv_mean': 0.8911, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8579, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8948, 'precision_test': 0.7382, 'recall_test': 0.8667, 'f1_score_test': 0.7973}, 'Naive Bayes': {'accuracy_cv_mean': 0.7796, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.7631, 'precision_cv_std': 0.0021, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.7863, 'f1_cv_std': 0.0035, 'accuracy_test': 0.7662, 'precision_test': 0.5062, 'recall_test': 0.8091, 'f1_score_test': 0.6228}}}
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 1559)
Shape of X_test after concatenation:  (21625, 1559)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1559)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1559)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [1559, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3955, Test Loss: 0.3075, F1: 0.8644, AUC: 0.9415
Epoch [10/30] Train Loss: 0.2580, Test Loss: 0.2754, F1: 0.8881, AUC: 0.9593
Epoch [20/30] Train Loss: 0.2489, Test Loss: 0.2561, F1: 0.8948, AUC: 0.9614
Mejores resultados en la época:  27
f1-score 0.8970445736434108
AUC según el mejor F1-score 0.9623824791741782

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4554, Test Loss: 0.3484, F1: 0.8432, AUC: 0.9272
Epoch [10/30] Train Loss: 0.2636, Test Loss: 0.2692, F1: 0.8787, AUC: 0.9555
Epoch [20/30] Train Loss: 0.2516, Test Loss: 0.2830, F1: 0.8690, AUC: 0.9590
Mejores resultados en la época:  24
f1-score 0.8915315315315315
AUC según el mejor F1-score 0.9598451830662971

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4088, Test Loss: 0.3160, F1: 0.8666, AUC: 0.9395
Epoch [10/30] Train Loss: 0.2620, Test Loss: 0.2849, F1: 0.8730, AUC: 0.9597
Epoch [20/30] Train Loss: 0.2511, Test Loss: 0.2565, F1: 0.8927, AUC: 0.9622
Mejores resultados en la época:  27
f1-score 0.8977746043079853
AUC según el mejor F1-score 0.9630417953022656

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4118, Test Loss: 0.3229, F1: 0.8662, AUC: 0.9396
Epoch [10/30] Train Loss: 0.2643, Test Loss: 0.2774, F1: 0.8738, AUC: 0.9610
Epoch [20/30] Train Loss: 0.2499, Test Loss: 0.2553, F1: 0.8944, AUC: 0.9632
Mejores resultados en la época:  28
f1-score 0.8966281073098695
AUC según el mejor F1-score 0.9640657548231255

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4219, Test Loss: 0.3375, F1: 0.8513, AUC: 0.9335
Epoch [10/30] Train Loss: 0.2594, Test Loss: 0.2728, F1: 0.8897, AUC: 0.9583
Epoch [20/30] Train Loss: 0.2499, Test Loss: 0.2507, F1: 0.8961, AUC: 0.9614
Mejores resultados en la época:  28
f1-score 0.8990847497919886
AUC según el mejor F1-score 0.9624764384850755
Epoch [0/30] Train Loss: 0.3849, Test Loss: 0.2689, F1: 0.7889, AUC: 0.9419
Epoch [10/30] Train Loss: 0.2583, Test Loss: 0.2228, F1: 0.8218, AUC: 0.9607
Epoch [20/30] Train Loss: 0.2438, Test Loss: 0.2759, F1: 0.7954, AUC: 0.9629
Mejores resultados en la época:  21
f1-score 0.8297225762180178
AUC según el mejor F1-score 0.9632972513930183
Confusion matrix Test saved: outputs_cv/2/gpt/cm_mlp_1.png

========================================
Entrenando red 5 con capas [1559, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3911, Test Loss: 0.2967, F1: 0.8754, AUC: 0.9460
Epoch [10/30] Train Loss: 0.2616, Test Loss: 0.2815, F1: 0.8870, AUC: 0.9603
Epoch [20/30] Train Loss: 0.2406, Test Loss: 0.2429, F1: 0.8943, AUC: 0.9641
Mejores resultados en la época:  27
f1-score 0.8966767371601209
AUC según el mejor F1-score 0.9650838854335678

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3718, Test Loss: 0.3247, F1: 0.8667, AUC: 0.9468
Epoch [10/30] Train Loss: 0.2615, Test Loss: 0.2686, F1: 0.8764, AUC: 0.9597
Epoch [20/30] Train Loss: 0.2543, Test Loss: 0.2550, F1: 0.8916, AUC: 0.9631
Mejores resultados en la época:  26
f1-score 0.895447484765205
AUC según el mejor F1-score 0.9640490202568206

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3975, Test Loss: 0.3136, F1: 0.8589, AUC: 0.9463
Epoch [10/30] Train Loss: 0.2604, Test Loss: 0.2565, F1: 0.8947, AUC: 0.9608
Epoch [20/30] Train Loss: 0.2512, Test Loss: 0.3313, F1: 0.8392, AUC: 0.9637
Mejores resultados en la época:  27
f1-score 0.9010164569215876
AUC según el mejor F1-score 0.9653746065816355

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3948, Test Loss: 0.3180, F1: 0.8520, AUC: 0.9486
Epoch [10/30] Train Loss: 0.2704, Test Loss: 0.2672, F1: 0.8789, AUC: 0.9625
Epoch [20/30] Train Loss: 0.2418, Test Loss: 0.2377, F1: 0.8959, AUC: 0.9656
Mejores resultados en la época:  25
f1-score 0.8972816935289872
AUC según el mejor F1-score 0.965962738526587

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3909, Test Loss: 0.3053, F1: 0.8681, AUC: 0.9429
Epoch [10/30] Train Loss: 0.2613, Test Loss: 0.2623, F1: 0.8839, AUC: 0.9605
Epoch [20/30] Train Loss: 0.2407, Test Loss: 0.2446, F1: 0.8933, AUC: 0.9641
Mejores resultados en la época:  26
f1-score 0.8989861976303897
AUC según el mejor F1-score 0.9650392081452639
Epoch [0/30] Train Loss: 0.3726, Test Loss: 0.2980, F1: 0.7846, AUC: 0.9487
Epoch [10/30] Train Loss: 0.2575, Test Loss: 0.3269, F1: 0.7461, AUC: 0.9621
Epoch [20/30] Train Loss: 0.2504, Test Loss: 0.2327, F1: 0.8235, AUC: 0.9652
Mejores resultados en la época:  16
f1-score 0.8307086614173228
AUC según el mejor F1-score 0.9643644376019603
Confusion matrix Test saved: outputs_cv/2/gpt/cm_mlp_5.png

========================================
Entrenando red 6 con capas [1559, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3949, Test Loss: 0.3099, F1: 0.8660, AUC: 0.9463
Epoch [10/30] Train Loss: 0.2626, Test Loss: 0.2882, F1: 0.8887, AUC: 0.9604
Epoch [20/30] Train Loss: 0.2450, Test Loss: 0.3212, F1: 0.8814, AUC: 0.9633
Mejores resultados en la época:  29
f1-score 0.8982988618284176
AUC según el mejor F1-score 0.9659752385861727

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3909, Test Loss: 0.3095, F1: 0.8686, AUC: 0.9434
Epoch [10/30] Train Loss: 0.2632, Test Loss: 0.2585, F1: 0.8833, AUC: 0.9596
Epoch [20/30] Train Loss: 0.2434, Test Loss: 0.2470, F1: 0.8903, AUC: 0.9634
Mejores resultados en la época:  25
f1-score 0.8968556455454979
AUC según el mejor F1-score 0.9641306205306172

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4107, Test Loss: 0.3162, F1: 0.8576, AUC: 0.9460
Epoch [10/30] Train Loss: 0.2593, Test Loss: 0.2691, F1: 0.8932, AUC: 0.9619
Epoch [20/30] Train Loss: 0.2490, Test Loss: 0.2420, F1: 0.8980, AUC: 0.9649
Mejores resultados en la época:  28
f1-score 0.9007798440311937
an': 0.811, 'recall_cv_std': 0.0104, 'f1_cv_mean': 0.8469, 'f1_cv_std': 0.0072, 'accuracy_test': 0.88, 'precision_test': 0.7214, 'recall_test': 0.8101, 'f1_score_test': 0.7632}, 'XGBoost': {'accuracy_cv_mean': 0.8765, 'accuracy_cv_std': 0.0045, 'precision_cv_mean': 0.8911, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8579, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8948, 'precision_test': 0.7382, 'recall_test': 0.8667, 'f1_score_test': 0.7973}, 'Naive Bayes': {'accuracy_cv_mean': 0.7796, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.7631, 'precision_cv_std': 0.0021, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.7863, 'f1_cv_std': 0.0035, 'accuracy_test': 0.7662, 'precision_test': 0.5062, 'recall_test': 0.8091, 'f1_score_test': 0.6228}}}
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 1559)
Shape of X_test after concatenation:  (21625, 1559)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1559)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1559)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [1559, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3955, Test Loss: 0.3075, F1: 0.8644, AUC: 0.9415
Epoch [10/30] Train Loss: 0.2580, Test Loss: 0.2754, F1: 0.8881, AUC: 0.9593
Epoch [20/30] Train Loss: 0.2489, Test Loss: 0.2561, F1: 0.8948, AUC: 0.9614
Mejores resultados en la época:  27
f1-score 0.8970445736434108
AUC según el mejor F1-score 0.9623824791741782

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4554, Test Loss: 0.3484, F1: 0.8432, AUC: 0.9272
Epoch [10/30] Train Loss: 0.2636, Test Loss: 0.2692, F1: 0.8787, AUC: 0.9555
Epoch [20/30] Train Loss: 0.2516, Test Loss: 0.2830, F1: 0.8690, AUC: 0.9590
Mejores resultados en la época:  24
f1-score 0.8915315315315315
AUC según el mejor F1-score 0.9598451830662971

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4088, Test Loss: 0.3160, F1: 0.8666, AUC: 0.9395
Epoch [10/30] Train Loss: 0.2620, Test Loss: 0.2849, F1: 0.8730, AUC: 0.9597
Epoch [20/30] Train Loss: 0.2511, Test Loss: 0.2565, F1: 0.8927, AUC: 0.9622
Mejores resultados en la época:  27
f1-score 0.8977746043079853
AUC según el mejor F1-score 0.9630417953022656

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4118, Test Loss: 0.3229, F1: 0.8662, AUC: 0.9396
Epoch [10/30] Train Loss: 0.2643, Test Loss: 0.2774, F1: 0.8738, AUC: 0.9610
Epoch [20/30] Train Loss: 0.2499, Test Loss: 0.2553, F1: 0.8944, AUC: 0.9632
Mejores resultados en la época:  28
f1-score 0.8966281073098695
AUC según el mejor F1-score 0.9640657548231255

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4219, Test Loss: 0.3375, F1: 0.8513, AUC: 0.9335
Epoch [10/30] Train Loss: 0.2594, Test Loss: 0.2728, F1: 0.8897, AUC: 0.9583
Epoch [20/30] Train Loss: 0.2499, Test Loss: 0.2507, F1: 0.8961, AUC: 0.9614
Mejores resultados en la época:  28
f1-score 0.8990847497919886
AUC según el mejor F1-score 0.9624764384850755
Epoch [0/30] Train Loss: 0.3849, Test Loss: 0.2689, F1: 0.7889, AUC: 0.9419
Epoch [10/30] Train Loss: 0.2583, Test Loss: 0.2228, F1: 0.8218, AUC: 0.9607
Epoch [20/30] Train Loss: 0.2438, Test Loss: 0.2759, F1: 0.7954, AUC: 0.9629
Mejores resultados en la época:  21
f1-score 0.8297225762180178
AUC según el mejor F1-score 0.9632972513930183
Confusion matrix Test saved: outputs_cv/2/gpt/cm_mlp_1.png

========================================
Entrenando red 5 con capas [1559, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3911, Test Loss: 0.2967, F1: 0.8754, AUC: 0.9460
Epoch [10/30] Train Loss: 0.2616, Test Loss: 0.2815, F1: 0.8870, AUC: 0.9603
Epoch [20/30] Train Loss: 0.2406, Test Loss: 0.2429, F1: 0.8943, AUC: 0.9641
Mejores resultados en la época:  27
f1-score 0.8966767371601209
AUC según el mejor F1-score 0.9650838854335678

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3718, Test Loss: 0.3247, F1: 0.8667, AUC: 0.9468
Epoch [10/30] Train Loss: 0.2615, Test Loss: 0.2686, F1: 0.8764, AUC: 0.9597
Epoch [20/30] Train Loss: 0.2543, Test Loss: 0.2550, F1: 0.8916, AUC: 0.9631
Mejores resultados en la época:  26
f1-score 0.895447484765205
AUC según el mejor F1-score 0.9640490202568206

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3975, Test Loss: 0.3136, F1: 0.8589, AUC: 0.9463
Epoch [10/30] Train Loss: 0.2604, Test Loss: 0.2565, F1: 0.8947, AUC: 0.9608
Epoch [20/30] Train Loss: 0.2512, Test Loss: 0.3313, F1: 0.8392, AUC: 0.9637
Mejores resultados en la época:  27
f1-score 0.9010164569215876
AUC según el mejor F1-score 0.9653746065816355

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3948, Test Loss: 0.3180, F1: 0.8520, AUC: 0.9486
Epoch [10/30] Train Loss: 0.2704, Test Loss: 0.2672, F1: 0.8789, AUC: 0.9625
Epoch [20/30] Train Loss: 0.2418, Test Loss: 0.2377, F1: 0.8959, AUC: 0.9656
Mejores resultados en la época:  25
f1-score 0.8972816935289872
AUC según el mejor F1-score 0.965962738526587

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3909, Test Loss: 0.3053, F1: 0.8681, AUC: 0.9429
Epoch [10/30] Train Loss: 0.2613, Test Loss: 0.2623, F1: 0.8839, AUC: 0.9605
Epoch [20/30] Train Loss: 0.2407, Test Loss: 0.2446, F1: 0.8933, AUC: 0.9641
Mejores resultados en la época:  26
f1-score 0.8989861976303897
AUC según el mejor F1-score 0.9650392081452639
Epoch [0/30] Train Loss: 0.3726, Test Loss: 0.2980, F1: 0.7846, AUC: 0.9487
Epoch [10/30] Train Loss: 0.2575, Test Loss: 0.3269, F1: 0.7461, AUC: 0.9621
Epoch [20/30] Train Loss: 0.2504, Test Loss: 0.2327, F1: 0.8235, AUC: 0.9652
Mejores resultados en la época:  16
f1-score 0.8307086614173228
AUC según el mejor F1-score 0.9643644376019603
Confusion matrix Test saved: outputs_cv/2/gpt/cm_mlp_5.png

========================================
Entrenando red 6 con capas [1559, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3949, Test Loss: 0.3099, F1: 0.8660, AUC: 0.9463
Epoch [10/30] Train Loss: 0.2626, Test Loss: 0.2882, F1: 0.8887, AUC: 0.9604
Epoch [20/30] Train Loss: 0.2450, Test Loss: 0.3212, F1: 0.8814, AUC: 0.9633
Mejores resultados en la época:  29
f1-score 0.8982988618284176
AUC según el mejor F1-score 0.9659752385861727

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3909, Test Loss: 0.3095, F1: 0.8686, AUC: 0.9434
Epoch [10/30] Train Loss: 0.2632, Test Loss: 0.2585, F1: 0.8833, AUC: 0.9596
Epoch [20/30] Train Loss: 0.2434, Test Loss: 0.2470, F1: 0.8903, AUC: 0.9634
Mejores resultados en la época:  25
f1-score 0.8968556455454979
AUC según el mejor F1-score 0.9641306205306172

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4107, Test Loss: 0.3162, F1: 0.8576, AUC: 0.9460
Epoch [10/30] Train Loss: 0.2593, Test Loss: 0.2691, F1: 0.8932, AUC: 0.9619
Epoch [20/30] Train Loss: 0.2490, Test Loss: 0.2420, F1: 0.8980, AUC: 0.9649
Mejores resultados en la época:  28
f1-score 0.9007798440311937
AUC según el mejor F1-score 0.9655945546767022

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4066, Test Loss: 0.3090, F1: 0.8723, AUC: 0.9469
Epoch [10/30] Train Loss: 0.2624, Test Loss: 0.2487, F1: 0.8907, AUC: 0.9628
Epoch [20/30] Train Loss: 0.2439, Test Loss: 0.2613, F1: 0.8831, AUC: 0.9651
Mejores resultados en la época:  22
f1-score 0.8960784313725491
AUC según el mejor F1-score 0.96545661206312

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3872, Test Loss: 0.3413, F1: 0.8352, AUC: 0.9458
Epoch [10/30] Train Loss: 0.2692, Test Loss: 0.2986, F1: 0.8611, AUC: 0.9598
Epoch [20/30] Train Loss: 0.2508, Test Loss: 0.2479, F1: 0.8970, AUC: 0.9632
Mejores resultados en la época:  29
f1-score 0.8987463837994214
AUC según el mejor F1-score 0.9647767091548753
Epoch [0/30] Train Loss: 0.3864, Test Loss: 0.3911, F1: 0.7366, AUC: 0.9463
Epoch [10/30] Train Loss: 0.2630, Test Loss: 0.2608, F1: 0.7992, AUC: 0.9621
Epoch [20/30] Train Loss: 0.2420, Test Loss: 0.2047, F1: 0.8286, AUC: 0.9652
Mejores resultados en la época:  17
f1-score 0.8289396602226128
AUC según el mejor F1-score 0.9645781102503078
Confusion matrix Test saved: outputs_cv/2/gpt/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.933, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0014, 'recall_cv_mean': 0.9367, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9332, 'f1_cv_std': 0.0018, 'params': 160801, 'accuracy_test': 0.9426, 'precision_test': 0.8578, 'recall_test': 0.9105, 'f1_score_test': 0.8833}, 'MLP_2744833': {'accuracy_cv_mean': 0.9438, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9412, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.9467, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.944, 'f1_cv_std': 0.0016, 'params': 2744833, 'accuracy_test': 0.9566, 'precision_test': 0.8952, 'recall_test': 0.9266, 'f1_score_test': 0.9106}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.945, 'precision_cv_std': 0.0049, 'recall_cv_mean': 0.9474, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9462, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9566, 'precision_test': 0.8847, 'recall_test': 0.9409, 'f1_score_test': 0.9119}, 'Logistic Regression': {'accuracy_cv_mean': 0.9287, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.937, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9191, 'recall_cv_std': 0.0045, 'f1_cv_mean': 0.928, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9353, 'precision_test': 0.8262, 'recall_test': 0.9229, 'f1_score_test': 0.8718}, 'SVM': {'accuracy_cv_mean': 0.818, 'accuracy_cv_std': 0.0219, 'precision_cv_mean': 0.7609, 'precision_cv_std': 0.03, 'recall_cv_mean': 0.9306, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.8368, 'f1_cv_std': 0.0159, 'accuracy_test': 0.6576, 'precision_test': 0.4073, 'recall_test': 0.9558, 'f1_score_test': 0.5712}, 'Decision Tree': {'accuracy_cv_mean': 0.8291, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0147, 'recall_cv_mean': 0.7558, 'recall_cv_std': 0.0138, 'f1_cv_mean': 0.8155, 'f1_cv_std': 0.0057, 'accuracy_test': 0.8674, 'precision_test': 0.7033, 'recall_test': 0.7684, 'f1_score_test': 0.7344}, 'Random Forest': {'accuracy_cv_mean': 0.8347, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8736, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.7827, 'recall_cv_std': 0.0068, 'f1_cv_mean': 0.8257, 'f1_cv_std': 0.0048, 'accuracy_test': 0.8568, 'precision_test': 0.6727, 'recall_test': 0.7789, 'f1_score_test': 0.7219}, 'XGBoost': {'accuracy_cv_mean': 0.9063, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.9269, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8821, 'recall_cv_std': 0.0074, 'f1_cv_mean': 0.9039, 'f1_cv_std': 0.0052, 'accuracy_test': 0.9232, 'precision_test': 0.8108, 'recall_test': 0.8845, 'f1_score_test': 0.846}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8737, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9009, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9183, 'precision_test': 0.8043, 'recall_test': 0.869, 'f1_score_test': 0.8354}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8481, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.8437, 'precision_cv_std': 0.0132, 'recall_cv_mean': 0.855, 'recall_cv_std': 0.0146, 'f1_cv_mean': 0.8491, 'f1_cv_std': 0.0025, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.7462, 'recall_test': 0.7798, 'f1_score_test': 0.7626}, 'MLP_338433': {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8547, 'precision_cv_std': 0.0068, 'recall_cv_mean': 0.8544, 'recall_cv_std': 0.014, 'f1_cv_mean': 0.8544, 'f1_cv_std': 0.0047, 'params': 338433, 'accuracy_test': 0.8884, 'precision_test': 0.7512, 'recall_test': 0.7957, 'f1_score_test': 0.7728}, 'MLP_1031169': {'accuracy_cv_mean': 0.8559, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.0191, 'recall_cv_mean': 0.8523, 'recall_cv_std': 0.0175, 'f1_cv_mean': 0.8554, 'f1_cv_std': 0.0037, 'params': 1031169, 'accuracy_test': 0.8998, 'precision_test': 0.8237, 'recall_test': 0.7382, 'f1_score_test': 0.7786}, 'Logistic Regression': {'accuracy_cv_mean': 0.8475, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8313, 'recall_cv_std': 0.0079, 'f1_cv_mean': 0.845, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8608, 'precision_test': 0.6666, 'recall_test': 0.8333, 'f1_score_test': 0.7407}, 'SVM': {'accuracy_cv_mean': 0.6815, 'accuracy_cv_std': 0.0227, 'precision_cv_mean': 0.6619, 'precision_cv_std': 0.0387, 'recall_cv_mean': 0.7619, 'recall_cv_std': 0.0757, 'f1_cv_mean': 0.7044, 'f1_cv_std': 0.0175, 'accuracy_test': 0.5634, 'precision_test': 0.3204, 'recall_test': 0.7399, 'f1_score_test': 0.4472}, 'Decision Tree': {'accuracy_cv_mean': 0.792, 'accuracy_cv_std': 0.0047, 'precision_cv_mean': 0.8112, 'precision_cv_std': 0.0162, 'recall_cv_mean': 0.7622, 'recall_cv_std': 0.0246, 'f1_cv_mean': 0.7855, 'f1_cv_std': 0.0072, 'accuracy_test': 0.8198, 'precision_test': 0.5927, 'recall_test': 0.7829, 'f1_score_test': 0.6747}, 'Random Forest': {'accuracy_cv_mean': 0.8534, 'accuracy_cv_std': 0.0064, 'precision_cv_mean': 0.8862, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0104, 'f1_cv_mean': 0.8469, 'f1_cv_std': 0.0072, 'accuracy_test': 0.88, 'precision_test': 0.7214, 'recall_test': 0.8101, 'f1_score_test': 0.7632}, 'XGBoost': {'accuracy_cv_mean': 0.8765, 'accuracy_cv_std': 0.0045, 'precision_cv_mean': 0.8911, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8579, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8948, 'precision_test': 0.7382, 'recall_test': 0.8667, 'f1_score_test': 0.7973}, 'Naive Bayes': {'accuracy_cv_mean': 0.7796, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.7631, 'precision_cv_std': 0.0021, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.7863, 'f1_cv_std': 0.0035, 'accuracy_test': 0.7662, 'precision_test': 0.5062, 'recall_test': 0.8091, 'f1_score_test': 0.6228}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8958, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8914, 'precision_cv_std': 0.0112, 'recall_cv_mean': 0.9017, 'recall_cv_std': 0.0122, 'f1_cv_mean': 0.8964, 'f1_cv_std': 0.0026, 'params': 49953, 'accuracy_test': 0.9197, 'precision_test': 0.8395, 'recall_test': 0.8202, 'f1_score_test': 0.8297}, 'MLP_971265': {'accuracy_cv_mean': 0.8975, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8952, 'precision_cv_std': 0.0079, 'recall_cv_mean': 0.9007, 'recall_cv_std': 0.0055, 'f1_cv_mean': 0.8979, 'f1_cv_std': 0.0019, 'params': 971265, 'accuracy_test': 0.9205, 'precision_test': 0.844, 'recall_test': 0.8178, 'f1_score_test': 0.8307}, 'MLP_2296833': {'accuracy_cv_mean': 0.898, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.8966, 'precision_cv_std': 0.0095, 'recall_cv_mean': 0.8999, 'recall_cv_std': 0.0106, 'f1_cv_mean': 0.8982, 'f1_cv_std': 0.0016, 'params': 2296833, 'accuracy_test': 0.919, 'precision_test': 0.8353, 'recall_test': 0.8227, 'f1_score_test': 0.8289}}}
Saved on: outputs_cv/2/gpt

==============================
Model: Logistic Regression

/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
AUC según el mejor F1-score 0.9655945546767022

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4066, Test Loss: 0.3090, F1: 0.8723, AUC: 0.9469
Epoch [10/30] Train Loss: 0.2624, Test Loss: 0.2487, F1: 0.8907, AUC: 0.9628
Epoch [20/30] Train Loss: 0.2439, Test Loss: 0.2613, F1: 0.8831, AUC: 0.9651
Mejores resultados en la época:  22
f1-score 0.8960784313725491
AUC según el mejor F1-score 0.96545661206312

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3872, Test Loss: 0.3413, F1: 0.8352, AUC: 0.9458
Epoch [10/30] Train Loss: 0.2692, Test Loss: 0.2986, F1: 0.8611, AUC: 0.9598
Epoch [20/30] Train Loss: 0.2508, Test Loss: 0.2479, F1: 0.8970, AUC: 0.9632
Mejores resultados en la época:  29
f1-score 0.8987463837994214
AUC según el mejor F1-score 0.9647767091548753
Epoch [0/30] Train Loss: 0.3864, Test Loss: 0.3911, F1: 0.7366, AUC: 0.9463
Epoch [10/30] Train Loss: 0.2630, Test Loss: 0.2608, F1: 0.7992, AUC: 0.9621
Epoch [20/30] Train Loss: 0.2420, Test Loss: 0.2047, F1: 0.8286, AUC: 0.9652
Mejores resultados en la época:  17
f1-score 0.8289396602226128
AUC según el mejor F1-score 0.9645781102503078
Confusion matrix Test saved: outputs_cv/2/gpt/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.933, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0014, 'recall_cv_mean': 0.9367, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9332, 'f1_cv_std': 0.0018, 'params': 160801, 'accuracy_test': 0.9426, 'precision_test': 0.8578, 'recall_test': 0.9105, 'f1_score_test': 0.8833}, 'MLP_2744833': {'accuracy_cv_mean': 0.9438, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9412, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.9467, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.944, 'f1_cv_std': 0.0016, 'params': 2744833, 'accuracy_test': 0.9566, 'precision_test': 0.8952, 'recall_test': 0.9266, 'f1_score_test': 0.9106}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.945, 'precision_cv_std': 0.0049, 'recall_cv_mean': 0.9474, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9462, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9566, 'precision_test': 0.8847, 'recall_test': 0.9409, 'f1_score_test': 0.9119}, 'Logistic Regression': {'accuracy_cv_mean': 0.9287, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.937, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9191, 'recall_cv_std': 0.0045, 'f1_cv_mean': 0.928, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9353, 'precision_test': 0.8262, 'recall_test': 0.9229, 'f1_score_test': 0.8718}, 'SVM': {'accuracy_cv_mean': 0.818, 'accuracy_cv_std': 0.0219, 'precision_cv_mean': 0.7609, 'precision_cv_std': 0.03, 'recall_cv_mean': 0.9306, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.8368, 'f1_cv_std': 0.0159, 'accuracy_test': 0.6576, 'precision_test': 0.4073, 'recall_test': 0.9558, 'f1_score_test': 0.5712}, 'Decision Tree': {'accuracy_cv_mean': 0.8291, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0147, 'recall_cv_mean': 0.7558, 'recall_cv_std': 0.0138, 'f1_cv_mean': 0.8155, 'f1_cv_std': 0.0057, 'accuracy_test': 0.8674, 'precision_test': 0.7033, 'recall_test': 0.7684, 'f1_score_test': 0.7344}, 'Random Forest': {'accuracy_cv_mean': 0.8347, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8736, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.7827, 'recall_cv_std': 0.0068, 'f1_cv_mean': 0.8257, 'f1_cv_std': 0.0048, 'accuracy_test': 0.8568, 'precision_test': 0.6727, 'recall_test': 0.7789, 'f1_score_test': 0.7219}, 'XGBoost': {'accuracy_cv_mean': 0.9063, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.9269, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8821, 'recall_cv_std': 0.0074, 'f1_cv_mean': 0.9039, 'f1_cv_std': 0.0052, 'accuracy_test': 0.9232, 'precision_test': 0.8108, 'recall_test': 0.8845, 'f1_score_test': 0.846}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8737, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9009, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9183, 'precision_test': 0.8043, 'recall_test': 0.869, 'f1_score_test': 0.8354}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8481, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.8437, 'precision_cv_std': 0.0132, 'recall_cv_mean': 0.855, 'recall_cv_std': 0.0146, 'f1_cv_mean': 0.8491, 'f1_cv_std': 0.0025, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.7462, 'recall_test': 0.7798, 'f1_score_test': 0.7626}, 'MLP_338433': {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8547, 'precision_cv_std': 0.0068, 'recall_cv_mean': 0.8544, 'recall_cv_std': 0.014, 'f1_cv_mean': 0.8544, 'f1_cv_std': 0.0047, 'params': 338433, 'accuracy_test': 0.8884, 'precision_test': 0.7512, 'recall_test': 0.7957, 'f1_score_test': 0.7728}, 'MLP_1031169': {'accuracy_cv_mean': 0.8559, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.0191, 'recall_cv_mean': 0.8523, 'recall_cv_std': 0.0175, 'f1_cv_mean': 0.8554, 'f1_cv_std': 0.0037, 'params': 1031169, 'accuracy_test': 0.8998, 'precision_test': 0.8237, 'recall_test': 0.7382, 'f1_score_test': 0.7786}, 'Logistic Regression': {'accuracy_cv_mean': 0.8475, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8313, 'recall_cv_std': 0.0079, 'f1_cv_mean': 0.845, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8608, 'precision_test': 0.6666, 'recall_test': 0.8333, 'f1_score_test': 0.7407}, 'SVM': {'accuracy_cv_mean': 0.6815, 'accuracy_cv_std': 0.0227, 'precision_cv_mean': 0.6619, 'precision_cv_std': 0.0387, 'recall_cv_mean': 0.7619, 'recall_cv_std': 0.0757, 'f1_cv_mean': 0.7044, 'f1_cv_std': 0.0175, 'accuracy_test': 0.5634, 'precision_test': 0.3204, 'recall_test': 0.7399, 'f1_score_test': 0.4472}, 'Decision Tree': {'accuracy_cv_mean': 0.792, 'accuracy_cv_std': 0.0047, 'precision_cv_mean': 0.8112, 'precision_cv_std': 0.0162, 'recall_cv_mean': 0.7622, 'recall_cv_std': 0.0246, 'f1_cv_mean': 0.7855, 'f1_cv_std': 0.0072, 'accuracy_test': 0.8198, 'precision_test': 0.5927, 'recall_test': 0.7829, 'f1_score_test': 0.6747}, 'Random Forest': {'accuracy_cv_mean': 0.8534, 'accuracy_cv_std': 0.0064, 'precision_cv_mean': 0.8862, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0104, 'f1_cv_mean': 0.8469, 'f1_cv_std': 0.0072, 'accuracy_test': 0.88, 'precision_test': 0.7214, 'recall_test': 0.8101, 'f1_score_test': 0.7632}, 'XGBoost': {'accuracy_cv_mean': 0.8765, 'accuracy_cv_std': 0.0045, 'precision_cv_mean': 0.8911, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8579, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8948, 'precision_test': 0.7382, 'recall_test': 0.8667, 'f1_score_test': 0.7973}, 'Naive Bayes': {'accuracy_cv_mean': 0.7796, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.7631, 'precision_cv_std': 0.0021, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.7863, 'f1_cv_std': 0.0035, 'accuracy_test': 0.7662, 'precision_test': 0.5062, 'recall_test': 0.8091, 'f1_score_test': 0.6228}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8958, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8914, 'precision_cv_std': 0.0112, 'recall_cv_mean': 0.9017, 'recall_cv_std': 0.0122, 'f1_cv_mean': 0.8964, 'f1_cv_std': 0.0026, 'params': 49953, 'accuracy_test': 0.9197, 'precision_test': 0.8395, 'recall_test': 0.8202, 'f1_score_test': 0.8297}, 'MLP_971265': {'accuracy_cv_mean': 0.8975, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8952, 'precision_cv_std': 0.0079, 'recall_cv_mean': 0.9007, 'recall_cv_std': 0.0055, 'f1_cv_mean': 0.8979, 'f1_cv_std': 0.0019, 'params': 971265, 'accuracy_test': 0.9205, 'precision_test': 0.844, 'recall_test': 0.8178, 'f1_score_test': 0.8307}, 'MLP_2296833': {'accuracy_cv_mean': 0.898, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.8966, 'precision_cv_std': 0.0095, 'recall_cv_mean': 0.8999, 'recall_cv_std': 0.0106, 'f1_cv_mean': 0.8982, 'f1_cv_std': 0.0016, 'params': 2296833, 'accuracy_test': 0.919, 'precision_test': 0.8353, 'recall_test': 0.8227, 'f1_score_test': 0.8289}}}
Saved on: outputs_cv/2/gpt

==============================
Model: Logistic Regression

/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:45:17] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:46:22] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:49:47] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:50:53] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:54:15] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:55:20] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:58:44] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:59:51] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:03:18] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:04:25] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:07:56] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:09:03] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9011, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.9059, 'precision_cv_std': 0.002, 'recall_cv_mean': 0.8952, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9005, 'f1_cv_std': 0.0028}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 44, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.91      0.94     16465
           1       0.76      0.89      0.82      5160

    accuracy                           0.91     21625
   macro avg       0.86      0.90      0.88     21625
weighted avg       0.91      0.91      0.91     21625

Confusion matrix Test saved as: outputs_cv/2/gpt/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/2/gpt/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8454, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.8394, 'precision_cv_std': 0.014, 'recall_cv_mean': 0.8551, 'recall_cv_std': 0.023, 'f1_cv_mean': 0.8468, 'f1_cv_std': 0.0067}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 44, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.79      0.87     16465
           1       0.57      0.90      0.70      5160

    accuracy                           0.82     21625
   macro avg       0.77      0.84      0.78     21625
weighted avg       0.87      0.82      0.83     21625

Confusion matrix Test saved as: outputs_cv/2/gpt/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/2/gpt/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.799, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8163, 'precision_cv_std': 0.0079, 'recall_cv_mean': 0.772, 'recall_cv_std': 0.0184, 'f1_cv_mean': 0.7933, 'f1_cv_std': 0.0084}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 44, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.83      0.87     16465
           1       0.59      0.77      0.67      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.80      0.77     21625
weighted avg       0.84      0.82      0.83     21625

Confusion matrix Test saved as: outputs_cv/2/gpt/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/2/gpt/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8734, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9028, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.837, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8686, 'f1_cv_std': 0.0033}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 44, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.91      0.93     16465
           1       0.75      0.84      0.79      5160

    accuracy                           0.90     21625
   macro avg       0.85      0.88      0.86     21625
weighted avg       0.90      0.90      0.90     21625

Confusion matrix Test saved as: outputs_cv/2/gpt/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/2/gpt/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9094, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.9222, 'precision_cv_std': 0.0015, 'recall_cv_mean': 0.8941, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.908, 'f1_cv_std': 0.004}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 44, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/2/gpt/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/2/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8338, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.8649, 'precision_cv_std': 0.0043, 'recall_cv_mean': 0.7913, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8264, 'f1_cv_std': 0.0035}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.79      0.72      5160

    accuracy                           0.85     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.85      0.86     21625

Confusion matrix Test saved as: outputs_cv/2/gpt/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/2/gpt/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.9094, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.9222, 'precision_cv_std': 0.0015, 'recall_cv_mean': 0.8941, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.908, 'f1_cv_std': 0.004, 'accuracy_test': 0.92, 'precision_test': 0.796, 'recall_test': 0.8936, 'f1_score_test': 0.842}
Logistic Regression: {'accuracy_cv_mean': 0.9011, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.9059, 'precision_cv_std': 0.002, 'recall_cv_mean': 0.8952, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9005, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9061, 'precision_test': 0.7577, 'recall_test': 0.8917, 'f1_score_test': 0.8193}
Random Forest: {'accuracy_cv_mean': 0.8734, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9028, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.837, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8686, 'f1_cv_std': 0.0033, 'accuracy_test': 0.896, 'precision_test': 0.7523, 'recall_test': 0.8411, 'f1_score_test': 0.7942}
Naive Bayes: {'accuracy_cv_mean': 0.8338, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.8649, 'precision_cv_std': 0.0043, 'recall_cv_mean': 0.7913, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8264, 'f1_cv_std': 0.0035, 'accuracy_test': 0.8548, 'precision_test': 0.6655, 'recall_test': 0.7866, 'f1_score_test': 0.721}
SVM: {'accuracy_cv_mean': 0.8454, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.8394, 'precision_cv_std': 0.014, 'recall_cv_mean': 0.8551, 'recall_cv_std': 0.023, 'f1_cv_mean': 0.8468, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8168, 'precision_test': 0.5745, 'recall_test': 0.8952, 'f1_score_test': 0.6998}
Decision Tree: {'accuracy_cv_mean': 0.799, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8163, 'precision_cv_std': 0.0079, 'recall_cv_mean': 0.772, 'recall_cv_std': 0.0184, 'f1_cv_mean': 0.7933, 'f1_cv_std': 0.0084, 'accuracy_test': 0.8188, 'precision_test': 0.5919, 'recall_test': 0.7748, 'f1_score_test': 0.6711}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.933, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0014, 'recall_cv_mean': 0.9367, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9332, 'f1_cv_std': 0.0018, 'params': 160801, 'accuracy_test': 0.9426, 'precision_test': 0.8578, 'recall_test': 0.9105, 'f1_score_test': 0.8833}, 'MLP_2744833': {'accuracy_cv_mean': 0.9438, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9412, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.9467, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.944, 'f1_cv_std': 0.0016, 'params': 2744833, 'accuracy_test': 0.9566, 'precision_test': 0.8952, 'recall_test': 0.9266, 'f1_score_test': 0.9106}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.945, 'precision_cv_std': 0.0049, 'recall_cv_mean': 0.9474, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9462, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9566, 'precision_test': 0.8847, 'recall_test': 0.9409, 'f1_score_test': 0.9119}, 'Logistic Regression': {'accuracy_cv_mean': 0.9287, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.937, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9191, 'recall_cv_std': 0.0045, 'f1_cv_mean': 0.928, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9353, 'precision_test': 0.8262, 'recall_test': 0.9229, 'f1_score_test': 0.8718}, 'SVM': {'accuracy_cv_mean': 0.818, 'accuracy_cv_std': 0.0219, 'precision_cv_mean': 0.7609, 'precision_cv_std': 0.03, 'recall_cv_mean': 0.9306, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.8368, 'f1_cv_std': 0.0159, 'accuracy_test': 0.6576, 'precision_test': 0.4073, 'recall_test': 0.9558, 'f1_score_test': 0.5712}, 'Decision Tree': {'accuracy_cv_mean': 0.8291, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0147, 'recall_cv_mean': 0.7558, 'recall_cv_std': 0.0138, 'f1_cv_mean': 0.8155, 'f1_cv_std': 0.0057, 'accuracy_test': 0.8674, 'precision_test': 0.7033, 'recall_test': 0.7684, 'f1_score_test': 0.7344}, 'Random Forest': {'accuracy_cv_mean': 0.8347, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8736, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.7827, 'recall_cv_std': 0.0068, 'f1_cv_mean': 0.8257, 'f1_cv_std': 0.0048, 'accuracy_test': 0.8568, 'precision_test': 0.6727, 'recall_test': 0.7789, 'f1_score_test': 0.7219}, 'XGBoost': {'accuracy_cv_mean': 0.9063, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.9269, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8821, 'recall_cv_std': 0.0074, 'f1_cv_mean': 0.9039, 'f1_cv_std': 0.0052, 'accuracy_test': 0.9232, 'precision_test': 0.8108, 'recall_test': 0.8845, 'f1_score_test': 0.846}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8737, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9009, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9183, 'precision_test': 0.8043, 'recall_test': 0.869, 'f1_score_test': 0.8354}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8481, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.8437, 'precision_cv_std': 0.0132, 'recall_cv_mean': 0.855, 'recall_cv_std': 0.0146, 'f1_cv_mean': 0.8491, 'f1_cv_std': 0.0025, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.7462, 'recall_test': 0.7798, 'f1_score_test': 0.7626}, 'MLP_338433': {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8547, 'precision_cv_std': 0.0068, 'recall_cv_mean': 0.8544, 'recall_cv_std': 0.014, 'f1_cv_mean': 0.8544, 'f1_cv_std': 0.0047, 'params': 338433, 'accuracy_test': 0.8884, 'precision_test': 0.7512, 'recall_test': 0.7957, 'f1_score_test': 0.7728}, 'MLP_1031169': {'accuracy_cv_mean': 0.8559, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.0191, 'recall_cv_mean': 0.8523, 'recall_cv_std': 0.0175, 'f1_cv_mean': 0.8554, 'f1_cv_std': 0.0037, 'params': 1031169, 'accuracy_test': 0.8998, 'precision_test': 0.8237, 'recall_test': 0.7382, 'f1_score_test': 0.7786}, 'Logistic Regression': {'accuracy_cv_mean': 0.8475, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8313, 'recall_cv_std': 0.0079, 'f1_cv_mean': 0.845, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8608, 'precision_test': 0.6666, 'recall_test': 0.8333, 'f1_score_test': 0.7407}, 'SVM': {'accuracy_cv_mean': 0.6815, 'accuracy_cv_std': 0.0227, 'precision_cv_mean': 0.6619, 'precision_cv_std': 0.0387, 'recall_cv_mean': 0.7619, 'recall_cv_std': 0.0757, 'f1_cv_mean': 0.7044, 'f1_cv_std': 0.0175, 'accuracy_test': 0.5634, 'precision_test': 0.3204, 'recall_test': 0.7399, 'f1_score_test': 0.4472}, 'Decision Tree': {'accuracy_cv_mean': 0.792, 'accuracy_cv_std': 0.0047, 'precision_cv_mean': 0.8112, 'precision_cv_std': 0.0162, 'recall_cv_mean': 0.7622, 'recall_cv_std': 0.0246, 'f1_cv_mean': 0.7855, 'f1_cv_std': 0.0072, 'accuracy_test': 0.8198, 'precision_test': 0.5927, 'recall_test': 0.7829, 'f1_score_test': 0.6747}, 'Random Forest': {'accuracy_cv_mean': 0.8534, 'accuracy_cv_std': 0.0064, 'precision_cv_mean': 0.8862, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0104, 'f1_cv_mean': 0.8469, 'f1_cv_std': 0.0072, 'accuracy_test': 0.88, 'precision_test': 0.7214, 'recall_test': 0.8101, 'f1_score_test': 0.7632}, 'XGBoost': {'accuracy_cv_mean': 0.8765, 'accuracy_cv_std': 0.0045, 'precision_cv_mean': 0.8911, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8579, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8948, 'precision_test': 0.7382, 'recall_test': 0.8667, 'f1_score_test': 0.7973}, 'Naive Bayes': {'accuracy_cv_mean': 0.7796, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.7631, 'precision_cv_std': 0.0021, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.7863, 'f1_cv_std': 0.0035, 'accuracy_test': 0.7662, 'precision_test': 0.5062, 'recall_test': 0.8091, 'f1_score_test': 0.6228}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8958, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8914, 'precision_cv_std': 0.0112, 'recall_cv_mean': 0.9017, 'recall_cv_std': 0.0122, 'f1_cv_mean': 0.8964, 'f1_cv_std': 0.0026, 'params': 49953, 'accuracy_test': 0.9197, 'precision_test': 0.8395, 'recall_test': 0.8202, 'f1_score_test': 0.8297}, 'MLP_971265': {'accuracy_cv_mean': 0.8975, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8952, 'precision_cv_std': 0.0079, 'recall_cv_mean': 0.9007, 'recall_cv_std': 0.0055, 'f1_cv_mean': 0.8979, 'f1_cv_std': 0.0019, 'params': 971265, 'accuracy_test': 0.9205, 'precision_test': 0.844, 'recall_test': 0.8178, 'f1_score_test': 0.8307}, 'MLP_2296833': {'accuracy_cv_mean': 0.898, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.8966, 'precision_cv_std': 0.0095, 'recall_cv_mean': 0.8999, 'recall_cv_std': 0.0106, 'f1_cv_mean': 0.8982, 'f1_cv_std': 0.0016, 'params': 2296833, 'accuracy_test': 0.919, 'precision_test': 0.8353, 'recall_test': 0.8227, 'f1_score_test': 0.8289}, 'Logistic Regression': {'accuracy_cv_mean': 0.9011, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.9059, 'precision_cv_std': 0.002, 'recall_cv_mean': 0.8952, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9005, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9061, 'precision_test': 0.7577, 'recall_test': 0.8917, 'f1_score_test': 0.8193}, 'SVM': {'accuracy_cv_mean': 0.8454, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.8394, 'precision_cv_std': 0.014, 'recall_cv_mean': 0.8551, 'recall_cv_std': 0.023, 'f1_cv_mean': 0.8468, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8168, 'precision_test': 0.5745, 'recall_test': 0.8952, 'f1_score_test': 0.6998}, 'Decision Tree': {'accuracy_cv_mean': 0.799, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8163, 'precision_cv_std': 0.0079, 'recall_cv_mean': 0.772, 'recall_cv_std': 0.0184, 'f1_cv_mean': 0.7933, 'f1_cv_std': 0.0084, 'accuracy_test': 0.8188, 'precision_test': 0.5919, 'recall_test': 0.7748, 'f1_score_test': 0.6711}, 'Random Forest': {'accuracy_cv_mean': 0.8734, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9028, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.837, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8686, 'f1_cv_std': 0.0033, 'accuracy_test': 0.896, 'precision_test': 0.7523, 'recall_test': 0.8411, 'f1_score_test': 0.7942}, 'XGBoost': {'accuracy_cv_mean': 0.9094, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.9222, 'precision_cv_std': 0.0015, 'recall_cv_mean': 0.8941, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.908, 'f1_cv_std': 0.004, 'accuracy_test': 0.92, 'precision_test': 0.796, 'recall_test': 0.8936, 'f1_score_test': 0.842}, 'Naive Bayes': {'accuracy_cv_mean': 0.8338, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.8649, 'precision_cv_std': 0.0043, 'recall_cv_mean': 0.7913, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8264, 'f1_cv_std': 0.0035, 'accuracy_test': 0.8548, 'precision_test': 0.6655, 'recall_test': 0.7866, 'f1_score_test': 0.721}}}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
MLP_5843969: {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.945, 'precision_cv_std': 0.0049, 'recall_cv_mean': 0.9474, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9462, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9566, 'precision_test': 0.8847, 'recall_test': 0.9409, 'f1_score_test': 0.9119}
MLP_2744833: {'accuracy_cv_mean': 0.9438, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9412, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.9467, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.944, 'f1_cv_std': 0.0016, 'params': 2744833, 'accuracy_test': 0.9566, 'precision_test': 0.8952, 'recall_test': 0.9266, 'f1_score_test': 0.9106}
MLP_160801: {'accuracy_cv_mean': 0.933, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0014, 'recall_cv_mean': 0.9367, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9332, 'f1_cv_std': 0.0018, 'params': 160801, 'accuracy_test': 0.9426, 'precision_test': 0.8578, 'recall_test': 0.9105, 'f1_score_test': 0.8833}
Logistic Regression: {'accuracy_cv_mean': 0.9287, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.937, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9191, 'recall_cv_std': 0.0045, 'f1_cv_mean': 0.928, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9353, 'precision_test': 0.8262, 'recall_test': 0.9229, 'f1_score_test': 0.8718}
XGBoost: {'accuracy_cv_mean': 0.9063, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.9269, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8821, 'recall_cv_std': 0.0074, 'f1_cv_mean': 0.9039, 'f1_cv_std': 0.0052, 'accuracy_test': 0.9232, 'precision_test': 0.8108, 'recall_test': 0.8845, 'f1_score_test': 0.846}
Naive Bayes: {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8737, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9009, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9183, 'precision_test': 0.8043, 'recall_test': 0.869, 'f1_score_test': 0.8354}
Decision Tree: {'accuracy_cv_mean': 0.8291, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0147, 'recall_cv_mean': 0.7558, 'recall_cv_std': 0.0138, 'f1_cv_mean': 0.8155, 'f1_cv_std': 0.0057, 'accuracy_test': 0.8674, 'precision_test': 0.7033, 'recall_test': 0.7684, 'f1_score_test': 0.7344}
Random Forest: {'accuracy_cv_mean': 0.8347, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8736, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.7827, 'recall_cv_std': 0.0068, 'f1_cv_mean': 0.8257, 'f1_cv_std': 0.0048, 'accuracy_test': 0.8568, 'precision_test': 0.6727, 'recall_test': 0.7789, 'f1_score_test': 0.7219}
SVM: {'accuracy_cv_mean': 0.818, 'accuracy_cv_std': 0.0219, 'precision_cv_mean': 0.7609, 'precision_cv_std': 0.03, 'recall_cv_mean': 0.9306, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.8368, 'f1_cv_std': 0.0159, 'accuracy_test': 0.6576, 'precision_test': 0.4073, 'recall_test': 0.9558, 'f1_score_test': 0.5712}


EMBEDDINGS TYPE: LYRICS_BERT
XGBoost: {'accuracy_cv_mean': 0.8765, 'accuracy_cv_std': 0.0045, 'precision_cv_mean': 0.8911, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8579, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8948, 'precision_test': 0.7382, 'recall_test': 0.8667, 'f1_score_test': 0.7973}
MLP_1031169: {'accuracy_cv_mean': 0.8559, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.0191, 'recall_cv_mean': 0.8523, 'recall_cv_std': 0.0175, 'f1_cv_mean': 0.8554, 'f1_cv_std': 0.0037, 'params': 1031169, 'accuracy_test': 0.8998, 'precision_test': 0.8237, 'recall_test': 0.7382, 'f1_score_test': 0.7786}
MLP_338433: {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8547, 'precision_cv_std': 0.0068, 'recall_cv_mean': 0.8544, 'recall_cv_std': 0.014, 'f1_cv_mean': 0.8544, 'f1_cv_std': 0.0047, 'params': 338433, 'accuracy_test': 0.8884, 'precision_test': 0.7512, 'recall_test': 0.7957, 'f1_score_test': 0.7728}
Random Forest: {'accuracy_cv_mean': 0.8534, 'accuracy_cv_std': 0.0064, 'precision_cv_mean': 0.8862, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0104, 'f1_cv_mean': 0.8469, 'f1_cv_std': 0.0072, 'accuracy_test': 0.88, 'precision_test': 0.7214, 'recall_test': 0.8101, 'f1_score_test': 0.7632}
MLP_10401: {'accuracy_cv_mean': 0.8481, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.8437, 'precision_cv_std': 0.0132, 'recall_cv_mean': 0.855, 'recall_cv_std': 0.0146, 'f1_cv_mean': 0.8491, 'f1_cv_std': 0.0025, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.7462, 'recall_test': 0.7798, 'f1_score_test': 0.7626}
Logistic Regression: {'accuracy_cv_mean': 0.8475, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8313, 'recall_cv_std': 0.0079, 'f1_cv_mean': 0.845, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8608, 'precision_test': 0.6666, 'recall_test': 0.8333, 'f1_score_test': 0.7407}
Decision Tree: {'accuracy_cv_mean': 0.792, 'accuracy_cv_std': 0.0047, 'precision_cv_mean': 0.8112, 'precision_cv_std': 0.0162, 'recall_cv_mean': 0.7622, 'recall_cv_std': 0.0246, 'f1_cv_mean': 0.7855, 'f1_cv_std': 0.0072, 'accuracy_test': 0.8198, 'precision_test': 0.5927, 'recall_test': 0.7829, 'f1_score_test': 0.6747}
Naive Bayes: {'accuracy_cv_mean': 0.7796, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.7631, 'precision_cv_std': 0.0021, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.7863, 'f1_cv_std': 0.0035, 'accuracy_test': 0.7662, 'precision_test': 0.5062, 'recall_test': 0.8091, 'f1_score_test': 0.6228}
SVM: {'accuracy_cv_mean': 0.6815, 'accuracy_cv_std': 0.0227, 'precision_cv_mean': 0.6619, 'precision_cv_std': 0.0387, 'recall_cv_mean': 0.7619, 'recall_cv_std': 0.0757, 'f1_cv_mean': 0.7044, 'f1_cv_std': 0.0175, 'accuracy_test': 0.5634, 'precision_test': 0.3204, 'recall_test': 0.7399, 'f1_score_test': 0.4472}


EMBEDDINGS TYPE: GPT
XGBoost: {'accuracy_cv_mean': 0.9094, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.9222, 'precision_cv_std': 0.0015, 'recall_cv_mean': 0.8941, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.908, 'f1_cv_std': 0.004, 'accuracy_test': 0.92, 'precision_test': 0.796, 'recall_test': 0.8936, 'f1_score_test': 0.842}
MLP_971265: {'accuracy_cv_mean': 0.8975, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8952, 'precision_cv_std': 0.0079, 'recall_cv_mean': 0.9007, 'recall_cv_std': 0.0055, 'f1_cv_mean': 0.8979, 'f1_cv_std': 0.0019, 'params': 971265, 'accuracy_test': 0.9205, 'precision_test': 0.844, 'recall_test': 0.8178, 'f1_score_test': 0.8307}
MLP_49953: {'accuracy_cv_mean': 0.8958, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8914, 'precision_cv_std': 0.0112, 'recall_cv_mean': 0.9017, 'recall_cv_std': 0.0122, 'f1_cv_mean': 0.8964, 'f1_cv_std': 0.0026, 'params': 49953, 'accuracy_test': 0.9197, 'precision_test': 0.8395, 'recall_test': 0.8202, 'f1_score_test': 0.8297}
MLP_2296833: {'accuracy_cv_mean': 0.898, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.8966, 'precision_cv_std': 0.0095, 'recall_cv_mean': 0.8999, 'recall_cv_std': 0.0106, 'f1_cv_mean': 0.8982, 'f1_cv_std': 0.0016, 'params': 2296833, 'accuracy_test': 0.919, 'precision_test': 0.8353, 'recall_test': 0.8227, 'f1_score_test': 0.8289}
Logistic Regression: {'accuracy_cv_mean': 0.9011, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.9059, 'precision_cv_std': 0.002, 'recall_cv_mean': 0.8952, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9005, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9061, 'precision_test': 0.7577, 'recall_test': 0.8917, 'f1_score_test': 0.8193}
Random Forest: {'accuracy_cv_mean': 0.8734, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9028, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.837, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8686, 'f1_cv_std': 0.0033, 'accuracy_test': 0.896, 'precision_test': 0.7523, 'recall_test': 0.8411, 'f1_score_test': 0.7942}
Naive Bayes: {'accuracy_cv_mean': 0.8338, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.8649, 'precision_cv_std': 0.0043, 'recall_cv_mean': 0.7913, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8264, 'f1_cv_std': 0.0035, 'accuracy_test': 0.8548, 'precision_test': 0.6655, 'recall_test': 0.7866, 'f1_score_test': 0.721}
SVM: {'accuracy_cv_mean': 0.8454, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.8394, 'precision_cv_std': 0.014, 'recall_cv_mean': 0.8551, 'recall_cv_std': 0.023, 'f1_cv_mean': 0.8468, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8168, 'precision_test': 0.5745, 'recall_test': 0.8952, 'f1_score_test': 0.6998}
Decision Tree: {'accuracy_cv_mean': 0.799, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8163, 'precision_cv_std': 0.0079, 'recall_cv_mean': 0.772, 'recall_cv_std': 0.0184, 'f1_cv_mean': 0.7933, 'f1_cv_std': 0.0084, 'accuracy_test': 0.8188, 'precision_test': 0.5919, 'recall_test': 0.7748, 'f1_score_test': 0.6711}
Diccionario global guardado en: outputs_cv/2/gpt/global_metrics.json

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
====================================

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9011, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.9059, 'precision_cv_std': 0.002, 'recall_cv_mean': 0.8952, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9005, 'f1_cv_std': 0.0028}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 44, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.91      0.94     16465
           1       0.76      0.89      0.82      5160

    accuracy                           0.91     21625
   macro avg       0.86      0.90      0.88     21625
weighted avg       0.91      0.91      0.91     21625

Confusion matrix Test saved as: outputs_cv/2/gpt/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/2/gpt/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8454, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.8394, 'precision_cv_std': 0.014, 'recall_cv_mean': 0.8551, 'recall_cv_std': 0.023, 'f1_cv_mean': 0.8468, 'f1_cv_std': 0.0067}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 44, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.79      0.87     16465
           1       0.57      0.90      0.70      5160

    accuracy                           0.82     21625
   macro avg       0.77      0.84      0.78     21625
weighted avg       0.87      0.82      0.83     21625

Confusion matrix Test saved as: outputs_cv/2/gpt/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/2/gpt/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.799, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8163, 'precision_cv_std': 0.0079, 'recall_cv_mean': 0.772, 'recall_cv_std': 0.0184, 'f1_cv_mean': 0.7933, 'f1_cv_std': 0.0084}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 44, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.83      0.87     16465
           1       0.59      0.77      0.67      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.80      0.77     21625
weighted avg       0.84      0.82      0.83     21625

Confusion matrix Test saved as: outputs_cv/2/gpt/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/2/gpt/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8734, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9028, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.837, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8686, 'f1_cv_std': 0.0033}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 44, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.91      0.93     16465
           1       0.75      0.84      0.79      5160

    accuracy                           0.90     21625
   macro avg       0.85      0.88      0.86     21625
weighted avg       0.90      0.90      0.90     21625

Confusion matrix Test saved as: outputs_cv/2/gpt/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/2/gpt/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9094, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.9222, 'precision_cv_std': 0.0015, 'recall_cv_mean': 0.8941, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.908, 'f1_cv_std': 0.004}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 44, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/2/gpt/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/2/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8338, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.8649, 'precision_cv_std': 0.0043, 'recall_cv_mean': 0.7913, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8264, 'f1_cv_std': 0.0035}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.79      0.72      5160

    accuracy                           0.85     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.85      0.86     21625

Confusion matrix Test saved as: outputs_cv/2/gpt/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/2/gpt/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.9094, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.9222, 'precision_cv_std': 0.0015, 'recall_cv_mean': 0.8941, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.908, 'f1_cv_std': 0.004, 'accuracy_test': 0.92, 'precision_test': 0.796, 'recall_test': 0.8936, 'f1_score_test': 0.842}
Logistic Regression: {'accuracy_cv_mean': 0.9011, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.9059, 'precision_cv_std': 0.002, 'recall_cv_mean': 0.8952, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9005, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9061, 'precision_test': 0.7577, 'recall_test': 0.8917, 'f1_score_test': 0.8193}
Random Forest: {'accuracy_cv_mean': 0.8734, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9028, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.837, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8686, 'f1_cv_std': 0.0033, 'accuracy_test': 0.896, 'precision_test': 0.7523, 'recall_test': 0.8411, 'f1_score_test': 0.7942}
Naive Bayes: {'accuracy_cv_mean': 0.8338, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.8649, 'precision_cv_std': 0.0043, 'recall_cv_mean': 0.7913, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8264, 'f1_cv_std': 0.0035, 'accuracy_test': 0.8548, 'precision_test': 0.6655, 'recall_test': 0.7866, 'f1_score_test': 0.721}
SVM: {'accuracy_cv_mean': 0.8454, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.8394, 'precision_cv_std': 0.014, 'recall_cv_mean': 0.8551, 'recall_cv_std': 0.023, 'f1_cv_mean': 0.8468, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8168, 'precision_test': 0.5745, 'recall_test': 0.8952, 'f1_score_test': 0.6998}
Decision Tree: {'accuracy_cv_mean': 0.799, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8163, 'precision_cv_std': 0.0079, 'recall_cv_mean': 0.772, 'recall_cv_std': 0.0184, 'f1_cv_mean': 0.7933, 'f1_cv_std': 0.0084, 'accuracy_test': 0.8188, 'precision_test': 0.5919, 'recall_test': 0.7748, 'f1_score_test': 0.6711}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.933, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0014, 'recall_cv_mean': 0.9367, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9332, 'f1_cv_std': 0.0018, 'params': 160801, 'accuracy_test': 0.9426, 'precision_test': 0.8578, 'recall_test': 0.9105, 'f1_score_test': 0.8833}, 'MLP_2744833': {'accuracy_cv_mean': 0.9438, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9412, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.9467, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.944, 'f1_cv_std': 0.0016, 'params': 2744833, 'accuracy_test': 0.9566, 'precision_test': 0.8952, 'recall_test': 0.9266, 'f1_score_test': 0.9106}, 'MLP_5843969': {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.945, 'precision_cv_std': 0.0049, 'recall_cv_mean': 0.9474, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9462, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9566, 'precision_test': 0.8847, 'recall_test': 0.9409, 'f1_score_test': 0.9119}, 'Logistic Regression': {'accuracy_cv_mean': 0.9287, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.937, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9191, 'recall_cv_std': 0.0045, 'f1_cv_mean': 0.928, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9353, 'precision_test': 0.8262, 'recall_test': 0.9229, 'f1_score_test': 0.8718}, 'SVM': {'accuracy_cv_mean': 0.818, 'accuracy_cv_std': 0.0219, 'precision_cv_mean': 0.7609, 'precision_cv_std': 0.03, 'recall_cv_mean': 0.9306, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.8368, 'f1_cv_std': 0.0159, 'accuracy_test': 0.6576, 'precision_test': 0.4073, 'recall_test': 0.9558, 'f1_score_test': 0.5712}, 'Decision Tree': {'accuracy_cv_mean': 0.8291, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0147, 'recall_cv_mean': 0.7558, 'recall_cv_std': 0.0138, 'f1_cv_mean': 0.8155, 'f1_cv_std': 0.0057, 'accuracy_test': 0.8674, 'precision_test': 0.7033, 'recall_test': 0.7684, 'f1_score_test': 0.7344}, 'Random Forest': {'accuracy_cv_mean': 0.8347, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8736, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.7827, 'recall_cv_std': 0.0068, 'f1_cv_mean': 0.8257, 'f1_cv_std': 0.0048, 'accuracy_test': 0.8568, 'precision_test': 0.6727, 'recall_test': 0.7789, 'f1_score_test': 0.7219}, 'XGBoost': {'accuracy_cv_mean': 0.9063, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.9269, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8821, 'recall_cv_std': 0.0074, 'f1_cv_mean': 0.9039, 'f1_cv_std': 0.0052, 'accuracy_test': 0.9232, 'precision_test': 0.8108, 'recall_test': 0.8845, 'f1_score_test': 0.846}, 'Naive Bayes': {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8737, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9009, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9183, 'precision_test': 0.8043, 'recall_test': 0.869, 'f1_score_test': 0.8354}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8481, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.8437, 'precision_cv_std': 0.0132, 'recall_cv_mean': 0.855, 'recall_cv_std': 0.0146, 'f1_cv_mean': 0.8491, 'f1_cv_std': 0.0025, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.7462, 'recall_test': 0.7798, 'f1_score_test': 0.7626}, 'MLP_338433': {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8547, 'precision_cv_std': 0.0068, 'recall_cv_mean': 0.8544, 'recall_cv_std': 0.014, 'f1_cv_mean': 0.8544, 'f1_cv_std': 0.0047, 'params': 338433, 'accuracy_test': 0.8884, 'precision_test': 0.7512, 'recall_test': 0.7957, 'f1_score_test': 0.7728}, 'MLP_1031169': {'accuracy_cv_mean': 0.8559, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.0191, 'recall_cv_mean': 0.8523, 'recall_cv_std': 0.0175, 'f1_cv_mean': 0.8554, 'f1_cv_std': 0.0037, 'params': 1031169, 'accuracy_test': 0.8998, 'precision_test': 0.8237, 'recall_test': 0.7382, 'f1_score_test': 0.7786}, 'Logistic Regression': {'accuracy_cv_mean': 0.8475, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8313, 'recall_cv_std': 0.0079, 'f1_cv_mean': 0.845, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8608, 'precision_test': 0.6666, 'recall_test': 0.8333, 'f1_score_test': 0.7407}, 'SVM': {'accuracy_cv_mean': 0.6815, 'accuracy_cv_std': 0.0227, 'precision_cv_mean': 0.6619, 'precision_cv_std': 0.0387, 'recall_cv_mean': 0.7619, 'recall_cv_std': 0.0757, 'f1_cv_mean': 0.7044, 'f1_cv_std': 0.0175, 'accuracy_test': 0.5634, 'precision_test': 0.3204, 'recall_test': 0.7399, 'f1_score_test': 0.4472}, 'Decision Tree': {'accuracy_cv_mean': 0.792, 'accuracy_cv_std': 0.0047, 'precision_cv_mean': 0.8112, 'precision_cv_std': 0.0162, 'recall_cv_mean': 0.7622, 'recall_cv_std': 0.0246, 'f1_cv_mean': 0.7855, 'f1_cv_std': 0.0072, 'accuracy_test': 0.8198, 'precision_test': 0.5927, 'recall_test': 0.7829, 'f1_score_test': 0.6747}, 'Random Forest': {'accuracy_cv_mean': 0.8534, 'accuracy_cv_std': 0.0064, 'precision_cv_mean': 0.8862, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0104, 'f1_cv_mean': 0.8469, 'f1_cv_std': 0.0072, 'accuracy_test': 0.88, 'precision_test': 0.7214, 'recall_test': 0.8101, 'f1_score_test': 0.7632}, 'XGBoost': {'accuracy_cv_mean': 0.8765, 'accuracy_cv_std': 0.0045, 'precision_cv_mean': 0.8911, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8579, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8948, 'precision_test': 0.7382, 'recall_test': 0.8667, 'f1_score_test': 0.7973}, 'Naive Bayes': {'accuracy_cv_mean': 0.7796, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.7631, 'precision_cv_std': 0.0021, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.7863, 'f1_cv_std': 0.0035, 'accuracy_test': 0.7662, 'precision_test': 0.5062, 'recall_test': 0.8091, 'f1_score_test': 0.6228}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8958, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8914, 'precision_cv_std': 0.0112, 'recall_cv_mean': 0.9017, 'recall_cv_std': 0.0122, 'f1_cv_mean': 0.8964, 'f1_cv_std': 0.0026, 'params': 49953, 'accuracy_test': 0.9197, 'precision_test': 0.8395, 'recall_test': 0.8202, 'f1_score_test': 0.8297}, 'MLP_971265': {'accuracy_cv_mean': 0.8975, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8952, 'precision_cv_std': 0.0079, 'recall_cv_mean': 0.9007, 'recall_cv_std': 0.0055, 'f1_cv_mean': 0.8979, 'f1_cv_std': 0.0019, 'params': 971265, 'accuracy_test': 0.9205, 'precision_test': 0.844, 'recall_test': 0.8178, 'f1_score_test': 0.8307}, 'MLP_2296833': {'accuracy_cv_mean': 0.898, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.8966, 'precision_cv_std': 0.0095, 'recall_cv_mean': 0.8999, 'recall_cv_std': 0.0106, 'f1_cv_mean': 0.8982, 'f1_cv_std': 0.0016, 'params': 2296833, 'accuracy_test': 0.919, 'precision_test': 0.8353, 'recall_test': 0.8227, 'f1_score_test': 0.8289}, 'Logistic Regression': {'accuracy_cv_mean': 0.9011, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.9059, 'precision_cv_std': 0.002, 'recall_cv_mean': 0.8952, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9005, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9061, 'precision_test': 0.7577, 'recall_test': 0.8917, 'f1_score_test': 0.8193}, 'SVM': {'accuracy_cv_mean': 0.8454, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.8394, 'precision_cv_std': 0.014, 'recall_cv_mean': 0.8551, 'recall_cv_std': 0.023, 'f1_cv_mean': 0.8468, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8168, 'precision_test': 0.5745, 'recall_test': 0.8952, 'f1_score_test': 0.6998}, 'Decision Tree': {'accuracy_cv_mean': 0.799, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8163, 'precision_cv_std': 0.0079, 'recall_cv_mean': 0.772, 'recall_cv_std': 0.0184, 'f1_cv_mean': 0.7933, 'f1_cv_std': 0.0084, 'accuracy_test': 0.8188, 'precision_test': 0.5919, 'recall_test': 0.7748, 'f1_score_test': 0.6711}, 'Random Forest': {'accuracy_cv_mean': 0.8734, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9028, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.837, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8686, 'f1_cv_std': 0.0033, 'accuracy_test': 0.896, 'precision_test': 0.7523, 'recall_test': 0.8411, 'f1_score_test': 0.7942}, 'XGBoost': {'accuracy_cv_mean': 0.9094, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.9222, 'precision_cv_std': 0.0015, 'recall_cv_mean': 0.8941, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.908, 'f1_cv_std': 0.004, 'accuracy_test': 0.92, 'precision_test': 0.796, 'recall_test': 0.8936, 'f1_score_test': 0.842}, 'Naive Bayes': {'accuracy_cv_mean': 0.8338, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.8649, 'precision_cv_std': 0.0043, 'recall_cv_mean': 0.7913, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8264, 'f1_cv_std': 0.0035, 'accuracy_test': 0.8548, 'precision_test': 0.6655, 'recall_test': 0.7866, 'f1_score_test': 0.721}}}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
MLP_5843969: {'accuracy_cv_mean': 0.9461, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.945, 'precision_cv_std': 0.0049, 'recall_cv_mean': 0.9474, 'recall_cv_std': 0.0037, 'f1_cv_mean': 0.9462, 'f1_cv_std': 0.0014, 'params': 5843969, 'accuracy_test': 0.9566, 'precision_test': 0.8847, 'recall_test': 0.9409, 'f1_score_test': 0.9119}
MLP_2744833: {'accuracy_cv_mean': 0.9438, 'accuracy_cv_std': 0.0016, 'precision_cv_mean': 0.9412, 'precision_cv_std': 0.0031, 'recall_cv_mean': 0.9467, 'recall_cv_std': 0.0036, 'f1_cv_mean': 0.944, 'f1_cv_std': 0.0016, 'params': 2744833, 'accuracy_test': 0.9566, 'precision_test': 0.8952, 'recall_test': 0.9266, 'f1_score_test': 0.9106}
MLP_160801: {'accuracy_cv_mean': 0.933, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0014, 'recall_cv_mean': 0.9367, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9332, 'f1_cv_std': 0.0018, 'params': 160801, 'accuracy_test': 0.9426, 'precision_test': 0.8578, 'recall_test': 0.9105, 'f1_score_test': 0.8833}
Logistic Regression: {'accuracy_cv_mean': 0.9287, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.937, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9191, 'recall_cv_std': 0.0045, 'f1_cv_mean': 0.928, 'f1_cv_std': 0.0036, 'accuracy_test': 0.9353, 'precision_test': 0.8262, 'recall_test': 0.9229, 'f1_score_test': 0.8718}
XGBoost: {'accuracy_cv_mean': 0.9063, 'accuracy_cv_std': 0.0049, 'precision_cv_mean': 0.9269, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.8821, 'recall_cv_std': 0.0074, 'f1_cv_mean': 0.9039, 'f1_cv_std': 0.0052, 'accuracy_test': 0.9232, 'precision_test': 0.8108, 'recall_test': 0.8845, 'f1_score_test': 0.846}
Naive Bayes: {'accuracy_cv_mean': 0.9039, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.9298, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8737, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9009, 'f1_cv_std': 0.0035, 'accuracy_test': 0.9183, 'precision_test': 0.8043, 'recall_test': 0.869, 'f1_score_test': 0.8354}
Decision Tree: {'accuracy_cv_mean': 0.8291, 'accuracy_cv_std': 0.0052, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0147, 'recall_cv_mean': 0.7558, 'recall_cv_std': 0.0138, 'f1_cv_mean': 0.8155, 'f1_cv_std': 0.0057, 'accuracy_test': 0.8674, 'precision_test': 0.7033, 'recall_test': 0.7684, 'f1_score_test': 0.7344}
Random Forest: {'accuracy_cv_mean': 0.8347, 'accuracy_cv_std': 0.0041, 'precision_cv_mean': 0.8736, 'precision_cv_std': 0.0037, 'recall_cv_mean': 0.7827, 'recall_cv_std': 0.0068, 'f1_cv_mean': 0.8257, 'f1_cv_std': 0.0048, 'accuracy_test': 0.8568, 'precision_test': 0.6727, 'recall_test': 0.7789, 'f1_score_test': 0.7219}
SVM: {'accuracy_cv_mean': 0.818, 'accuracy_cv_std': 0.0219, 'precision_cv_mean': 0.7609, 'precision_cv_std': 0.03, 'recall_cv_mean': 0.9306, 'recall_cv_std': 0.0085, 'f1_cv_mean': 0.8368, 'f1_cv_std': 0.0159, 'accuracy_test': 0.6576, 'precision_test': 0.4073, 'recall_test': 0.9558, 'f1_score_test': 0.5712}


EMBEDDINGS TYPE: LYRICS_BERT
XGBoost: {'accuracy_cv_mean': 0.8765, 'accuracy_cv_std': 0.0045, 'precision_cv_mean': 0.8911, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8579, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8742, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8948, 'precision_test': 0.7382, 'recall_test': 0.8667, 'f1_score_test': 0.7973}
MLP_1031169: {'accuracy_cv_mean': 0.8559, 'accuracy_cv_std': 0.0051, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.0191, 'recall_cv_mean': 0.8523, 'recall_cv_std': 0.0175, 'f1_cv_mean': 0.8554, 'f1_cv_std': 0.0037, 'params': 1031169, 'accuracy_test': 0.8998, 'precision_test': 0.8237, 'recall_test': 0.7382, 'f1_score_test': 0.7786}
MLP_338433: {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8547, 'precision_cv_std': 0.0068, 'recall_cv_mean': 0.8544, 'recall_cv_std': 0.014, 'f1_cv_mean': 0.8544, 'f1_cv_std': 0.0047, 'params': 338433, 'accuracy_test': 0.8884, 'precision_test': 0.7512, 'recall_test': 0.7957, 'f1_score_test': 0.7728}
Random Forest: {'accuracy_cv_mean': 0.8534, 'accuracy_cv_std': 0.0064, 'precision_cv_mean': 0.8862, 'precision_cv_std': 0.0051, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0104, 'f1_cv_mean': 0.8469, 'f1_cv_std': 0.0072, 'accuracy_test': 0.88, 'precision_test': 0.7214, 'recall_test': 0.8101, 'f1_score_test': 0.7632}
MLP_10401: {'accuracy_cv_mean': 0.8481, 'accuracy_cv_std': 0.0031, 'precision_cv_mean': 0.8437, 'precision_cv_std': 0.0132, 'recall_cv_mean': 0.855, 'recall_cv_std': 0.0146, 'f1_cv_mean': 0.8491, 'f1_cv_std': 0.0025, 'params': 10401, 'accuracy_test': 0.8842, 'precision_test': 0.7462, 'recall_test': 0.7798, 'f1_score_test': 0.7626}
Logistic Regression: {'accuracy_cv_mean': 0.8475, 'accuracy_cv_std': 0.0043, 'precision_cv_mean': 0.8592, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8313, 'recall_cv_std': 0.0079, 'f1_cv_mean': 0.845, 'f1_cv_std': 0.0049, 'accuracy_test': 0.8608, 'precision_test': 0.6666, 'recall_test': 0.8333, 'f1_score_test': 0.7407}
Decision Tree: {'accuracy_cv_mean': 0.792, 'accuracy_cv_std': 0.0047, 'precision_cv_mean': 0.8112, 'precision_cv_std': 0.0162, 'recall_cv_mean': 0.7622, 'recall_cv_std': 0.0246, 'f1_cv_mean': 0.7855, 'f1_cv_std': 0.0072, 'accuracy_test': 0.8198, 'precision_test': 0.5927, 'recall_test': 0.7829, 'f1_score_test': 0.6747}
Naive Bayes: {'accuracy_cv_mean': 0.7796, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.7631, 'precision_cv_std': 0.0021, 'recall_cv_mean': 0.811, 'recall_cv_std': 0.0084, 'f1_cv_mean': 0.7863, 'f1_cv_std': 0.0035, 'accuracy_test': 0.7662, 'precision_test': 0.5062, 'recall_test': 0.8091, 'f1_score_test': 0.6228}
SVM: {'accuracy_cv_mean': 0.6815, 'accuracy_cv_std': 0.0227, 'precision_cv_mean': 0.6619, 'precision_cv_std': 0.0387, 'recall_cv_mean': 0.7619, 'recall_cv_std': 0.0757, 'f1_cv_mean': 0.7044, 'f1_cv_std': 0.0175, 'accuracy_test': 0.5634, 'precision_test': 0.3204, 'recall_test': 0.7399, 'f1_score_test': 0.4472}


EMBEDDINGS TYPE: GPT
XGBoost: {'accuracy_cv_mean': 0.9094, 'accuracy_cv_std': 0.0036, 'precision_cv_mean': 0.9222, 'precision_cv_std': 0.0015, 'recall_cv_mean': 0.8941, 'recall_cv_std': 0.0073, 'f1_cv_mean': 0.908, 'f1_cv_std': 0.004, 'accuracy_test': 0.92, 'precision_test': 0.796, 'recall_test': 0.8936, 'f1_score_test': 0.842}
MLP_971265: {'accuracy_cv_mean': 0.8975, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8952, 'precision_cv_std': 0.0079, 'recall_cv_mean': 0.9007, 'recall_cv_std': 0.0055, 'f1_cv_mean': 0.8979, 'f1_cv_std': 0.0019, 'params': 971265, 'accuracy_test': 0.9205, 'precision_test': 0.844, 'recall_test': 0.8178, 'f1_score_test': 0.8307}
MLP_49953: {'accuracy_cv_mean': 0.8958, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8914, 'precision_cv_std': 0.0112, 'recall_cv_mean': 0.9017, 'recall_cv_std': 0.0122, 'f1_cv_mean': 0.8964, 'f1_cv_std': 0.0026, 'params': 49953, 'accuracy_test': 0.9197, 'precision_test': 0.8395, 'recall_test': 0.8202, 'f1_score_test': 0.8297}
MLP_2296833: {'accuracy_cv_mean': 0.898, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.8966, 'precision_cv_std': 0.0095, 'recall_cv_mean': 0.8999, 'recall_cv_std': 0.0106, 'f1_cv_mean': 0.8982, 'f1_cv_std': 0.0016, 'params': 2296833, 'accuracy_test': 0.919, 'precision_test': 0.8353, 'recall_test': 0.8227, 'f1_score_test': 0.8289}
Logistic Regression: {'accuracy_cv_mean': 0.9011, 'accuracy_cv_std': 0.0024, 'precision_cv_mean': 0.9059, 'precision_cv_std': 0.002, 'recall_cv_mean': 0.8952, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9005, 'f1_cv_std': 0.0028, 'accuracy_test': 0.9061, 'precision_test': 0.7577, 'recall_test': 0.8917, 'f1_score_test': 0.8193}
Random Forest: {'accuracy_cv_mean': 0.8734, 'accuracy_cv_std': 0.0029, 'precision_cv_mean': 0.9028, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.837, 'recall_cv_std': 0.0069, 'f1_cv_mean': 0.8686, 'f1_cv_std': 0.0033, 'accuracy_test': 0.896, 'precision_test': 0.7523, 'recall_test': 0.8411, 'f1_score_test': 0.7942}
Naive Bayes: {'accuracy_cv_mean': 0.8338, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.8649, 'precision_cv_std': 0.0043, 'recall_cv_mean': 0.7913, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8264, 'f1_cv_std': 0.0035, 'accuracy_test': 0.8548, 'precision_test': 0.6655, 'recall_test': 0.7866, 'f1_score_test': 0.721}
SVM: {'accuracy_cv_mean': 0.8454, 'accuracy_cv_std': 0.0053, 'precision_cv_mean': 0.8394, 'precision_cv_std': 0.014, 'recall_cv_mean': 0.8551, 'recall_cv_std': 0.023, 'f1_cv_mean': 0.8468, 'f1_cv_std': 0.0067, 'accuracy_test': 0.8168, 'precision_test': 0.5745, 'recall_test': 0.8952, 'f1_score_test': 0.6998}
Decision Tree: {'accuracy_cv_mean': 0.799, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8163, 'precision_cv_std': 0.0079, 'recall_cv_mean': 0.772, 'recall_cv_std': 0.0184, 'f1_cv_mean': 0.7933, 'f1_cv_std': 0.0084, 'accuracy_test': 0.8188, 'precision_test': 0.5919, 'recall_test': 0.7748, 'f1_score_test': 0.6711}
Diccionario global guardado en: outputs_cv/2/gpt/global_metrics.json

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
====================================

