2025-10-23 01:44:36.128282: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-23 01:44:36.128282: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-23 01:44:36.178728: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-23 01:44:36.178728: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-23 01:44:38.029952: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-23 01:44:38.030700: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_3.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_3.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 15 ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']

CAT_LOW 3 ['emotion', 'Key', 'Time signature']
CAT_HIGH 12 ['Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
After removing some columns that ignorated, for TF-IDF embbedings you are selecteing this columns:
--> ['text']
After removing some columns that ignorated, for numeric cols you are selecteing this columns:
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
--> PaTH:  ../../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy
Running experiment with TFIDF embeddings
Contaning the categorical cols
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 5023)
Shape of X_test after concatenation:  (21625, 5023)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [5023, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3166, Test Loss: 0.1976, F1: 0.9192, AUC: 0.9779
Epoch [10/30] Train Loss: 0.0848, Test Loss: 0.2076, F1: 0.9312, AUC: 0.9809
Epoch [20/30] Train Loss: 0.0680, Test Loss: 0.2683, F1: 0.9259, AUC: 0.9786
Mejores resultados en la época:  3
f1-score 0.9353500853866796
AUC según el mejor F1-score 0.9838067029475392

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3251, Test Loss: 0.1986, F1: 0.9229, AUC: 0.9777
Epoch [10/30] Train Loss: 0.0829, Test Loss: 0.1985, F1: 0.9319, AUC: 0.9821
Epoch [20/30] Train Loss: 0.0545, Test Loss: 0.2733, F1: 0.9272, AUC: 0.9794
Mejores resultados en la época:  4
f1-score 0.9362887596899225
AUC según el mejor F1-score 0.9839436423498439

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3194, Test Loss: 0.2020, F1: 0.9235, AUC: 0.9763
Epoch [10/30] Train Loss: 0.0832, Test Loss: 0.2142, F1: 0.9293, AUC: 0.9801
Epoch [20/30] Train Loss: 0.0674, Test Loss: 0.2753, F1: 0.9273, AUC: 0.9779
Mejores resultados en la época:  3
f1-score 0.9316413264695319
AUC según el mejor F1-score 0.982074288936212

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3217, Test Loss: 0.2051, F1: 0.9158, AUC: 0.9756
Epoch [10/30] Train Loss: 0.0825, Test Loss: 0.2200, F1: 0.9271, AUC: 0.9800
Epoch [20/30] Train Loss: 0.0661, Test Loss: 0.2856, F1: 0.9237, AUC: 0.9780
Mejores resultados en la época:  2
f1-score 0.9342636797649652
AUC según el mejor F1-score 0.9820232802324642

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3163, Test Loss: 0.2053, F1: 0.9169, AUC: 0.9753
Epoch [10/30] Train Loss: 0.0799, Test Loss: 0.2308, F1: 0.9274, AUC: 0.9787
Epoch [20/30] Train Loss: 0.0588, Test Loss: 0.3081, F1: 0.9213, AUC: 0.9760
Mejores resultados en la época:  4
f1-score 0.930322893906288
AUC según el mejor F1-score 0.9808924273032762
Epoch [0/30] Train Loss: 0.2944, Test Loss: 0.1920, F1: 0.8577, AUC: 0.9799
Epoch [10/30] Train Loss: 0.0909, Test Loss: 0.2016, F1: 0.8627, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0641, Test Loss: 0.2384, F1: 0.8576, AUC: 0.9815
Mejores resultados en la época:  8
f1-score 0.880638574345647
AUC según el mejor F1-score 0.9828963716787077
Confusion matrix Test saved: outputs_cv/4/tfidf/cm_mlp_1.png

========================================
Entrenando red 5 con capas [5023, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2329, Test Loss: 0.1757, F1: 0.9309, AUC: 0.9820
Epoch [10/30] Train Loss: 0.0047, Test Loss: 0.2839, F1: 0.9412, AUC: 0.9855
Epoch [20/30] Train Loss: 0.0004, Test Loss: 0.4501, F1: 0.9467, AUC: 0.9864
Mejores resultados en la época:  20
f1-score 0.9466553767993227
AUC según el mejor F1-score 0.9864291497186918

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2381, Test Loss: 0.1812, F1: 0.9246, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0027, Test Loss: 0.4331, F1: 0.9436, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5275, F1: 0.9486, AUC: 0.9858
Mejores resultados en la época:  18
f1-score 0.9486992462922441
AUC según el mejor F1-score 0.9859854977446516

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2387, Test Loss: 0.1901, F1: 0.9264, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.4327, F1: 0.9430, AUC: 0.9845
Epoch [20/30] Train Loss: 0.0003, Test Loss: 0.4752, F1: 0.9467, AUC: 0.9856
Mejores resultados en la época:  29
f1-score 0.9470886687605702
AUC según el mejor F1-score 0.98120183207139

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2354, Test Loss: 0.1850, F1: 0.9253, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0010, Test Loss: 0.4383, F1: 0.9410, AUC: 0.9839
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5763, F1: 0.9445, AUC: 0.9829
Mejores resultados en la época:  18
f1-score 0.9458869206503276
AUC según el mejor F1-score 0.9842388785423277

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2329, Test Loss: 0.1885, F1: 0.9218, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.4714, F1: 0.9353, AUC: 0.9841
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5846, F1: 0.9394, AUC: 0.9832
Mejores resultados en la época:  27
f1-score 0.9408054342552159
AUC según el mejor F1-score 0.9823280713790636
Epoch [0/30] Train Loss: 0.2230, Test Loss: 0.1518, F1: 0.8799, AUC: 0.9830
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 15 ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']

CAT_LOW 3 ['emotion', 'Key', 'Time signature']
CAT_HIGH 12 ['Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
After removing some columns that ignorated, for TF-IDF embbedings you are selecteing this columns:
--> ['text']
After removing some columns that ignorated, for numeric cols you are selecteing this columns:
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
--> PaTH:  ../../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy
Running experiment with TFIDF embeddings
Contaning the categorical cols
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 5023)
Shape of X_test after concatenation:  (21625, 5023)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [5023, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3166, Test Loss: 0.1976, F1: 0.9192, AUC: 0.9779
Epoch [10/30] Train Loss: 0.0848, Test Loss: 0.2076, F1: 0.9312, AUC: 0.9809
Epoch [20/30] Train Loss: 0.0680, Test Loss: 0.2683, F1: 0.9259, AUC: 0.9786
Mejores resultados en la época:  3
f1-score 0.9353500853866796
AUC según el mejor F1-score 0.9838067029475392

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3251, Test Loss: 0.1986, F1: 0.9229, AUC: 0.9777
Epoch [10/30] Train Loss: 0.0829, Test Loss: 0.1985, F1: 0.9319, AUC: 0.9821
Epoch [20/30] Train Loss: 0.0545, Test Loss: 0.2733, F1: 0.9272, AUC: 0.9794
Mejores resultados en la época:  4
f1-score 0.9362887596899225
AUC según el mejor F1-score 0.9839436423498439

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3194, Test Loss: 0.2020, F1: 0.9235, AUC: 0.9763
Epoch [10/30] Train Loss: 0.0832, Test Loss: 0.2142, F1: 0.9293, AUC: 0.9801
Epoch [20/30] Train Loss: 0.0674, Test Loss: 0.2753, F1: 0.9273, AUC: 0.9779
Mejores resultados en la época:  3
f1-score 0.9316413264695319
AUC según el mejor F1-score 0.982074288936212

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3217, Test Loss: 0.2051, F1: 0.9158, AUC: 0.9756
Epoch [10/30] Train Loss: 0.0825, Test Loss: 0.2200, F1: 0.9271, AUC: 0.9800
Epoch [20/30] Train Loss: 0.0661, Test Loss: 0.2856, F1: 0.9237, AUC: 0.9780
Mejores resultados en la época:  2
f1-score 0.9342636797649652
AUC según el mejor F1-score 0.9820232802324642

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3163, Test Loss: 0.2053, F1: 0.9169, AUC: 0.9753
Epoch [10/30] Train Loss: 0.0799, Test Loss: 0.2308, F1: 0.9274, AUC: 0.9787
Epoch [20/30] Train Loss: 0.0588, Test Loss: 0.3081, F1: 0.9213, AUC: 0.9760
Mejores resultados en la época:  4
f1-score 0.930322893906288
AUC según el mejor F1-score 0.9808924273032762
Epoch [0/30] Train Loss: 0.2944, Test Loss: 0.1920, F1: 0.8577, AUC: 0.9799
Epoch [10/30] Train Loss: 0.0909, Test Loss: 0.2016, F1: 0.8627, AUC: 0.9825
Epoch [20/30] Train Loss: 0.0641, Test Loss: 0.2384, F1: 0.8576, AUC: 0.9815
Mejores resultados en la época:  8
f1-score 0.880638574345647
AUC según el mejor F1-score 0.9828963716787077
Confusion matrix Test saved: outputs_cv/4/tfidf/cm_mlp_1.png

========================================
Entrenando red 5 con capas [5023, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2329, Test Loss: 0.1757, F1: 0.9309, AUC: 0.9820
Epoch [10/30] Train Loss: 0.0047, Test Loss: 0.2839, F1: 0.9412, AUC: 0.9855
Epoch [20/30] Train Loss: 0.0004, Test Loss: 0.4501, F1: 0.9467, AUC: 0.9864
Mejores resultados en la época:  20
f1-score 0.9466553767993227
AUC según el mejor F1-score 0.9864291497186918

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2381, Test Loss: 0.1812, F1: 0.9246, AUC: 0.9829
Epoch [10/30] Train Loss: 0.0027, Test Loss: 0.4331, F1: 0.9436, AUC: 0.9858
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5275, F1: 0.9486, AUC: 0.9858
Mejores resultados en la época:  18
f1-score 0.9486992462922441
AUC según el mejor F1-score 0.9859854977446516

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2387, Test Loss: 0.1901, F1: 0.9264, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.4327, F1: 0.9430, AUC: 0.9845
Epoch [20/30] Train Loss: 0.0003, Test Loss: 0.4752, F1: 0.9467, AUC: 0.9856
Mejores resultados en la época:  29
f1-score 0.9470886687605702
AUC según el mejor F1-score 0.98120183207139

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2354, Test Loss: 0.1850, F1: 0.9253, AUC: 0.9797
Epoch [10/30] Train Loss: 0.0010, Test Loss: 0.4383, F1: 0.9410, AUC: 0.9839
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5763, F1: 0.9445, AUC: 0.9829
Mejores resultados en la época:  18
f1-score 0.9458869206503276
AUC según el mejor F1-score 0.9842388785423277

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2329, Test Loss: 0.1885, F1: 0.9218, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0038, Test Loss: 0.4714, F1: 0.9353, AUC: 0.9841
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5846, F1: 0.9394, AUC: 0.9832
Mejores resultados en la época:  27
f1-score 0.9408054342552159
AUC según el mejor F1-score 0.9823280713790636
Epoch [0/30] Train Loss: 0.2230, Test Loss: 0.1518, F1: 0.8799, AUC: 0.9830
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.3765, F1: 0.8910, AUC: 0.9876
Epoch [20/30] Train Loss: 0.0034, Test Loss: 0.3872, F1: 0.8878, AUC: 0.9875
Mejores resultados en la época:  8
f1-score 0.9118825100133512
AUC según el mejor F1-score 0.9875765718684454
Confusion matrix Test saved: outputs_cv/4/tfidf/cm_mlp_5.png

========================================
Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2380, Test Loss: 0.1718, F1: 0.9308, AUC: 0.9824
Epoch [10/30] Train Loss: 0.0036, Test Loss: 0.3521, F1: 0.9459, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8337, F1: 0.9426, AUC: 0.9798
Mejores resultados en la época:  7
f1-score 0.9468866674695893
AUC según el mejor F1-score 0.9843085402300793

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2370, Test Loss: 0.1705, F1: 0.9319, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0054, Test Loss: 0.4684, F1: 0.9460, AUC: 0.9842
Epoch [20/30] Train Loss: 0.0027, Test Loss: 0.5343, F1: 0.9478, AUC: 0.9874
Mejores resultados en la época:  22
f1-score 0.9498295177788602
AUC según el mejor F1-score 0.9882725940917763

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2370, Test Loss: 0.1805, F1: 0.9239, AUC: 0.9806
Epoch [10/30] Train Loss: 0.0030, Test Loss: 0.4139, F1: 0.9394, AUC: 0.9835
Epoch [20/30] Train Loss: 0.0013, Test Loss: 0.4762, F1: 0.9449, AUC: 0.9849
Mejores resultados en la época:  23
f1-score 0.9467512265166926
AUC según el mejor F1-score 0.9838334629078782

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2444, Test Loss: 0.1815, F1: 0.9289, AUC: 0.9805
Epoch [10/30] Train Loss: 0.0010, Test Loss: 0.4902, F1: 0.9414, AUC: 0.9837
Epoch [20/30] Train Loss: 0.0013, Test Loss: 0.5841, F1: 0.9434, AUC: 0.9846
Mejores resultados en la época:  23
f1-score 0.9463355542095699
AUC según el mejor F1-score 0.9839881192205611

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2377, Test Loss: 0.1824, F1: 0.9252, AUC: 0.9800
Epoch [10/30] Train Loss: 0.0029, Test Loss: 0.6700, F1: 0.9407, AUC: 0.9828
Epoch [20/30] Train Loss: 0.0025, Test Loss: 0.4500, F1: 0.9400, AUC: 0.9846
Mejores resultados en la época:  19
f1-score 0.9422104502769083
AUC según el mejor F1-score 0.9848511316101379
Epoch [0/30] Train Loss: 0.2233, Test Loss: 0.2113, F1: 0.8548, AUC: 0.9822
Epoch [10/30] Train Loss: 0.0027, Test Loss: 0.4571, F1: 0.8901, AUC: 0.9871
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5069, F1: 0.9044, AUC: 0.9861
Mejores resultados en la época:  29
f1-score 0.9086822141050857
AUC según el mejor F1-score 0.9846653754616911
Confusion matrix Test saved: outputs_cv/4/tfidf/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9337, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9321, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9336, 'f1_cv_std': 0.0022, 'params': 160801, 'accuracy_test': 0.9405, 'precision_test': 0.845, 'recall_test': 0.9194, 'f1_score_test': 0.8806}, 'MLP_2744833': {'accuracy_cv_mean': 0.9459, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9463, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9453, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9458, 'f1_cv_std': 0.0027, 'params': 2744833, 'accuracy_test': 0.9573, 'precision_test': 0.8977, 'recall_test': 0.9266, 'f1_score_test': 0.9119}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9429, 'precision_cv_std': 0.0071, 'recall_cv_mean': 0.9499, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9464, 'f1_cv_std': 0.0024, 'params': 5843969, 'accuracy_test': 0.9549, 'precision_test': 0.8793, 'recall_test': 0.9401, 'f1_score_test': 0.9087}}}
Saved on: outputs_cv/4/tfidf

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.0005, 'precision_cv_mean': 0.9364, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.9202, 'recall_cv_std': 0.0012, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0005}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 46, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

Confusion matrix Test saved as: outputs_cv/4/tfidf/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/4/tfidf/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8348, 'accuracy_cv_std': 0.0146, 'precision_cv_mean': 0.8051, 'precision_cv_std': 0.0362, 'recall_cv_mean': 0.8891, 'recall_cv_std': 0.0395, 'f1_cv_mean': 0.8434, 'f1_cv_std': 0.0099}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 46, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.81      0.87     16465
           1       0.58      0.86      0.70      5160

    accuracy                           0.82     21625
   macro avg       0.77      0.83      0.78     21625
weighted avg       0.86      0.82      0.83     21625

Confusion matrix Test saved as: outputs_cv/4/tfidf/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/4/tfidf/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8312, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7441, 'recall_cv_std': 0.0134, 'f1_cv_mean': 0.8151, 'f1_cv_std': 0.0043}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 46, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.93      0.92     16465
           1       0.75      0.73      0.74      5160

    accuracy                           0.88     21625
   macro avg       0.84      0.83      0.83     21625
weighted avg       0.88      0.88      0.88     21625

Confusion matrix Test saved as: outputs_cv/4/tfidf/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/4/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8401, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.8781, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.7898, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0027}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 46, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.70      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

Confusion matrix Test saved as: outputs_cv/4/tfidf/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/4/tfidf/random_forest_model.pkl

Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.3765, F1: 0.8910, AUC: 0.9876
Epoch [20/30] Train Loss: 0.0034, Test Loss: 0.3872, F1: 0.8878, AUC: 0.9875
Mejores resultados en la época:  8
f1-score 0.9118825100133512
AUC según el mejor F1-score 0.9875765718684454
Confusion matrix Test saved: outputs_cv/4/tfidf/cm_mlp_5.png

========================================
Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.2380, Test Loss: 0.1718, F1: 0.9308, AUC: 0.9824
Epoch [10/30] Train Loss: 0.0036, Test Loss: 0.3521, F1: 0.9459, AUC: 0.9868
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8337, F1: 0.9426, AUC: 0.9798
Mejores resultados en la época:  7
f1-score 0.9468866674695893
AUC según el mejor F1-score 0.9843085402300793

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.2370, Test Loss: 0.1705, F1: 0.9319, AUC: 0.9828
Epoch [10/30] Train Loss: 0.0054, Test Loss: 0.4684, F1: 0.9460, AUC: 0.9842
Epoch [20/30] Train Loss: 0.0027, Test Loss: 0.5343, F1: 0.9478, AUC: 0.9874
Mejores resultados en la época:  22
f1-score 0.9498295177788602
AUC según el mejor F1-score 0.9882725940917763

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.2370, Test Loss: 0.1805, F1: 0.9239, AUC: 0.9806
Epoch [10/30] Train Loss: 0.0030, Test Loss: 0.4139, F1: 0.9394, AUC: 0.9835
Epoch [20/30] Train Loss: 0.0013, Test Loss: 0.4762, F1: 0.9449, AUC: 0.9849
Mejores resultados en la época:  23
f1-score 0.9467512265166926
AUC según el mejor F1-score 0.9838334629078782

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.2444, Test Loss: 0.1815, F1: 0.9289, AUC: 0.9805
Epoch [10/30] Train Loss: 0.0010, Test Loss: 0.4902, F1: 0.9414, AUC: 0.9837
Epoch [20/30] Train Loss: 0.0013, Test Loss: 0.5841, F1: 0.9434, AUC: 0.9846
Mejores resultados en la época:  23
f1-score 0.9463355542095699
AUC según el mejor F1-score 0.9839881192205611

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.2377, Test Loss: 0.1824, F1: 0.9252, AUC: 0.9800
Epoch [10/30] Train Loss: 0.0029, Test Loss: 0.6700, F1: 0.9407, AUC: 0.9828
Epoch [20/30] Train Loss: 0.0025, Test Loss: 0.4500, F1: 0.9400, AUC: 0.9846
Mejores resultados en la época:  19
f1-score 0.9422104502769083
AUC según el mejor F1-score 0.9848511316101379
Epoch [0/30] Train Loss: 0.2233, Test Loss: 0.2113, F1: 0.8548, AUC: 0.9822
Epoch [10/30] Train Loss: 0.0027, Test Loss: 0.4571, F1: 0.8901, AUC: 0.9871
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.5069, F1: 0.9044, AUC: 0.9861
Mejores resultados en la época:  29
f1-score 0.9086822141050857
AUC según el mejor F1-score 0.9846653754616911
Confusion matrix Test saved: outputs_cv/4/tfidf/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9337, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9321, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9336, 'f1_cv_std': 0.0022, 'params': 160801, 'accuracy_test': 0.9405, 'precision_test': 0.845, 'recall_test': 0.9194, 'f1_score_test': 0.8806}, 'MLP_2744833': {'accuracy_cv_mean': 0.9459, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9463, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9453, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9458, 'f1_cv_std': 0.0027, 'params': 2744833, 'accuracy_test': 0.9573, 'precision_test': 0.8977, 'recall_test': 0.9266, 'f1_score_test': 0.9119}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9429, 'precision_cv_std': 0.0071, 'recall_cv_mean': 0.9499, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9464, 'f1_cv_std': 0.0024, 'params': 5843969, 'accuracy_test': 0.9549, 'precision_test': 0.8793, 'recall_test': 0.9401, 'f1_score_test': 0.9087}}}
Saved on: outputs_cv/4/tfidf

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.0005, 'precision_cv_mean': 0.9364, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.9202, 'recall_cv_std': 0.0012, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0005}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 46, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

Confusion matrix Test saved as: outputs_cv/4/tfidf/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/4/tfidf/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8348, 'accuracy_cv_std': 0.0146, 'precision_cv_mean': 0.8051, 'precision_cv_std': 0.0362, 'recall_cv_mean': 0.8891, 'recall_cv_std': 0.0395, 'f1_cv_mean': 0.8434, 'f1_cv_std': 0.0099}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 46, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.81      0.87     16465
           1       0.58      0.86      0.70      5160

    accuracy                           0.82     21625
   macro avg       0.77      0.83      0.78     21625
weighted avg       0.86      0.82      0.83     21625

Confusion matrix Test saved as: outputs_cv/4/tfidf/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/4/tfidf/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8312, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7441, 'recall_cv_std': 0.0134, 'f1_cv_mean': 0.8151, 'f1_cv_std': 0.0043}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 46, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.93      0.92     16465
           1       0.75      0.73      0.74      5160

    accuracy                           0.88     21625
   macro avg       0.84      0.83      0.83     21625
weighted avg       0.88      0.88      0.88     21625

Confusion matrix Test saved as: outputs_cv/4/tfidf/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/4/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8401, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.8781, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.7898, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0027}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 46, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.70      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

Confusion matrix Test saved as: outputs_cv/4/tfidf/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/4/tfidf/random_forest_model.pkl

/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:15:08] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:15:11] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:19:07] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:19:11] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:23:08] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:23:11] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:27:08] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:27:13] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:31:03] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:31:09] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:35:09] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:35:15] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9077, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9274, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8846, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.9055, 'f1_cv_std': 0.0018}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 46, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/4/tfidf/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/4/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9045, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9301, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.9016, 'f1_cv_std': 0.0018}
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.94      0.95     16465
           1       0.81      0.86      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.90      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/4/tfidf/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/4/tfidf/naive_bayes_model.pkl


Resumen Final:
Logistic Regression: {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.0005, 'precision_cv_mean': 0.9364, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.9202, 'recall_cv_std': 0.0012, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0005, 'accuracy_test': 0.9354, 'precision_test': 0.8255, 'recall_test': 0.9248, 'f1_score_test': 0.8723}
XGBoost: {'accuracy_cv_mean': 0.9077, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9274, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8846, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.9055, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9216, 'precision_test': 0.8038, 'recall_test': 0.8884, 'f1_score_test': 0.844}
Naive Bayes: {'accuracy_cv_mean': 0.9045, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9301, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.9016, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9191, 'precision_test': 0.8094, 'recall_test': 0.8643, 'f1_score_test': 0.836}
Decision Tree: {'accuracy_cv_mean': 0.8312, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7441, 'recall_cv_std': 0.0134, 'f1_cv_mean': 0.8151, 'f1_cv_std': 0.0043, 'accuracy_test': 0.8791, 'precision_test': 0.7538, 'recall_test': 0.7324, 'f1_score_test': 0.7429}
Random Forest: {'accuracy_cv_mean': 0.8401, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.8781, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.7898, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8684, 'precision_test': 0.6998, 'recall_test': 0.7855, 'f1_score_test': 0.7401}
SVM: {'accuracy_cv_mean': 0.8348, 'accuracy_cv_std': 0.0146, 'precision_cv_mean': 0.8051, 'precision_cv_std': 0.0362, 'recall_cv_mean': 0.8891, 'recall_cv_std': 0.0395, 'f1_cv_mean': 0.8434, 'f1_cv_std': 0.0099, 'accuracy_test': 0.8203, 'precision_test': 0.5836, 'recall_test': 0.8624, 'f1_score_test': 0.6961}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9337, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9321, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9336, 'f1_cv_std': 0.0022, 'params': 160801, 'accuracy_test': 0.9405, 'precision_test': 0.845, 'recall_test': 0.9194, 'f1_score_test': 0.8806}, 'MLP_2744833': {'accuracy_cv_mean': 0.9459, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9463, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9453, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9458, 'f1_cv_std': 0.0027, 'params': 2744833, 'accuracy_test': 0.9573, 'precision_test': 0.8977, 'recall_test': 0.9266, 'f1_score_test': 0.9119}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9429, 'precision_cv_std': 0.0071, 'recall_cv_mean': 0.9499, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9464, 'f1_cv_std': 0.0024, 'params': 5843969, 'accuracy_test': 0.9549, 'precision_test': 0.8793, 'recall_test': 0.9401, 'f1_score_test': 0.9087}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.0005, 'precision_cv_mean': 0.9364, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.9202, 'recall_cv_std': 0.0012, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0005, 'accuracy_test': 0.9354, 'precision_test': 0.8255, 'recall_test': 0.9248, 'f1_score_test': 0.8723}, 'SVM': {'accuracy_cv_mean': 0.8348, 'accuracy_cv_std': 0.0146, 'precision_cv_mean': 0.8051, 'precision_cv_std': 0.0362, 'recall_cv_mean': 0.8891, 'recall_cv_std': 0.0395, 'f1_cv_mean': 0.8434, 'f1_cv_std': 0.0099, 'accuracy_test': 0.8203, 'precision_test': 0.5836, 'recall_test': 0.8624, 'f1_score_test': 0.6961}, 'Decision Tree': {'accuracy_cv_mean': 0.8312, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7441, 'recall_cv_std': 0.0134, 'f1_cv_mean': 0.8151, 'f1_cv_std': 0.0043, 'accuracy_test': 0.8791, 'precision_test': 0.7538, 'recall_test': 0.7324, 'f1_score_test': 0.7429}, 'Random Forest': {'accuracy_cv_mean': 0.8401, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.8781, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.7898, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8684, 'precision_test': 0.6998, 'recall_test': 0.7855, 'f1_score_test': 0.7401}, 'XGBoost': {'accuracy_cv_mean': 0.9077, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9274, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8846, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.9055, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9216, 'precision_test': 0.8038, 'recall_test': 0.8884, 'f1_score_test': 0.844}, 'Naive Bayes': {'accuracy_cv_mean': 0.9045, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9301, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.9016, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9191, 'precision_test': 0.8094, 'recall_test': 0.8643, 'f1_score_test': 0.836}}}
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_3.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9077, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9274, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8846, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.9055, 'f1_cv_std': 0.0018}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 46, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/4/tfidf/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/4/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9045, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9301, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.9016, 'f1_cv_std': 0.0018}
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.94      0.95     16465
           1       0.81      0.86      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.90      0.89     21625
weighted avg       0.92      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/4/tfidf/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/4/tfidf/naive_bayes_model.pkl


Resumen Final:
Logistic Regression: {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.0005, 'precision_cv_mean': 0.9364, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.9202, 'recall_cv_std': 0.0012, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0005, 'accuracy_test': 0.9354, 'precision_test': 0.8255, 'recall_test': 0.9248, 'f1_score_test': 0.8723}
XGBoost: {'accuracy_cv_mean': 0.9077, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9274, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8846, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.9055, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9216, 'precision_test': 0.8038, 'recall_test': 0.8884, 'f1_score_test': 0.844}
Naive Bayes: {'accuracy_cv_mean': 0.9045, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9301, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.9016, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9191, 'precision_test': 0.8094, 'recall_test': 0.8643, 'f1_score_test': 0.836}
Decision Tree: {'accuracy_cv_mean': 0.8312, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7441, 'recall_cv_std': 0.0134, 'f1_cv_mean': 0.8151, 'f1_cv_std': 0.0043, 'accuracy_test': 0.8791, 'precision_test': 0.7538, 'recall_test': 0.7324, 'f1_score_test': 0.7429}
Random Forest: {'accuracy_cv_mean': 0.8401, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.8781, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.7898, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8684, 'precision_test': 0.6998, 'recall_test': 0.7855, 'f1_score_test': 0.7401}
SVM: {'accuracy_cv_mean': 0.8348, 'accuracy_cv_std': 0.0146, 'precision_cv_mean': 0.8051, 'precision_cv_std': 0.0362, 'recall_cv_mean': 0.8891, 'recall_cv_std': 0.0395, 'f1_cv_mean': 0.8434, 'f1_cv_std': 0.0099, 'accuracy_test': 0.8203, 'precision_test': 0.5836, 'recall_test': 0.8624, 'f1_score_test': 0.6961}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9337, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9321, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9336, 'f1_cv_std': 0.0022, 'params': 160801, 'accuracy_test': 0.9405, 'precision_test': 0.845, 'recall_test': 0.9194, 'f1_score_test': 0.8806}, 'MLP_2744833': {'accuracy_cv_mean': 0.9459, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9463, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9453, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9458, 'f1_cv_std': 0.0027, 'params': 2744833, 'accuracy_test': 0.9573, 'precision_test': 0.8977, 'recall_test': 0.9266, 'f1_score_test': 0.9119}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9429, 'precision_cv_std': 0.0071, 'recall_cv_mean': 0.9499, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9464, 'f1_cv_std': 0.0024, 'params': 5843969, 'accuracy_test': 0.9549, 'precision_test': 0.8793, 'recall_test': 0.9401, 'f1_score_test': 0.9087}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.0005, 'precision_cv_mean': 0.9364, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.9202, 'recall_cv_std': 0.0012, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0005, 'accuracy_test': 0.9354, 'precision_test': 0.8255, 'recall_test': 0.9248, 'f1_score_test': 0.8723}, 'SVM': {'accuracy_cv_mean': 0.8348, 'accuracy_cv_std': 0.0146, 'precision_cv_mean': 0.8051, 'precision_cv_std': 0.0362, 'recall_cv_mean': 0.8891, 'recall_cv_std': 0.0395, 'f1_cv_mean': 0.8434, 'f1_cv_std': 0.0099, 'accuracy_test': 0.8203, 'precision_test': 0.5836, 'recall_test': 0.8624, 'f1_score_test': 0.6961}, 'Decision Tree': {'accuracy_cv_mean': 0.8312, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7441, 'recall_cv_std': 0.0134, 'f1_cv_mean': 0.8151, 'f1_cv_std': 0.0043, 'accuracy_test': 0.8791, 'precision_test': 0.7538, 'recall_test': 0.7324, 'f1_score_test': 0.7429}, 'Random Forest': {'accuracy_cv_mean': 0.8401, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.8781, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.7898, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8684, 'precision_test': 0.6998, 'recall_test': 0.7855, 'f1_score_test': 0.7401}, 'XGBoost': {'accuracy_cv_mean': 0.9077, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9274, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8846, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.9055, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9216, 'precision_test': 0.8038, 'recall_test': 0.8884, 'f1_score_test': 0.844}, 'Naive Bayes': {'accuracy_cv_mean': 0.9045, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9301, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.9016, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9191, 'precision_test': 0.8094, 'recall_test': 0.8643, 'f1_score_test': 0.836}}}
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_3.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 323)
Shape of X_test after concatenation:  (21625, 323)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 323)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 323)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [323, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.5579, Test Loss: 0.4470, F1: 0.8037, AUC: 0.8858
Epoch [10/30] Train Loss: 0.3642, Test Loss: 0.3502, F1: 0.8482, AUC: 0.9238
Epoch [20/30] Train Loss: 0.3508, Test Loss: 0.3465, F1: 0.8471, AUC: 0.9270
Mejores resultados en la época:  27
f1-score 0.8543263964950711
AUC según el mejor F1-score 0.9282371805705786

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.5527, Test Loss: 0.4415, F1: 0.8010, AUC: 0.8836
Epoch [10/30] Train Loss: 0.3616, Test Loss: 0.3584, F1: 0.8460, AUC: 0.9234
Epoch [20/30] Train Loss: 0.3451, Test Loss: 0.3412, F1: 0.8415, AUC: 0.9288
Mejores resultados en la época:  28
f1-score 0.8554913294797688
AUC según el mejor F1-score 0.9319299377291029

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.5669, Test Loss: 0.4552, F1: 0.7943, AUC: 0.8795
Epoch [10/30] Train Loss: 0.3633, Test Loss: 0.3961, F1: 0.8315, AUC: 0.9216
Epoch [20/30] Train Loss: 0.3482, Test Loss: 0.3426, F1: 0.8497, AUC: 0.9271
Mejores resultados en la época:  25
f1-score 0.8526328473310038
AUC según el mejor F1-score 0.9287058613233128

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.5325, Test Loss: 0.4378, F1: 0.8044, AUC: 0.8841
Epoch [10/30] Train Loss: 0.3605, Test Loss: 0.3735, F1: 0.8413, AUC: 0.9209
Epoch [20/30] Train Loss: 0.3436, Test Loss: 0.3425, F1: 0.8406, AUC: 0.9280
Mejores resultados en la época:  26
f1-score 0.8527150408457472
AUC según el mejor F1-score 0.9303535354246849

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.5319, Test Loss: 0.4359, F1: 0.7984, AUC: 0.8865
Epoch [10/30] Train Loss: 0.3579, Test Loss: 0.3690, F1: 0.8389, AUC: 0.9202
Epoch [20/30] Train Loss: 0.3404, Test Loss: 0.3545, F1: 0.8363, AUC: 0.9251
Mejores resultados en la época:  25
f1-score 0.8475439660400242
AUC según el mejor F1-score 0.9275015296788214
Epoch [0/30] Train Loss: 0.5262, Test Loss: 0.3979, F1: 0.6931, AUC: 0.8896
Epoch [10/30] Train Loss: 0.3562, Test Loss: 0.3224, F1: 0.7476, AUC: 0.9239
Epoch [20/30] Train Loss: 0.3449, Test Loss: 0.3778, F1: 0.7200, AUC: 0.9278
Mejores resultados en la época:  29
f1-score 0.762088601486343
AUC según el mejor F1-score 0.9298588090311372
Confusion matrix Test saved: outputs_cv/4/lyrics_bert/cm_mlp_1.png

========================================
Entrenando red 5 con capas [323, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4883, Test Loss: 0.3977, F1: 0.8156, AUC: 0.9057
Epoch [10/30] Train Loss: 0.3581, Test Loss: 0.3376, F1: 0.8525, AUC: 0.9309
Epoch [20/30] Train Loss: 0.3346, Test Loss: 0.3718, F1: 0.8547, AUC: 0.9319
Mejores resultados en la época:  24
f1-score 0.8608818674840839
AUC según el mejor F1-score 0.9374811623963404

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4797, Test Loss: 0.4025, F1: 0.7974, AUC: 0.9057
Epoch [10/30] Train Loss: 0.3541, Test Loss: 0.3419, F1: 0.8409, AUC: 0.9290
Epoch [20/30] Train Loss: 0.3339, Test Loss: 0.3689, F1: 0.8086, AUC: 0.9314
Mejores resultados en la época:  28
f1-score 0.8615238321645073
AUC según el mejor F1-score 0.9367607267535755

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4737, Test Loss: 0.3856, F1: 0.8267, AUC: 0.9067
Epoch [10/30] Train Loss: 0.3594, Test Loss: 0.3490, F1: 0.8298, AUC: 0.9279
Epoch [20/30] Train Loss: 0.3365, Test Loss: 0.3442, F1: 0.8521, AUC: 0.9330
Mejores resultados en la época:  28
f1-score 0.857347157668931
AUC según el mejor F1-score 0.9362985012544318

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4622, Test Loss: 0.3964, F1: 0.8111, AUC: 0.9043
Epoch [10/30] Train Loss: 0.3490, Test Loss: 0.3916, F1: 0.8410, AUC: 0.9272
Epoch [20/30] Train Loss: 0.3299, Test Loss: 0.3344, F1: 0.8479, AUC: 0.9339
Mejores resultados en la época:  29
f1-score 0.8583469913369234
AUC según el mejor F1-score 0.9355586109999756

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4800, Test Loss: 0.4218, F1: 0.7782, AUC: 0.9054
Epoch [10/30] Train Loss: 0.3557, Test Loss: 0.3676, F1: 0.8405, AUC: 0.9258
Epoch [20/30] Train Loss: 0.3255, Test Loss: 0.3550, F1: 0.8401, AUC: 0.9316
Mejores resultados en la época:  26
f1-score 0.8542957832057358
AUC según el mejor F1-score 0.9338734989659698
Epoch [0/30] Train Loss: 0.4642, Test Loss: 0.3132, F1: 0.7240, AUC: 0.9086
Epoch [10/30] Train Loss: 0.3516, Test Loss: 0.3000, F1: 0.7552, AUC: 0.9317
Epoch [20/30] Train Loss: 0.3253, Test Loss: 0.2958, F1: 0.7628, AUC: 0.9364
Mejores resultados en la época:  28
f1-score 0.7757930697901415
AUC según el mejor F1-score 0.9390464504222017
Confusion matrix Test saved: outputs_cv/4/lyrics_bert/cm_mlp_5.png

========================================
Entrenando red 6 con capas [323, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4912, Test Loss: 0.3892, F1: 0.8277, AUC: 0.9087
Epoch [10/30] Train Loss: 0.3528, Test Loss: 0.3417, F1: 0.8366, AUC: 0.9312
Epoch [20/30] Train Loss: 0.3341, Test Loss: 0.3590, F1: 0.8450, AUC: 0.9340
Mejores resultados en la época:  29
f1-score 0.8607594936708861
AUC según el mejor F1-score 0.9370306150377832

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4864, Test Loss: 0.4641, F1: 0.8104, AUC: 0.9052
Epoch [10/30] Train Loss: 0.3579, Test Loss: 0.3449, F1: 0.8318, AUC: 0.9293
Epoch [20/30] Train Loss: 0.3317, Test Loss: 0.3438, F1: 0.8235, AUC: 0.9341
Mejores resultados en la época:  26
f1-score 0.8569760653823701
AUC según el mejor F1-score 0.9348708925808245

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4787, Test Loss: 0.4447, F1: 0.8287, AUC: 0.9065
Epoch [10/30] Train Loss: 0.3532, Test Loss: 0.3880, F1: 0.8044, AUC: 0.9284
Epoch [20/30] Train Loss: 0.3352, Test Loss: 0.3432, F1: 0.8397, AUC: 0.9342
Mejores resultados en la época:  27
f1-score 0.8584778064674782
AUC según el mejor F1-score 0.9361057239085692

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.5360, Test Loss: 0.4445, F1: 0.7428, AUC: 0.9009
Epoch [10/30] Train Loss: 0.3561, Test Loss: 0.3739, F1: 0.8450, AUC: 0.9248
Epoch [20/30] Train Loss: 0.3411, Test Loss: 0.3437, F1: 0.8452, AUC: 0.9310
Mejores resultados en la época:  28
f1-score 0.856384262611634
AUC según el mejor F1-score 0.9337558087880341

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4911, Test Loss: 0.4046, F1: 0.8238, AUC: 0.9030
Epoch [10/30] Train Loss: 0.3557, Test Loss: 0.3862, F1: 0.8124, AUC: 0.9249
Epoch [20/30] Train Loss: 0.3342, Test Loss: 0.3581, F1: 0.8470, AUC: 0.9286
Mejores resultados en la época:  25
f1-score 0.8521928761812455
AUC según el mejor F1-score 0.9330000969696628
Epoch [0/30] Train Loss: 0.4641, Test Loss: 0.4469, F1: 0.6677, AUC: 0.9102
Epoch [10/30] Train Loss: 0.3498, Test Loss: 0.2943, F1: 0.7599, AUC: 0.9313
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 323)
Shape of X_test after concatenation:  (21625, 323)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 323)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 323)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [323, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.5579, Test Loss: 0.4470, F1: 0.8037, AUC: 0.8858
Epoch [10/30] Train Loss: 0.3642, Test Loss: 0.3502, F1: 0.8482, AUC: 0.9238
Epoch [20/30] Train Loss: 0.3508, Test Loss: 0.3465, F1: 0.8471, AUC: 0.9270
Mejores resultados en la época:  27
f1-score 0.8543263964950711
AUC según el mejor F1-score 0.9282371805705786

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.5527, Test Loss: 0.4415, F1: 0.8010, AUC: 0.8836
Epoch [10/30] Train Loss: 0.3616, Test Loss: 0.3584, F1: 0.8460, AUC: 0.9234
Epoch [20/30] Train Loss: 0.3451, Test Loss: 0.3412, F1: 0.8415, AUC: 0.9288
Mejores resultados en la época:  28
f1-score 0.8554913294797688
AUC según el mejor F1-score 0.9319299377291029

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.5669, Test Loss: 0.4552, F1: 0.7943, AUC: 0.8795
Epoch [10/30] Train Loss: 0.3633, Test Loss: 0.3961, F1: 0.8315, AUC: 0.9216
Epoch [20/30] Train Loss: 0.3482, Test Loss: 0.3426, F1: 0.8497, AUC: 0.9271
Mejores resultados en la época:  25
f1-score 0.8526328473310038
AUC según el mejor F1-score 0.9287058613233128

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.5325, Test Loss: 0.4378, F1: 0.8044, AUC: 0.8841
Epoch [10/30] Train Loss: 0.3605, Test Loss: 0.3735, F1: 0.8413, AUC: 0.9209
Epoch [20/30] Train Loss: 0.3436, Test Loss: 0.3425, F1: 0.8406, AUC: 0.9280
Mejores resultados en la época:  26
f1-score 0.8527150408457472
AUC según el mejor F1-score 0.9303535354246849

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.5319, Test Loss: 0.4359, F1: 0.7984, AUC: 0.8865
Epoch [10/30] Train Loss: 0.3579, Test Loss: 0.3690, F1: 0.8389, AUC: 0.9202
Epoch [20/30] Train Loss: 0.3404, Test Loss: 0.3545, F1: 0.8363, AUC: 0.9251
Mejores resultados en la época:  25
f1-score 0.8475439660400242
AUC según el mejor F1-score 0.9275015296788214
Epoch [0/30] Train Loss: 0.5262, Test Loss: 0.3979, F1: 0.6931, AUC: 0.8896
Epoch [10/30] Train Loss: 0.3562, Test Loss: 0.3224, F1: 0.7476, AUC: 0.9239
Epoch [20/30] Train Loss: 0.3449, Test Loss: 0.3778, F1: 0.7200, AUC: 0.9278
Mejores resultados en la época:  29
f1-score 0.762088601486343
AUC según el mejor F1-score 0.9298588090311372
Confusion matrix Test saved: outputs_cv/4/lyrics_bert/cm_mlp_1.png

========================================
Entrenando red 5 con capas [323, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4883, Test Loss: 0.3977, F1: 0.8156, AUC: 0.9057
Epoch [10/30] Train Loss: 0.3581, Test Loss: 0.3376, F1: 0.8525, AUC: 0.9309
Epoch [20/30] Train Loss: 0.3346, Test Loss: 0.3718, F1: 0.8547, AUC: 0.9319
Mejores resultados en la época:  24
f1-score 0.8608818674840839
AUC según el mejor F1-score 0.9374811623963404

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4797, Test Loss: 0.4025, F1: 0.7974, AUC: 0.9057
Epoch [10/30] Train Loss: 0.3541, Test Loss: 0.3419, F1: 0.8409, AUC: 0.9290
Epoch [20/30] Train Loss: 0.3339, Test Loss: 0.3689, F1: 0.8086, AUC: 0.9314
Mejores resultados en la época:  28
f1-score 0.8615238321645073
AUC según el mejor F1-score 0.9367607267535755

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4737, Test Loss: 0.3856, F1: 0.8267, AUC: 0.9067
Epoch [10/30] Train Loss: 0.3594, Test Loss: 0.3490, F1: 0.8298, AUC: 0.9279
Epoch [20/30] Train Loss: 0.3365, Test Loss: 0.3442, F1: 0.8521, AUC: 0.9330
Mejores resultados en la época:  28
f1-score 0.857347157668931
AUC según el mejor F1-score 0.9362985012544318

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4622, Test Loss: 0.3964, F1: 0.8111, AUC: 0.9043
Epoch [10/30] Train Loss: 0.3490, Test Loss: 0.3916, F1: 0.8410, AUC: 0.9272
Epoch [20/30] Train Loss: 0.3299, Test Loss: 0.3344, F1: 0.8479, AUC: 0.9339
Mejores resultados en la época:  29
f1-score 0.8583469913369234
AUC según el mejor F1-score 0.9355586109999756

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4800, Test Loss: 0.4218, F1: 0.7782, AUC: 0.9054
Epoch [10/30] Train Loss: 0.3557, Test Loss: 0.3676, F1: 0.8405, AUC: 0.9258
Epoch [20/30] Train Loss: 0.3255, Test Loss: 0.3550, F1: 0.8401, AUC: 0.9316
Mejores resultados en la época:  26
f1-score 0.8542957832057358
AUC según el mejor F1-score 0.9338734989659698
Epoch [0/30] Train Loss: 0.4642, Test Loss: 0.3132, F1: 0.7240, AUC: 0.9086
Epoch [10/30] Train Loss: 0.3516, Test Loss: 0.3000, F1: 0.7552, AUC: 0.9317
Epoch [20/30] Train Loss: 0.3253, Test Loss: 0.2958, F1: 0.7628, AUC: 0.9364
Mejores resultados en la época:  28
f1-score 0.7757930697901415
AUC según el mejor F1-score 0.9390464504222017
Confusion matrix Test saved: outputs_cv/4/lyrics_bert/cm_mlp_5.png

========================================
Entrenando red 6 con capas [323, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4912, Test Loss: 0.3892, F1: 0.8277, AUC: 0.9087
Epoch [10/30] Train Loss: 0.3528, Test Loss: 0.3417, F1: 0.8366, AUC: 0.9312
Epoch [20/30] Train Loss: 0.3341, Test Loss: 0.3590, F1: 0.8450, AUC: 0.9340
Mejores resultados en la época:  29
f1-score 0.8607594936708861
AUC según el mejor F1-score 0.9370306150377832

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4864, Test Loss: 0.4641, F1: 0.8104, AUC: 0.9052
Epoch [10/30] Train Loss: 0.3579, Test Loss: 0.3449, F1: 0.8318, AUC: 0.9293
Epoch [20/30] Train Loss: 0.3317, Test Loss: 0.3438, F1: 0.8235, AUC: 0.9341
Mejores resultados en la época:  26
f1-score 0.8569760653823701
AUC según el mejor F1-score 0.9348708925808245

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4787, Test Loss: 0.4447, F1: 0.8287, AUC: 0.9065
Epoch [10/30] Train Loss: 0.3532, Test Loss: 0.3880, F1: 0.8044, AUC: 0.9284
Epoch [20/30] Train Loss: 0.3352, Test Loss: 0.3432, F1: 0.8397, AUC: 0.9342
Mejores resultados en la época:  27
f1-score 0.8584778064674782
AUC según el mejor F1-score 0.9361057239085692

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.5360, Test Loss: 0.4445, F1: 0.7428, AUC: 0.9009
Epoch [10/30] Train Loss: 0.3561, Test Loss: 0.3739, F1: 0.8450, AUC: 0.9248
Epoch [20/30] Train Loss: 0.3411, Test Loss: 0.3437, F1: 0.8452, AUC: 0.9310
Mejores resultados en la época:  28
f1-score 0.856384262611634
AUC según el mejor F1-score 0.9337558087880341

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.4911, Test Loss: 0.4046, F1: 0.8238, AUC: 0.9030
Epoch [10/30] Train Loss: 0.3557, Test Loss: 0.3862, F1: 0.8124, AUC: 0.9249
Epoch [20/30] Train Loss: 0.3342, Test Loss: 0.3581, F1: 0.8470, AUC: 0.9286
Mejores resultados en la época:  25
f1-score 0.8521928761812455
AUC según el mejor F1-score 0.9330000969696628
Epoch [0/30] Train Loss: 0.4641, Test Loss: 0.4469, F1: 0.6677, AUC: 0.9102
Epoch [10/30] Train Loss: 0.3498, Test Loss: 0.2943, F1: 0.7599, AUC: 0.9313
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
Epoch [20/30] Train Loss: 0.3296, Test Loss: 0.3954, F1: 0.7172, AUC: 0.9320
Mejores resultados en la época:  25
f1-score 0.7734399016292653
AUC según el mejor F1-score 0.9374115283300023
Confusion matrix Test saved: outputs_cv/4/lyrics_bert/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9337, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9321, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9336, 'f1_cv_std': 0.0022, 'params': 160801, 'accuracy_test': 0.9405, 'precision_test': 0.845, 'recall_test': 0.9194, 'f1_score_test': 0.8806}, 'MLP_2744833': {'accuracy_cv_mean': 0.9459, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9463, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9453, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9458, 'f1_cv_std': 0.0027, 'params': 2744833, 'accuracy_test': 0.9573, 'precision_test': 0.8977, 'recall_test': 0.9266, 'f1_score_test': 0.9119}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9429, 'precision_cv_std': 0.0071, 'recall_cv_mean': 0.9499, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9464, 'f1_cv_std': 0.0024, 'params': 5843969, 'accuracy_test': 0.9549, 'precision_test': 0.8793, 'recall_test': 0.9401, 'f1_score_test': 0.9087}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.0005, 'precision_cv_mean': 0.9364, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.9202, 'recall_cv_std': 0.0012, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0005, 'accuracy_test': 0.9354, 'precision_test': 0.8255, 'recall_test': 0.9248, 'f1_score_test': 0.8723}, 'SVM': {'accuracy_cv_mean': 0.8348, 'accuracy_cv_std': 0.0146, 'precision_cv_mean': 0.8051, 'precision_cv_std': 0.0362, 'recall_cv_mean': 0.8891, 'recall_cv_std': 0.0395, 'f1_cv_mean': 0.8434, 'f1_cv_std': 0.0099, 'accuracy_test': 0.8203, 'precision_test': 0.5836, 'recall_test': 0.8624, 'f1_score_test': 0.6961}, 'Decision Tree': {'accuracy_cv_mean': 0.8312, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7441, 'recall_cv_std': 0.0134, 'f1_cv_mean': 0.8151, 'f1_cv_std': 0.0043, 'accuracy_test': 0.8791, 'precision_test': 0.7538, 'recall_test': 0.7324, 'f1_score_test': 0.7429}, 'Random Forest': {'accuracy_cv_mean': 0.8401, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.8781, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.7898, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8684, 'precision_test': 0.6998, 'recall_test': 0.7855, 'f1_score_test': 0.7401}, 'XGBoost': {'accuracy_cv_mean': 0.9077, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9274, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8846, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.9055, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9216, 'precision_test': 0.8038, 'recall_test': 0.8884, 'f1_score_test': 0.844}, 'Naive Bayes': {'accuracy_cv_mean': 0.9045, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9301, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.9016, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9191, 'precision_test': 0.8094, 'recall_test': 0.8643, 'f1_score_test': 0.836}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8521, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8503, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8548, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.8525, 'f1_cv_std': 0.0027, 'params': 10401, 'accuracy_test': 0.886, 'precision_test': 0.7591, 'recall_test': 0.7651, 'f1_score_test': 0.7621}, 'MLP_338433': {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0088, 'recall_cv_mean': 0.8747, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8585, 'f1_cv_std': 0.0026, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7815, 'recall_test': 0.7702, 'f1_score_test': 0.7758}, 'MLP_1031169': {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8506, 'precision_cv_std': 0.0142, 'recall_cv_mean': 0.8639, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.857, 'f1_cv_std': 0.0028, 'params': 1031169, 'accuracy_test': 0.8978, 'precision_test': 0.8206, 'recall_test': 0.7314, 'f1_score_test': 0.7734}}}
Saved on: outputs_cv/4/lyrics_bert

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8599, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.834, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8467, 'f1_cv_std': 0.0019}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 46, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.87      0.90     16465
           1       0.66      0.83      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.85      0.82     21625
weighted avg       0.87      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/4/lyrics_bert/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/4/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.6625, 'accuracy_cv_std': 0.0376, 'precision_cv_mean': 0.6361, 'precision_cv_std': 0.0336, 'recall_cv_mean': 0.7617, 'recall_cv_std': 0.0525, 'f1_cv_mean': 0.6926, 'f1_cv_std': 0.0352}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 46, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.85      0.55      0.67     16465
           1       0.32      0.68      0.44      5160

    accuracy                           0.58     21625
   macro avg       0.58      0.61      0.55     21625
weighted avg       0.72      0.58      0.61     21625

Confusion matrix Test saved as: outputs_cv/4/lyrics_bert/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/4/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8012, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8097, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7878, 'recall_cv_std': 0.0116, 'f1_cv_mean': 0.7985, 'f1_cv_std': 0.0034}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 46, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.83      0.87     16465
           1       0.59      0.78      0.67      5160

    accuracy                           0.82     21625
   macro avg       0.75      0.80      0.77     21625
weighted avg       0.84      0.82      0.82     21625

Confusion matrix Test saved as: outputs_cv/4/lyrics_bert/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/4/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0011, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8137, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.8483, 'f1_cv_std': 0.0009}
Epoch [20/30] Train Loss: 0.3296, Test Loss: 0.3954, F1: 0.7172, AUC: 0.9320
Mejores resultados en la época:  25
f1-score 0.7734399016292653
AUC según el mejor F1-score 0.9374115283300023
Confusion matrix Test saved: outputs_cv/4/lyrics_bert/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9337, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9321, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9336, 'f1_cv_std': 0.0022, 'params': 160801, 'accuracy_test': 0.9405, 'precision_test': 0.845, 'recall_test': 0.9194, 'f1_score_test': 0.8806}, 'MLP_2744833': {'accuracy_cv_mean': 0.9459, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9463, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9453, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9458, 'f1_cv_std': 0.0027, 'params': 2744833, 'accuracy_test': 0.9573, 'precision_test': 0.8977, 'recall_test': 0.9266, 'f1_score_test': 0.9119}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9429, 'precision_cv_std': 0.0071, 'recall_cv_mean': 0.9499, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9464, 'f1_cv_std': 0.0024, 'params': 5843969, 'accuracy_test': 0.9549, 'precision_test': 0.8793, 'recall_test': 0.9401, 'f1_score_test': 0.9087}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.0005, 'precision_cv_mean': 0.9364, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.9202, 'recall_cv_std': 0.0012, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0005, 'accuracy_test': 0.9354, 'precision_test': 0.8255, 'recall_test': 0.9248, 'f1_score_test': 0.8723}, 'SVM': {'accuracy_cv_mean': 0.8348, 'accuracy_cv_std': 0.0146, 'precision_cv_mean': 0.8051, 'precision_cv_std': 0.0362, 'recall_cv_mean': 0.8891, 'recall_cv_std': 0.0395, 'f1_cv_mean': 0.8434, 'f1_cv_std': 0.0099, 'accuracy_test': 0.8203, 'precision_test': 0.5836, 'recall_test': 0.8624, 'f1_score_test': 0.6961}, 'Decision Tree': {'accuracy_cv_mean': 0.8312, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7441, 'recall_cv_std': 0.0134, 'f1_cv_mean': 0.8151, 'f1_cv_std': 0.0043, 'accuracy_test': 0.8791, 'precision_test': 0.7538, 'recall_test': 0.7324, 'f1_score_test': 0.7429}, 'Random Forest': {'accuracy_cv_mean': 0.8401, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.8781, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.7898, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8684, 'precision_test': 0.6998, 'recall_test': 0.7855, 'f1_score_test': 0.7401}, 'XGBoost': {'accuracy_cv_mean': 0.9077, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9274, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8846, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.9055, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9216, 'precision_test': 0.8038, 'recall_test': 0.8884, 'f1_score_test': 0.844}, 'Naive Bayes': {'accuracy_cv_mean': 0.9045, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9301, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.9016, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9191, 'precision_test': 0.8094, 'recall_test': 0.8643, 'f1_score_test': 0.836}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8521, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8503, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8548, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.8525, 'f1_cv_std': 0.0027, 'params': 10401, 'accuracy_test': 0.886, 'precision_test': 0.7591, 'recall_test': 0.7651, 'f1_score_test': 0.7621}, 'MLP_338433': {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0088, 'recall_cv_mean': 0.8747, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8585, 'f1_cv_std': 0.0026, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7815, 'recall_test': 0.7702, 'f1_score_test': 0.7758}, 'MLP_1031169': {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8506, 'precision_cv_std': 0.0142, 'recall_cv_mean': 0.8639, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.857, 'f1_cv_std': 0.0028, 'params': 1031169, 'accuracy_test': 0.8978, 'precision_test': 0.8206, 'recall_test': 0.7314, 'f1_score_test': 0.7734}}}
Saved on: outputs_cv/4/lyrics_bert

==============================
Model: Logistic Regression

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8599, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.834, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8467, 'f1_cv_std': 0.0019}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 46, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.87      0.90     16465
           1       0.66      0.83      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.85      0.82     21625
weighted avg       0.87      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/4/lyrics_bert/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/4/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.6625, 'accuracy_cv_std': 0.0376, 'precision_cv_mean': 0.6361, 'precision_cv_std': 0.0336, 'recall_cv_mean': 0.7617, 'recall_cv_std': 0.0525, 'f1_cv_mean': 0.6926, 'f1_cv_std': 0.0352}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 46, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.85      0.55      0.67     16465
           1       0.32      0.68      0.44      5160

    accuracy                           0.58     21625
   macro avg       0.58      0.61      0.55     21625
weighted avg       0.72      0.58      0.61     21625

Confusion matrix Test saved as: outputs_cv/4/lyrics_bert/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/4/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8012, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8097, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7878, 'recall_cv_std': 0.0116, 'f1_cv_mean': 0.7985, 'f1_cv_std': 0.0034}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 46, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.92      0.83      0.87     16465
           1       0.59      0.78      0.67      5160

    accuracy                           0.82     21625
   macro avg       0.75      0.80      0.77     21625
weighted avg       0.84      0.82      0.82     21625

Confusion matrix Test saved as: outputs_cv/4/lyrics_bert/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/4/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0011, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8137, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.8483, 'f1_cv_std': 0.0009}
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:25:56] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:26:08] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:26:44] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:26:58] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:27:33] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:27:46] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:28:22] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:28:35] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:29:11] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:29:24] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:30:00] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:30:13] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 46, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.90      0.92     16465
           1       0.72      0.82      0.77      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.86      0.84     21625
weighted avg       0.89      0.88      0.88     21625

Confusion matrix Test saved as: outputs_cv/4/lyrics_bert/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/4/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8932, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8595, 'recall_cv_std': 0.004, 'f1_cv_mean': 0.876, 'f1_cv_std': 0.0024}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 46, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.90      0.93     16465
           1       0.73      0.87      0.79      5160

    accuracy                           0.89     21625
   macro avg       0.84      0.88      0.86     21625
weighted avg       0.90      0.89      0.89     21625

Confusion matrix Test saved as: outputs_cv/4/lyrics_bert/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/4/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7819, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.7663, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8112, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.7881, 'f1_cv_std': 0.0026}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.75      0.83     16465
           1       0.50      0.81      0.62      5160

    accuracy                           0.76     21625
   macro avg       0.71      0.78      0.73     21625
weighted avg       0.82      0.76      0.78     21625

Confusion matrix Test saved as: outputs_cv/4/lyrics_bert/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/4/lyrics_bert/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8932, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8595, 'recall_cv_std': 0.004, 'f1_cv_mean': 0.876, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8918, 'precision_test': 0.7303, 'recall_test': 0.8665, 'f1_score_test': 0.7926}
Random Forest: {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0011, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8137, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.8483, 'f1_cv_std': 0.0009, 'accuracy_test': 0.8821, 'precision_test': 0.7233, 'recall_test': 0.8192, 'f1_score_test': 0.7683}
Logistic Regression: {'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8599, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.834, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8467, 'f1_cv_std': 0.0019, 'accuracy_test': 0.8576, 'precision_test': 0.6603, 'recall_test': 0.83, 'f1_score_test': 0.7355}
Decision Tree: {'accuracy_cv_mean': 0.8012, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8097, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7878, 'recall_cv_std': 0.0116, 'f1_cv_mean': 0.7985, 'f1_cv_std': 0.0034, 'accuracy_test': 0.8157, 'precision_test': 0.5856, 'recall_test': 0.7787, 'f1_score_test': 0.6685}
Naive Bayes: {'accuracy_cv_mean': 0.7819, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.7663, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8112, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.7881, 'f1_cv_std': 0.0026, 'accuracy_test': 0.7647, 'precision_test': 0.5044, 'recall_test': 0.8066, 'f1_score_test': 0.6206}
SVM: {'accuracy_cv_mean': 0.6625, 'accuracy_cv_std': 0.0376, 'precision_cv_mean': 0.6361, 'precision_cv_std': 0.0336, 'recall_cv_mean': 0.7617, 'recall_cv_std': 0.0525, 'f1_cv_mean': 0.6926, 'f1_cv_std': 0.0352, 'accuracy_test': 0.5809, 'precision_test': 0.3211, 'recall_test': 0.6787, 'f1_score_test': 0.4359}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9337, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9321, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9336, 'f1_cv_std': 0.0022, 'params': 160801, 'accuracy_test': 0.9405, 'precision_test': 0.845, 'recall_test': 0.9194, 'f1_score_test': 0.8806}, 'MLP_2744833': {'accuracy_cv_mean': 0.9459, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9463, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9453, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9458, 'f1_cv_std': 0.0027, 'params': 2744833, 'accuracy_test': 0.9573, 'precision_test': 0.8977, 'recall_test': 0.9266, 'f1_score_test': 0.9119}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9429, 'precision_cv_std': 0.0071, 'recall_cv_mean': 0.9499, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9464, 'f1_cv_std': 0.0024, 'params': 5843969, 'accuracy_test': 0.9549, 'precision_test': 0.8793, 'recall_test': 0.9401, 'f1_score_test': 0.9087}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.0005, 'precision_cv_mean': 0.9364, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.9202, 'recall_cv_std': 0.0012, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0005, 'accuracy_test': 0.9354, 'precision_test': 0.8255, 'recall_test': 0.9248, 'f1_score_test': 0.8723}, 'SVM': {'accuracy_cv_mean': 0.8348, 'accuracy_cv_std': 0.0146, 'precision_cv_mean': 0.8051, 'precision_cv_std': 0.0362, 'recall_cv_mean': 0.8891, 'recall_cv_std': 0.0395, 'f1_cv_mean': 0.8434, 'f1_cv_std': 0.0099, 'accuracy_test': 0.8203, 'precision_test': 0.5836, 'recall_test': 0.8624, 'f1_score_test': 0.6961}, 'Decision Tree': {'accuracy_cv_mean': 0.8312, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7441, 'recall_cv_std': 0.0134, 'f1_cv_mean': 0.8151, 'f1_cv_std': 0.0043, 'accuracy_test': 0.8791, 'precision_test': 0.7538, 'recall_test': 0.7324, 'f1_score_test': 0.7429}, 'Random Forest': {'accuracy_cv_mean': 0.8401, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.8781, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.7898, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8684, 'precision_test': 0.6998, 'recall_test': 0.7855, 'f1_score_test': 0.7401}, 'XGBoost': {'accuracy_cv_mean': 0.9077, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9274, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8846, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.9055, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9216, 'precision_test': 0.8038, 'recall_test': 0.8884, 'f1_score_test': 0.844}, 'Naive Bayes': {'accuracy_cv_mean': 0.9045, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9301, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.9016, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9191, 'precision_test': 0.8094, 'recall_test': 0.8643, 'f1_score_test': 0.836}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8521, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8503, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8548, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.8525, 'f1_cv_std': 0.0027, 'params': 10401, 'accuracy_test': 0.886, 'precision_test': 0.7591, 'recall_test': 0.7651, 'f1_score_test': 0.7621}, 'MLP_338433': {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0088, 'recall_cv_mean': 0.8747, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8585, 'f1_cv_std': 0.0026, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7815, 'recall_test': 0.7702, 'f1_score_test': 0.7758}, 'MLP_1031169': {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8506, 'precision_cv_std': 0.0142, 'recall_cv_mean': 0.8639, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.857, 'f1_cv_std': 0.0028, 'params': 1031169, 'accuracy_test': 0.8978, 'precision_test': 0.8206, 'recall_test': 0.7314, 'f1_score_test': 0.7734}, 'Logistic Regression': {'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8599, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.834, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8467, 'f1_cv_std': 0.0019, 'accuracy_test': 0.8576, 'precision_test': 0.6603, 'recall_test': 0.83, 'f1_score_test': 0.7355}, 'SVM': {'accuracy_cv_mean': 0.6625, 'accuracy_cv_std': 0.0376, 'precision_cv_mean': 0.6361, 'precision_cv_std': 0.0336, 'recall_cv_mean': 0.7617, 'recall_cv_std': 0.0525, 'f1_cv_mean': 0.6926, 'f1_cv_std': 0.0352, 'accuracy_test': 0.5809, 'precision_test': 0.3211, 'recall_test': 0.6787, 'f1_score_test': 0.4359}, 'Decision Tree': {'accuracy_cv_mean': 0.8012, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8097, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7878, 'recall_cv_std': 0.0116, 'f1_cv_mean': 0.7985, 'f1_cv_std': 0.0034, 'accuracy_test': 0.8157, 'precision_test': 0.5856, 'recall_test': 0.7787, 'f1_score_test': 0.6685}, 'Random Forest': {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0011, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0057, 'recall_cv/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_3.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 46, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.94      0.90      0.92     16465
           1       0.72      0.82      0.77      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.86      0.84     21625
weighted avg       0.89      0.88      0.88     21625

Confusion matrix Test saved as: outputs_cv/4/lyrics_bert/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/4/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8932, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8595, 'recall_cv_std': 0.004, 'f1_cv_mean': 0.876, 'f1_cv_std': 0.0024}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 46, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.96      0.90      0.93     16465
           1       0.73      0.87      0.79      5160

    accuracy                           0.89     21625
   macro avg       0.84      0.88      0.86     21625
weighted avg       0.90      0.89      0.89     21625

Confusion matrix Test saved as: outputs_cv/4/lyrics_bert/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/4/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.7819, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.7663, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8112, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.7881, 'f1_cv_std': 0.0026}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.75      0.83     16465
           1       0.50      0.81      0.62      5160

    accuracy                           0.76     21625
   macro avg       0.71      0.78      0.73     21625
weighted avg       0.82      0.76      0.78     21625

Confusion matrix Test saved as: outputs_cv/4/lyrics_bert/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/4/lyrics_bert/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8932, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8595, 'recall_cv_std': 0.004, 'f1_cv_mean': 0.876, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8918, 'precision_test': 0.7303, 'recall_test': 0.8665, 'f1_score_test': 0.7926}
Random Forest: {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0011, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8137, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.8483, 'f1_cv_std': 0.0009, 'accuracy_test': 0.8821, 'precision_test': 0.7233, 'recall_test': 0.8192, 'f1_score_test': 0.7683}
Logistic Regression: {'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8599, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.834, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8467, 'f1_cv_std': 0.0019, 'accuracy_test': 0.8576, 'precision_test': 0.6603, 'recall_test': 0.83, 'f1_score_test': 0.7355}
Decision Tree: {'accuracy_cv_mean': 0.8012, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8097, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7878, 'recall_cv_std': 0.0116, 'f1_cv_mean': 0.7985, 'f1_cv_std': 0.0034, 'accuracy_test': 0.8157, 'precision_test': 0.5856, 'recall_test': 0.7787, 'f1_score_test': 0.6685}
Naive Bayes: {'accuracy_cv_mean': 0.7819, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.7663, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8112, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.7881, 'f1_cv_std': 0.0026, 'accuracy_test': 0.7647, 'precision_test': 0.5044, 'recall_test': 0.8066, 'f1_score_test': 0.6206}
SVM: {'accuracy_cv_mean': 0.6625, 'accuracy_cv_std': 0.0376, 'precision_cv_mean': 0.6361, 'precision_cv_std': 0.0336, 'recall_cv_mean': 0.7617, 'recall_cv_std': 0.0525, 'f1_cv_mean': 0.6926, 'f1_cv_std': 0.0352, 'accuracy_test': 0.5809, 'precision_test': 0.3211, 'recall_test': 0.6787, 'f1_score_test': 0.4359}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9337, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9321, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9336, 'f1_cv_std': 0.0022, 'params': 160801, 'accuracy_test': 0.9405, 'precision_test': 0.845, 'recall_test': 0.9194, 'f1_score_test': 0.8806}, 'MLP_2744833': {'accuracy_cv_mean': 0.9459, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9463, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9453, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9458, 'f1_cv_std': 0.0027, 'params': 2744833, 'accuracy_test': 0.9573, 'precision_test': 0.8977, 'recall_test': 0.9266, 'f1_score_test': 0.9119}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9429, 'precision_cv_std': 0.0071, 'recall_cv_mean': 0.9499, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9464, 'f1_cv_std': 0.0024, 'params': 5843969, 'accuracy_test': 0.9549, 'precision_test': 0.8793, 'recall_test': 0.9401, 'f1_score_test': 0.9087}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.0005, 'precision_cv_mean': 0.9364, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.9202, 'recall_cv_std': 0.0012, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0005, 'accuracy_test': 0.9354, 'precision_test': 0.8255, 'recall_test': 0.9248, 'f1_score_test': 0.8723}, 'SVM': {'accuracy_cv_mean': 0.8348, 'accuracy_cv_std': 0.0146, 'precision_cv_mean': 0.8051, 'precision_cv_std': 0.0362, 'recall_cv_mean': 0.8891, 'recall_cv_std': 0.0395, 'f1_cv_mean': 0.8434, 'f1_cv_std': 0.0099, 'accuracy_test': 0.8203, 'precision_test': 0.5836, 'recall_test': 0.8624, 'f1_score_test': 0.6961}, 'Decision Tree': {'accuracy_cv_mean': 0.8312, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7441, 'recall_cv_std': 0.0134, 'f1_cv_mean': 0.8151, 'f1_cv_std': 0.0043, 'accuracy_test': 0.8791, 'precision_test': 0.7538, 'recall_test': 0.7324, 'f1_score_test': 0.7429}, 'Random Forest': {'accuracy_cv_mean': 0.8401, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.8781, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.7898, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8684, 'precision_test': 0.6998, 'recall_test': 0.7855, 'f1_score_test': 0.7401}, 'XGBoost': {'accuracy_cv_mean': 0.9077, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9274, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8846, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.9055, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9216, 'precision_test': 0.8038, 'recall_test': 0.8884, 'f1_score_test': 0.844}, 'Naive Bayes': {'accuracy_cv_mean': 0.9045, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9301, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.9016, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9191, 'precision_test': 0.8094, 'recall_test': 0.8643, 'f1_score_test': 0.836}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8521, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8503, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8548, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.8525, 'f1_cv_std': 0.0027, 'params': 10401, 'accuracy_test': 0.886, 'precision_test': 0.7591, 'recall_test': 0.7651, 'f1_score_test': 0.7621}, 'MLP_338433': {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0088, 'recall_cv_mean': 0.8747, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8585, 'f1_cv_std': 0.0026, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7815, 'recall_test': 0.7702, 'f1_score_test': 0.7758}, 'MLP_1031169': {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8506, 'precision_cv_std': 0.0142, 'recall_cv_mean': 0.8639, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.857, 'f1_cv_std': 0.0028, 'params': 1031169, 'accuracy_test': 0.8978, 'precision_test': 0.8206, 'recall_test': 0.7314, 'f1_score_test': 0.7734}, 'Logistic Regression': {'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8599, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.834, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8467, 'f1_cv_std': 0.0019, 'accuracy_test': 0.8576, 'precision_test': 0.6603, 'recall_test': 0.83, 'f1_score_test': 0.7355}, 'SVM': {'accuracy_cv_mean': 0.6625, 'accuracy_cv_std': 0.0376, 'precision_cv_mean': 0.6361, 'precision_cv_std': 0.0336, 'recall_cv_mean': 0.7617, 'recall_cv_std': 0.0525, 'f1_cv_mean': 0.6926, 'f1_cv_std': 0.0352, 'accuracy_test': 0.5809, 'precision_test': 0.3211, 'recall_test': 0.6787, 'f1_score_test': 0.4359}, 'Decision Tree': {'accuracy_cv_mean': 0.8012, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8097, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7878, 'recall_cv_std': 0.0116, 'f1_cv_mean': 0.7985, 'f1_cv_std': 0.0034, 'accuracy_test': 0.8157, 'precision_test': 0.5856, 'recall_test': 0.7787, 'f1_score_test': 0.6685}, 'Random Forest': {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0011, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0057, 'recall_cv/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/__cross_validation_metadata_only/cv_3.py:289: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
_mean': 0.8137, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.8483, 'f1_cv_std': 0.0009, 'accuracy_test': 0.8821, 'precision_test': 0.7233, 'recall_test': 0.8192, 'f1_score_test': 0.7683}, 'XGBoost': {'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8932, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8595, 'recall_cv_std': 0.004, 'f1_cv_mean': 0.876, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8918, 'precision_test': 0.7303, 'recall_test': 0.8665, 'f1_score_test': 0.7926}, 'Naive Bayes': {'accuracy_cv_mean': 0.7819, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.7663, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8112, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.7881, 'f1_cv_std': 0.0026, 'accuracy_test': 0.7647, 'precision_test': 0.5044, 'recall_test': 0.8066, 'f1_score_test': 0.6206}}}
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 1559)
Shape of X_test after concatenation:  (21625, 1559)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1559)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1559)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [1559, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4138, Test Loss: 0.3170, F1: 0.8627, AUC: 0.9401
Epoch [10/30] Train Loss: 0.2573, Test Loss: 0.3087, F1: 0.8624, AUC: 0.9594
Epoch [20/30] Train Loss: 0.2448, Test Loss: 0.2787, F1: 0.8744, AUC: 0.9618
Mejores resultados en la época:  29
f1-score 0.8969931167733366
AUC según el mejor F1-score 0.9630317603171384

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4344, Test Loss: 0.3263, F1: 0.8624, AUC: 0.9378
Epoch [10/30] Train Loss: 0.2625, Test Loss: 0.2525, F1: 0.8916, AUC: 0.9612
Epoch [20/30] Train Loss: 0.2447, Test Loss: 0.2426, F1: 0.8956, AUC: 0.9644
Mejores resultados en la época:  28
f1-score 0.8982121573301549
AUC según el mejor F1-score 0.9654689706523045

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4021, Test Loss: 0.3147, F1: 0.8599, AUC: 0.9418
Epoch [10/30] Train Loss: 0.2572, Test Loss: 0.2527, F1: 0.8951, AUC: 0.9611
Epoch [20/30] Train Loss: 0.2454, Test Loss: 0.2507, F1: 0.8996, AUC: 0.9633
Mejores resultados en la época:  23
f1-score 0.8999758978067004
AUC según el mejor F1-score 0.9635996172386726

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4613, Test Loss: 0.3491, F1: 0.8490, AUC: 0.9283
Epoch [10/30] Train Loss: 0.2597, Test Loss: 0.2662, F1: 0.8886, AUC: 0.9566
Epoch [20/30] Train Loss: 0.2434, Test Loss: 0.2621, F1: 0.8877, AUC: 0.9590
Mejores resultados en la época:  26
f1-score 0.892975011786893
AUC según el mejor F1-score 0.9598516540253915

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3853, Test Loss: 0.3697, F1: 0.8186, AUC: 0.9392
Epoch [10/30] Train Loss: 0.2608, Test Loss: 0.2787, F1: 0.8779, AUC: 0.9588
Epoch [20/30] Train Loss: 0.2451, Test Loss: 0.2508, F1: 0.8956, AUC: 0.9613
Mejores resultados en la época:  29
f1-score 0.8974831184775937
AUC según el mejor F1-score 0.9625837390562809
Epoch [0/30] Train Loss: 0.6934, Test Loss: 0.6912, F1: 0.0000, AUC: 0.5000
Epoch [10/30] Train Loss: 0.6932, Test Loss: 0.6907, F1: 0.0000, AUC: 0.5000
Epoch [20/30] Train Loss: 0.6932, Test Loss: 0.6901, F1: 0.0000, AUC: 0.5000
Mejores resultados en la época:  29
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5
Confusion matrix Test saved: outputs_cv/4/gpt/cm_mlp_1.png

========================================
Entrenando red 5 con capas [1559, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3810, Test Loss: 0.3038, F1: 0.8766, AUC: 0.9482
Epoch [10/30] Train Loss: 0.2638, Test Loss: 0.2813, F1: 0.8908, AUC: 0.9604
Epoch [20/30] Train Loss: 0.2434, Test Loss: 0.2477, F1: 0.8956, AUC: 0.9636
Mejores resultados en la época:  28
f1-score 0.8967980002380669
AUC según el mejor F1-score 0.9646551685689712

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3983, Test Loss: 0.3484, F1: 0.8252, AUC: 0.9475
Epoch [10/30] Train Loss: 0.2643, Test Loss: 0.2829, F1: 0.8781, AUC: 0.9636
Epoch [20/30] Train Loss: 0.2464, Test Loss: 0.2515, F1: 0.8895, AUC: 0.9659
Mejores resultados en la época:  28
f1-score 0.8987748304983942
AUC según el mejor F1-score 0.9674244723593084

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3860, Test Loss: 0.2904, F1: 0.8750, AUC: 0.9476
Epoch [10/30] Train Loss: 0.2597, Test Loss: 0.2670, F1: 0.8960, AUC: 0.9624
Epoch [20/30] Train Loss: 0.2422, Test Loss: 0.2915, F1: 0.8480, AUC: 0.9642
Mejores resultados en la época:  25
f1-score 0.9006638800098352
AUC según el mejor F1-score 0.965867142430593

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4063, Test Loss: 0.3245, F1: 0.8662, AUC: 0.9432
Epoch [10/30] Train Loss: 0.2653, Test Loss: 0.2657, F1: 0.8880, AUC: 0.9586
Epoch [20/30] Train Loss: 0.2408, Test Loss: 0.3033, F1: 0.8806, AUC: 0.9621
Mejores resultados en la época:  26
f1-score 0.8963500233972859
AUC según el mejor F1-score 0.9629174978352052

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3736, Test Loss: 0.3932, F1: 0.7982, AUC: 0.9470
Epoch [10/30] Train Loss: 0.2600, Test Loss: 0.2657, F1: 0.8887, AUC: 0.9600
Epoch [20/30] Train Loss: 0.2455, Test Loss: 0.2821, F1: 0.8759, AUC: 0.9630
Mejores resultados en la época:  24
f1-score 0.8984796468857283
AUC según el mejor F1-score 0.9644373740333557
Epoch [0/30] Train Loss: 0.3671, Test Loss: 0.3089, F1: 0.7716, AUC: 0.9491
Epoch [10/30] Train Loss: 0.2576, Test Loss: 0.2269, F1: 0.8203, AUC: 0.9625
Epoch [20/30] Train Loss: 0.2429, Test Loss: 0.2887, F1: 0.7868, AUC: 0.9650
Mejores resultados en la época:  25
f1-score 0.8325031741380994
AUC según el mejor F1-score 0.9661337415283064
Confusion matrix Test saved: outputs_cv/4/gpt/cm_mlp_5.png

========================================
Entrenando red 6 con capas [1559, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4032, Test Loss: 0.2951, F1: 0.8762, AUC: 0.9467
Epoch [10/30] Train Loss: 0.2608, Test Loss: 0.2845, F1: 0.8911, AUC: 0.9612
Epoch [20/30] Train Loss: 0.2418, Test Loss: 0.2613, F1: 0.8948, AUC: 0.9639
Mejores resultados en la época:  21
f1-score 0.8966594568613314
AUC según el mejor F1-score 0.9640147780707289

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3891, Test Loss: 0.2929, F1: 0.8681, AUC: 0.9498
Epoch [10/30] Train Loss: 0.2595, Test Loss: 0.2469, F1: 0.8931, AUC: 0.9633
Epoch [20/30] Train Loss: 0.2469, Test Loss: 0.2635, F1: 0.8956, AUC: 0.9661
Mejores resultados en la época:  26
f1-score 0.8998548621190131
AUC según el mejor F1-score 0.9674239735442582

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3958, Test Loss: 0.3010, F1: 0.8627, AUC: 0.9470
Epoch [10/30] Train Loss: 0.2677, Test Loss: 0.2504, F1: 0.8939, AUC: 0.9617
Epoch [20/30] Train Loss: 0.2413, Test Loss: 0.2496, F1: 0.8960, AUC: 0.9646
Mejores resultados en la época:  24
f1-score 0.8991575817641229
AUC según el mejor F1-score 0.9658076367293131

_mean': 0.8137, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.8483, 'f1_cv_std': 0.0009, 'accuracy_test': 0.8821, 'precision_test': 0.7233, 'recall_test': 0.8192, 'f1_score_test': 0.7683}, 'XGBoost': {'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8932, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8595, 'recall_cv_std': 0.004, 'f1_cv_mean': 0.876, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8918, 'precision_test': 0.7303, 'recall_test': 0.8665, 'f1_score_test': 0.7926}, 'Naive Bayes': {'accuracy_cv_mean': 0.7819, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.7663, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8112, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.7881, 'f1_cv_std': 0.0026, 'accuracy_test': 0.7647, 'precision_test': 0.5044, 'recall_test': 0.8066, 'f1_score_test': 0.6206}}}
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
El valor de y[0] es 108138
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_Numeric:  (86500, 23)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 1559)
Shape of X_test after concatenation:  (21625, 1559)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1559)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1559)
y shape: (41278,)
Comenzando trainning de MLP...

========================================
Entrenando red 1 con capas [1559, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4138, Test Loss: 0.3170, F1: 0.8627, AUC: 0.9401
Epoch [10/30] Train Loss: 0.2573, Test Loss: 0.3087, F1: 0.8624, AUC: 0.9594
Epoch [20/30] Train Loss: 0.2448, Test Loss: 0.2787, F1: 0.8744, AUC: 0.9618
Mejores resultados en la época:  29
f1-score 0.8969931167733366
AUC según el mejor F1-score 0.9630317603171384

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.4344, Test Loss: 0.3263, F1: 0.8624, AUC: 0.9378
Epoch [10/30] Train Loss: 0.2625, Test Loss: 0.2525, F1: 0.8916, AUC: 0.9612
Epoch [20/30] Train Loss: 0.2447, Test Loss: 0.2426, F1: 0.8956, AUC: 0.9644
Mejores resultados en la época:  28
f1-score 0.8982121573301549
AUC según el mejor F1-score 0.9654689706523045

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.4021, Test Loss: 0.3147, F1: 0.8599, AUC: 0.9418
Epoch [10/30] Train Loss: 0.2572, Test Loss: 0.2527, F1: 0.8951, AUC: 0.9611
Epoch [20/30] Train Loss: 0.2454, Test Loss: 0.2507, F1: 0.8996, AUC: 0.9633
Mejores resultados en la época:  23
f1-score 0.8999758978067004
AUC según el mejor F1-score 0.9635996172386726

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4613, Test Loss: 0.3491, F1: 0.8490, AUC: 0.9283
Epoch [10/30] Train Loss: 0.2597, Test Loss: 0.2662, F1: 0.8886, AUC: 0.9566
Epoch [20/30] Train Loss: 0.2434, Test Loss: 0.2621, F1: 0.8877, AUC: 0.9590
Mejores resultados en la época:  26
f1-score 0.892975011786893
AUC según el mejor F1-score 0.9598516540253915

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3853, Test Loss: 0.3697, F1: 0.8186, AUC: 0.9392
Epoch [10/30] Train Loss: 0.2608, Test Loss: 0.2787, F1: 0.8779, AUC: 0.9588
Epoch [20/30] Train Loss: 0.2451, Test Loss: 0.2508, F1: 0.8956, AUC: 0.9613
Mejores resultados en la época:  29
f1-score 0.8974831184775937
AUC según el mejor F1-score 0.9625837390562809
Epoch [0/30] Train Loss: 0.6934, Test Loss: 0.6912, F1: 0.0000, AUC: 0.5000
Epoch [10/30] Train Loss: 0.6932, Test Loss: 0.6907, F1: 0.0000, AUC: 0.5000
Epoch [20/30] Train Loss: 0.6932, Test Loss: 0.6901, F1: 0.0000, AUC: 0.5000
Mejores resultados en la época:  29
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.5
Confusion matrix Test saved: outputs_cv/4/gpt/cm_mlp_1.png

========================================
Entrenando red 5 con capas [1559, 512, 256, 128, 64, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.3810, Test Loss: 0.3038, F1: 0.8766, AUC: 0.9482
Epoch [10/30] Train Loss: 0.2638, Test Loss: 0.2813, F1: 0.8908, AUC: 0.9604
Epoch [20/30] Train Loss: 0.2434, Test Loss: 0.2477, F1: 0.8956, AUC: 0.9636
Mejores resultados en la época:  28
f1-score 0.8967980002380669
AUC según el mejor F1-score 0.9646551685689712

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3983, Test Loss: 0.3484, F1: 0.8252, AUC: 0.9475
Epoch [10/30] Train Loss: 0.2643, Test Loss: 0.2829, F1: 0.8781, AUC: 0.9636
Epoch [20/30] Train Loss: 0.2464, Test Loss: 0.2515, F1: 0.8895, AUC: 0.9659
Mejores resultados en la época:  28
f1-score 0.8987748304983942
AUC según el mejor F1-score 0.9674244723593084

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3860, Test Loss: 0.2904, F1: 0.8750, AUC: 0.9476
Epoch [10/30] Train Loss: 0.2597, Test Loss: 0.2670, F1: 0.8960, AUC: 0.9624
Epoch [20/30] Train Loss: 0.2422, Test Loss: 0.2915, F1: 0.8480, AUC: 0.9642
Mejores resultados en la época:  25
f1-score 0.9006638800098352
AUC según el mejor F1-score 0.965867142430593

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.4063, Test Loss: 0.3245, F1: 0.8662, AUC: 0.9432
Epoch [10/30] Train Loss: 0.2653, Test Loss: 0.2657, F1: 0.8880, AUC: 0.9586
Epoch [20/30] Train Loss: 0.2408, Test Loss: 0.3033, F1: 0.8806, AUC: 0.9621
Mejores resultados en la época:  26
f1-score 0.8963500233972859
AUC según el mejor F1-score 0.9629174978352052

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3736, Test Loss: 0.3932, F1: 0.7982, AUC: 0.9470
Epoch [10/30] Train Loss: 0.2600, Test Loss: 0.2657, F1: 0.8887, AUC: 0.9600
Epoch [20/30] Train Loss: 0.2455, Test Loss: 0.2821, F1: 0.8759, AUC: 0.9630
Mejores resultados en la época:  24
f1-score 0.8984796468857283
AUC según el mejor F1-score 0.9644373740333557
Epoch [0/30] Train Loss: 0.3671, Test Loss: 0.3089, F1: 0.7716, AUC: 0.9491
Epoch [10/30] Train Loss: 0.2576, Test Loss: 0.2269, F1: 0.8203, AUC: 0.9625
Epoch [20/30] Train Loss: 0.2429, Test Loss: 0.2887, F1: 0.7868, AUC: 0.9650
Mejores resultados en la época:  25
f1-score 0.8325031741380994
AUC según el mejor F1-score 0.9661337415283064
Confusion matrix Test saved: outputs_cv/4/gpt/cm_mlp_5.png

========================================
Entrenando red 6 con capas [1559, 1024, 512, 256, 128, 64, 32, 1]

--- Fold 1/5 ---
Epoch [0/30] Train Loss: 0.4032, Test Loss: 0.2951, F1: 0.8762, AUC: 0.9467
Epoch [10/30] Train Loss: 0.2608, Test Loss: 0.2845, F1: 0.8911, AUC: 0.9612
Epoch [20/30] Train Loss: 0.2418, Test Loss: 0.2613, F1: 0.8948, AUC: 0.9639
Mejores resultados en la época:  21
f1-score 0.8966594568613314
AUC según el mejor F1-score 0.9640147780707289

--- Fold 2/5 ---
Epoch [0/30] Train Loss: 0.3891, Test Loss: 0.2929, F1: 0.8681, AUC: 0.9498
Epoch [10/30] Train Loss: 0.2595, Test Loss: 0.2469, F1: 0.8931, AUC: 0.9633
Epoch [20/30] Train Loss: 0.2469, Test Loss: 0.2635, F1: 0.8956, AUC: 0.9661
Mejores resultados en la época:  26
f1-score 0.8998548621190131
AUC según el mejor F1-score 0.9674239735442582

--- Fold 3/5 ---
Epoch [0/30] Train Loss: 0.3958, Test Loss: 0.3010, F1: 0.8627, AUC: 0.9470
Epoch [10/30] Train Loss: 0.2677, Test Loss: 0.2504, F1: 0.8939, AUC: 0.9617
Epoch [20/30] Train Loss: 0.2413, Test Loss: 0.2496, F1: 0.8960, AUC: 0.9646
Mejores resultados en la época:  24
f1-score 0.8991575817641229
AUC según el mejor F1-score 0.9658076367293131

--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3988, Test Loss: 0.3112, F1: 0.8658, AUC: 0.9437
Epoch [10/30] Train Loss: 0.2622, Test Loss: 0.2668, F1: 0.8917, AUC: 0.9596
Epoch [20/30] Train Loss: 0.2382, Test Loss: 0.2823, F1: 0.8909, AUC: 0.9626
Mejores resultados en la época:  27
f1-score 0.8944009632751354
AUC según el mejor F1-score 0.9633553874748066

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3976, Test Loss: 0.3145, F1: 0.8685, AUC: 0.9434
Epoch [10/30] Train Loss: 0.2650, Test Loss: 0.2825, F1: 0.8608, AUC: 0.9597
Epoch [20/30] Train Loss: 0.2448, Test Loss: 0.2611, F1: 0.8860, AUC: 0.9633
Mejores resultados en la época:  22
f1-score 0.898876404494382
AUC según el mejor F1-score 0.9641562676682013
Epoch [0/30] Train Loss: 0.3830, Test Loss: 0.2342, F1: 0.7979, AUC: 0.9488
Epoch [10/30] Train Loss: 0.2580, Test Loss: 0.2040, F1: 0.8260, AUC: 0.9626
Epoch [20/30] Train Loss: 0.2372, Test Loss: 0.2850, F1: 0.7998, AUC: 0.9644
Mejores resultados en la época:  26
f1-score 0.8305808127667471
AUC según el mejor F1-score 0.9671093840116574
Confusion matrix Test saved: outputs_cv/4/gpt/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9337, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9321, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9336, 'f1_cv_std': 0.0022, 'params': 160801, 'accuracy_test': 0.9405, 'precision_test': 0.845, 'recall_test': 0.9194, 'f1_score_test': 0.8806}, 'MLP_2744833': {'accuracy_cv_mean': 0.9459, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9463, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9453, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9458, 'f1_cv_std': 0.0027, 'params': 2744833, 'accuracy_test': 0.9573, 'precision_test': 0.8977, 'recall_test': 0.9266, 'f1_score_test': 0.9119}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9429, 'precision_cv_std': 0.0071, 'recall_cv_mean': 0.9499, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9464, 'f1_cv_std': 0.0024, 'params': 5843969, 'accuracy_test': 0.9549, 'precision_test': 0.8793, 'recall_test': 0.9401, 'f1_score_test': 0.9087}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.0005, 'precision_cv_mean': 0.9364, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.9202, 'recall_cv_std': 0.0012, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0005, 'accuracy_test': 0.9354, 'precision_test': 0.8255, 'recall_test': 0.9248, 'f1_score_test': 0.8723}, 'SVM': {'accuracy_cv_mean': 0.8348, 'accuracy_cv_std': 0.0146, 'precision_cv_mean': 0.8051, 'precision_cv_std': 0.0362, 'recall_cv_mean': 0.8891, 'recall_cv_std': 0.0395, 'f1_cv_mean': 0.8434, 'f1_cv_std': 0.0099, 'accuracy_test': 0.8203, 'precision_test': 0.5836, 'recall_test': 0.8624, 'f1_score_test': 0.6961}, 'Decision Tree': {'accuracy_cv_mean': 0.8312, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7441, 'recall_cv_std': 0.0134, 'f1_cv_mean': 0.8151, 'f1_cv_std': 0.0043, 'accuracy_test': 0.8791, 'precision_test': 0.7538, 'recall_test': 0.7324, 'f1_score_test': 0.7429}, 'Random Forest': {'accuracy_cv_mean': 0.8401, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.8781, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.7898, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8684, 'precision_test': 0.6998, 'recall_test': 0.7855, 'f1_score_test': 0.7401}, 'XGBoost': {'accuracy_cv_mean': 0.9077, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9274, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8846, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.9055, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9216, 'precision_test': 0.8038, 'recall_test': 0.8884, 'f1_score_test': 0.844}, 'Naive Bayes': {'accuracy_cv_mean': 0.9045, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9301, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.9016, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9191, 'precision_test': 0.8094, 'recall_test': 0.8643, 'f1_score_test': 0.836}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8521, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8503, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8548, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.8525, 'f1_cv_std': 0.0027, 'params': 10401, 'accuracy_test': 0.886, 'precision_test': 0.7591, 'recall_test': 0.7651, 'f1_score_test': 0.7621}, 'MLP_338433': {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0088, 'recall_cv_mean': 0.8747, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8585, 'f1_cv_std': 0.0026, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7815, 'recall_test': 0.7702, 'f1_score_test': 0.7758}, 'MLP_1031169': {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8506, 'precision_cv_std': 0.0142, 'recall_cv_mean': 0.8639, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.857, 'f1_cv_std': 0.0028, 'params': 1031169, 'accuracy_test': 0.8978, 'precision_test': 0.8206, 'recall_test': 0.7314, 'f1_score_test': 0.7734}, 'Logistic Regression': {'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8599, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.834, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8467, 'f1_cv_std': 0.0019, 'accuracy_test': 0.8576, 'precision_test': 0.6603, 'recall_test': 0.83, 'f1_score_test': 0.7355}, 'SVM': {'accuracy_cv_mean': 0.6625, 'accuracy_cv_std': 0.0376, 'precision_cv_mean': 0.6361, 'precision_cv_std': 0.0336, 'recall_cv_mean': 0.7617, 'recall_cv_std': 0.0525, 'f1_cv_mean': 0.6926, 'f1_cv_std': 0.0352, 'accuracy_test': 0.5809, 'precision_test': 0.3211, 'recall_test': 0.6787, 'f1_score_test': 0.4359}, 'Decision Tree': {'accuracy_cv_mean': 0.8012, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8097, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7878, 'recall_cv_std': 0.0116, 'f1_cv_mean': 0.7985, 'f1_cv_std': 0.0034, 'accuracy_test': 0.8157, 'precision_test': 0.5856, 'recall_test': 0.7787, 'f1_score_test': 0.6685}, 'Random Forest': {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0011, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8137, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.8483, 'f1_cv_std': 0.0009, 'accuracy_test': 0.8821, 'precision_test': 0.7233, 'recall_test': 0.8192, 'f1_score_test': 0.7683}, 'XGBoost': {'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8932, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8595, 'recall_cv_std': 0.004, 'f1_cv_mean': 0.876, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8918, 'precision_test': 0.7303, 'recall_test': 0.8665, 'f1_score_test': 0.7926}, 'Naive Bayes': {'accuracy_cv_mean': 0.7819, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.7663, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8112, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.7881, 'f1_cv_std': 0.0026, 'accuracy_test': 0.7647, 'precision_test': 0.5044, 'recall_test': 0.8066, 'f1_score_test': 0.6206}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8963, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8906, 'precision_cv_std': 0.0134, 'recall_cv_mean': 0.9041, 'recall_cv_std': 0.0113, 'f1_cv_mean': 0.8971, 'f1_cv_std': 0.0023, 'params': 49953, 'accuracy_test': 0.2386, 'precision_test': 0.2386, 'recall_test': 1.0, 'f1_score_test': 0.3853}, 'MLP_971265': {'accuracy_cv_mean': 0.8973, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.891, 'precision_cv_std': 0.0181, 'recall_cv_mean': 0.9062, 'recall_cv_std': 0.0162, 'f1_cv_mean': 0.8982, 'f1_cv_std': 0.0015, 'params': 971265, 'accuracy_test': 0.9207, 'precision_test': 0.8391, 'recall_test': 0.826, 'f1_score_test': 0.8325}, 'MLP_2296833': {'accuracy_cv_mean': 0.8981, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9007, 'precision_cv_std': 0.0116, 'recall_cv_mean': 0.8951, 'recall_cv_std': 0.009, 'f1_cv_mean': 0.8978, 'f1_cv_std': 0.002, 'params': 2296833, 'accuracy_test': 0.9156, 'precision_test': 0.7967, 'recall_test': 0.8674, 'f1_score_test': 0.8306}}}
Saved on: outputs_cv/4/gpt

==============================
Model: Logistic Regression

Promedio CV (validación interna):
--- Fold 4/5 ---
Epoch [0/30] Train Loss: 0.3988, Test Loss: 0.3112, F1: 0.8658, AUC: 0.9437
Epoch [10/30] Train Loss: 0.2622, Test Loss: 0.2668, F1: 0.8917, AUC: 0.9596
Epoch [20/30] Train Loss: 0.2382, Test Loss: 0.2823, F1: 0.8909, AUC: 0.9626
Mejores resultados en la época:  27
f1-score 0.8944009632751354
AUC según el mejor F1-score 0.9633553874748066

--- Fold 5/5 ---
Epoch [0/30] Train Loss: 0.3976, Test Loss: 0.3145, F1: 0.8685, AUC: 0.9434
Epoch [10/30] Train Loss: 0.2650, Test Loss: 0.2825, F1: 0.8608, AUC: 0.9597
Epoch [20/30] Train Loss: 0.2448, Test Loss: 0.2611, F1: 0.8860, AUC: 0.9633
Mejores resultados en la época:  22
f1-score 0.898876404494382
AUC según el mejor F1-score 0.9641562676682013
Epoch [0/30] Train Loss: 0.3830, Test Loss: 0.2342, F1: 0.7979, AUC: 0.9488
Epoch [10/30] Train Loss: 0.2580, Test Loss: 0.2040, F1: 0.8260, AUC: 0.9626
Epoch [20/30] Train Loss: 0.2372, Test Loss: 0.2850, F1: 0.7998, AUC: 0.9644
Mejores resultados en la época:  26
f1-score 0.8305808127667471
AUC según el mejor F1-score 0.9671093840116574
Confusion matrix Test saved: outputs_cv/4/gpt/cm_mlp_6.png


resultados_globales con DL {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9337, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9321, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9336, 'f1_cv_std': 0.0022, 'params': 160801, 'accuracy_test': 0.9405, 'precision_test': 0.845, 'recall_test': 0.9194, 'f1_score_test': 0.8806}, 'MLP_2744833': {'accuracy_cv_mean': 0.9459, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9463, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9453, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9458, 'f1_cv_std': 0.0027, 'params': 2744833, 'accuracy_test': 0.9573, 'precision_test': 0.8977, 'recall_test': 0.9266, 'f1_score_test': 0.9119}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9429, 'precision_cv_std': 0.0071, 'recall_cv_mean': 0.9499, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9464, 'f1_cv_std': 0.0024, 'params': 5843969, 'accuracy_test': 0.9549, 'precision_test': 0.8793, 'recall_test': 0.9401, 'f1_score_test': 0.9087}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.0005, 'precision_cv_mean': 0.9364, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.9202, 'recall_cv_std': 0.0012, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0005, 'accuracy_test': 0.9354, 'precision_test': 0.8255, 'recall_test': 0.9248, 'f1_score_test': 0.8723}, 'SVM': {'accuracy_cv_mean': 0.8348, 'accuracy_cv_std': 0.0146, 'precision_cv_mean': 0.8051, 'precision_cv_std': 0.0362, 'recall_cv_mean': 0.8891, 'recall_cv_std': 0.0395, 'f1_cv_mean': 0.8434, 'f1_cv_std': 0.0099, 'accuracy_test': 0.8203, 'precision_test': 0.5836, 'recall_test': 0.8624, 'f1_score_test': 0.6961}, 'Decision Tree': {'accuracy_cv_mean': 0.8312, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7441, 'recall_cv_std': 0.0134, 'f1_cv_mean': 0.8151, 'f1_cv_std': 0.0043, 'accuracy_test': 0.8791, 'precision_test': 0.7538, 'recall_test': 0.7324, 'f1_score_test': 0.7429}, 'Random Forest': {'accuracy_cv_mean': 0.8401, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.8781, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.7898, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8684, 'precision_test': 0.6998, 'recall_test': 0.7855, 'f1_score_test': 0.7401}, 'XGBoost': {'accuracy_cv_mean': 0.9077, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9274, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8846, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.9055, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9216, 'precision_test': 0.8038, 'recall_test': 0.8884, 'f1_score_test': 0.844}, 'Naive Bayes': {'accuracy_cv_mean': 0.9045, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9301, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.9016, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9191, 'precision_test': 0.8094, 'recall_test': 0.8643, 'f1_score_test': 0.836}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8521, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8503, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8548, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.8525, 'f1_cv_std': 0.0027, 'params': 10401, 'accuracy_test': 0.886, 'precision_test': 0.7591, 'recall_test': 0.7651, 'f1_score_test': 0.7621}, 'MLP_338433': {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0088, 'recall_cv_mean': 0.8747, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8585, 'f1_cv_std': 0.0026, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7815, 'recall_test': 0.7702, 'f1_score_test': 0.7758}, 'MLP_1031169': {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8506, 'precision_cv_std': 0.0142, 'recall_cv_mean': 0.8639, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.857, 'f1_cv_std': 0.0028, 'params': 1031169, 'accuracy_test': 0.8978, 'precision_test': 0.8206, 'recall_test': 0.7314, 'f1_score_test': 0.7734}, 'Logistic Regression': {'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8599, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.834, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8467, 'f1_cv_std': 0.0019, 'accuracy_test': 0.8576, 'precision_test': 0.6603, 'recall_test': 0.83, 'f1_score_test': 0.7355}, 'SVM': {'accuracy_cv_mean': 0.6625, 'accuracy_cv_std': 0.0376, 'precision_cv_mean': 0.6361, 'precision_cv_std': 0.0336, 'recall_cv_mean': 0.7617, 'recall_cv_std': 0.0525, 'f1_cv_mean': 0.6926, 'f1_cv_std': 0.0352, 'accuracy_test': 0.5809, 'precision_test': 0.3211, 'recall_test': 0.6787, 'f1_score_test': 0.4359}, 'Decision Tree': {'accuracy_cv_mean': 0.8012, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8097, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7878, 'recall_cv_std': 0.0116, 'f1_cv_mean': 0.7985, 'f1_cv_std': 0.0034, 'accuracy_test': 0.8157, 'precision_test': 0.5856, 'recall_test': 0.7787, 'f1_score_test': 0.6685}, 'Random Forest': {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0011, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8137, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.8483, 'f1_cv_std': 0.0009, 'accuracy_test': 0.8821, 'precision_test': 0.7233, 'recall_test': 0.8192, 'f1_score_test': 0.7683}, 'XGBoost': {'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8932, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8595, 'recall_cv_std': 0.004, 'f1_cv_mean': 0.876, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8918, 'precision_test': 0.7303, 'recall_test': 0.8665, 'f1_score_test': 0.7926}, 'Naive Bayes': {'accuracy_cv_mean': 0.7819, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.7663, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8112, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.7881, 'f1_cv_std': 0.0026, 'accuracy_test': 0.7647, 'precision_test': 0.5044, 'recall_test': 0.8066, 'f1_score_test': 0.6206}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8963, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8906, 'precision_cv_std': 0.0134, 'recall_cv_mean': 0.9041, 'recall_cv_std': 0.0113, 'f1_cv_mean': 0.8971, 'f1_cv_std': 0.0023, 'params': 49953, 'accuracy_test': 0.2386, 'precision_test': 0.2386, 'recall_test': 1.0, 'f1_score_test': 0.3853}, 'MLP_971265': {'accuracy_cv_mean': 0.8973, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.891, 'precision_cv_std': 0.0181, 'recall_cv_mean': 0.9062, 'recall_cv_std': 0.0162, 'f1_cv_mean': 0.8982, 'f1_cv_std': 0.0015, 'params': 971265, 'accuracy_test': 0.9207, 'precision_test': 0.8391, 'recall_test': 0.826, 'f1_score_test': 0.8325}, 'MLP_2296833': {'accuracy_cv_mean': 0.8981, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9007, 'precision_cv_std': 0.0116, 'recall_cv_mean': 0.8951, 'recall_cv_std': 0.009, 'f1_cv_mean': 0.8978, 'f1_cv_std': 0.002, 'params': 2296833, 'accuracy_test': 0.9156, 'precision_test': 0.7967, 'recall_test': 0.8674, 'f1_score_test': 0.8306}}}
Saved on: outputs_cv/4/gpt

==============================
Model: Logistic Regression

Promedio CV (validación interna):
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:56:39] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:56:50] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:01:16] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:01:28] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:05:55] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:06:07] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:10:33] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:10:47] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:15:12] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:15:26] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:19:50] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [06:20:04] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
{'accuracy_cv_mean': 0.9014, 'accuracy_cv_std': 0.0012, 'precision_cv_mean': 0.907, 'precision_cv_std': 0.0028, 'recall_cv_mean': 0.8946, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.0015}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 46, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.91      0.94     16465
           1       0.76      0.90      0.82      5160

    accuracy                           0.91     21625
   macro avg       0.86      0.90      0.88     21625
weighted avg       0.92      0.91      0.91     21625

Confusion matrix Test saved as: outputs_cv/4/gpt/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/4/gpt/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8463, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8424, 'precision_cv_std': 0.0167, 'recall_cv_mean': 0.8529, 'recall_cv_std': 0.0185, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0058}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 46, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.88      0.95      0.91     16465
           1       0.79      0.59      0.67      5160

    accuracy                           0.86     21625
   macro avg       0.84      0.77      0.79     21625
weighted avg       0.86      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/4/gpt/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/4/gpt/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8052, 'accuracy_cv_std': 0.005, 'precision_cv_mean': 0.8206, 'precision_cv_std': 0.0138, 'recall_cv_mean': 0.782, 'recall_cv_std': 0.0151, 'f1_cv_mean': 0.8006, 'f1_cv_std': 0.0051}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 46, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.84      0.88     16465
           1       0.61      0.80      0.69      5160

    accuracy                           0.83     21625
   macro avg       0.77      0.82      0.79     21625
weighted avg       0.85      0.83      0.84     21625

Confusion matrix Test saved as: outputs_cv/4/gpt/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/4/gpt/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8757, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9043, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8404, 'recall_cv_std': 0.0055, 'f1_cv_mean': 0.8711, 'f1_cv_std': 0.0028}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 46, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.91      0.93     16465
           1       0.75      0.84      0.79      5160

    accuracy                           0.90     21625
   macro avg       0.85      0.88      0.86     21625
weighted avg       0.90      0.90      0.90     21625

Confusion matrix Test saved as: outputs_cv/4/gpt/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/4/gpt/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9087, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9208, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8944, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.9074, 'f1_cv_std': 0.0027}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 46, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.90      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/4/gpt/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/4/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8345, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8661, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.7915, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8271, 'f1_cv_std': 0.0026}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.79      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/4/gpt/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/4/gpt/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.9087, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9208, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8944, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.9074, 'f1_cv_std': 0.0027, 'accuracy_test': 0.9229, 'precision_test': 0.8026, 'recall_test': 0.8977, 'f1_score_test': 0.8475}
Logistic Regression: {'accuracy_cv_mean': 0.9014, 'accuracy_cv_std': 0.0012, 'precision_cv_mean': 0.907, 'precision_cv_std': 0.0028, 'recall_cv_mean': 0.8946, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9062, 'precision_test': 0.7552, 'recall_test': 0.8981, 'f1_score_test': 0.8205}
Random Forest: {'accuracy_cv_mean': 0.8757, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9043, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8404, 'recall_cv_std': 0.0055, 'f1_cv_mean': 0.8711, 'f1_cv_std': 0.0028, 'accuracy_test': 0.8956, 'precision_test': 0.7523, 'recall_test': 0.8388, 'f1_score_test': 0.7932}
Naive Bayes: {'accuracy_cv_mean': 0.8345, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8661, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.7915, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8271, 'f1_cv_std': 0.0026, 'accuracy_test': 0.8551, 'precision_test': 0.6656, 'recall_test': 0.7893, 'f1_score_test': 0.7222}
Decision Tree: {'accuracy_cv_mean': 0.8052, 'accuracy_cv_std': 0.005, 'precision_cv_mean': 0.8206, 'precision_cv_std': 0.0138, 'recall_cv_mean': 0.782, 'recall_cv_std': 0.0151, 'f1_cv_mean': 0.8006, 'f1_cv_std': 0.0051, 'accuracy_test': 0.8285, 'precision_test': 0.6071, 'recall_test': 0.7971, 'f1_score_test': 0.6892}
SVM: {'accuracy_cv_mean': 0.8463, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8424, 'precision_cv_std': 0.0167, 'recall_cv_mean': 0.8529, 'recall_cv_std': 0.0185, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8647, 'precision_test': 0.7931, 'recall_test': 0.586, 'f1_score_test': 0.674}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9337, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9321, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9336, 'f1_cv_std': 0.0022, 'params': 160801, 'accuracy_test': 0.9405, 'precision_test': 0.845, 'recall_test': 0.9194, 'f1_score_test': 0.8806}, 'MLP_2744833': {'accuracy_cv_mean': 0.9459, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9463, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9453, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9458, 'f1_cv_std': 0.0027, 'params': 2744833, 'accuracy_test': 0.9573, 'precision_test': 0.8977, 'recall_test': 0.9266, 'f1_score_test': 0.9119}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9429, 'precision_cv_std': 0.0071, 'recall_cv_mean': 0.9499, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9464, 'f1_cv_std': 0.0024, 'params': 5843969, 'accuracy_test': 0.9549, 'precision_test': 0.8793, 'recall_test': 0.9401, 'f1_score_test': 0.9087}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.0005, 'precision_cv_mean': 0.9364, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.9202, 'recall_cv_std': 0.0012, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0005, 'accuracy_test': 0.9354, 'precision_test': 0.8255, 'recall_test': 0.9248, 'f1_score_test': 0.8723}, 'SVM': {'accuracy_cv_mean': 0.8348, 'accuracy_cv_std': 0.0146, 'precision_cv_mean': 0.8051, 'precision_cv_std': 0.0362, 'recall_cv_mean': 0.8891, 'recall_cv_std': 0.0395, 'f1_cv_mean': 0.8434, 'f1_cv_std': 0.0099, 'accuracy_test': 0.8203, 'precision_test': 0.5836, 'recall_test': 0.8624, 'f1_score_test': 0.6961}, 'Decision Tree': {'accuracy_cv_mean': 0.8312, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7441, 'recall_cv_std': 0.0134, 'f1_cv_mean': 0.8151, 'f1_cv_std': 0.0043, 'accuracy_test': 0.8791, 'precision_test': 0.7538, 'recall_test': 0.7324, 'f1_score_test': 0.7429}, 'Random Forest': {'accuracy_cv_mean': 0.8401, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.8781, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.7898, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8684, 'precision_test': 0.6998, 'recall_test': 0.7855, 'f1_score_test': 0.7401}, 'XGBoost': {'accuracy_cv_mean': 0.9077, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9274, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8846, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.9055, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9216, 'precision_test': 0.8038, 'recall_test': 0.8884, 'f1_score_test': 0.844}, 'Naive Bayes': {'accuracy_cv_mean': 0.9045, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9301, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.9016, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9191, 'precision_test': 0.8094, 'recall_test': 0.8643, 'f1_score_test': 0.836}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8521, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8503, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8548, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.8525, 'f1_cv_std': 0.0027, 'params': 10401, 'accuracy_test': 0.886, 'precision_test': 0.7591, 'recall_test': 0.7651, 'f1_score_test': 0.7621}, 'MLP_338433': {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0088, 'recall_cv_mean': 0.8747, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8585, 'f1_cv_std': 0.0026, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7815, 'recall_test': 0.7702, 'f1_score_test': 0.7758}, 'MLP_1031169': {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8506, 'precision_cv_std': 0.0142, 'recall_cv_mean': 0.8639, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.857, 'f1_cv_std': 0.0028, 'params': 1031169, 'accuracy_test': 0.8978, 'precision_test': 0.8206, 'recall_test': 0.7314, 'f1_score_test': 0.7734}, 'Logistic Regression': {'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8599, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.834, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8467, 'f1_cv_std': 0.0019, 'accuracy_test': 0.8576, 'precision_test': 0.6603, 'recall_test': 0.83, 'f1_score_test': 0.7355}, 'SVM': {'accuracy_cv_mean': 0.6625, 'accuracy_cv_std': 0.0376, 'precision_cv_mean': 0.6361, 'precision_cv_std': 0.0336, 'recall_cv_mean': 0.7617, 'recall_cv_std': 0.0525, 'f1_cv_mean': 0.6926, 'f1_cv_std': 0.0352, 'accuracy_test': 0.5809, 'precision_test': 0.3211, 'recall_test': 0.6787, 'f1_score_test': 0.4359}, 'Decision Tree': {'accuracy_cv_mean': 0.8012, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8097, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7878, 'recall_cv_std': 0.0116, 'f1_cv_mean': 0.7985, 'f1_cv_std': 0.0034, 'accuracy_test': 0.8157, 'precision_test': 0.5856, 'recall_test': 0.7787, 'f1_score_test': 0.6685}, 'Random Forest': {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0011, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8137, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.8483, 'f1_cv_std': 0.0009, 'accuracy_test': 0.8821, 'precision_test': 0.7233, 'recall_test': 0.8192, 'f1_score_test': 0.7683}, 'XGBoost': {'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8932, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8595, 'recall_cv_std': 0.004, 'f1_cv_mean': 0.876, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8918, 'precision_test': 0.7303, 'recall_test': 0.8665, 'f1_score_test': 0.7926}, 'Naive Bayes': {'accuracy_cv_mean': 0.7819, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.7663, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8112, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.7881, 'f1_cv_std': 0.0026, 'accuracy_test': 0.7647, 'precision_test': 0.5044, 'recall_test': 0.8066, 'f1_score_test': 0.6206}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8963, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8906, 'precision_cv_std': 0.0134, 'recall_cv_mean': 0.9041, 'recall_cv_std': 0.0113, 'f1_cv_mean': 0.8971, 'f1_cv_std': 0.0023, 'params': 49953, 'accuracy_test': 0.2386, 'precision_test': 0.2386, 'recall_test': 1.0, 'f1_score_test': 0.3853}, 'MLP_971265': {'accuracy_cv_mean': 0.8973, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.891, 'precision_cv_std': 0.0181, 'recall_cv_mean': 0.9062, 'recall_cv_std': 0.0162, 'f1_cv_mean': 0.8982, 'f1_cv_std': 0.0015, 'params': 971265, 'accuracy_test': 0.9207, 'precision_test': 0.8391, 'recall_test': 0.826, 'f1_score_test': 0.8325}, 'MLP_2296833': {'accuracy_cv_mean': 0.8981, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9007, 'precision_cv_std': 0.0116, 'recall_cv_mean': 0.8951, 'recall_cv_std': 0.009, 'f1_cv_mean': 0.8978, 'f1_cv_std': 0.002, 'params': 2296833, 'accuracy_test': 0.9156, 'precision_test': 0.7967, 'recall_test': 0.8674, 'f1_score_test': 0.8306}, 'Logistic Regression': {'accuracy_cv_mean': 0.9014, 'accuracy_cv_std': 0.0012, 'precision_cv_mean': 0.907, 'precision_cv_std': 0.0028, 'recall_cv_mean': 0.8946, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9062, 'precision_test': 0.7552, 'recall_test': 0.8981, 'f1_score_test': 0.8205}, 'SVM': {'accuracy_cv_mean': 0.8463, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8424, 'precision_cv_std': 0.0167, 'recall_cv_mean': 0.8529, 'recall_cv_std': 0.0185, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8647, 'precision_test': 0.7931, 'recall_test': 0.586, 'f1_score_test': 0.674}, 'Decision Tree': {'accuracy_cv_mean': 0.8052, 'accuracy_cv_std': 0.005, 'precision_cv_mean': 0.8206, 'precision_cv_std': 0.0138, 'recall_cv_mean': 0.782, 'recall_cv_std': 0.0151, 'f1_cv_mean': 0.8006, 'f1_cv_std': 0.0051, 'accuracy_test': 0.8285, 'precision_test': 0.6071, 'recall_test': 0.7971, 'f1_score_test': 0.6892}, 'Random Forest': {'accuracy_cv_mean': 0.8757, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9043, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8404, 'recall_cv_std': 0.0055, 'f1_cv_mean': 0.8711, 'f1_cv_std': 0.0028, 'accuracy_test': 0.8956, 'precision_test': 0.7523, 'recall_test': 0.8388, 'f1_score_test': 0.7932}, 'XGBoost': {'accuracy_cv_mean': 0.9087, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9208, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8944, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.9074, 'f1_cv_std': 0.0027, 'accuracy_test': 0.9229, 'precision_test': 0.8026, 'recall_test': 0.8977, 'f1_score_test': 0.8475}, 'Naive Bayes': {'accuracy_cv_mean': 0.8345, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8661, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.7915, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8271, 'f1_cv_std': 0.0026, 'accuracy_test': 0.8551, 'precision_test': 0.6656, 'recall_test': 0.7893, 'f1_score_test': 0.7222}}}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
MLP_2744833: {'accuracy_cv_mean': 0.9459, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9463, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9453, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9458, 'f1_cv_std': 0.0027, 'params': 2744833, 'accuracy_test': 0.9573, 'precision_test': 0.8977, 'recall_test': 0.9266, 'f1_score_test': 0.9119}
MLP_5843969: {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9429, 'precision_cv_std': 0.0071, 'recall_cv_mean': 0.9499, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9464, 'f1_cv_std': 0.0024, 'params': 5843969, 'accuracy_test': 0.9549, 'precision_test': 0.8793, 'recall_test': 0.9401, 'f1_score_test': 0.9087}
MLP_160801: {'accuracy_cv_mean': 0.9337, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9321, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9336, 'f1_cv_std': 0.0022, 'params': 160801, 'accuracy_test': 0.9405, 'precision_test': 0.845, 'recall_test': 0.9194, 'f1_score_test': 0.8806}
Logistic Regression: {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.0005, 'precision_cv_mean': 0.9364, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.9202, 'recall_cv_std': 0.0012, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0005, 'accuracy_test': 0.9354, 'precision_test': 0.8255, 'recall_test': 0.9248, 'f1_score_test': 0.8723}
XGBoost: {'accuracy_cv_mean': 0.9077, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9274, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8846, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.9055, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9216, 'precision_test': 0.8038, 'recall_test': 0.8884, 'f1_score_test': 0.844}
Naive Bayes: {'accuracy_cv_mean': 0.9045, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9301, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.9016, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9191, 'precision_test': 0.8094, 'recall_test': 0.8643, 'f1_score_test': 0.836}
Decision Tree: {'accuracy_cv_mean': 0.8312, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7441, 'recall_cv_std': 0.0134, 'f1_cv_mean': 0.8151, 'f1_cv_std': 0.0043, 'accuracy_test': 0.8791, 'precision_test': 0.7538, 'recall_test': 0.7324, 'f1_score_test': 0.7429}
Random Forest: {'accuracy_cv_mean': 0.8401, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.8781, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.7898, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8684, 'precision_test': 0.6998, 'recall_test': 0.7855, 'f1_score_test': 0.7401}
SVM: {'accuracy_cv_mean': 0.8348, 'accuracy_cv_std': 0.0146, 'precision_cv_mean': 0.8051, 'precision_cv_std': 0.0362, 'recall_cv_mean': 0.8891, 'recall_cv_std': 0.0395, 'f1_cv_mean': 0.8434, 'f1_cv_std': 0.0099, 'accuracy_test': 0.8203, 'precision_test': 0.5836, 'recall_test': 0.8624, 'f1_score_test': 0.6961}


EMBEDDINGS TYPE: LYRICS_BERT
XGBoost: {'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8932, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8595, 'recall_cv_std': 0.004, 'f1_cv_mean': 0.876, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8918, 'precision_test': 0.7303, 'recall_test': 0.8665, 'f1_score_test': 0.7926}
MLP_338433: {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0088, 'recall_cv_mean': 0.8747, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8585, 'f1_cv_std': 0.0026, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7815, 'recall_test': 0.7702, 'f1_score_test': 0.7758}
MLP_1031169: {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8506, 'precision_cv_std': 0.0142, 'recall_cv_mean': 0.8639, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.857, 'f1_cv_std': 0.0028, 'params': 1031169, 'accuracy_test': 0.8978, 'precision_test': 0.8206, 'recall_test': 0.7314, 'f1_score_test': 0.7734}
Random Forest: {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0011, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8137, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.8483, 'f1_cv_std': 0.0009, 'accuracy_test': 0.8821, 'precision_test': 0.7233, 'recall_test': 0.8192, 'f1_score_test': 0.7683}
MLP_10401: {'accuracy_cv_mean': 0.8521, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8503, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8548, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.8525, 'f1_cv_std': 0.0027, 'params': 10401, 'accuracy_test': 0.886, 'precision_test': 0.7591, 'recall_test': 0.7651, 'f1_score_test': 0.7621}
Logistic Regression: {'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8599, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.834, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8467, 'f1_cv_std': 0.0019, 'accuracy_test': 0.8576, 'precision_test': 0.6603, 'recall_test': 0.83, 'f1_score_test': 0.7355}
Decision Tree: {'accuracy_cv_mean': 0.8012, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8097, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7878, 'recall_cv_std': 0.0116, 'f1_cv_mean': 0.7985, 'f1_cv_std': 0.0034, 'accuracy_test': 0.8157, 'precision_test': 0.5856, 'recall_test': 0.7787, 'f1_score_test': 0.6685}
Naive Bayes: {'accuracy_cv_mean': 0.7819, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.7663, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8112, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.7881, 'f1_cv_std': 0.0026, 'accuracy_test': 0.7647, 'precision_test': 0.5044, 'recall_test': 0.8066, 'f1_score_test': 0.6206}
SVM: {'accuracy_cv_mean': 0.6625, 'accuracy_cv_std': 0.0376, 'precision_cv_mean': 0.6361, 'precision_cv_std': 0.0336, 'recall_cv_mean': 0.7617, 'recall_cv_std': 0.0525, 'f1_cv_mean': 0.6926, 'f1_cv_std': 0.0352, 'accuracy_test': 0.5809, 'precision_test': 0.3211, 'recall_test': 0.6787, 'f1_score_test': 0.4359}


EMBEDDINGS TYPE: GPT
XGBoost: {'accuracy_cv_mean': 0.9087, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9208, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8944, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.9074, 'f1_cv_std': 0.0027, 'accuracy_test': 0.9229, 'precision_test': 0.8026, 'recall_test': 0.8977, 'f1_score_test': 0.8475}
MLP_971265: {'accuracy_cv_mean': 0.8973, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.891, 'precision_cv_std': 0.0181, 'recall_cv_mean': 0.9062, 'recall_cv_std': 0.0162, 'f1_cv_mean': 0.8982, 'f1_cv_std': 0.0015, 'params': 971265, 'accuracy_test': 0.9207, 'precision_test': 0.8391, 'recall_test': 0.826, 'f1_score_test': 0.8325}
MLP_2296833: {'accuracy_cv_mean': 0.8981, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9007, 'precision_cv_std': 0.0116, 'recall_cv_mean': 0.8951, 'recall_cv_std': 0.009, 'f1_cv_mean': 0.8978, 'f1_cv_std': 0.002, 'params': 2296833, 'accuracy_test': 0.9156, 'precision_test': 0.7967, 'recall_test': 0.8674, 'f1_score_test': 0.8306}
Logistic Regression: {'accuracy_cv_mean': 0.9014, 'accuracy_cv_std': 0.0012, 'precision_cv_mean': 0.907, 'precision_cv_std': 0.0028, 'recall_cv_mean': 0.8946, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9062, 'precision_test': 0.7552, 'recall_test': 0.8981, 'f1_score_test': 0.8205}
Random Forest: {'accuracy_cv_mean': 0.8757, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9043, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8404, 'recall_cv_std': 0.0055, 'f1_cv_mean': 0.8711, 'f1_cv_std': 0.0028, 'accuracy_test': 0.8956, 'precision_test': 0.7523, 'recall_test': 0.8388, 'f1_score_test': 0.7932}
Naive Bayes: {'accuracy_cv_mean': 0.8345, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8661, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.7915, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8271, 'f1_cv_std': 0.0026, 'accuracy_test': 0.8551, 'precision_test': 0.6656, 'recall_test': 0.7893, 'f1_score_test': 0.7222}
Decision Tree: {'accuracy_cv_mean': 0.8052, 'accuracy_cv_std': 0.005, 'precision_cv_mean': 0.8206, 'precision_cv_std': 0.0138, 'recall_cv_mean': 0.782, 'recall_cv_std': 0.0151, 'f1_cv_mean': 0.8006, 'f1_cv_std': 0.0051, 'accuracy_test': 0.8285, 'precision_test': 0.6071, 'recall_test': 0.7971, 'f1_score_test': 0.6892}
SVM: {'accuracy_cv_mean': 0.8463, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8424, 'precision_cv_std': 0.0167, 'recall_cv_mean': 0.8529, 'recall_cv_std': 0.0185, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8647, 'precision_test': 0.7931, 'recall_test': 0.586, 'f1_score_test': 0.674}
MLP_49953: {'accuracy_cv_mean': 0.8963, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8906, 'precision_cv_std': 0.0134, 'recall_cv_mean': 0.9041, 'recall_cv_std': 0.0113, 'f1_cv_mean': 0.8971, 'f1_cv_std': 0.0023, 'params': 49953, 'accuracy_test': 0.2386, 'precision_test': 0.2386, 'recall_test': 1.0, 'f1_score_test': 0.3853}
Diccionario global guardado en: outputs_cv/4/gpt/global_metrics.json

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
====================================

{'accuracy_cv_mean': 0.9014, 'accuracy_cv_std': 0.0012, 'precision_cv_mean': 0.907, 'precision_cv_std': 0.0028, 'recall_cv_mean': 0.8946, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.0015}
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 46, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.91      0.94     16465
           1       0.76      0.90      0.82      5160

    accuracy                           0.91     21625
   macro avg       0.86      0.90      0.88     21625
weighted avg       0.92      0.91      0.91     21625

Confusion matrix Test saved as: outputs_cv/4/gpt/conf_matrix_test_logistic_regression.png
Modelo guardado como: outputs_cv/4/gpt/logistic_regression_model.pkl

==============================
Model: SVM

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8463, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8424, 'precision_cv_std': 0.0167, 'recall_cv_mean': 0.8529, 'recall_cv_std': 0.0185, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0058}
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 46, 'shrinking': True, 'tol': 0.001, 'verbose': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.88      0.95      0.91     16465
           1       0.79      0.59      0.67      5160

    accuracy                           0.86     21625
   macro avg       0.84      0.77      0.79     21625
weighted avg       0.86      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/4/gpt/conf_matrix_test_svm.png
Modelo guardado como: outputs_cv/4/gpt/svm_model.pkl

==============================
Model: Decision Tree

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8052, 'accuracy_cv_std': 0.005, 'precision_cv_mean': 0.8206, 'precision_cv_std': 0.0138, 'recall_cv_mean': 0.782, 'recall_cv_std': 0.0151, 'f1_cv_mean': 0.8006, 'f1_cv_std': 0.0051}
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 46, 'splitter': 'best'}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.84      0.88     16465
           1       0.61      0.80      0.69      5160

    accuracy                           0.83     21625
   macro avg       0.77      0.82      0.79     21625
weighted avg       0.85      0.83      0.84     21625

Confusion matrix Test saved as: outputs_cv/4/gpt/conf_matrix_test_decision_tree.png
Modelo guardado como: outputs_cv/4/gpt/decision_tree_model.pkl

==============================
Model: Random Forest

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8757, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9043, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8404, 'recall_cv_std': 0.0055, 'f1_cv_mean': 0.8711, 'f1_cv_std': 0.0028}
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 46, 'verbose': 0, 'warm_start': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.95      0.91      0.93     16465
           1       0.75      0.84      0.79      5160

    accuracy                           0.90     21625
   macro avg       0.85      0.88      0.86     21625
weighted avg       0.90      0.90      0.90     21625

Confusion matrix Test saved as: outputs_cv/4/gpt/conf_matrix_test_random_forest.png
Modelo guardado como: outputs_cv/4/gpt/random_forest_model.pkl

==============================
Model: XGBoost

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.9087, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9208, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8944, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.9074, 'f1_cv_std': 0.0027}
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 46, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.97      0.93      0.95     16465
           1       0.80      0.90      0.85      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

Confusion matrix Test saved as: outputs_cv/4/gpt/conf_matrix_test_xgboost.png
Modelo guardado como: outputs_cv/4/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes

Promedio CV (validación interna):
{'accuracy_cv_mean': 0.8345, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8661, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.7915, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8271, 'f1_cv_std': 0.0026}
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}

Evaluación en TEST:
              precision    recall  f1-score   support

           0       0.93      0.88      0.90     16465
           1       0.67      0.79      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.80      0.83      0.81     21625
weighted avg       0.87      0.86      0.86     21625

Confusion matrix Test saved as: outputs_cv/4/gpt/conf_matrix_test_naive_bayes.png
Modelo guardado como: outputs_cv/4/gpt/naive_bayes_model.pkl


Resumen Final:
XGBoost: {'accuracy_cv_mean': 0.9087, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9208, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8944, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.9074, 'f1_cv_std': 0.0027, 'accuracy_test': 0.9229, 'precision_test': 0.8026, 'recall_test': 0.8977, 'f1_score_test': 0.8475}
Logistic Regression: {'accuracy_cv_mean': 0.9014, 'accuracy_cv_std': 0.0012, 'precision_cv_mean': 0.907, 'precision_cv_std': 0.0028, 'recall_cv_mean': 0.8946, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9062, 'precision_test': 0.7552, 'recall_test': 0.8981, 'f1_score_test': 0.8205}
Random Forest: {'accuracy_cv_mean': 0.8757, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9043, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8404, 'recall_cv_std': 0.0055, 'f1_cv_mean': 0.8711, 'f1_cv_std': 0.0028, 'accuracy_test': 0.8956, 'precision_test': 0.7523, 'recall_test': 0.8388, 'f1_score_test': 0.7932}
Naive Bayes: {'accuracy_cv_mean': 0.8345, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8661, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.7915, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8271, 'f1_cv_std': 0.0026, 'accuracy_test': 0.8551, 'precision_test': 0.6656, 'recall_test': 0.7893, 'f1_score_test': 0.7222}
Decision Tree: {'accuracy_cv_mean': 0.8052, 'accuracy_cv_std': 0.005, 'precision_cv_mean': 0.8206, 'precision_cv_std': 0.0138, 'recall_cv_mean': 0.782, 'recall_cv_std': 0.0151, 'f1_cv_mean': 0.8006, 'f1_cv_std': 0.0051, 'accuracy_test': 0.8285, 'precision_test': 0.6071, 'recall_test': 0.7971, 'f1_score_test': 0.6892}
SVM: {'accuracy_cv_mean': 0.8463, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8424, 'precision_cv_std': 0.0167, 'recall_cv_mean': 0.8529, 'recall_cv_std': 0.0185, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8647, 'precision_test': 0.7931, 'recall_test': 0.586, 'f1_score_test': 0.674}
resultados_globales con ML {'tfidf': {'MLP_160801': {'accuracy_cv_mean': 0.9337, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9321, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9336, 'f1_cv_std': 0.0022, 'params': 160801, 'accuracy_test': 0.9405, 'precision_test': 0.845, 'recall_test': 0.9194, 'f1_score_test': 0.8806}, 'MLP_2744833': {'accuracy_cv_mean': 0.9459, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9463, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9453, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9458, 'f1_cv_std': 0.0027, 'params': 2744833, 'accuracy_test': 0.9573, 'precision_test': 0.8977, 'recall_test': 0.9266, 'f1_score_test': 0.9119}, 'MLP_5843969': {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9429, 'precision_cv_std': 0.0071, 'recall_cv_mean': 0.9499, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9464, 'f1_cv_std': 0.0024, 'params': 5843969, 'accuracy_test': 0.9549, 'precision_test': 0.8793, 'recall_test': 0.9401, 'f1_score_test': 0.9087}, 'Logistic Regression': {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.0005, 'precision_cv_mean': 0.9364, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.9202, 'recall_cv_std': 0.0012, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0005, 'accuracy_test': 0.9354, 'precision_test': 0.8255, 'recall_test': 0.9248, 'f1_score_test': 0.8723}, 'SVM': {'accuracy_cv_mean': 0.8348, 'accuracy_cv_std': 0.0146, 'precision_cv_mean': 0.8051, 'precision_cv_std': 0.0362, 'recall_cv_mean': 0.8891, 'recall_cv_std': 0.0395, 'f1_cv_mean': 0.8434, 'f1_cv_std': 0.0099, 'accuracy_test': 0.8203, 'precision_test': 0.5836, 'recall_test': 0.8624, 'f1_score_test': 0.6961}, 'Decision Tree': {'accuracy_cv_mean': 0.8312, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7441, 'recall_cv_std': 0.0134, 'f1_cv_mean': 0.8151, 'f1_cv_std': 0.0043, 'accuracy_test': 0.8791, 'precision_test': 0.7538, 'recall_test': 0.7324, 'f1_score_test': 0.7429}, 'Random Forest': {'accuracy_cv_mean': 0.8401, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.8781, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.7898, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8684, 'precision_test': 0.6998, 'recall_test': 0.7855, 'f1_score_test': 0.7401}, 'XGBoost': {'accuracy_cv_mean': 0.9077, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9274, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8846, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.9055, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9216, 'precision_test': 0.8038, 'recall_test': 0.8884, 'f1_score_test': 0.844}, 'Naive Bayes': {'accuracy_cv_mean': 0.9045, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9301, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.9016, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9191, 'precision_test': 0.8094, 'recall_test': 0.8643, 'f1_score_test': 0.836}}, 'lyrics_bert': {'MLP_10401': {'accuracy_cv_mean': 0.8521, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8503, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8548, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.8525, 'f1_cv_std': 0.0027, 'params': 10401, 'accuracy_test': 0.886, 'precision_test': 0.7591, 'recall_test': 0.7651, 'f1_score_test': 0.7621}, 'MLP_338433': {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0088, 'recall_cv_mean': 0.8747, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8585, 'f1_cv_std': 0.0026, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7815, 'recall_test': 0.7702, 'f1_score_test': 0.7758}, 'MLP_1031169': {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8506, 'precision_cv_std': 0.0142, 'recall_cv_mean': 0.8639, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.857, 'f1_cv_std': 0.0028, 'params': 1031169, 'accuracy_test': 0.8978, 'precision_test': 0.8206, 'recall_test': 0.7314, 'f1_score_test': 0.7734}, 'Logistic Regression': {'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8599, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.834, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8467, 'f1_cv_std': 0.0019, 'accuracy_test': 0.8576, 'precision_test': 0.6603, 'recall_test': 0.83, 'f1_score_test': 0.7355}, 'SVM': {'accuracy_cv_mean': 0.6625, 'accuracy_cv_std': 0.0376, 'precision_cv_mean': 0.6361, 'precision_cv_std': 0.0336, 'recall_cv_mean': 0.7617, 'recall_cv_std': 0.0525, 'f1_cv_mean': 0.6926, 'f1_cv_std': 0.0352, 'accuracy_test': 0.5809, 'precision_test': 0.3211, 'recall_test': 0.6787, 'f1_score_test': 0.4359}, 'Decision Tree': {'accuracy_cv_mean': 0.8012, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8097, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7878, 'recall_cv_std': 0.0116, 'f1_cv_mean': 0.7985, 'f1_cv_std': 0.0034, 'accuracy_test': 0.8157, 'precision_test': 0.5856, 'recall_test': 0.7787, 'f1_score_test': 0.6685}, 'Random Forest': {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0011, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8137, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.8483, 'f1_cv_std': 0.0009, 'accuracy_test': 0.8821, 'precision_test': 0.7233, 'recall_test': 0.8192, 'f1_score_test': 0.7683}, 'XGBoost': {'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8932, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8595, 'recall_cv_std': 0.004, 'f1_cv_mean': 0.876, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8918, 'precision_test': 0.7303, 'recall_test': 0.8665, 'f1_score_test': 0.7926}, 'Naive Bayes': {'accuracy_cv_mean': 0.7819, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.7663, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8112, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.7881, 'f1_cv_std': 0.0026, 'accuracy_test': 0.7647, 'precision_test': 0.5044, 'recall_test': 0.8066, 'f1_score_test': 0.6206}}, 'gpt': {'MLP_49953': {'accuracy_cv_mean': 0.8963, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8906, 'precision_cv_std': 0.0134, 'recall_cv_mean': 0.9041, 'recall_cv_std': 0.0113, 'f1_cv_mean': 0.8971, 'f1_cv_std': 0.0023, 'params': 49953, 'accuracy_test': 0.2386, 'precision_test': 0.2386, 'recall_test': 1.0, 'f1_score_test': 0.3853}, 'MLP_971265': {'accuracy_cv_mean': 0.8973, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.891, 'precision_cv_std': 0.0181, 'recall_cv_mean': 0.9062, 'recall_cv_std': 0.0162, 'f1_cv_mean': 0.8982, 'f1_cv_std': 0.0015, 'params': 971265, 'accuracy_test': 0.9207, 'precision_test': 0.8391, 'recall_test': 0.826, 'f1_score_test': 0.8325}, 'MLP_2296833': {'accuracy_cv_mean': 0.8981, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9007, 'precision_cv_std': 0.0116, 'recall_cv_mean': 0.8951, 'recall_cv_std': 0.009, 'f1_cv_mean': 0.8978, 'f1_cv_std': 0.002, 'params': 2296833, 'accuracy_test': 0.9156, 'precision_test': 0.7967, 'recall_test': 0.8674, 'f1_score_test': 0.8306}, 'Logistic Regression': {'accuracy_cv_mean': 0.9014, 'accuracy_cv_std': 0.0012, 'precision_cv_mean': 0.907, 'precision_cv_std': 0.0028, 'recall_cv_mean': 0.8946, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9062, 'precision_test': 0.7552, 'recall_test': 0.8981, 'f1_score_test': 0.8205}, 'SVM': {'accuracy_cv_mean': 0.8463, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8424, 'precision_cv_std': 0.0167, 'recall_cv_mean': 0.8529, 'recall_cv_std': 0.0185, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8647, 'precision_test': 0.7931, 'recall_test': 0.586, 'f1_score_test': 0.674}, 'Decision Tree': {'accuracy_cv_mean': 0.8052, 'accuracy_cv_std': 0.005, 'precision_cv_mean': 0.8206, 'precision_cv_std': 0.0138, 'recall_cv_mean': 0.782, 'recall_cv_std': 0.0151, 'f1_cv_mean': 0.8006, 'f1_cv_std': 0.0051, 'accuracy_test': 0.8285, 'precision_test': 0.6071, 'recall_test': 0.7971, 'f1_score_test': 0.6892}, 'Random Forest': {'accuracy_cv_mean': 0.8757, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9043, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8404, 'recall_cv_std': 0.0055, 'f1_cv_mean': 0.8711, 'f1_cv_std': 0.0028, 'accuracy_test': 0.8956, 'precision_test': 0.7523, 'recall_test': 0.8388, 'f1_score_test': 0.7932}, 'XGBoost': {'accuracy_cv_mean': 0.9087, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9208, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8944, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.9074, 'f1_cv_std': 0.0027, 'accuracy_test': 0.9229, 'precision_test': 0.8026, 'recall_test': 0.8977, 'f1_score_test': 0.8475}, 'Naive Bayes': {'accuracy_cv_mean': 0.8345, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8661, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.7915, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8271, 'f1_cv_std': 0.0026, 'accuracy_test': 0.8551, 'precision_test': 0.6656, 'recall_test': 0.7893, 'f1_score_test': 0.7222}}}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
MLP_2744833: {'accuracy_cv_mean': 0.9459, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9463, 'precision_cv_std': 0.0034, 'recall_cv_mean': 0.9453, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.9458, 'f1_cv_std': 0.0027, 'params': 2744833, 'accuracy_test': 0.9573, 'precision_test': 0.8977, 'recall_test': 0.9266, 'f1_score_test': 0.9119}
MLP_5843969: {'accuracy_cv_mean': 0.9462, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9429, 'precision_cv_std': 0.0071, 'recall_cv_mean': 0.9499, 'recall_cv_std': 0.0049, 'f1_cv_mean': 0.9464, 'f1_cv_std': 0.0024, 'params': 5843969, 'accuracy_test': 0.9549, 'precision_test': 0.8793, 'recall_test': 0.9401, 'f1_score_test': 0.9087}
MLP_160801: {'accuracy_cv_mean': 0.9337, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.9351, 'precision_cv_std': 0.0082, 'recall_cv_mean': 0.9321, 'recall_cv_std': 0.0065, 'f1_cv_mean': 0.9336, 'f1_cv_std': 0.0022, 'params': 160801, 'accuracy_test': 0.9405, 'precision_test': 0.845, 'recall_test': 0.9194, 'f1_score_test': 0.8806}
Logistic Regression: {'accuracy_cv_mean': 0.9288, 'accuracy_cv_std': 0.0005, 'precision_cv_mean': 0.9364, 'precision_cv_std': 0.0013, 'recall_cv_mean': 0.9202, 'recall_cv_std': 0.0012, 'f1_cv_mean': 0.9282, 'f1_cv_std': 0.0005, 'accuracy_test': 0.9354, 'precision_test': 0.8255, 'recall_test': 0.9248, 'f1_score_test': 0.8723}
XGBoost: {'accuracy_cv_mean': 0.9077, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.9274, 'precision_cv_std': 0.0033, 'recall_cv_mean': 0.8846, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.9055, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9216, 'precision_test': 0.8038, 'recall_test': 0.8884, 'f1_score_test': 0.844}
Naive Bayes: {'accuracy_cv_mean': 0.9045, 'accuracy_cv_std': 0.0017, 'precision_cv_mean': 0.9301, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.8748, 'recall_cv_std': 0.0034, 'f1_cv_mean': 0.9016, 'f1_cv_std': 0.0018, 'accuracy_test': 0.9191, 'precision_test': 0.8094, 'recall_test': 0.8643, 'f1_score_test': 0.836}
Decision Tree: {'accuracy_cv_mean': 0.8312, 'accuracy_cv_std': 0.002, 'precision_cv_mean': 0.9014, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7441, 'recall_cv_std': 0.0134, 'f1_cv_mean': 0.8151, 'f1_cv_std': 0.0043, 'accuracy_test': 0.8791, 'precision_test': 0.7538, 'recall_test': 0.7324, 'f1_score_test': 0.7429}
Random Forest: {'accuracy_cv_mean': 0.8401, 'accuracy_cv_std': 0.003, 'precision_cv_mean': 0.8781, 'precision_cv_std': 0.0061, 'recall_cv_mean': 0.7898, 'recall_cv_std': 0.0022, 'f1_cv_mean': 0.8316, 'f1_cv_std': 0.0027, 'accuracy_test': 0.8684, 'precision_test': 0.6998, 'recall_test': 0.7855, 'f1_score_test': 0.7401}
SVM: {'accuracy_cv_mean': 0.8348, 'accuracy_cv_std': 0.0146, 'precision_cv_mean': 0.8051, 'precision_cv_std': 0.0362, 'recall_cv_mean': 0.8891, 'recall_cv_std': 0.0395, 'f1_cv_mean': 0.8434, 'f1_cv_std': 0.0099, 'accuracy_test': 0.8203, 'precision_test': 0.5836, 'recall_test': 0.8624, 'f1_score_test': 0.6961}


EMBEDDINGS TYPE: LYRICS_BERT
XGBoost: {'accuracy_cv_mean': 0.8783, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.8932, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8595, 'recall_cv_std': 0.004, 'f1_cv_mean': 0.876, 'f1_cv_std': 0.0024, 'accuracy_test': 0.8918, 'precision_test': 0.7303, 'recall_test': 0.8665, 'f1_score_test': 0.7926}
MLP_338433: {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0019, 'precision_cv_mean': 0.8431, 'precision_cv_std': 0.0088, 'recall_cv_mean': 0.8747, 'recall_cv_std': 0.013, 'f1_cv_mean': 0.8585, 'f1_cv_std': 0.0026, 'params': 338433, 'accuracy_test': 0.8938, 'precision_test': 0.7815, 'recall_test': 0.7702, 'f1_score_test': 0.7758}
MLP_1031169: {'accuracy_cv_mean': 0.8558, 'accuracy_cv_std': 0.0035, 'precision_cv_mean': 0.8506, 'precision_cv_std': 0.0142, 'recall_cv_mean': 0.8639, 'recall_cv_std': 0.0155, 'f1_cv_mean': 0.857, 'f1_cv_std': 0.0028, 'params': 1031169, 'accuracy_test': 0.8978, 'precision_test': 0.8206, 'recall_test': 0.7314, 'f1_score_test': 0.7734}
Random Forest: {'accuracy_cv_mean': 0.8545, 'accuracy_cv_std': 0.0011, 'precision_cv_mean': 0.886, 'precision_cv_std': 0.0057, 'recall_cv_mean': 0.8137, 'recall_cv_std': 0.0054, 'f1_cv_mean': 0.8483, 'f1_cv_std': 0.0009, 'accuracy_test': 0.8821, 'precision_test': 0.7233, 'recall_test': 0.8192, 'f1_score_test': 0.7683}
MLP_10401: {'accuracy_cv_mean': 0.8521, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.8503, 'precision_cv_std': 0.0044, 'recall_cv_mean': 0.8548, 'recall_cv_std': 0.0056, 'f1_cv_mean': 0.8525, 'f1_cv_std': 0.0027, 'params': 10401, 'accuracy_test': 0.886, 'precision_test': 0.7591, 'recall_test': 0.7651, 'f1_score_test': 0.7621}
Logistic Regression: {'accuracy_cv_mean': 0.849, 'accuracy_cv_std': 0.0018, 'precision_cv_mean': 0.8599, 'precision_cv_std': 0.0035, 'recall_cv_mean': 0.834, 'recall_cv_std': 0.0041, 'f1_cv_mean': 0.8467, 'f1_cv_std': 0.0019, 'accuracy_test': 0.8576, 'precision_test': 0.6603, 'recall_test': 0.83, 'f1_score_test': 0.7355}
Decision Tree: {'accuracy_cv_mean': 0.8012, 'accuracy_cv_std': 0.0032, 'precision_cv_mean': 0.8097, 'precision_cv_std': 0.0094, 'recall_cv_mean': 0.7878, 'recall_cv_std': 0.0116, 'f1_cv_mean': 0.7985, 'f1_cv_std': 0.0034, 'accuracy_test': 0.8157, 'precision_test': 0.5856, 'recall_test': 0.7787, 'f1_score_test': 0.6685}
Naive Bayes: {'accuracy_cv_mean': 0.7819, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.7663, 'precision_cv_std': 0.003, 'recall_cv_mean': 0.8112, 'recall_cv_std': 0.0035, 'f1_cv_mean': 0.7881, 'f1_cv_std': 0.0026, 'accuracy_test': 0.7647, 'precision_test': 0.5044, 'recall_test': 0.8066, 'f1_score_test': 0.6206}
SVM: {'accuracy_cv_mean': 0.6625, 'accuracy_cv_std': 0.0376, 'precision_cv_mean': 0.6361, 'precision_cv_std': 0.0336, 'recall_cv_mean': 0.7617, 'recall_cv_std': 0.0525, 'f1_cv_mean': 0.6926, 'f1_cv_std': 0.0352, 'accuracy_test': 0.5809, 'precision_test': 0.3211, 'recall_test': 0.6787, 'f1_score_test': 0.4359}


EMBEDDINGS TYPE: GPT
XGBoost: {'accuracy_cv_mean': 0.9087, 'accuracy_cv_std': 0.0027, 'precision_cv_mean': 0.9208, 'precision_cv_std': 0.0039, 'recall_cv_mean': 0.8944, 'recall_cv_std': 0.0033, 'f1_cv_mean': 0.9074, 'f1_cv_std': 0.0027, 'accuracy_test': 0.9229, 'precision_test': 0.8026, 'recall_test': 0.8977, 'f1_score_test': 0.8475}
MLP_971265: {'accuracy_cv_mean': 0.8973, 'accuracy_cv_std': 0.0033, 'precision_cv_mean': 0.891, 'precision_cv_std': 0.0181, 'recall_cv_mean': 0.9062, 'recall_cv_std': 0.0162, 'f1_cv_mean': 0.8982, 'f1_cv_std': 0.0015, 'params': 971265, 'accuracy_test': 0.9207, 'precision_test': 0.8391, 'recall_test': 0.826, 'f1_score_test': 0.8325}
MLP_2296833: {'accuracy_cv_mean': 0.8981, 'accuracy_cv_std': 0.0028, 'precision_cv_mean': 0.9007, 'precision_cv_std': 0.0116, 'recall_cv_mean': 0.8951, 'recall_cv_std': 0.009, 'f1_cv_mean': 0.8978, 'f1_cv_std': 0.002, 'params': 2296833, 'accuracy_test': 0.9156, 'precision_test': 0.7967, 'recall_test': 0.8674, 'f1_score_test': 0.8306}
Logistic Regression: {'accuracy_cv_mean': 0.9014, 'accuracy_cv_std': 0.0012, 'precision_cv_mean': 0.907, 'precision_cv_std': 0.0028, 'recall_cv_mean': 0.8946, 'recall_cv_std': 0.005, 'f1_cv_mean': 0.9007, 'f1_cv_std': 0.0015, 'accuracy_test': 0.9062, 'precision_test': 0.7552, 'recall_test': 0.8981, 'f1_score_test': 0.8205}
Random Forest: {'accuracy_cv_mean': 0.8757, 'accuracy_cv_std': 0.0026, 'precision_cv_mean': 0.9043, 'precision_cv_std': 0.0045, 'recall_cv_mean': 0.8404, 'recall_cv_std': 0.0055, 'f1_cv_mean': 0.8711, 'f1_cv_std': 0.0028, 'accuracy_test': 0.8956, 'precision_test': 0.7523, 'recall_test': 0.8388, 'f1_score_test': 0.7932}
Naive Bayes: {'accuracy_cv_mean': 0.8345, 'accuracy_cv_std': 0.0025, 'precision_cv_mean': 0.8661, 'precision_cv_std': 0.0046, 'recall_cv_mean': 0.7915, 'recall_cv_std': 0.0046, 'f1_cv_mean': 0.8271, 'f1_cv_std': 0.0026, 'accuracy_test': 0.8551, 'precision_test': 0.6656, 'recall_test': 0.7893, 'f1_score_test': 0.7222}
Decision Tree: {'accuracy_cv_mean': 0.8052, 'accuracy_cv_std': 0.005, 'precision_cv_mean': 0.8206, 'precision_cv_std': 0.0138, 'recall_cv_mean': 0.782, 'recall_cv_std': 0.0151, 'f1_cv_mean': 0.8006, 'f1_cv_std': 0.0051, 'accuracy_test': 0.8285, 'precision_test': 0.6071, 'recall_test': 0.7971, 'f1_score_test': 0.6892}
SVM: {'accuracy_cv_mean': 0.8463, 'accuracy_cv_std': 0.0061, 'precision_cv_mean': 0.8424, 'precision_cv_std': 0.0167, 'recall_cv_mean': 0.8529, 'recall_cv_std': 0.0185, 'f1_cv_mean': 0.8473, 'f1_cv_std': 0.0058, 'accuracy_test': 0.8647, 'precision_test': 0.7931, 'recall_test': 0.586, 'f1_score_test': 0.674}
MLP_49953: {'accuracy_cv_mean': 0.8963, 'accuracy_cv_std': 0.0034, 'precision_cv_mean': 0.8906, 'precision_cv_std': 0.0134, 'recall_cv_mean': 0.9041, 'recall_cv_std': 0.0113, 'f1_cv_mean': 0.8971, 'f1_cv_std': 0.0023, 'params': 49953, 'accuracy_test': 0.2386, 'precision_test': 0.2386, 'recall_test': 1.0, 'f1_score_test': 0.3853}
Diccionario global guardado en: outputs_cv/4/gpt/global_metrics.json

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
====================================

