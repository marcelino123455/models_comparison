2025-10-12 02:01:32.234757: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text_plus_numerical.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
For both embbedings your are adding this columns: 
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
--> PaTH:  ../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy

##################################################
Running experiment with TFIDF embeddings
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_text:  (86500, 5000)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 5023)
Shape of X_test after concatenation:  (21625, 5023)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5023, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4493, Test Loss: 0.4025, F1: 0.7039, AUC: 0.9048
Epoch [10/30] Train Loss: 0.2022, Test Loss: 0.3863, F1: 0.7292, AUC: 0.9159
Epoch [20/30] Train Loss: 0.0741, Test Loss: 0.5615, F1: 0.7035, AUC: 0.9098
Mejores resultados en la época:  10
f1-score 0.7292035398230089
AUC según el mejor F1-score 0.9158914375572332
Confusion Matrix:
 [[14086  2379]
 [  834  4326]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_160801.png
Accuracy:   0.8514
Precision:  0.6452
Recall:     0.8384
F1-score:   0.7292

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4465, Test Loss: 0.4153, F1: 0.7019, AUC: 0.9053
Epoch [10/30] Train Loss: 0.2036, Test Loss: 0.4200, F1: 0.7108, AUC: 0.9154
Epoch [20/30] Train Loss: 0.0673, Test Loss: 0.5883, F1: 0.6943, AUC: 0.9087
Mejores resultados en la época:  8
f1-score 0.7311003153498679
AUC según el mejor F1-score 0.9164212259031962
Confusion Matrix:
 [[14181  2284]
 [  871  4289]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_160801.png
Accuracy:   0.8541
Precision:  0.6525
Recall:     0.8312
F1-score:   0.7311

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4514, Test Loss: 0.3884, F1: 0.7120, AUC: 0.9048
Epoch [10/30] Train Loss: 0.1839, Test Loss: 0.4208, F1: 0.7182, AUC: 0.9147
Epoch [20/30] Train Loss: 0.0612, Test Loss: 0.5874, F1: 0.7072, AUC: 0.9084
Mejores resultados en la época:  4
f1-score 0.7281971733884868
AUC según el mejor F1-score 0.9135942403077234
Confusion Matrix:
 [[14246  2219]
 [  935  4225]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_160801.png
Accuracy:   0.8542
Precision:  0.6556
Recall:     0.8188
F1-score:   0.7282

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4518, Test Loss: 0.4347, F1: 0.6940, AUC: 0.9038
Epoch [10/30] Train Loss: 0.1889, Test Loss: 0.4327, F1: 0.7099, AUC: 0.9131
Epoch [20/30] Train Loss: 0.0451, Test Loss: 0.6341, F1: 0.7024, AUC: 0.9062
Mejores resultados en la época:  14
f1-score 0.7217451047749914
AUC según el mejor F1-score 0.9102500429616969
Confusion Matrix:
 [[14183  2282]
 [  958  4202]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_160801.png
Accuracy:   0.8502
Precision:  0.6481
Recall:     0.8143
F1-score:   0.7217
Tiempo total para red 1: 488.96 segundos

Entrenando red 2 con capas [5023, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4287, Test Loss: 0.3620, F1: 0.7192, AUC: 0.9077
Epoch [10/30] Train Loss: 0.0103, Test Loss: 0.9739, F1: 0.6979, AUC: 0.9029
Epoch [20/30] Train Loss: 0.0004, Test Loss: 1.6646, F1: 0.6935, AUC: 0.9018
Mejores resultados en la época:  4
f1-score 0.7313279781233977
AUC según el mejor F1-score 0.91716475163431
Confusion Matrix:
 [[14202  2263]
 [  881  4279]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_323649.png
Accuracy:   0.8546
Precision:  0.6541
Recall:     0.8293
F1-score:   0.7313

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4271, Test Loss: 0.3860, F1: 0.7123, AUC: 0.9084
Epoch [10/30] Train Loss: 0.0113, Test Loss: 1.0132, F1: 0.6915, AUC: 0.9020
Epoch [20/30] Train Loss: 0.0040, Test Loss: 1.5082, F1: 0.6917, AUC: 0.8993
Mejores resultados en la época:  2
f1-score 0.7165109034267912
AUC según el mejor F1-score 0.915623050539434
Confusion Matrix:
 [[13797  2668]
 [  790  4370]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_323649.png
Accuracy:   0.8401
Precision:  0.6209
Recall:     0.8469
F1-score:   0.7165

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4267, Test Loss: 0.3875, F1: 0.7123, AUC: 0.9090
Epoch [10/30] Train Loss: 0.0111, Test Loss: 0.9881, F1: 0.7030, AUC: 0.9013
Epoch [20/30] Train Loss: 0.0008, Test Loss: 1.5776, F1: 0.6925, AUC: 0.8988
Mejores resultados en la época:  3
f1-score 0.7381057463147674
AUC según el mejor F1-score 0.9171811006198255
Confusion Matrix:
 [[14477  1988]
 [  979  4181]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_323649.png
Accuracy:   0.8628
Precision:  0.6777
Recall:     0.8103
F1-score:   0.7381

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4237, Test Loss: 0.3744, F1: 0.7147, AUC: 0.9084
Epoch [10/30] Train Loss: 0.0143, Test Loss: 0.9834, F1: 0.6947, AUC: 0.9005
Epoch [20/30] Train Loss: 0.0096, Test Loss: 1.5791, F1: 0.6983, AUC: 0.9009
Mejores resultados en la época:  5
f1-score 0.7281207598371777
AUC según el mejor F1-score 0.9135875841872707
Confusion Matrix:
 [[14126  2339]
 [  867  4293]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_323649.png
Accuracy:   0.8517
Precision:  0.6473
Recall:     0.8320
F1-score:   0.7281
Tiempo total para red 2: 493.21 segundos

Entrenando red 3 con capas [5023, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4265, Test Loss: 0.3466, F1: 0.7248, AUC: 0.9077
Epoch [10/30] Train Loss: 0.0137, Test Loss: 1.2050, F1: 0.6787, AUC: 0.8972
Epoch [20/30] Train Loss: 0.0021, Test Loss: 1.5467, F1: 0.7118, AUC: 0.9037
Mejores resultados en la época:  0
f1-score 0.724822436714624
AUC según el mejor F1-score 0.9076960995487255
Confusion Matrix:
 [[14623  1842]
 [ 1180  3980]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_653441.png
Accuracy:   0.8603
Precision:  0.6836
Recall:     0.7713
F1-score:   0.7248

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4218, Test Loss: 0.3828, F1: 0.7099, AUC: 0.9110
Epoch [10/30] Train Loss: 0.0102, Test Loss: 1.3009, F1: 0.6980, AUC: 0.9029
Epoch [20/30] Train Loss: 0.0006, Test Loss: 1.6662, F1: 0.7010, AUC: 0.9034
Mejores resultados en la época:  1
f1-score 0.7344126765415088
AUC según el mejor F1-score 0.9187389741452976
Confusion Matrix:
 [[14277  2188]
 [  896  4264]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_653441.png
Accuracy:   0.8574
Precision:  0.6609
Recall:     0.8264
F1-score:   0.7344

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4210, Test Loss: 0.3780, F1: 0.7173, AUC: 0.9101
Epoch [10/30] Train Loss: 0.0087, Test Loss: 1.1717, F1: 0.6864, AUC: 0.8956
Epoch [20/30] Train Loss: 0.0006, Test Loss: 1.5655, F1: 0.6919, AUC: 0.9012
Mejores resultados en la época:  1
f1-score 0.7335994446372788
AUC según el mejor F1-score 0.918927905564305
Confusion Matrix:
 [[14328  2137]
 [  933  4227]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_653441.png
Accuracy:   0.8580
Precision:  0.6642
Recall:     0.8192
F1-score:   0.7336

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4219, Test Loss: 0.3887, F1: 0.7106, AUC: 0.9109
Epoch [10/30] Train Loss: 0.0108, Test Loss: 1.2728, F1: 0.6901, AUC: 0.8974
Epoch [20/30] Train Loss: 0.0056, Test Loss: 1.4901, F1: 0.7006, AUC: 0.9030
Mejores resultados en la época:  2
f1-score 0.7391607173950897
AUC según el mejor F1-score 0.9171022806187427
Confusion Matrix:
 [[14384  2081]
 [  915  4245]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_653441.png
Accuracy:   0.8615
Precision:  0.6710
Recall:     0.8227
F1-score:   0.7392
Tiempo total para red 3: 497.69 segundos

Entrenando red 4 con capas [5023, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4219, Test Loss: 0.3915, F1: 0.7074, AUC: 0.9109
Epoch [10/30] Train Loss: 0.0096, Test Loss: 1.0964, F1: 0.7128, AUC: 0.9031
Epoch [20/30] Train Loss: 0.0017, Test Loss: 1.4811, F1: 0.7039, AUC: 0.9041
Mejores resultados en la época:  1
f1-score 0.7420156139105749
AUC según el mejor F1-score 0.9179881625811859
Confusion Matrix:
 [[14535  1930]
 [  978  4182]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_1329409.png
Accuracy:   0.8655
Precision:  0.6842
Recall:     0.8105
F1-score:   0.7420

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4208, Test Loss: 0.3717, F1: 0.7154, AUC: 0.9116
Epoch [10/30] Train Loss: 0.0094, Test Loss: 1.4644, F1: 0.6894, AUC: 0.9032
Epoch [20/30] Train Loss: 0.0042, Test Loss: 1.5174, F1: 0.6878, AUC: 0.9037
Mejores resultados en la época:  9
f1-score 0.7180480930150621
AUC según el mejor F1-score 0.9037745911576589
Confusion Matrix:
 [[14348  2117]
 [ 1084  4076]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_1329409.png
Accuracy:   0.8520
Precision:  0.6582
Recall:     0.7899
F1-score:   0.7180

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4205, Test Loss: 0.3632, F1: 0.7204, AUC: 0.9123
Epoch [10/30] Train Loss: 0.0042, Test Loss: 1.4811, F1: 0.7034, AUC: 0.9056
Epoch [20/30] Train Loss: 0.0048, Test Loss: 1.5418, F1: 0.6836, AUC: 0.9021
Mejores resultados en la época:  1
f1-score 0.733229329173167
AUC según el mejor F1-score 0.9183085391375173
Confusion Matrix:
 [[14317  2148]
 [  930  4230]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_1329409.png
Accuracy:   0.8577
Precision:  0.6632
Recall:     0.8198
F1-score:   0.7332

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4201, Test Loss: 0.4014, F1: 0.7112, AUC: 0.9138
Epoch [10/30] Train Loss: 0.0090, Test Loss: 1.3128, F1: 0.6986, AUC: 0.9024
Epoch [20/30] Train Loss: 0.0040, Test Loss: 1.6053, F1: 0.6869, AUC: 0.9030
Mejores resultados en la época:  1
f1-score 0.7277059223961879
AUC según el mejor F1-score 0.9175726641195677
Confusion Matrix:
 [[14149  2316]
 [  884  4276]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_1329409.png
Accuracy:   0.8520
Precision:  0.6487
Recall:     0.8287
F1-score:   0.7277
Tiempo total para red 4: 516.54 segundos

Entrenando red 5 con capas [5023, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4120, Test Loss: 0.4108, F1: 0.7098, AUC: 0.9158
Epoch [10/30] Train Loss: 0.0051, Test Loss: 1.2897, F1: 0.7029, AUC: 0.9066
Epoch [20/30] Train Loss: 0.0049, Test Loss: 1.3160, F1: 0.7045, AUC: 0.9056
Mejores resultados en la época:  1
f1-score 0.7267534649307014
AUC según el mejor F1-score 0.9180039877871078
Confusion Matrix:
 [[14046  2419]
 [  834  4326]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_2744833.png
Accuracy:   0.8496
Precision:  0.6414
Recall:     0.8384
F1-score:   0.7268

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4105, Test Loss: 0.3654, F1: 0.7277, AUC: 0.9166
Epoch [10/30] Train Loss: 0.0075, Test Loss: 1.6373, F1: 0.6881, AUC: 0.9076
Epoch [20/30] Train Loss: 0.0052, Test Loss: 1.7320, F1: 0.7105, AUC: 0.9076
Mejores resultados en la época:  2
f1-score 0.7303664921465969
AUC según el mejor F1-score 0.9091810146964316
Confusion Matrix:
 [[14350  2115]
 [  975  4185]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_2744833.png
Accuracy:   0.8571
Precision:  0.6643
Recall:     0.8110
F1-score:   0.7304

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4131, Test Loss: 0.4498, F1: 0.6760, AUC: 0.9137
Epoch [10/30] Train Loss: 0.0069, Test Loss: 1.3115, F1: 0.6847, AUC: 0.9052
Epoch [20/30] Train Loss: 0.0035, Test Loss: 1.5553, F1: 0.7003, AUC: 0.9077
Mejores resultados en la época:  1
f1-score 0.7229333553555207
AUC según el mejor F1-score 0.9199098804840901
Confusion Matrix:
 [[13893  2572]
 [  783  4377]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_2744833.png
Accuracy:   0.8449
Precision:  0.6299
Recall:     0.8483
F1-score:   0.7229

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4147, Test Loss: 0.3411, F1: 0.7341, AUC: 0.9159
Epoch [10/30] Train Loss: 0.0058, Test Loss: 1.3953, F1: 0.6922, AUC: 0.9041
Epoch [20/30] Train Loss: 0.0033, Test Loss: 2.2005, F1: 0.6688, AUC: 0.9024
Mejores resultados en la época:  0
f1-score 0.7340549244118436
AUC según el mejor F1-score 0.9159194215119223
Confusion Matrix:
 [[14549  1916]
 [ 1057  4103]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_2744833.png
Accuracy:   0.8625
Precision:  0.6817
Recall:     0.7952
F1-score:   0.7341
Tiempo total para red 5: 516.63 segundos

Entrenando red 6 con capas [5023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4157, Test Loss: 0.3716, F1: 0.7310, AUC: 0.9156
Epoch [10/30] Train Loss: 0.0052, Test Loss: 1.9018, F1: 0.6825, AUC: 0.9068
Epoch [20/30] Train Loss: 0.0049, Test Loss: 1.3388, F1: 0.6998, AUC: 0.9066
Mejores resultados en la época:  1
f1-score 0.736703988803359
AUC según el mejor F1-score 0.9187909519135022
Confusion Matrix:
 [[14404  2061]
 [  949  4211]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_5843969.png
Accuracy:   0.8608
Precision:  0.6714
Recall:     0.8161
F1-score:   0.7367

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4136, Test Loss: 0.4450, F1: 0.6972, AUC: 0.9165
Epoch [10/30] Train Loss: 0.0084, Test Loss: 1.5438, F1: 0.6869, AUC: 0.9082
Epoch [20/30] Train Loss: 0.0028, Test Loss: 1.6722, F1: 0.6690, AUC: 0.9049
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:05:06] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Mejores resultados en la época:  1
f1-score 0.7302960889115127
AUC según el mejor F1-score 0.9188195538104081
Confusion Matrix:
 [[14142  2323]
 [  856  4304]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_5843969.png
Accuracy:   0.8530
Precision:  0.6495
Recall:     0.8341
F1-score:   0.7303

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4150, Test Loss: 0.3748, F1: 0.7318, AUC: 0.9151
Epoch [10/30] Train Loss: 0.0068, Test Loss: 1.7668, F1: 0.6886, AUC: 0.9074
Epoch [20/30] Train Loss: 0.0066, Test Loss: 1.7847, F1: 0.6816, AUC: 0.9066
Mejores resultados en la época:  0
f1-score 0.7318031615611325
AUC según el mejor F1-score 0.9151150314150053
Confusion Matrix:
 [[14525  1940]
 [ 1063  4097]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_5843969.png
Accuracy:   0.8611
Precision:  0.6786
Recall:     0.7940
F1-score:   0.7318

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4146, Test Loss: 0.3601, F1: 0.7195, AUC: 0.9167
Epoch [10/30] Train Loss: 0.0076, Test Loss: 1.3246, F1: 0.7036, AUC: 0.9092
Epoch [20/30] Train Loss: 0.0040, Test Loss: 1.9310, F1: 0.6891, AUC: 0.9024
Mejores resultados en la época:  1
f1-score 0.7384014902865254
AUC según el mejor F1-score 0.9199552550983174
Confusion Matrix:
 [[14514  1951]
 [  998  4162]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/tfidf/confusion_matrix_param_5843969.png
Accuracy:   0.8636
Precision:  0.6808
Recall:     0.8066
F1-score:   0.7384
Tiempo total para red 6: 543.33 segundos
Saved on: outputs_text_plus_numerical/3/tfidf

==============================
Model: Logistic Regression
Accuracy:  0.8401
Precision: 0.6278
Recall:    0.8107
F1-score:  0.7076
              precision    recall  f1-score   support

           0       0.93      0.85      0.89     16465
           1       0.63      0.81      0.71      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.83      0.80     21625
weighted avg       0.86      0.84      0.85     21625

[[13985  2480]
 [  977  4183]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical/3/tfidf/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_text_plus_numerical/3/tfidf/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.6605
Precision: 0.3927
Recall:    0.7731
F1-score:  0.5208
              precision    recall  f1-score   support

           0       0.90      0.63      0.74     16465
           1       0.39      0.77      0.52      5160

    accuracy                           0.66     21625
   macro avg       0.65      0.70      0.63     21625
weighted avg       0.78      0.66      0.69     21625

[[10295  6170]
 [ 1171  3989]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_text_plus_numerical/3/tfidf/conf_matrix_svm.png
Modelo guardado como: outputs_text_plus_numerical/3/tfidf/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8875
Precision: 0.7224
Recall:    0.8587
F1-score:  0.7847
              precision    recall  f1-score   support

           0       0.95      0.90      0.92     16465
           1       0.72      0.86      0.78      5160

    accuracy                           0.89     21625
   macro avg       0.84      0.88      0.85     21625
weighted avg       0.90      0.89      0.89     21625

[[14762  1703]
 [  729  4431]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_text_plus_numerical/3/tfidf/conf_matrix_decision_tree.png
Modelo guardado como: outputs_text_plus_numerical/3/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8646
Precision: 0.6931
Recall:    0.7760
F1-score:  0.7322
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.69      0.78      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.83      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14692  1773]
 [ 1156  4004]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical/3/tfidf/conf_matrix_random_forest.png
Modelo guardado como: outputs_text_plus_numerical/3/tfidf/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9086
Precision: 0.7622
Recall:    0.8965
F1-score:  0.8239
              precision    recall  f1-score   support

           0       0.97      0.91      0.94     16465
           1       0.76      0.90      0.82      5160

    accuracy                           0.91     21625
   macro avg       0.86      0.90      0.88     21625
weighted avg       0.92      0.91      0.91     21625

[[15022  1443]
 [  534  4626]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_text_plus_numerical/3/tfidf/conf_matrix_xgboost.png
Modelo guardado como: outputs_text_plus_numerical/3/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8283
Precision: 0.6200
Recall:    0.7238
F1-score:  0.6679
              precision    recall  f1-score   support

           0       0.91      0.86      0.88     16465
           1       0.62      0.72      0.67      5160

    accuracy                           0.83     21625
   macro avg       0.76      0.79      0.78     21625
weighted avg       0.84      0.83      0.83     21625

[[14176  2289]
 [ 1425  3735]]
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}
Confusion matrix saved as: outputs_text_plus_numerical/3/tfidf/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_text_plus_numerical/3/tfidf/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.9086, 'precision': 0.7622, 'recall': 0.8965, 'f1_score': 0.8239}
Decision Tree: {'accuracy': 0.8875, 'precision': 0.7224, 'recall': 0.8587, 'f1_score': 0.7847}
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text_plus_numerical.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Random Forest: {'accuracy': 0.8646, 'precision': 0.6931, 'recall': 0.776, 'f1_score': 0.7322}
Logistic Regression: {'accuracy': 0.8401, 'precision': 0.6278, 'recall': 0.8107, 'f1_score': 0.7076}
Naive Bayes: {'accuracy': 0.8283, 'precision': 0.62, 'recall': 0.7238, 'f1_score': 0.6679}
SVM: {'accuracy': 0.6605, 'precision': 0.3927, 'recall': 0.7731, 'f1_score': 0.5208}

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_text:  (86500, 300)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 323)
Shape of X_test after concatenation:  (21625, 323)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 323)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 323)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [323, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5760, Test Loss: 0.4921, F1: 0.6359, AUC: 0.8790
Epoch [10/30] Train Loss: 0.4025, Test Loss: 0.4651, F1: 0.6637, AUC: 0.9036
Epoch [20/30] Train Loss: 0.3922, Test Loss: 0.4227, F1: 0.6801, AUC: 0.9067
Mejores resultados en la época:  22
f1-score 0.7193240264511389
AUC según el mejor F1-score 0.9067884307092564
Confusion Matrix:
 [[14653  1812]
 [ 1244  3916]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_10401.png
Accuracy:   0.8587
Precision:  0.6837
Recall:     0.7589
F1-score:   0.7193

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5790, Test Loss: 0.5573, F1: 0.5842, AUC: 0.8785
Epoch [10/30] Train Loss: 0.3988, Test Loss: 0.3888, F1: 0.7022, AUC: 0.9048
Epoch [20/30] Train Loss: 0.3905, Test Loss: 0.3899, F1: 0.7001, AUC: 0.9082
Mejores resultados en la época:  23
f1-score 0.7224568138195777
AUC según el mejor F1-score 0.9090912424051959
Confusion Matrix:
 [[14969  1496]
 [ 1396  3764]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_10401.png
Accuracy:   0.8663
Precision:  0.7156
Recall:     0.7295
F1-score:   0.7225

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5778, Test Loss: 0.4033, F1: 0.6865, AUC: 0.8783
Epoch [10/30] Train Loss: 0.4000, Test Loss: 0.3770, F1: 0.7030, AUC: 0.9036
Epoch [20/30] Train Loss: 0.3913, Test Loss: 0.4582, F1: 0.6677, AUC: 0.9064
Mejores resultados en la época:  25
f1-score 0.720499243570348
AUC según el mejor F1-score 0.9071200891249234
Confusion Matrix:
 [[14859  1606]
 [ 1350  3810]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_10401.png
Accuracy:   0.8633
Precision:  0.7035
Recall:     0.7384
F1-score:   0.7205

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6347, Test Loss: 0.5836, F1: 0.5745, AUC: 0.8390
Epoch [10/30] Train Loss: 0.4010, Test Loss: 0.4310, F1: 0.6805, AUC: 0.9014
Epoch [20/30] Train Loss: 0.3948, Test Loss: 0.3502, F1: 0.7175, AUC: 0.9054
Mejores resultados en la época:  21
f1-score 0.7181335356600911
AUC según el mejor F1-score 0.9054675880479381
Confusion Matrix:
 [[14867  1598]
 [ 1374  3786]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_10401.png
Accuracy:   0.8626
Precision:  0.7032
Recall:     0.7337
F1-score:   0.7181
Tiempo total para red 1: 223.92 segundos

Entrenando red 2 con capas [323, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.5314, Test Loss: 0.3804, F1: 0.6968, AUC: 0.8921
Epoch [10/30] Train Loss: 0.3958, Test Loss: 0.4280, F1: 0.6790, AUC: 0.9063
Epoch [20/30] Train Loss: 0.3857, Test Loss: 0.3341, F1: 0.7259, AUC: 0.9097
Mejores resultados en la época:  29
f1-score 0.7269408147578785
AUC según el mejor F1-score 0.9106068604533459
Confusion Matrix:
 [[15000  1465]
 [ 1377  3783]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_22849.png
Accuracy:   0.8686
Precision:  0.7208
Recall:     0.7331
F1-score:   0.7269

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.5517, Test Loss: 0.4673, F1: 0.6486, AUC: 0.8871
Epoch [10/30] Train Loss: 0.3995, Test Loss: 0.3331, F1: 0.7177, AUC: 0.9050
Epoch [20/30] Train Loss: 0.3938, Test Loss: 0.4053, F1: 0.6897, AUC: 0.9071
Mejores resultados en la época:  15
f1-score 0.7178396374734116
AUC según el mejor F1-score 0.9064450019656448
Confusion Matrix:
 [[14693  1772]
 [ 1279  3881]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_22849.png
Accuracy:   0.8589
Precision:  0.6865
Recall:     0.7521
F1-score:   0.7178

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.5200, Test Loss: 0.4162, F1: 0.6826, AUC: 0.8923
Epoch [10/30] Train Loss: 0.3981, Test Loss: 0.3857, F1: 0.7024, AUC: 0.9059
Epoch [20/30] Train Loss: 0.3877, Test Loss: 0.3998, F1: 0.6907, AUC: 0.9088
Mejores resultados en la época:  22
f1-score 0.723562880125564
AUC según el mejor F1-score 0.9094231362274215
Confusion Matrix:
 [[15119  1346]
 [ 1472  3688]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_22849.png
Accuracy:   0.8697
Precision:  0.7326
Recall:     0.7147
F1-score:   0.7236

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.5472, Test Loss: 0.3905, F1: 0.6934, AUC: 0.8895
Epoch [10/30] Train Loss: 0.3994, Test Loss: 0.4831, F1: 0.6557, AUC: 0.9047
Epoch [20/30] Train Loss: 0.3936, Test Loss: 0.3937, F1: 0.6976, AUC: 0.9069
Mejores resultados en la época:  23
f1-score 0.722572153802764
AUC según el mejor F1-score 0.9085683691269004
Confusion Matrix:
 [[14831  1634]
 [ 1317  3843]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_22849.png
Accuracy:   0.8635
Precision:  0.7017
Recall:     0.7448
F1-score:   0.7226
Tiempo total para red 2: 231.07 segundos

Entrenando red 3 con capas [323, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.5217, Test Loss: 0.4025, F1: 0.6865, AUC: 0.8923
Epoch [10/30] Train Loss: 0.4022, Test Loss: 0.4292, F1: 0.6761, AUC: 0.9059
Epoch [20/30] Train Loss: 0.3912, Test Loss: 0.4498, F1: 0.6668, AUC: 0.9082
Mejores resultados en la época:  18
f1-score 0.7226010475343413
AUC según el mejor F1-score 0.9076165144763263
Confusion Matrix:
 [[15162  1303]
 [ 1504  3656]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_51841.png
Accuracy:   0.8702
Precision:  0.7372
Recall:     0.7085
F1-score:   0.7226

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.5230, Test Loss: 0.3373, F1: 0.6998, AUC: 0.8927
Epoch [10/30] Train Loss: 0.4019, Test Loss: 0.3564, F1: 0.7148, AUC: 0.9055
Epoch [20/30] Train Loss: 0.3872, Test Loss: 0.3210, F1: 0.7215, AUC: 0.9080
Mejores resultados en la época:  29
f1-score 0.7265727699530516
AUC según el mejor F1-score 0.9107889591969811
Confusion Matrix:
 [[14844  1621]
 [ 1291  3869]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_51841.png
Accuracy:   0.8653
Precision:  0.7047
Recall:     0.7498
F1-score:   0.7266

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.5044, Test Loss: 0.3895, F1: 0.6987, AUC: 0.8948
Epoch [10/30] Train Loss: 0.4011, Test Loss: 0.4011, F1: 0.6883, AUC: 0.9051
Epoch [20/30] Train Loss: 0.3886, Test Loss: 0.3438, F1: 0.7166, AUC: 0.9083
Mejores resultados en la época:  27
f1-score 0.7244386713676007
AUC según el mejor F1-score 0.9107564730918533
Confusion Matrix:
 [[14751  1714]
 [ 1256  3904]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_51841.png
Accuracy:   0.8627
Precision:  0.6949
Recall:     0.7566
F1-score:   0.7244

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.5123, Test Loss: 0.3921, F1: 0.6979, AUC: 0.8935
Epoch [10/30] Train Loss: 0.3974, Test Loss: 0.3450, F1: 0.7186, AUC: 0.9062
Epoch [20/30] Train Loss: 0.3864, Test Loss: 0.3886, F1: 0.6957, AUC: 0.9096
Mejores resultados en la época:  16
f1-score 0.7227167182662538
AUC según el mejor F1-score 0.9085344764675833
Confusion Matrix:
 [[15024  1441]
 [ 1425  3735]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_51841.png
Accuracy:   0.8675
Precision:  0.7216
Recall:     0.7238
F1-score:   0.7227
Tiempo total para red 3: 264.53 segundos

Entrenando red 4 con capas [323, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.5240, Test Loss: 0.4076, F1: 0.6962, AUC: 0.8931
Epoch [10/30] Train Loss: 0.4016, Test Loss: 0.3472, F1: 0.7155, AUC: 0.9056
Epoch [20/30] Train Loss: 0.3912, Test Loss: 0.3469, F1: 0.7191, AUC: 0.9088
Mejores resultados en la época:  28
f1-score 0.7297271049820336
AUC según el mejor F1-score 0.9115063724555494
Confusion Matrix:
 [[15085  1380]
 [ 1403  3757]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_126209.png
Accuracy:   0.8713
Precision:  0.7314
Recall:     0.7281
F1-score:   0.7297

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.5147, Test Loss: 0.4488, F1: 0.6783, AUC: 0.8928
Epoch [10/30] Train Loss: 0.3977, Test Loss: 0.3948, F1: 0.6987, AUC: 0.9067
Epoch [20/30] Train Loss: 0.3888, Test Loss: 0.3655, F1: 0.7193, AUC: 0.9108
Mejores resultados en la época:  27
f1-score 0.7294851166532582
AUC según el mejor F1-score 0.9121248207967572
Confusion Matrix:
 [[15308  1157]
 [ 1533  3627]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_126209.png
Accuracy:   0.8756
Precision:  0.7582
Recall:     0.7029
F1-score:   0.7295

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.5077, Test Loss: 0.3475, F1: 0.7080, AUC: 0.8934
Epoch [10/30] Train Loss: 0.3981, Test Loss: 0.4909, F1: 0.6448, AUC: 0.9067
Epoch [20/30] Train Loss: 0.3884, Test Loss: 0.3354, F1: 0.7228, AUC: 0.9096
Mejores resultados en la época:  26
f1-score 0.7261747499528213
AUC según el mejor F1-score 0.9102208348929017
Confusion Matrix:
 [[14875  1590]
 [ 1312  3848]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_126209.png
Accuracy:   0.8658
Precision:  0.7076
Recall:     0.7457
F1-score:   0.7262

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.5135, Test Loss: 0.4209, F1: 0.6885, AUC: 0.8938
Epoch [10/30] Train Loss: 0.3982, Test Loss: 0.4083, F1: 0.6990, AUC: 0.9071
Epoch [20/30] Train Loss: 0.3856, Test Loss: 0.3087, F1: 0.7187, AUC: 0.9103
Mejores resultados en la época:  29
f1-score 0.7282302050028211
AUC según el mejor F1-score 0.9124329915230098
Confusion Matrix:
 [[14863  1602]
 [ 1288  3872]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_126209.png
Accuracy:   0.8664
Precision:  0.7073
Recall:     0.7504
F1-score:   0.7282
Tiempo total para red 4: 295.84 segundos

Entrenando red 5 con capas [323, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.5042, Test Loss: 0.6416, F1: 0.5813, AUC: 0.8951
Epoch [10/30] Train Loss: 0.3981, Test Loss: 0.3402, F1: 0.7221, AUC: 0.9075
Epoch [20/30] Train Loss: 0.3858, Test Loss: 0.3066, F1: 0.7179, AUC: 0.9107
Mejores resultados en la época:  28
f1-score 0.7311683320522675
AUC según el mejor F1-score 0.9128358721930709
Confusion Matrix:
 [[15022  1443]
 [ 1355  3805]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_338433.png
Accuracy:   0.8706
Precision:  0.7250
Recall:     0.7374
F1-score:   0.7312

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.5004, Test Loss: 0.5523, F1: 0.6177, AUC: 0.8950
Epoch [10/30] Train Loss: 0.4025, Test Loss: 0.3905, F1: 0.7129, AUC: 0.9067
Epoch [20/30] Train Loss: 0.3911, Test Loss: 0.3539, F1: 0.7189, AUC: 0.9097
Mejores resultados en la época:  18
f1-score 0.7255329227854097
AUC según el mejor F1-score 0.9089562720546521
Confusion Matrix:
 [[14899  1566]
 [ 1331  3829]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_338433.png
Accuracy:   0.8660
Precision:  0.7097
Recall:     0.7421
F1-score:   0.7255

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4904, Test Loss: 0.3833, F1: 0.7014, AUC: 0.8955
Epoch [10/30] Train Loss: 0.4053, Test Loss: 0.4068, F1: 0.6912, AUC: 0.9068
Epoch [20/30] Train Loss: 0.3873, Test Loss: 0.3533, F1: 0.7129, AUC: 0.9099
Mejores resultados en la época:  19
f1-score 0.7229290084490628
AUC según el mejor F1-score 0.910073646941951
Confusion Matrix:
 [[15050  1415]
 [ 1438  3722]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_338433.png
Accuracy:   0.8681
Precision:  0.7245
Recall:     0.7213
F1-score:   0.7229

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.5138, Test Loss: 0.4500, F1: 0.6709, AUC: 0.8935
Epoch [10/30] Train Loss: 0.3986, Test Loss: 0.3863, F1: 0.7045, AUC: 0.9071
Epoch [20/30] Train Loss: 0.3878, Test Loss: 0.4881, F1: 0.6641, AUC: 0.9091
Mejores resultados en la época:  25
f1-score 0.7259779533704029
AUC según el mejor F1-score 0.9103696059529609
Confusion Matrix:
 [[15095  1370]
 [ 1439  3721]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_338433.png
Accuracy:   0.8701
Precision:  0.7309
Recall:     0.7211
F1-score:   0.7260
Tiempo total para red 5: 301.36 segundos

Entrenando red 6 con capas [323, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.5251, Test Loss: 0.5186, F1: 0.6433, AUC: 0.8944
Epoch [10/30] Train Loss: 0.4038, Test Loss: 0.5476, F1: 0.6384, AUC: 0.9069
Epoch [20/30] Train Loss: 0.3944, Test Loss: 0.3291, F1: 0.7249, AUC: 0.9088
Mejores resultados en la época:  27
f1-score 0.7249636451769268
AUC según el mejor F1-score 0.9096649399595572
Confusion Matrix:
 [[15049  1416]
 [ 1421  3739]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_1031169.png
Accuracy:   0.8688
Precision:  0.7253
Recall:     0.7246
F1-score:   0.7250

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.5281, Test Loss: 0.3639, F1: 0.7102, AUC: 0.8923
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:42:53] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [10/30] Train Loss: 0.3980, Test Loss: 0.3608, F1: 0.7056, AUC: 0.9076
Epoch [20/30] Train Loss: 0.3929, Test Loss: 0.3327, F1: 0.7220, AUC: 0.9099
Mejores resultados en la época:  27
f1-score 0.726196160825136
AUC según el mejor F1-score 0.9115735221764748
Confusion Matrix:
 [[14956  1509]
 [ 1358  3802]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_1031169.png
Accuracy:   0.8674
Precision:  0.7159
Recall:     0.7368
F1-score:   0.7262

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.5061, Test Loss: 0.4610, F1: 0.6951, AUC: 0.8943
Epoch [10/30] Train Loss: 0.4047, Test Loss: 0.3241, F1: 0.6645, AUC: 0.9047
Epoch [20/30] Train Loss: 0.3872, Test Loss: 0.3492, F1: 0.7290, AUC: 0.9103
Mejores resultados en la época:  24
f1-score 0.7302530674846626
AUC según el mejor F1-score 0.9119969538391278
Confusion Matrix:
 [[15002  1463]
 [ 1351  3809]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_1031169.png
Accuracy:   0.8699
Precision:  0.7225
Recall:     0.7382
F1-score:   0.7303

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.5202, Test Loss: 0.3769, F1: 0.7011, AUC: 0.8938
Epoch [10/30] Train Loss: 0.3945, Test Loss: 0.4121, F1: 0.7078, AUC: 0.9081
Epoch [20/30] Train Loss: 0.3892, Test Loss: 0.3388, F1: 0.7226, AUC: 0.9095
Mejores resultados en la época:  28
f1-score 0.7284543656400113
AUC según el mejor F1-score 0.9118397728797519
Confusion Matrix:
 [[14875  1590]
 [ 1293  3867]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/lyrics_bert/confusion_matrix_param_1031169.png
Accuracy:   0.8667
Precision:  0.7086
Recall:     0.7494
F1-score:   0.7285
Tiempo total para red 6: 311.37 segundos
Saved on: outputs_text_plus_numerical/3/lyrics_bert

==============================
Model: Logistic Regression
Accuracy:  0.8369
Precision: 0.6220
Recall:    0.8070
F1-score:  0.7025
              precision    recall  f1-score   support

           0       0.93      0.85      0.89     16465
           1       0.62      0.81      0.70      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.83      0.80     21625
weighted avg       0.86      0.84      0.84     21625

[[13935  2530]
 [  996  4164]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical/3/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_text_plus_numerical/3/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.3204
Precision: 0.2546
Recall:    0.9587
F1-score:  0.4023
              precision    recall  f1-score   support

           0       0.90      0.12      0.21     16465
           1       0.25      0.96      0.40      5160

    accuracy                           0.32     21625
   macro avg       0.58      0.54      0.31     21625
weighted avg       0.75      0.32      0.26     21625

[[ 1981 14484]
 [  213  4947]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_text_plus_numerical/3/lyrics_bert/conf_matrix_svm.png
Modelo guardado como: outputs_text_plus_numerical/3/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8034
Precision: 0.5638
Recall:    0.7783
F1-score:  0.6539
              precision    recall  f1-score   support

           0       0.92      0.81      0.86     16465
           1       0.56      0.78      0.65      5160

    accuracy                           0.80     21625
   macro avg       0.74      0.79      0.76     21625
weighted avg       0.84      0.80      0.81     21625

[[13358  3107]
 [ 1144  4016]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_text_plus_numerical/3/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs_text_plus_numerical/3/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8453
Precision: 0.6469
Recall:    0.7742
F1-score:  0.7048
              precision    recall  f1-score   support

           0       0.92      0.87      0.90     16465
           1       0.65      0.77      0.70      5160

    accuracy                           0.85     21625
   macro avg       0.79      0.82      0.80     21625
weighted avg       0.86      0.85      0.85     21625

[[14284  2181]
 [ 1165  3995]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical/3/lyrics_bert/conf_matrix_random_forest.png
Modelo guardado como: outputs_text_plus_numerical/3/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8700
Precision: 0.6901
Recall:    0.8260
F1-score:  0.7519
              precision    recall  f1-score   support

           0       0.94      0.88      0.91     16465
           1       0.69      0.83      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.85      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14551  1914]
 [  898  4262]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_text_plus_numerical/3/lyrics_bert/conf_matrix_xgboost.png
Modelo guardado como: outputs_text_plus_numerical/3/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6702
Precision: 0.4035
Recall:    0.7988
F1-score:  0.5362
              precision    recall  f1-score   support

           0       0.91      0.63      0.74     16465
           1       0.40      0.80      0.54      5160

    accuracy                           0.67     21625
   macro avg       0.66      0.71      0.64     21625
weighted avg       0.79      0.67      0.69     21625

[[10371  6094]
 [ 1038  4122]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_text_plus_numerical/3/lyrics_bert/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_text_plus_numerical/3/lyrics_bert/naive_bayes_model.pkl


Resumen de métricas:
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text_plus_numerical.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
XGBoost: {'accuracy': 0.87, 'precision': 0.6901, 'recall': 0.826, 'f1_score': 0.7519}
Random Forest: {'accuracy': 0.8453, 'precision': 0.6469, 'recall': 0.7742, 'f1_score': 0.7048}
Logistic Regression: {'accuracy': 0.8369, 'precision': 0.622, 'recall': 0.807, 'f1_score': 0.7025}
Decision Tree: {'accuracy': 0.8034, 'precision': 0.5638, 'recall': 0.7783, 'f1_score': 0.6539}
Naive Bayes: {'accuracy': 0.6702, 'precision': 0.4035, 'recall': 0.7988, 'f1_score': 0.5362}
SVM: {'accuracy': 0.3204, 'precision': 0.2546, 'recall': 0.9587, 'f1_score': 0.4023}

##################################################
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_text:  (86500, 1536)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 1559)
Shape of X_test after concatenation:  (21625, 1559)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1559)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1559)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1559, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4638, Test Loss: 0.3661, F1: 0.7153, AUC: 0.8987
Epoch [10/30] Train Loss: 0.3465, Test Loss: 0.4622, F1: 0.6838, AUC: 0.9321
Epoch [20/30] Train Loss: 0.3306, Test Loss: 0.3803, F1: 0.7292, AUC: 0.9357
Mejores resultados en la época:  22
f1-score 0.7663099115867845
AUC según el mejor F1-score 0.9355911176397198
Confusion Matrix:
 [[14997  1468]
 [ 1043  4117]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_49953.png
Accuracy:   0.8839
Precision:  0.7372
Recall:     0.7979
F1-score:   0.7663

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4611, Test Loss: 0.3770, F1: 0.7188, AUC: 0.9003
Epoch [10/30] Train Loss: 0.3557, Test Loss: 0.3309, F1: 0.7512, AUC: 0.9272
Epoch [20/30] Train Loss: 0.3413, Test Loss: 0.4374, F1: 0.7030, AUC: 0.9307
Mejores resultados en la época:  28
f1-score 0.7617676176761767
AUC según el mejor F1-score 0.9336617843346351
Confusion Matrix:
 [[14746  1719]
 [  928  4232]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_49953.png
Accuracy:   0.8776
Precision:  0.7111
Recall:     0.8202
F1-score:   0.7618

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4713, Test Loss: 0.4083, F1: 0.7049, AUC: 0.8941
Epoch [10/30] Train Loss: 0.3385, Test Loss: 0.3769, F1: 0.7326, AUC: 0.9312
Epoch [20/30] Train Loss: 0.3254, Test Loss: 0.3286, F1: 0.7511, AUC: 0.9354
Mejores resultados en la época:  27
f1-score 0.7647899399828523
AUC según el mejor F1-score 0.9353076763724791
Confusion Matrix:
 [[15142  1323]
 [ 1146  4014]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_49953.png
Accuracy:   0.8858
Precision:  0.7521
Recall:     0.7779
F1-score:   0.7648

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4631, Test Loss: 0.4295, F1: 0.7000, AUC: 0.8988
Epoch [10/30] Train Loss: 0.3431, Test Loss: 0.3137, F1: 0.7557, AUC: 0.9325
Epoch [20/30] Train Loss: 0.3292, Test Loss: 0.2737, F1: 0.7674, AUC: 0.9349
Mejores resultados en la época:  24
f1-score 0.7709157846295444
AUC según el mejor F1-score 0.9359502774266296
Confusion Matrix:
 [[14948  1517]
 [  972  4188]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_49953.png
Accuracy:   0.8849
Precision:  0.7341
Recall:     0.8116
F1-score:   0.7709
Tiempo total para red 1: 276.11 segundos

Entrenando red 2 con capas [1559, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4604, Test Loss: 0.4654, F1: 0.6835, AUC: 0.9012
Epoch [10/30] Train Loss: 0.3433, Test Loss: 0.2783, F1: 0.7584, AUC: 0.9317
Epoch [20/30] Train Loss: 0.3261, Test Loss: 0.3374, F1: 0.7501, AUC: 0.9361
Mejores resultados en la época:  25
f1-score 0.7740345110928513
AUC según el mejor F1-score 0.9373678074468509
Confusion Matrix:
 [[14911  1554]
 [  921  4239]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_101953.png
Accuracy:   0.8855
Precision:  0.7317
Recall:     0.8215
F1-score:   0.7740

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4530, Test Loss: 0.3374, F1: 0.7159, AUC: 0.9025
Epoch [10/30] Train Loss: 0.3410, Test Loss: 0.3006, F1: 0.6728, AUC: 0.9323
Epoch [20/30] Train Loss: 0.3254, Test Loss: 0.2636, F1: 0.7682, AUC: 0.9366
Mejores resultados en la época:  27
f1-score 0.7779696446423703
AUC según el mejor F1-score 0.938865310960294
Confusion Matrix:
 [[14902  1563]
 [  880  4280]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_101953.png
Accuracy:   0.8870
Precision:  0.7325
Recall:     0.8295
F1-score:   0.7780

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4479, Test Loss: 0.5234, F1: 0.6582, AUC: 0.9048
Epoch [10/30] Train Loss: 0.3423, Test Loss: 0.4214, F1: 0.7187, AUC: 0.9334
Epoch [20/30] Train Loss: 0.3267, Test Loss: 0.3881, F1: 0.7349, AUC: 0.9373
Mejores resultados en la época:  27
f1-score 0.766546762589928
AUC según el mejor F1-score 0.9374618288264748
Confusion Matrix:
 [[14767  1698]
 [  898  4262]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_101953.png
Accuracy:   0.8800
Precision:  0.7151
Recall:     0.8260
F1-score:   0.7665

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4628, Test Loss: 0.3554, F1: 0.7159, AUC: 0.9005
Epoch [10/30] Train Loss: 0.3381, Test Loss: 0.2768, F1: 0.7596, AUC: 0.9321
Epoch [20/30] Train Loss: 0.3264, Test Loss: 0.3200, F1: 0.7605, AUC: 0.9355
Mejores resultados en la época:  25
f1-score 0.7710046478369682
AUC según el mejor F1-score 0.9375279133327212
Confusion Matrix:
 [[14750  1715]
 [  847  4313]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_101953.png
Accuracy:   0.8815
Precision:  0.7155
Recall:     0.8359
F1-score:   0.7710
Tiempo total para red 2: 284.77 segundos

Entrenando red 3 con capas [1559, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4495, Test Loss: 0.3345, F1: 0.7233, AUC: 0.9057
Epoch [10/30] Train Loss: 0.3452, Test Loss: 0.2990, F1: 0.7596, AUC: 0.9323
Epoch [20/30] Train Loss: 0.3270, Test Loss: 0.3185, F1: 0.7655, AUC: 0.9367
Mejores resultados en la época:  29
f1-score 0.7775043295962082
AUC según el mejor F1-score 0.9384267897372157
Confusion Matrix:
 [[14919  1546]
 [  895  4265]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_210049.png
Accuracy:   0.8871
Precision:  0.7340
Recall:     0.8266
F1-score:   0.7775

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4648, Test Loss: 0.3457, F1: 0.7163, AUC: 0.9020
Epoch [10/30] Train Loss: 0.3480, Test Loss: 0.3357, F1: 0.7481, AUC: 0.9319
Epoch [20/30] Train Loss: 0.3281, Test Loss: 0.2863, F1: 0.7699, AUC: 0.9368
Mejores resultados en la época:  25
f1-score 0.7742525372588461
AUC según el mejor F1-score 0.9381778826121654
Confusion Matrix:
 [[14922  1543]
 [  926  4234]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_210049.png
Accuracy:   0.8858
Precision:  0.7329
Recall:     0.8205
F1-score:   0.7743

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4508, Test Loss: 0.3527, F1: 0.7235, AUC: 0.9043
Epoch [10/30] Train Loss: 0.3377, Test Loss: 0.3860, F1: 0.7284, AUC: 0.9326
Epoch [20/30] Train Loss: 0.3225, Test Loss: 0.4800, F1: 0.6749, AUC: 0.9372
Mejores resultados en la época:  15
f1-score 0.7728241727685605
AUC según el mejor F1-score 0.9363584253184462
Confusion Matrix:
 [[15005  1460]
 [  991  4169]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_210049.png
Accuracy:   0.8867
Precision:  0.7406
Recall:     0.8079
F1-score:   0.7728

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4573, Test Loss: 0.3546, F1: 0.7228, AUC: 0.9034
Epoch [10/30] Train Loss: 0.3404, Test Loss: 0.3233, F1: 0.7519, AUC: 0.9344
Epoch [20/30] Train Loss: 0.3314, Test Loss: 0.3586, F1: 0.7390, AUC: 0.9365
Mejores resultados en la época:  26
f1-score 0.7778593721314485
AUC según el mejor F1-score 0.9390102331231153
Confusion Matrix:
 [[14968  1497]
 [  923  4237]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_210049.png
Accuracy:   0.8881
Precision:  0.7389
Recall:     0.8211
F1-score:   0.7779
Tiempo total para red 3: 338.59 segundos

Entrenando red 4 con capas [1559, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4549, Test Loss: 0.3771, F1: 0.7244, AUC: 0.9049
Epoch [10/30] Train Loss: 0.3465, Test Loss: 0.3036, F1: 0.7458, AUC: 0.9282
Epoch [20/30] Train Loss: 0.3309, Test Loss: 0.3547, F1: 0.7454, AUC: 0.9357
Mejores resultados en la época:  24
f1-score 0.7751452014381857
AUC según el mejor F1-score 0.937716709392957
Confusion Matrix:
 [[14982  1483]
 [  956  4204]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_442625.png
Accuracy:   0.8872
Precision:  0.7392
Recall:     0.8147
F1-score:   0.7751

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4503, Test Loss: 0.3595, F1: 0.7028, AUC: 0.9065
Epoch [10/30] Train Loss: 0.3434, Test Loss: 0.3598, F1: 0.7492, AUC: 0.9337
Epoch [20/30] Train Loss: 0.3269, Test Loss: 0.2683, F1: 0.7498, AUC: 0.9364
Mejores resultados en la época:  22
f1-score 0.775112443778111
AUC según el mejor F1-score 0.9377265023058073
Confusion Matrix:
 [[15089  1376]
 [ 1024  4136]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_442625.png
Accuracy:   0.8890
Precision:  0.7504
Recall:     0.8016
F1-score:   0.7751

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4573, Test Loss: 0.4241, F1: 0.7117, AUC: 0.9037
Epoch [10/30] Train Loss: 0.3490, Test Loss: 0.4001, F1: 0.7242, AUC: 0.9328
Epoch [20/30] Train Loss: 0.3257, Test Loss: 0.2762, F1: 0.7669, AUC: 0.9355
Mejores resultados en la época:  26
f1-score 0.7724680432645035
AUC según el mejor F1-score 0.9384078277388964
Confusion Matrix:
 [[15383  1082]
 [ 1232  3928]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_442625.png
Accuracy:   0.8930
Precision:  0.7840
Recall:     0.7612
F1-score:   0.7725

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4562, Test Loss: 0.3545, F1: 0.7260, AUC: 0.9056
Epoch [10/30] Train Loss: 0.3480, Test Loss: 0.4022, F1: 0.7376, AUC: 0.9328
Epoch [20/30] Train Loss: 0.3311, Test Loss: 0.3231, F1: 0.7549, AUC: 0.9370
Mejores resultados en la época:  29
f1-score 0.7764182424916574
AUC según el mejor F1-score 0.9386915161830239
Confusion Matrix:
 [[15025  1440]
 [  972  4188]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_442625.png
Accuracy:   0.8885
Precision:  0.7441
Recall:     0.8116
F1-score:   0.7764
Tiempo total para red 4: 351.86 segundos

Entrenando red 5 con capas [1559, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4515, Test Loss: 0.3779, F1: 0.7225, AUC: 0.9062
Epoch [10/30] Train Loss: 0.3464, Test Loss: 0.3518, F1: 0.7484, AUC: 0.9324
Epoch [20/30] Train Loss: 0.3267, Test Loss: 0.3001, F1: 0.7617, AUC: 0.9363
Mejores resultados en la época:  25
f1-score 0.7764859082876011
AUC según el mejor F1-score 0.9381157058547966
Confusion Matrix:
 [[15048  1417]
 [  986  4174]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_971265.png
Accuracy:   0.8889
Precision:  0.7466
Recall:     0.8089
F1-score:   0.7765

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4513, Test Loss: 0.3496, F1: 0.7227, AUC: 0.9061
Epoch [10/30] Train Loss: 0.3460, Test Loss: 0.2900, F1: 0.7600, AUC: 0.9322
Epoch [20/30] Train Loss: 0.3325, Test Loss: 0.3033, F1: 0.7652, AUC: 0.9358
Mejores resultados en la época:  29
f1-score 0.7704754294846184
AUC según el mejor F1-score 0.9382913485735539
Confusion Matrix:
 [[15470   995]
 [ 1303  3857]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_971265.png
Accuracy:   0.8937
Precision:  0.7949
Recall:     0.7475
F1-score:   0.7705

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4499, Test Loss: 0.4447, F1: 0.6848, AUC: 0.9073
Epoch [10/30] Train Loss: 0.3450, Test Loss: 0.2857, F1: 0.7579, AUC: 0.9330
Epoch [20/30] Train Loss: 0.3311, Test Loss: 0.3015, F1: 0.7637, AUC: 0.9357
Mejores resultados en la época:  22
f1-score 0.7735172413793103
AUC según el mejor F1-score 0.9372753574060081
Confusion Matrix:
 [[14956  1509]
 [  954  4206]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_971265.png
Accuracy:   0.8861
Precision:  0.7360
Recall:     0.8151
F1-score:   0.7735

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4531, Test Loss: 0.3749, F1: 0.7257, AUC: 0.9056
Epoch [10/30] Train Loss: 0.3444, Test Loss: 0.2726, F1: 0.7496, AUC: 0.9324
Epoch [20/30] Train Loss: 0.3313, Test Loss: 0.4370, F1: 0.7249, AUC: 0.9355
Mejores resultados en la época:  24
f1-score 0.7762351244139921
AUC según el mejor F1-score 0.9382454501797328
Confusion Matrix:
 [[14838  1627]
 [  855  4305]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_971265.png
Accuracy:   0.8852
Precision:  0.7257
Recall:     0.8343
F1-score:   0.7762
Tiempo total para red 5: 349.57 segundos

Entrenando red 6 con capas [1559, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4562, Test Loss: 0.6534, F1: 0.6125, AUC: 0.9061
Epoch [10/30] Train Loss: 0.3468, Test Loss: 0.2849, F1: 0.7519, AUC: 0.9307
Epoch [20/30] Train Loss: 0.3311, Test Loss: 0.3854, F1: 0.7284, AUC: 0.9360
Mejores resultados en la época:  28
f1-score 0.7724741447891806
AUC según el mejor F1-score 0.9382427312339776
Confusion Matrix:
 [[15453  1012]
 [ 1276  3884]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_2296833.png
Accuracy:   0.8942
Precision:  0.7933
Recall:     0.7527
F1-score:   0.7725

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4638, Test Loss: 0.5123, F1: 0.6434, AUC: 0.9067
Epoch [10/30] Train Loss: 0.3420, Test Loss: 0.2907, F1: 0.7599, AUC: 0.9333
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:28:18] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch [20/30] Train Loss: 0.3264, Test Loss: 0.2911, F1: 0.7667, AUC: 0.9371
Mejores resultados en la época:  18
f1-score 0.7718183517099607
AUC según el mejor F1-score 0.9362256619043919
Confusion Matrix:
 [[15053  1412]
 [ 1030  4130]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_2296833.png
Accuracy:   0.8871
Precision:  0.7452
Recall:     0.8004
F1-score:   0.7718

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4618, Test Loss: 0.5009, F1: 0.6785, AUC: 0.9073
Epoch [10/30] Train Loss: 0.3514, Test Loss: 0.2984, F1: 0.7565, AUC: 0.9310
Epoch [20/30] Train Loss: 0.3295, Test Loss: 0.3614, F1: 0.7505, AUC: 0.9351
Mejores resultados en la época:  24
f1-score 0.7752280895769975
AUC según el mejor F1-score 0.9375641777131194
Confusion Matrix:
 [[14980  1485]
 [  954  4206]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_2296833.png
Accuracy:   0.8872
Precision:  0.7391
Recall:     0.8151
F1-score:   0.7752

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4683, Test Loss: 0.4624, F1: 0.6719, AUC: 0.9038
Epoch [10/30] Train Loss: 0.3452, Test Loss: 0.3249, F1: 0.7483, AUC: 0.9307
Epoch [20/30] Train Loss: 0.3326, Test Loss: 0.3020, F1: 0.7616, AUC: 0.9360
Mejores resultados en la época:  24
f1-score 0.7755294117647059
AUC según el mejor F1-score 0.937279836015791
Confusion Matrix:
 [[15120  1345]
 [ 1040  4120]]
Matriz de confusión guardada en: outputs_text_plus_numerical/3/gpt/confusion_matrix_param_2296833.png
Accuracy:   0.8897
Precision:  0.7539
Recall:     0.7984
F1-score:   0.7755
Tiempo total para red 6: 364.01 segundos
Saved on: outputs_text_plus_numerical/3/gpt

==============================
Model: Logistic Regression
Accuracy:  0.8611
Precision: 0.6627
Recall:    0.8510
F1-score:  0.7451
              precision    recall  f1-score   support

           0       0.95      0.86      0.90     16465
           1       0.66      0.85      0.75      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.86      0.82     21625
weighted avg       0.88      0.86      0.87     21625

[[14230  2235]
 [  769  4391]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical/3/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_text_plus_numerical/3/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.6540
Precision: 0.3875
Recall:    0.7752
F1-score:  0.5167
              precision    recall  f1-score   support

           0       0.90      0.62      0.73     16465
           1       0.39      0.78      0.52      5160

    accuracy                           0.65     21625
   macro avg       0.64      0.70      0.62     21625
weighted avg       0.78      0.65      0.68     21625

[[10142  6323]
 [ 1160  4000]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_text_plus_numerical/3/gpt/conf_matrix_svm.png
Modelo guardado como: outputs_text_plus_numerical/3/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7984
Precision: 0.5542
Recall:    0.7932
F1-score:  0.6525
              precision    recall  f1-score   support

           0       0.93      0.80      0.86     16465
           1       0.55      0.79      0.65      5160

    accuracy                           0.80     21625
   macro avg       0.74      0.80      0.76     21625
weighted avg       0.84      0.80      0.81     21625

[[13173  3292]
 [ 1067  4093]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_text_plus_numerical/3/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputs_text_plus_numerical/3/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8482
Precision: 0.6579
Recall:    0.7579
F1-score:  0.7044
              precision    recall  f1-score   support

           0       0.92      0.88      0.90     16465
           1       0.66      0.76      0.70      5160

    accuracy                           0.85     21625
   macro avg       0.79      0.82      0.80     21625
weighted avg       0.86      0.85      0.85     21625

[[14431  2034]
 [ 1249  3911]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical/3/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputs_text_plus_numerical/3/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8844
Precision: 0.7186
Recall:    0.8477
F1-score:  0.7778
              precision    recall  f1-score   support

           0       0.95      0.90      0.92     16465
           1       0.72      0.85      0.78      5160

    accuracy                           0.88     21625
   macro avg       0.83      0.87      0.85     21625
weighted avg       0.89      0.88      0.89     21625

[[14752  1713]
 [  786  4374]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_text_plus_numerical/3/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputs_text_plus_numerical/3/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8338
Precision: 0.6315
Recall:    0.7289
F1-score:  0.6767
              precision    recall  f1-score   support

           0       0.91      0.87      0.89     16465
           1       0.63      0.73      0.68      5160

    accuracy                           0.83     21625
   macro avg       0.77      0.80      0.78     21625
weighted avg       0.84      0.83      0.84     21625

[[14270  2195]
 [ 1399  3761]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_text_plus_numerical/3/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_text_plus_numerical/3/gpt/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.8844, 'precision': 0.7186, 'recall': 0.8477, 'f1_score': 0.7778}
Logistic Regression: {'accuracy': 0.8611, 'precision': 0.6627, 'recall': 0.851, 'f1_score': 0.7451}
Random Forest: {'accuracy': 0.8482, 'precision': 0.6579, 'recall': 0.7579, 'f1_score': 0.7044}
Naive Bayes: {'accuracy': 0.8338, 'precision': 0.6315, 'recall': 0.7289, 'f1_score': 0.6767}
Decision Tree: {'accuracy': 0.7984, 'precision': 0.5542, 'recall': 0.7932, 'f1_score': 0.6525}
SVM: {'accuracy': 0.654, 'precision': 0.3875, 'recall': 0.7752, 'f1_score': 0.5167}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
XGBoost: {'accuracy': 0.9086, 'precision': 0.7622, 'recall': 0.8965, 'f1_score': 0.8239}
Decision Tree: {'accuracy': 0.8875, 'precision': 0.7224, 'recall': 0.8587, 'f1_score': 0.7847}
MLP_1329409: {'accuracy': 0.8520231213872832, 'precision': 0.6486650485436893, 'recall': 0.8286821705426357, 'f1_score': 0.7420156139105749, 'f1_score_avg': 0.730249739623748}
MLP_2744833: {'accuracy': 0.8625202312138728, 'precision': 0.6816746967934872, 'recall': 0.79515503875969, 'f1_score': 0.7420156139105749, 'f1_score_avg': 0.7285270592111657}
MLP_5843969: {'accuracy': 0.8636300578034682, 'precision': 0.6808441027318829, 'recall': 0.8065891472868217, 'f1_score': 0.7420156139105749, 'f1_score_avg': 0.7343011823906324}
MLP_653441: {'accuracy': 0.861456647398844, 'precision': 0.6710401517546632, 'recall': 0.8226744186046512, 'f1_score': 0.7391607173950897, 'f1_score_avg': 0.7329988188221253}
MLP_323649: {'accuracy': 0.8517456647398844, 'precision': 0.6473160434258143, 'recall': 0.8319767441860465, 'f1_score': 0.7381057463147674, 'f1_score_avg': 0.7285163469255335}
Random Forest: {'accuracy': 0.8646, 'precision': 0.6931, 'recall': 0.776, 'f1_score': 0.7322}
MLP_160801: {'accuracy': 0.8501734104046242, 'precision': 0.6480567550894509, 'recall': 0.8143410852713179, 'f1_score': 0.7311003153498679, 'f1_score_avg': 0.7275615333340888}
Logistic Regression: {'accuracy': 0.8401, 'precision': 0.6278, 'recall': 0.8107, 'f1_score': 0.7076}
Naive Bayes: {'accuracy': 0.8283, 'precision': 0.62, 'recall': 0.7238, 'f1_score': 0.6679}
SVM: {'accuracy': 0.6605, 'precision': 0.3927, 'recall': 0.7731, 'f1_score': 0.5208}


EMBEDDINGS TYPE: LYRICS_BERT
XGBoost: {'accuracy': 0.87, 'precision': 0.6901, 'recall': 0.826, 'f1_score': 0.7519}
MLP_338433: {'accuracy': 0.8701040462427746, 'precision': 0.7308976625417404, 'recall': 0.7211240310077519, 'f1_score': 0.7311683320522675, 'f1_score_avg': 0.7264020541642857}
MLP_1031169: {'accuracy': 0.8666820809248555, 'precision': 0.708631115997801, 'recall': 0.7494186046511628, 'f1_score': 0.7311683320522675, 'f1_score_avg': 0.7274668097816842}
MLP_126209: {'accuracy': 0.8663583815028901, 'precision': 0.7073438070880527, 'recall': 0.7503875968992249, 'f1_score': 0.7297271049820336, 'f1_score_avg': 0.7284042941477336}
MLP_22849: {'accuracy': 0.8635375722543353, 'precision': 0.7016614935183495, 'recall': 0.7447674418604651, 'f1_score': 0.7269408147578785, 'f1_score_avg': 0.7227288715399045}
MLP_51841: {'accuracy': 0.8674682080924856, 'precision': 0.7215996908809892, 'recall': 0.7238372093023255, 'f1_score': 0.7269408147578785, 'f1_score_avg': 0.7240823017803119}
MLP_10401: {'accuracy': 0.8625664739884393, 'precision': 0.7031946508172363, 'recall': 0.7337209302325581, 'f1_score': 0.7224568138195777, 'f1_score_avg': 0.720103404875289}
Random Forest: {'accuracy': 0.8453, 'precision': 0.6469, 'recall': 0.7742, 'f1_score': 0.7048}
Logistic Regression: {'accuracy': 0.8369, 'precision': 0.622, 'recall': 0.807, 'f1_score': 0.7025}
Decision Tree: {'accuracy': 0.8034, 'precision': 0.5638, 'recall': 0.7783, 'f1_score': 0.6539}
Naive Bayes: {'accuracy': 0.6702, 'precision': 0.4035, 'recall': 0.7988, 'f1_score': 0.5362}
SVM: {'accuracy': 0.3204, 'precision': 0.2546, 'recall': 0.9587, 'f1_score': 0.4023}


EMBEDDINGS TYPE: GPT
MLP_101953: {'accuracy': 0.8815260115606937, 'precision': 0.7154943596549436, 'recall': 0.8358527131782946, 'f1_score': 0.7779696446423703, 'f1_score_avg': 0.7723888915405294}
MLP_210049: {'accuracy': 0.8880924855491329, 'precision': 0.7389257063132194, 'recall': 0.8211240310077519, 'f1_score': 0.7779696446423703, 'f1_score_avg': 0.7756101029387659}
MLP_442625: {'accuracy': 0.8884624277456648, 'precision': 0.744136460554371, 'recall': 0.8116279069767441, 'f1_score': 0.7779696446423703, 'f1_score_avg': 0.7747859827431144}
MLP_971265: {'accuracy': 0.8852254335260116, 'precision': 0.7257248819959542, 'recall': 0.8343023255813954, 'f1_score': 0.7779696446423703, 'f1_score_avg': 0.7741784258913805}
MLP_2296833: {'accuracy': 0.8897109826589595, 'precision': 0.7538883806038427, 'recall': 0.7984496124031008, 'f1_score': 0.7779696446423703, 'f1_score_avg': 0.7737624994602113}
XGBoost: {'accuracy': 0.8844, 'precision': 0.7186, 'recall': 0.8477, 'f1_score': 0.7778}
MLP_49953: {'accuracy': 0.8849017341040463, 'precision': 0.7340929009640667, 'recall': 0.8116279069767441, 'f1_score': 0.7709157846295444, 'f1_score_avg': 0.7659458134688395}
Logistic Regression: {'accuracy': 0.8611, 'precision': 0.6627, 'recall': 0.851, 'f1_score': 0.7451}
Random Forest: {'accuracy': 0.8482, 'precision': 0.6579, 'recall': 0.7579, 'f1_score': 0.7044}
Naive Bayes: {'accuracy': 0.8338, 'precision': 0.6315, 'recall': 0.7289, 'f1_score': 0.6767}
Decision Tree: {'accuracy': 0.7984, 'precision': 0.5542, 'recall': 0.7932, 'f1_score': 0.6525}
SVM: {'accuracy': 0.654, 'precision': 0.3875, 'recall': 0.7752, 'f1_score': 0.5167}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
====================================

