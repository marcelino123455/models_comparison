2025-09-29 00:37:59.629123: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-09-29 00:37:59.629123: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
Your are not adding cols numercis
You are executing with [ALL] dataset
--> PaTH:  ../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy

##################################################
Running experiment with TFIDF embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5000)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5000)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5000, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4833, Test Loss: 0.4286, F1: 0.6902, AUC: 0.8789
Epoch [10/30] Train Loss: 0.2839, Test Loss: 0.4643, F1: 0.6870, AUC: 0.8793
Epoch [20/30] Train Loss: 0.1975, Test Loss: 0.5703, F1: 0.6751, AUC: 0.8729
Mejores resultados en la época:  1
f1-score 0.6910569105691057
AUC según el mejor F1-score 0.8830987153864079
Confusion Matrix:
 [[13897  2568]
 [ 1080  4080]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_160065.png
Accuracy:   0.8313
Precision:  0.6137
Recall:     0.7907
F1-score:   0.6911

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4827, Test Loss: 0.4270, F1: 0.6919, AUC: 0.8791
Epoch [10/30] Train Loss: 0.2853, Test Loss: 0.4772, F1: 0.6771, AUC: 0.8787
Epoch [20/30] Train Loss: 0.2059, Test Loss: 0.5805, F1: 0.6628, AUC: 0.8726
Mejores resultados en la época:  1
f1-score 0.6921115266916015
AUC según el mejor F1-score 0.8825691683321681
Confusion Matrix:
 [[13932  2533]
 [ 1089  4071]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_160065.png
Accuracy:   0.8325
Precision:  0.6164
Recall:     0.7890
F1-score:   0.6921

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4814, Test Loss: 0.4244, F1: 0.6931, AUC: 0.8800
Epoch [10/30] Train Loss: 0.2866, Test Loss: 0.4778, F1: 0.6768, AUC: 0.8790
Epoch [20/30] Train Loss: 0.2136, Test Loss: 0.5476, F1: 0.6723, AUC: 0.8727
Mejores resultados en la época:  0
f1-score 0.6930763109028207
AUC según el mejor F1-score 0.8799839570430111
Confusion Matrix:
 [[14235  2230]
 [ 1241  3919]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_160065.png
Accuracy:   0.8395
Precision:  0.6373
Recall:     0.7595
F1-score:   0.6931

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4809, Test Loss: 0.4268, F1: 0.6911, AUC: 0.8804
Epoch [10/30] Train Loss: 0.2849, Test Loss: 0.4680, F1: 0.6790, AUC: 0.8793
Epoch [20/30] Train Loss: 0.2029, Test Loss: 0.5446, F1: 0.6769, AUC: 0.8753
Mejores resultados en la época:  0
f1-score 0.6910667823070251
AUC según el mejor F1-score 0.8803879794348831
Confusion Matrix:
 [[14079  2386]
 [ 1176  3984]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_160065.png
Accuracy:   0.8353
Precision:  0.6254
Recall:     0.7721
F1-score:   0.6911
Tiempo total para red 1: 1335.95 segundos

Entrenando red 2 con capas [5000, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4648, Test Loss: 0.4308, F1: 0.6906, AUC: 0.8824
Epoch [10/30] Train Loss: 0.0789, Test Loss: 1.0170, F1: 0.6402, AUC: 0.8487
Epoch [20/30] Train Loss: 0.0054, Test Loss: 1.8083, F1: 0.6484, AUC: 0.8534
Mejores resultados en la época:  1
f1-score 0.6976002685014264
AUC según el mejor F1-score 0.8854152571698953
Confusion Matrix:
 [[13864  2601]
 [ 1003  4157]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_322177.png
Accuracy:   0.8333
Precision:  0.6151
Recall:     0.8056
F1-score:   0.6976

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4607, Test Loss: 0.4127, F1: 0.6946, AUC: 0.8839
Epoch [10/30] Train Loss: 0.0407, Test Loss: 1.0830, F1: 0.6376, AUC: 0.8484
Epoch [20/30] Train Loss: 0.0051, Test Loss: 2.0947, F1: 0.6342, AUC: 0.8468
Mejores resultados en la época:  0
f1-score 0.6946199829205807
AUC según el mejor F1-score 0.8839403585712704
Confusion Matrix:
 [[13982  2483]
 [ 1093  4067]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_322177.png
Accuracy:   0.8346
Precision:  0.6209
Recall:     0.7882
F1-score:   0.6946

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4634, Test Loss: 0.4328, F1: 0.6863, AUC: 0.8824
Epoch [10/30] Train Loss: 0.1009, Test Loss: 0.8446, F1: 0.6271, AUC: 0.8463
Epoch [20/30] Train Loss: 0.0156, Test Loss: 1.9732, F1: 0.6269, AUC: 0.8449
Mejores resultados en la época:  2
f1-score 0.69826435246996
AUC según el mejor F1-score 0.8862245084122534
Confusion Matrix:
 [[13825  2640]
 [  976  4184]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_322177.png
Accuracy:   0.8328
Precision:  0.6131
Recall:     0.8109
F1-score:   0.6983

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4630, Test Loss: 0.3986, F1: 0.6998, AUC: 0.8836
Epoch [10/30] Train Loss: 0.0321, Test Loss: 1.2354, F1: 0.6331, AUC: 0.8456
Epoch [20/30] Train Loss: 0.0035, Test Loss: 2.0326, F1: 0.6434, AUC: 0.8508
Mejores resultados en la época:  0
f1-score 0.6997742663656885
AUC según el mejor F1-score 0.8836108835514375
Confusion Matrix:
 [[14137  2328]
 [ 1130  4030]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_322177.png
Accuracy:   0.8401
Precision:  0.6338
Recall:     0.7810
F1-score:   0.6998
Tiempo total para red 2: 1367.02 segundos

Entrenando red 3 con capas [5000, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4570, Test Loss: 0.4268, F1: 0.6861, AUC: 0.8825
Epoch [10/30] Train Loss: 0.0135, Test Loss: 1.7986, F1: 0.6308, AUC: 0.8459
Epoch [20/30] Train Loss: 0.0020, Test Loss: 1.8974, F1: 0.6431, AUC: 0.8539
Mejores resultados en la época:  1
f1-score 0.6930086149768059
AUC según el mejor F1-score 0.8863916294135786
Confusion Matrix:
 [[13736  2729]
 [  977  4183]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_650497.png
Accuracy:   0.8286
Precision:  0.6052
Recall:     0.8107
F1-score:   0.6930

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4582, Test Loss: 0.4377, F1: 0.6831, AUC: 0.8826
Epoch [10/30] Train Loss: 0.0160, Test Loss: 1.7467, F1: 0.6202, AUC: 0.8421
Epoch [20/30] Train Loss: 0.0035, Test Loss: 2.0551, F1: 0.6329, AUC: 0.8490
Mejores resultados en la época:  2
f1-score 0.6973077245659649
AUC según el mejor F1-score 0.8845750264243863
Confusion Matrix:
 [[13859  2606]
 [ 1003  4157]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_650497.png
Accuracy:   0.8331
Precision:  0.6147
Recall:     0.8056
F1-score:   0.6973

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4565, Test Loss: 0.4116, F1: 0.6906, AUC: 0.8843
Epoch [10/30] Train Loss: 0.0117, Test Loss: 1.6433, F1: 0.6466, AUC: 0.8531
Epoch [20/30] Train Loss: 0.0033, Test Loss: 2.2838, F1: 0.6519, AUC: 0.8586
Mejores resultados en la época:  1
f1-score 0.7016920473773266
AUC según el mejor F1-score 0.885599380409937
Confusion Matrix:
 [[13952  2513]
 [ 1013  4147]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_650497.png
Accuracy:   0.8369
Precision:  0.6227
Recall:     0.8037
F1-score:   0.7017

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4587, Test Loss: 0.4281, F1: 0.6841, AUC: 0.8839
Epoch [10/30] Train Loss: 0.0109, Test Loss: 1.7029, F1: 0.6358, AUC: 0.8473
Epoch [20/30] Train Loss: 0.0022, Test Loss: 2.0342, F1: 0.6485, AUC: 0.8544
Mejores resultados en la época:  1
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
Your are not adding cols numercis
You are executing with [ALL] dataset
--> PaTH:  ../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy

##################################################
Running experiment with TFIDF embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5000)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5000)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5000, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4821, Test Loss: 0.4258, F1: 0.6921, AUC: 0.8794
Epoch [10/30] Train Loss: 0.2862, Test Loss: 0.4855, F1: 0.6735, AUC: 0.8795
Epoch [20/30] Train Loss: 0.2144, Test Loss: 0.5401, F1: 0.6741, AUC: 0.8736
Mejores resultados en la época:  2
f1-score 0.6951343994555972
AUC según el mejor F1-score 0.8837596722669887
Confusion Matrix:
 [[13955  2510]
 [ 1074  4086]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_160065.png
Accuracy:   0.8343
Precision:  0.6195
Recall:     0.7919
F1-score:   0.6951

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4852, Test Loss: 0.4228, F1: 0.6929, AUC: 0.8779
Epoch [10/30] Train Loss: 0.2878, Test Loss: 0.4733, F1: 0.6802, AUC: 0.8783
Epoch [20/30] Train Loss: 0.2052, Test Loss: 0.5821, F1: 0.6642, AUC: 0.8707
Mejores resultados en la época:  0
f1-score 0.6928506787330316
AUC según el mejor F1-score 0.8778998910067632
Confusion Matrix:
 [[14403  2062]
 [ 1332  3828]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_160065.png
Accuracy:   0.8431
Precision:  0.6499
Recall:     0.7419
F1-score:   0.6929

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4819, Test Loss: 0.4136, F1: 0.6939, AUC: 0.8798
Epoch [10/30] Train Loss: 0.2855, Test Loss: 0.4521, F1: 0.6897, AUC: 0.8814
Epoch [20/30] Train Loss: 0.2138, Test Loss: 0.5502, F1: 0.6678, AUC: 0.8723
Mejores resultados en la época:  0
f1-score 0.6938812129912076
AUC según el mejor F1-score 0.8798235392434505
Confusion Matrix:
 [[14346  2119]
 [ 1293  3867]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_160065.png
Accuracy:   0.8422
Precision:  0.6460
Recall:     0.7494
F1-score:   0.6939

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4817, Test Loss: 0.4363, F1: 0.6882, AUC: 0.8797
Epoch [10/30] Train Loss: 0.2858, Test Loss: 0.4968, F1: 0.6698, AUC: 0.8791
Epoch [20/30] Train Loss: 0.2097, Test Loss: 0.5725, F1: 0.6636, AUC: 0.8734
Mejores resultados en la época:  2
f1-score 0.6907878282617758
AUC según el mejor F1-score 0.883717952339588
Confusion Matrix:
 [[13773  2692]
 [ 1017  4143]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_160065.png
Accuracy:   0.8285
Precision:  0.6061
Recall:     0.8029
F1-score:   0.6908
Tiempo total para red 1: 1336.92 segundos

Entrenando red 2 con capas [5000, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4621, Test Loss: 0.4372, F1: 0.6862, AUC: 0.8826
Epoch [10/30] Train Loss: 0.0405, Test Loss: 1.2897, F1: 0.6494, AUC: 0.8551
Epoch [20/30] Train Loss: 0.0112, Test Loss: 1.9443, F1: 0.6331, AUC: 0.8467
Mejores resultados en la época:  1
f1-score 0.6966537750126114
AUC según el mejor F1-score 0.8861353540632348
Confusion Matrix:
 [[13874  2591]
 [ 1017  4143]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_322177.png
Accuracy:   0.8332
Precision:  0.6152
Recall:     0.8029
F1-score:   0.6967

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4622, Test Loss: 0.4323, F1: 0.6819, AUC: 0.8820
Epoch [10/30] Train Loss: 0.0902, Test Loss: 1.0049, F1: 0.6446, AUC: 0.8518
Epoch [20/30] Train Loss: 0.0241, Test Loss: 2.1890, F1: 0.6275, AUC: 0.8496
Mejores resultados en la época:  2
f1-score 0.6910244934494263
AUC según el mejor F1-score 0.8865429369793101
Confusion Matrix:
 [[13582  2883]
 [  914  4246]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_322177.png
Accuracy:   0.8244
Precision:  0.5956
Recall:     0.8229
F1-score:   0.6910

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4618, Test Loss: 0.4320, F1: 0.6861, AUC: 0.8836
Epoch [10/30] Train Loss: 0.0813, Test Loss: 0.9900, F1: 0.6446, AUC: 0.8491
Epoch [20/30] Train Loss: 0.0102, Test Loss: 2.1273, F1: 0.6155, AUC: 0.8374
Mejores resultados en la época:  1
f1-score 0.6990241832838354
AUC según el mejor F1-score 0.8864540651181623
Confusion Matrix:
 [[13959  2506]
 [ 1041  4119]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_322177.png
Accuracy:   0.8360
Precision:  0.6217
Recall:     0.7983
F1-score:   0.6990

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4632, Test Loss: 0.4105, F1: 0.6979, AUC: 0.8835
Epoch [10/30] Train Loss: 0.0427, Test Loss: 1.2200, F1: 0.6323, AUC: 0.8463
Epoch [20/30] Train Loss: 0.0032, Test Loss: 2.1284, F1: 0.6313, AUC: 0.8438
Mejores resultados en la época:  0
f1-score 0.6978554133517814
AUC según el mejor F1-score 0.8834929860615777
Confusion Matrix:
 [[14096  2369]
 [ 1125  4035]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_322177.png
Accuracy:   0.8384
Precision:  0.6301
Recall:     0.7820
F1-score:   0.6979
Tiempo total para red 2: 1366.22 segundos

Entrenando red 3 con capas [5000, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4588, Test Loss: 0.4550, F1: 0.6714, AUC: 0.8828
Epoch [10/30] Train Loss: 0.0157, Test Loss: 1.8030, F1: 0.6289, AUC: 0.8473
Epoch [20/30] Train Loss: 0.0015, Test Loss: 2.3693, F1: 0.6215, AUC: 0.8468
Mejores resultados en la época:  1
f1-score 0.6965132728634434
AUC según el mejor F1-score 0.8866433202211881
Confusion Matrix:
 [[13793  2672]
 [  975  4185]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_650497.png
Accuracy:   0.8314
Precision:  0.6103
Recall:     0.8110
F1-score:   0.6965

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4583, Test Loss: 0.4174, F1: 0.6944, AUC: 0.8852
Epoch [10/30] Train Loss: 0.0160, Test Loss: 2.1567, F1: 0.6300, AUC: 0.8488
Epoch [20/30] Train Loss: 0.0113, Test Loss: 2.2567, F1: 0.6279, AUC: 0.8523
Mejores resultados en la época:  2
f1-score 0.7069831918211749
AUC según el mejor F1-score 0.8842589872339023
Confusion Matrix:
 [[14163  2302]
 [ 1080  4080]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_650497.png
Accuracy:   0.8436
Precision:  0.6393
Recall:     0.7907
F1-score:   0.7070

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4585, Test Loss: 0.4758, F1: 0.6669, AUC: 0.8828
Epoch [10/30] Train Loss: 0.0129, Test Loss: 2.0467, F1: 0.6360, AUC: 0.8497
Epoch [20/30] Train Loss: 0.0086, Test Loss: 2.0057, F1: 0.6346, AUC: 0.8479
Mejores resultados en la época:  1
f1-score 0.6923961241583183
AUC según el mejor F1-score 0.886490653182579
Confusion Matrix:
 [[13663  2802]
 [  944  4216]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_650497.png
Accuracy:   0.8268
Precision:  0.6007
Recall:     0.8171
F1-score:   0.6924

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4585, Test Loss: 0.4295, F1: 0.6876, AUC: 0.8839
Epoch [10/30] Train Loss: 0.0076, Test Loss: 1.7373, F1: 0.6355, AUC: 0.8477
Epoch [20/30] Train Loss: 0.0115, Test Loss: 1.7251, F1: 0.6358, AUC: 0.8488
Mejores resultados en la época:  3
f1-score 0.6990503403647366
AUC según el mejor F1-score 0.8855055179297406
Confusion Matrix:
 [[13885  2580]
 [ 1001  4159]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_650497.png
Accuracy:   0.8344
Precision:  0.6172
Recall:     0.8060
F1-score:   0.6991
Tiempo total para red 3: 1334.89 segundos

Entrenando red 4 con capas [5000, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4545, Test Loss: 0.3919, F1: 0.7007, AUC: 0.8854
Epoch [10/30] Train Loss: 0.0065, Test Loss: 1.9891, F1: 0.6322, AUC: 0.8529
Epoch [20/30] Train Loss: 0.0042, Test Loss: 2.4635, F1: 0.6237, AUC: 0.8556
Mejores resultados en la época:  1
f1-score 0.7008196721311475
AUC según el mejor F1-score 0.8855698780829431
Confusion Matrix:
 [[14017  2448]
 [ 1056  4104]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_1323521.png
Accuracy:   0.8380
Precision:  0.6264
Recall:     0.7953
F1-score:   0.7008

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4576, Test Loss: 0.3912, F1: 0.6983, AUC: 0.8845
Epoch [10/30] Train Loss: 0.0052, Test Loss: 1.6653, F1: 0.6494, AUC: 0.8539
Epoch [20/30] Train Loss: 0.0026, Test Loss: 1.8192, F1: 0.6430, AUC: 0.8578
Mejores resultados en la época:  1
f1-score 0.7002630016119453
AUC según el mejor F1-score 0.8841895422990276
Confusion Matrix:
 [[13965  2500]
 [ 1033  4127]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_1323521.png
Accuracy:   0.8366
Precision:  0.6228
Recall:     0.7998
F1-score:   0.7003

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4574, Test Loss: 0.3767, F1: 0.7033, AUC: 0.8840
Epoch [10/30] Train Loss: 0.0078, Test Loss: 1.9034, F1: 0.6176, AUC: 0.8432
Epoch [20/30] Train Loss: 0.0047, Test Loss: 1.9318, F1: 0.6319, AUC: 0.8541
Mejores resultados en la época:  0
f1-score 0.7032887603903144
AUC según el mejor F1-score 0.8840432547781645
Confusion Matrix:
 [[14449  2016]
 [ 1268  3892]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_1323521.png
Accuracy:   0.8481
Precision:  0.6588
Recall:     0.7543
F1-score:   0.7033

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4570, Test Loss: 0.4181, F1: 0.6940, AUC: 0.8854
Epoch [10/30] Train Loss: 0.0070, Test Loss: 1.7058, F1: 0.6359, AUC: 0.8510
Epoch [20/30] Train Loss: 0.0023, Test Loss: 2.0758, F1: 0.6453, AUC: 0.8564
Mejores resultados en la época:  1
f1-score 0.6981435958294482
AUC según el mejor F1-score 0.8854929531046594
Confusion Matrix:
 [[13946  2519]
 [ 1042  4118]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_1323521.png
Accuracy:   0.8353
Precision:  0.6205
Recall:     0.7981
F1-score:   0.6981
Tiempo total para red 4: 1441.16 segundos

Entrenando red 5 con capas [5000, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4537, Test Loss: 0.3927, F1: 0.7001, AUC: 0.8840
Epoch [10/30] Train Loss: 0.0098, Test Loss: 1.6602, F1: 0.6367, AUC: 0.8578
Epoch [20/30] Train Loss: 0.0077, Test Loss: 1.3819, F1: 0.6364, AUC: 0.8547
Mejores resultados en la época:  0
f1-score 0.700103591160221
AUC según el mejor F1-score 0.8840216032599101
Confusion Matrix:
 [[14096  2369]
 [ 1105  4055]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_2733057.png
Accuracy:   0.8394
Precision:  0.6312
Recall:     0.7859
F1-score:   0.7001

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4530, Test Loss: 0.4027, F1: 0.6953, AUC: 0.8850
Epoch [10/30] Train Loss: 0.0096, Test Loss: 1.6987, F1: 0.6546, AUC: 0.8593
Epoch [20/30] Train Loss: 0.0045, Test Loss: 1.7770, F1: 0.6547, AUC: 0.8635
Mejores resultados en la época:  0
f1-score 0.6953058803592611
AUC según el mejor F1-score 0.8849777423098563
Confusion Matrix:
 [[13926  2539]
 [ 1057  4103]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_2733057.png
Accuracy:   0.8337
Precision:  0.6177
Recall:     0.7952
F1-score:   0.6953

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4536, Test Loss: 0.4010, F1: 0.7028, AUC: 0.8861
Epoch [10/30] Train Loss: 0.0056, Test Loss: 1.6121, F1: 0.6438, AUC: 0.8582
Epoch [20/30] Train Loss: 0.0043, Test Loss: 1.7563, F1: 0.6299, AUC: 0.8572
Mejores resultados en la época:  0
f1-score 0.7028226522279425
AUC según el mejor F1-score 0.8861387144918631
Confusion Matrix:
 [[14318  2147]
 [ 1201  3959]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_2733057.png
Accuracy:   0.8452
Precision:  0.6484
Recall:     0.7672
F1-score:   0.7028

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4543, Test Loss: 0.4047, F1: 0.6982, AUC: 0.8858
Epoch [10/30] Train Loss: 0.0086, Test Loss: 1.8409, F1: 0.6315, AUC: 0.8506
Epoch [20/30] Train Loss: 0.0043, Test Loss: 1.6280, F1: 0.6572, AUC: 0.8644
Mejores resultados en la época:  2
f1-score 0.7034259417078575
AUC según el mejor F1-score 0.8817730351203046
Confusion Matrix:
 [[14018  2447]
 [ 1033  4127]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_2733057.png
Accuracy:   0.8391
Precision:  0.6278
Recall:     0.7998
F1-score:   0.7034
Tiempo total para red 5: 1357.60 segundos

Entrenando red 6 con capas [5000, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4568, Test Loss: 0.3748, F1: 0.7023, AUC: 0.8862
Epoch [10/30] Train Loss: 0.0092, Test Loss: 1.6304, F1: 0.6494, AUC: 0.8650
Epoch [20/30] Train Loss: 0.0035, Test Loss: 1.7184, F1: 0.6574, AUC: 0.8654
Mejores resultados en la época:  0
f1-score 0.702257636122178
AUC según el mejor F1-score 0.8861703825591988
Confusion Matrix:
 [[14296  2169]
 [ 1194  3966]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_5820417.png
Accuracy:   0.8445
Precision:  0.6465
Recall:     0.7686
F1-score:   0.7023

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4590, Test Loss: 0.4380, F1: 0.6867, AUC: 0.8830
Epoch [10/30] Train Loss: 0.0059, Test Loss: 2.3665, F1: 0.6437, AUC: 0.8583
Epoch [20/30] Train Loss: 0.0042, Test Loss: 1.8320, F1: 0.6453, AUC: 0.8596
Mejores resultados en la época:  1
f1-score 0.7031425850575708
AUC según el mejor F1-score 0.8844253667045673
Confusion Matrix:
 [[14135  2330]
 [ 1099  4061]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_5820417.png
Accuracy:   0.8414
Precision:  0.6354
Recall:     0.7870
F1-score:   0.7031

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4572, Test Loss: 0.4316, F1: 0.6976, AUC: 0.8845
Epoch [10/30] Train Loss: 0.0063, Test Loss: 2.1311, F1: 0.6630, AUC: 0.8618
Epoch [20/30] Train Loss: 0.0059, Test Loss: 2.3952, F1: 0.6319, AUC: 0.8583
Mejores resultados en la época:  0
f1-score 0.69755759570414
AUC según el mejor F1-score 0.8844663568716351
Confusion Matrix:
 [[14106  2359]
 [ 1133  4027]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_5820417.png
Accuracy:   0.8385
Precision:  0.6306
Recall:     0.7804
F1-score:   0.6976

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4587, Test Loss: 0.4528, F1: 0.6949, AUC: 0.8841
Epoch [10/30] Train Loss: 0.0060, Test Loss: 1.7228, F1: 0.6411, AUC: 0.8598
Epoch [20/30] Train Loss: 0.0045, Test Loss: 1.8638, F1: 0.6389, AUC: 0.8580
Mejores resultados en la época:  0
f1-score 0.6949313530783179
AUC según el mejor F1-score 0.8841370113254096
Confusion Matrix:
 [[14068  2397]
 [ 1136  4024]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_5820417.png
Accuracy:   0.8366
Precision:  0.6267
Recall:     0.7798
F1-score:   0.6949
Tiempo total para red 6: 1905.38 segundos
Saved on: outputs_only_text/0/tfidf

==============================
Model: Logistic Regression
Accuracy:  0.8250
Precision: 0.6070
Recall:    0.7566
F1-score:  0.6736
              precision    recall  f1-score   support

           0       0.92      0.85      0.88     16465
           1       0.61      0.76      0.67      5160

    accuracy                           0.83     21625
   macro avg       0.76      0.80      0.78     21625
weighted avg       0.84      0.83      0.83     21625
f1-score 0.6968736616702356
AUC según el mejor F1-score 0.8750719225889072
Confusion Matrix:
 [[14018  2447]
 [ 1092  4068]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_650497.png
Accuracy:   0.8363
Precision:  0.6244
Recall:     0.7884
F1-score:   0.6969
Tiempo total para red 3: 1345.99 segundos

Entrenando red 4 con capas [5000, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4559, Test Loss: 0.4244, F1: 0.6913, AUC: 0.8853
Epoch [10/30] Train Loss: 0.0127, Test Loss: 1.4429, F1: 0.6400, AUC: 0.8556
Epoch [20/30] Train Loss: 0.0024, Test Loss: 2.0289, F1: 0.6247, AUC: 0.8495
Mejores resultados en la época:  1
f1-score 0.6970310391363023
AUC según el mejor F1-score 0.8858574154243085
Confusion Matrix:
 [[13901  2564]
 [ 1028  4132]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_1323521.png
Accuracy:   0.8339
Precision:  0.6171
Recall:     0.8008
F1-score:   0.6970

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4561, Test Loss: 0.4231, F1: 0.6922, AUC: 0.8850
Epoch [10/30] Train Loss: 0.0083, Test Loss: 1.5805, F1: 0.6494, AUC: 0.8552
Epoch [20/30] Train Loss: 0.0011, Test Loss: 2.4699, F1: 0.6479, AUC: 0.8567
Mejores resultados en la época:  0
f1-score 0.6922109499455018
AUC según el mejor F1-score 0.8849951270842308
Confusion Matrix:
 [[13826  2639]
 [ 1032  4128]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_1323521.png
Accuracy:   0.8302
Precision:  0.6100
Recall:     0.8000
F1-score:   0.6922

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4562, Test Loss: 0.4002, F1: 0.6985, AUC: 0.8853
Epoch [10/30] Train Loss: 0.0085, Test Loss: 1.5294, F1: 0.6365, AUC: 0.8529
Epoch [20/30] Train Loss: 0.0024, Test Loss: 1.7310, F1: 0.6391, AUC: 0.8542
Mejores resultados en la época:  0
f1-score 0.6985130746880875
AUC según el mejor F1-score 0.8853128847425946
Confusion Matrix:
 [[14010  2455]
 [ 1073  4087]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_1323521.png
Accuracy:   0.8369
Precision:  0.6247
Recall:     0.7921
F1-score:   0.6985

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4558, Test Loss: 0.4192, F1: 0.6929, AUC: 0.8858
Epoch [10/30] Train Loss: 0.0081, Test Loss: 1.6493, F1: 0.6313, AUC: 0.8501
Epoch [20/30] Train Loss: 0.0023, Test Loss: 1.8362, F1: 0.6401, AUC: 0.8594
Mejores resultados en la época:  1
f1-score 0.7016342945152734
AUC según el mejor F1-score 0.8866144181809195
Confusion Matrix:
 [[14038  2427]
 [ 1060  4100]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_1323521.png
Accuracy:   0.8388
Precision:  0.6282
Recall:     0.7946
F1-score:   0.7016
Tiempo total para red 4: 1467.71 segundos

Entrenando red 5 con capas [5000, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4535, Test Loss: 0.4274, F1: 0.6853, AUC: 0.8852
Epoch [10/30] Train Loss: 0.0089, Test Loss: 1.6216, F1: 0.6519, AUC: 0.8610
Epoch [20/30] Train Loss: 0.0039, Test Loss: 1.7643, F1: 0.6383, AUC: 0.8574
Mejores resultados en la época:  1
f1-score 0.6970775726467423
AUC según el mejor F1-score 0.8847818252012137
Confusion Matrix:
 [[13756  2709]
 [  950  4210]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_2733057.png
Accuracy:   0.8308
Precision:  0.6085
Recall:     0.8159
F1-score:   0.6971

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4580, Test Loss: 0.4006, F1: 0.6974, AUC: 0.8843
Epoch [10/30] Train Loss: 0.0033, Test Loss: 1.8135, F1: 0.6613, AUC: 0.8638
Epoch [20/30] Train Loss: 0.0013, Test Loss: 2.6124, F1: 0.6202, AUC: 0.8609
Mejores resultados en la época:  1
f1-score 0.7023688663282572
AUC según el mejor F1-score 0.8840559019955414
Confusion Matrix:
 [[13956  2509]
 [ 1009  4151]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_2733057.png
Accuracy:   0.8373
Precision:  0.6233
Recall:     0.8045
F1-score:   0.7024

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4556, Test Loss: 0.4071, F1: 0.6959, AUC: 0.8860
Epoch [10/30] Train Loss: 0.0093, Test Loss: 1.4838, F1: 0.6363, AUC: 0.8542
Epoch [20/30] Train Loss: 0.0037, Test Loss: 1.5904, F1: 0.6450, AUC: 0.8601
Mejores resultados en la época:  1
f1-score 0.7015243123562974
AUC según el mejor F1-score 0.8838971496973849
Confusion Matrix:
 [[14001  2464]
 [ 1041  4119]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_2733057.png
Accuracy:   0.8379
Precision:  0.6257
Recall:     0.7983
F1-score:   0.7015

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4548, Test Loss: 0.4173, F1: 0.6976, AUC: 0.8862
Epoch [10/30] Train Loss: 0.0089, Test Loss: 1.7042, F1: 0.6517, AUC: 0.8611
Epoch [20/30] Train Loss: 0.0027, Test Loss: 2.2155, F1: 0.6116, AUC: 0.8498
Mejores resultados en la época:  1
f1-score 0.7041506204535729
AUC según el mejor F1-score 0.8858830100024246
Confusion Matrix:
 [[14054  2411]
 [ 1046  4114]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_2733057.png
Accuracy:   0.8401
Precision:  0.6305
Recall:     0.7973
F1-score:   0.7042
Tiempo total para red 5: 1377.35 segundos

Entrenando red 6 con capas [5000, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4542, Test Loss: 0.4041, F1: 0.7037, AUC: 0.8858
Epoch [10/30] Train Loss: 0.0058, Test Loss: 1.9722, F1: 0.6280, AUC: 0.8553
Epoch [20/30] Train Loss: 0.0036, Test Loss: 2.0534, F1: 0.6460, AUC: 0.8557
Mejores resultados en la época:  0
f1-score 0.7036866783120346
AUC según el mejor F1-score 0.8857647594027264
Confusion Matrix:
 [[14575  1890]
 [ 1333  3827]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_5820417.png
Accuracy:   0.8510
Precision:  0.6694
Recall:     0.7417
F1-score:   0.7037

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4586, Test Loss: 0.4098, F1: 0.6868, AUC: 0.8838
Epoch [10/30] Train Loss: 0.0063, Test Loss: 1.9300, F1: 0.6442, AUC: 0.8576
Epoch [20/30] Train Loss: 0.0031, Test Loss: 2.4832, F1: 0.6568, AUC: 0.8621
Mejores resultados en la época:  2
f1-score 0.6886577835554111
AUC según el mejor F1-score 0.8789296358025127
Confusion Matrix:
 [[13555  2910]
 [  922  4238]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_5820417.png
Accuracy:   0.8228
Precision:  0.5929
Recall:     0.8213
F1-score:   0.6887

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4557, Test Loss: 0.4332, F1: 0.6853, AUC: 0.8837
Epoch [10/30] Train Loss: 0.0042, Test Loss: 1.8629, F1: 0.6410, AUC: 0.8657
Epoch [20/30] Train Loss: 0.0049, Test Loss: 2.2637, F1: 0.6190, AUC: 0.8588
Mejores resultados en la época:  1
f1-score 0.7028566496936222
AUC según el mejor F1-score 0.8859745125318681
Confusion Matrix:
 [[14110  2355]
 [ 1088  4072]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_5820417.png
Accuracy:   0.8408
Precision:  0.6336
Recall:     0.7891
F1-score:   0.7029

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4569, Test Loss: 0.3811, F1: 0.7014, AUC: 0.8841
Epoch [10/30] Train Loss: 0.0054, Test Loss: 2.3035, F1: 0.6407, AUC: 0.8590
Epoch [20/30] Train Loss: 0.0048, Test Loss: 1.9831, F1: 0.6212, AUC: 0.8588
Mejores resultados en la época:  0
f1-score 0.7013570594050508
AUC según el mejor F1-score 0.8840798075315974
Confusion Matrix:
 [[14400  2065]
 [ 1258  3902]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_5820417.png
Accuracy:   0.8463
Precision:  0.6539
Recall:     0.7562
F1-score:   0.7014
Tiempo total para red 6: 1903.97 segundos
Saved on: outputs_only_text/0/tfidf

==============================
Model: Logistic Regression
Accuracy:  0.8250
Precision: 0.6070
Recall:    0.7566
F1-score:  0.6736
              precision    recall  f1-score   support

           0       0.92      0.85      0.88     16465
           1       0.61      0.76      0.67      5160

    accuracy                           0.83     21625
   macro avg       0.76      0.80      0.78     21625
weighted avg       0.84      0.83      0.83     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:30:24] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:31:21] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])

[[13937  2528]
 [ 1256  3904]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text/0/tfidf/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.6767
Precision: 0.3854
Recall:    0.5969
F1-score:  0.4684
              precision    recall  f1-score   support

           0       0.85      0.70      0.77     16465
           1       0.39      0.60      0.47      5160

    accuracy                           0.68     21625
   macro avg       0.62      0.65      0.62     21625
weighted avg       0.74      0.68      0.70     21625

[[11554  4911]
 [ 2080  3080]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_svm.png
Modelo guardado como: outputs_only_text/0/tfidf/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8637
Precision: 0.6782
Recall:    0.8157
F1-score:  0.7406
              precision    recall  f1-score   support

           0       0.94      0.88      0.91     16465
           1       0.68      0.82      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.85      0.82     21625
weighted avg       0.88      0.86      0.87     21625

[[14468  1997]
 [  951  4209]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text/0/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8530
Precision: 0.6721
Recall:    0.7500
F1-score:  0.7089
              precision    recall  f1-score   support

           0       0.92      0.89      0.90     16465
           1       0.67      0.75      0.71      5160

    accuracy                           0.85     21625
   macro avg       0.80      0.82      0.81     21625
weighted avg       0.86      0.85      0.86     21625

[[14577  1888]
 [ 1290  3870]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text/0/tfidf/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8670
Precision: 0.6775
Recall:    0.8444
F1-score:  0.7518
              precision    recall  f1-score   support

           0       0.95      0.87      0.91     16465
           1       0.68      0.84      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.86      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14391  2074]
 [  803  4357]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text/0/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8214
Precision: 0.6084
Recall:    0.7054
F1-score:  0.6533
              precision    recall  f1-score   support

           0       0.90      0.86      0.88     16465
           1       0.61      0.71      0.65      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.78      0.77     21625
weighted avg       0.83      0.82      0.83     21625

[[14122  2343]
 [ 1520  3640]]
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text/0/tfidf/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.867, 'precision': 0.6775, 'recall': 0.8444, 'f1_score': 0.7518}
Decision Tree: {'accuracy': 0.8637, 'precision': 0.6782, 'recall': 0.8157, 'f1_score': 0.7406}
Random Forest: {'accuracy': 0.853, 'precision': 0.6721, 'recall': 0.75, 'f1_score': 0.7089}
Logistic Regression: {'accuracy': 0.825, 'precision': 0.607, 'recall': 0.7566, 'f1_score': 0.6736}
Naive Bayes: {'accuracy': 0.8214, 'precision': 0.6084, 'recall': 0.7054, 'f1_score': 0.6533}
SVM: {'accuracy': 0.6767, 'precision': 0.3854, 'recall': 0.5969, 'f1_score': 0.4684}

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 300)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 300)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [300, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6626, Test Loss: 0.5975, F1: 0.5968, AUC: 0.8199
Epoch [10/30] Train Loss: 0.4941, Test Loss: 0.4940, F1: 0.6325, AUC: 0.8475
Epoch [20/30] Train Loss: 0.4857, Test Loss: 0.5090, F1: 0.6264, AUC: 0.8522
Mejores resultados en la época:  22
f1-score 0.6476036327668375
AUC según el mejor F1-score 0.8525663611089532
Confusion Matrix:
 [[14105  2360]
 [ 1559  3601]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8188
Precision:  0.6041
Recall:     0.6979
F1-score:   0.6476

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6775, Test Loss: 0.6551, F1: 0.5554, AUC: 0.8188
Epoch [10/30] Train Loss: 0.4911, Test Loss: 0.4878, F1: 0.6300, AUC: 0.8462
Epoch [20/30] Train Loss: 0.4825, Test Loss: 0.4644, F1: 0.6394, AUC: 0.8517
Mejores resultados en la época:  22
f1-score 0.6496364708194144
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])

[[13937  2528]
 [ 1256  3904]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text/0/tfidf/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.6767
Precision: 0.3854
Recall:    0.5969
F1-score:  0.4684
              precision    recall  f1-score   support

           0       0.85      0.70      0.77     16465
           1       0.39      0.60      0.47      5160

    accuracy                           0.68     21625
   macro avg       0.62      0.65      0.62     21625
weighted avg       0.74      0.68      0.70     21625

[[11554  4911]
 [ 2080  3080]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_svm.png
Modelo guardado como: outputs_only_text/0/tfidf/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8637
Precision: 0.6782
Recall:    0.8157
F1-score:  0.7406
              precision    recall  f1-score   support

           0       0.94      0.88      0.91     16465
           1       0.68      0.82      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.85      0.82     21625
weighted avg       0.88      0.86      0.87     21625

[[14468  1997]
 [  951  4209]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text/0/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8530
Precision: 0.6721
Recall:    0.7500
F1-score:  0.7089
              precision    recall  f1-score   support

           0       0.92      0.89      0.90     16465
           1       0.67      0.75      0.71      5160

    accuracy                           0.85     21625
   macro avg       0.80      0.82      0.81     21625
weighted avg       0.86      0.85      0.86     21625

[[14577  1888]
 [ 1290  3870]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text/0/tfidf/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8670
Precision: 0.6775
Recall:    0.8444
F1-score:  0.7518
              precision    recall  f1-score   support

           0       0.95      0.87      0.91     16465
           1       0.68      0.84      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.86      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14391  2074]
 [  803  4357]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text/0/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8214
Precision: 0.6084
Recall:    0.7054
F1-score:  0.6533
              precision    recall  f1-score   support

           0       0.90      0.86      0.88     16465
           1       0.61      0.71      0.65      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.78      0.77     21625
weighted avg       0.83      0.82      0.83     21625

[[14122  2343]
 [ 1520  3640]]
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text/0/tfidf/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.867, 'precision': 0.6775, 'recall': 0.8444, 'f1_score': 0.7518}
Decision Tree: {'accuracy': 0.8637, 'precision': 0.6782, 'recall': 0.8157, 'f1_score': 0.7406}
Random Forest: {'accuracy': 0.853, 'precision': 0.6721, 'recall': 0.75, 'f1_score': 0.7089}
Logistic Regression: {'accuracy': 0.825, 'precision': 0.607, 'recall': 0.7566, 'f1_score': 0.6736}
Naive Bayes: {'accuracy': 0.8214, 'precision': 0.6084, 'recall': 0.7054, 'f1_score': 0.6533}
SVM: {'accuracy': 0.6767, 'precision': 0.3854, 'recall': 0.5969, 'f1_score': 0.4684}

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 300)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 300)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [300, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6474, Test Loss: 0.6023, F1: 0.5661, AUC: 0.8225
Epoch [10/30] Train Loss: 0.4952, Test Loss: 0.5452, F1: 0.6117, AUC: 0.8484
Epoch [20/30] Train Loss: 0.4947, Test Loss: 0.4815, F1: 0.6399, AUC: 0.8521
Mejores resultados en la época:  18
f1-score 0.6498812351543943
AUC según el mejor F1-score 0.85177314693842
Confusion Matrix:
 [[14520  1945]
 [ 1740  3420]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8296
Precision:  0.6375
Recall:     0.6628
F1-score:   0.6499

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6938, Test Loss: 0.6926, F1: 0.0000, AUC: 0.4999
Epoch [10/30] Train Loss: 0.6932, Test Loss: 0.6938, F1: 0.3852, AUC: 0.4999
Epoch [20/30] Train Loss: 0.6932, Test Loss: 0.6927, F1: 0.0000, AUC: 0.4999
Mejores resultados en la época:  13
f1-score 0.3852902744073175
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
AUC según el mejor F1-score 0.8526076631897118
Confusion Matrix:
 [[14753  1712]
 [ 1854  3306]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8351
Precision:  0.6588
Recall:     0.6407
F1-score:   0.6496

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6541, Test Loss: 0.5854, F1: 0.5928, AUC: 0.8216
Epoch [10/30] Train Loss: 0.4902, Test Loss: 0.6213, F1: 0.5644, AUC: 0.8490
Epoch [20/30] Train Loss: 0.4820, Test Loss: 0.4258, F1: 0.6499, AUC: 0.8533
Mejores resultados en la época:  27
f1-score 0.6526375928394591
AUC según el mejor F1-score 0.854297052474476
Confusion Matrix:
 [[14550  1915]
 [ 1733  3427]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8313
Precision:  0.6415
Recall:     0.6641
F1-score:   0.6526

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6935, Test Loss: 0.6896, F1: 0.0000, AUC: 0.5002
Epoch [10/30] Train Loss: 0.6932, Test Loss: 0.6917, F1: 0.0000, AUC: 0.5000
Epoch [20/30] Train Loss: 0.6932, Test Loss: 0.6947, F1: 0.3853, AUC: 0.5000
Mejores resultados en la época:  1
f1-score 0.3852902744073175
AUC según el mejor F1-score 0.49993926510780445
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853
Tiempo total para red 1: 848.24 segundos

Entrenando red 2 con capas [300, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6298, Test Loss: 0.4942, F1: 0.6117, AUC: 0.8262
Epoch [10/30] Train Loss: 0.4900, Test Loss: 0.4276, F1: 0.6481, AUC: 0.8506
Epoch [20/30] Train Loss: 0.4847, Test Loss: 0.4721, F1: 0.6425, AUC: 0.8539
Mejores resultados en la época:  21
f1-score 0.6546647913736522
AUC según el mejor F1-score 0.8542174203207651
Confusion Matrix:
 [[14451  2014]
 [ 1669  3491]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8297
Precision:  0.6342
Recall:     0.6766
F1-score:   0.6547

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6432, Test Loss: 0.5195, F1: 0.6029, AUC: 0.8243
Epoch [10/30] Train Loss: 0.4855, Test Loss: 0.4551, F1: 0.6443, AUC: 0.8505
Epoch [20/30] Train Loss: 0.4842, Test Loss: 0.5050, F1: 0.6305, AUC: 0.8539
Mejores resultados en la época:  28
f1-score 0.6565551237084084
AUC según el mejor F1-score 0.8556178421693186
Confusion Matrix:
 [[14539  1926]
 [ 1697  3463]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8325
Precision:  0.6426
Recall:     0.6711
F1-score:   0.6566

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6470, Test Loss: 0.6595, F1: 0.5190, AUC: 0.8229
Epoch [10/30] Train Loss: 0.4868, Test Loss: 0.4688, F1: 0.6392, AUC: 0.8499
Epoch [20/30] Train Loss: 0.4818, Test Loss: 0.4561, F1: 0.6429, AUC: 0.8540
Mejores resultados en la época:  25
f1-score 0.6527302920777571
AUC según el mejor F1-score 0.8548891176255954
Confusion Matrix:
 [[14729  1736]
 [ 1819  3341]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8356
Precision:  0.6581
Recall:     0.6475
F1-score:   0.6527

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6422, Test Loss: 0.4837, F1: 0.6134, AUC: 0.8244
Epoch [10/30] Train Loss: 0.4937, Test Loss: 0.5359, F1: 0.6108, AUC: 0.8498
Epoch [20/30] Train Loss: 0.4822, Test Loss: 0.3971, F1: 0.6271, AUC: 0.8535
Mejores resultados en la época:  21
f1-score 0.6529324424647365
AUC según el mejor F1-score 0.8535186924578093
Confusion Matrix:
 [[14367  2098]
 [ 1642  3518]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8271
Precision:  0.6264
Recall:     0.6818
F1-score:   0.6529
Tiempo total para red 2: 926.72 segundos

Entrenando red 3 con capas [300, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6170, Test Loss: 0.4175, F1: 0.5582, AUC: 0.8279
Epoch [10/30] Train Loss: 0.4902, Test Loss: 0.4605, F1: 0.6418, AUC: 0.8513
Epoch [20/30] Train Loss: 0.4836, Test Loss: 0.4228, F1: 0.6547, AUC: 0.8545
Mejores resultados en la época:  20
f1-score 0.6547481737793156
AUC según el mejor F1-score 0.8545278980312949
Confusion Matrix:
 [[14627  1838]
 [ 1754  3406]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8339
Precision:  0.6495
Recall:     0.6601
F1-score:   0.6547

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6103, Test Loss: 0.6287, F1: 0.5485, AUC: 0.8303
Epoch [10/30] Train Loss: 0.4889, Test Loss: 0.4686, F1: 0.6410, AUC: 0.8514
Epoch [20/30] Train Loss: 0.4869, Test Loss: 0.4088, F1: 0.6442, AUC: 0.8543
Mejores resultados en la época:  25
f1-score 0.6540530157510565
AUC según el mejor F1-score 0.8557781246101079
Confusion Matrix:
 [[14618  1847]
 [ 1755  3405]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8334
Precision:  0.6483
Recall:     0.6599
F1-score:   0.6541

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6315, Test Loss: 0.4527, F1: 0.6180, AUC: 0.8263
Epoch [10/30] Train Loss: 0.4889, Test Loss: 0.4834, F1: 0.6356, AUC: 0.8506
Epoch [20/30] Train Loss: 0.4819, Test Loss: 0.4100, F1: 0.6522, AUC: 0.8536
Mejores resultados en la época:  28
f1-score 0.6544754571703562
AUC según el mejor F1-score 0.8552316577094472
Confusion Matrix:
 [[14635  1830]
 [ 1760  3400]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8340
Precision:  0.6501
Recall:     0.6589
F1-score:   0.6545

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6099, Test Loss: 0.5011, F1: 0.6083, AUC: 0.8282
Epoch [10/30] Train Loss: 0.4933, Test Loss: 0.4884, F1: 0.6368, AUC: 0.8505
Epoch [20/30] Train Loss: 0.4808, Test Loss: 0.5218, F1: 0.6271, AUC: 0.8541
Mejores resultados en la época:  29
f1-score 0.6521011364637477
AUC según el mejor F1-score 0.8555571661287628
Confusion Matrix:
 [[13975  2490]
 [ 1459  3701]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8174
Precision:  0.5978
Recall:     0.7172
F1-score:   0.6521
Tiempo total para red 3: 983.04 segundos

Entrenando red 4 con capas [300, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.5951, Test Loss: 0.4403, F1: 0.5150, AUC: 0.8316
Epoch [10/30] Train Loss: 0.4942, Test Loss: 0.4100, F1: 0.6271, AUC: 0.8511
Epoch [20/30] Train Loss: 0.4858, Test Loss: 0.5273, F1: 0.6233, AUC: 0.8541
Mejores resultados en la época:  25
f1-score 0.6539799072642968
AUC según el mejor F1-score 0.8557153887621617
Confusion Matrix:
 [[14658  1807]
 [ 1775  3385]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8344
Precision:  0.6520
Recall:     0.6560
F1-score:   0.6540

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6287, Test Loss: 0.5418, F1: 0.5900, AUC: 0.8282
Epoch [10/30] Train Loss: 0.4884, Test Loss: 0.4857, F1: 0.6339, AUC: 0.8513
Epoch [20/30] Train Loss: 0.4826, Test Loss: 0.5677, F1: 0.6055, AUC: 0.8537
Mejores resultados en la época:  29
f1-score 0.655992596020361
AUC según el mejor F1-score 0.8552685047210784
Confusion Matrix:
 [[14364  2101]
 [ 1616  3544]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8281
Precision:  0.6278
Recall:     0.6868
F1-score:   0.6560

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.5976, Test Loss: 0.4312, F1: 0.6166, AUC: 0.8305
Epoch [10/30] Train Loss: 0.4914, Test Loss: 0.5101, F1: 0.6226, AUC: 0.8525
Epoch [20/30] Train Loss: 0.4877, Test Loss: 0.6084, F1: 0.5827, AUC: 0.8539
Mejores resultados en la época:  27
f1-score 0.6575035063113605
AUC según el mejor F1-score 0.855412755975207
Confusion Matrix:
AUC según el mejor F1-score 0.4999031007751938
Confusion Matrix:
 [[    0 16465]
 [    0  5160]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.2386
Precision:  0.2386
Recall:     1.0000
F1-score:   0.3853

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6592, Test Loss: 0.6751, F1: 0.4938, AUC: 0.8209
Epoch [10/30] Train Loss: 0.4908, Test Loss: 0.4506, F1: 0.6417, AUC: 0.8489
Epoch [20/30] Train Loss: 0.4858, Test Loss: 0.4539, F1: 0.6461, AUC: 0.8539
Mejores resultados en la época:  25
f1-score 0.652349245052987
AUC según el mejor F1-score 0.8538754393274905
Confusion Matrix:
 [[14440  2025]
 [ 1682  3478]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8286
Precision:  0.6320
Recall:     0.6740
F1-score:   0.6523

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6581, Test Loss: 0.7122, F1: 0.4610, AUC: 0.8203
Epoch [10/30] Train Loss: 0.4893, Test Loss: 0.4630, F1: 0.6409, AUC: 0.8491
Epoch [20/30] Train Loss: 0.4829, Test Loss: 0.4964, F1: 0.6315, AUC: 0.8541
Mejores resultados en la época:  27
f1-score 0.6499598680103451
AUC según el mejor F1-score 0.8548982219742605
Confusion Matrix:
 [[14056  2409]
 [ 1516  3644]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8185
Precision:  0.6020
Recall:     0.7062
F1-score:   0.6500
Tiempo total para red 1: 861.86 segundos

Entrenando red 2 con capas [300, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6464, Test Loss: 0.5004, F1: 0.6121, AUC: 0.8226
Epoch [10/30] Train Loss: 0.4891, Test Loss: 0.4642, F1: 0.6404, AUC: 0.8500
Epoch [20/30] Train Loss: 0.4822, Test Loss: 0.5033, F1: 0.6279, AUC: 0.8536
Mejores resultados en la época:  29
f1-score 0.6541054770481204
AUC según el mejor F1-score 0.8556032646181588
Confusion Matrix:
 [[14339  2126]
 [ 1619  3541]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8268
Precision:  0.6248
Recall:     0.6862
F1-score:   0.6541

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6101, Test Loss: 0.6657, F1: 0.5287, AUC: 0.8283
Epoch [10/30] Train Loss: 0.4919, Test Loss: 0.4731, F1: 0.6396, AUC: 0.8511
Epoch [20/30] Train Loss: 0.4904, Test Loss: 0.4768, F1: 0.6421, AUC: 0.8540
Mejores resultados en la época:  17
f1-score 0.6528506954038868
AUC según el mejor F1-score 0.8534044967360881
Confusion Matrix:
 [[14312  2153]
 [ 1616  3544]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8257
Precision:  0.6221
Recall:     0.6868
F1-score:   0.6529

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6465, Test Loss: 0.4358, F1: 0.5077, AUC: 0.8216
Epoch [10/30] Train Loss: 0.4893, Test Loss: 0.4440, F1: 0.6421, AUC: 0.8491
Epoch [20/30] Train Loss: 0.4827, Test Loss: 0.4334, F1: 0.6508, AUC: 0.8536
Mejores resultados en la época:  24
f1-score 0.6544278512318082
AUC según el mejor F1-score 0.8539969620783575
Confusion Matrix:
 [[14552  1913]
 [ 1720  3440]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8320
Precision:  0.6426
Recall:     0.6667
F1-score:   0.6544

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.6174, Test Loss: 0.4751, F1: 0.6166, AUC: 0.8281
Epoch [10/30] Train Loss: 0.4874, Test Loss: 0.4516, F1: 0.6435, AUC: 0.8508
Epoch [20/30] Train Loss: 0.4813, Test Loss: 0.4577, F1: 0.6466, AUC: 0.8542
Mejores resultados en la época:  29
f1-score 0.6529015826814626
AUC según el mejor F1-score 0.8553989140695437
Confusion Matrix:
 [[14220  2245]
 [ 1571  3589]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8235
Precision:  0.6152
Recall:     0.6955
F1-score:   0.6529
Tiempo total para red 2: 926.66 segundos

Entrenando red 3 con capas [300, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6238, Test Loss: 0.4142, F1: 0.5064, AUC: 0.8269
Epoch [10/30] Train Loss: 0.4919, Test Loss: 0.5287, F1: 0.6164, AUC: 0.8507
Epoch [20/30] Train Loss: 0.4893, Test Loss: 0.4780, F1: 0.6415, AUC: 0.8538
Mejores resultados en la época:  26
f1-score 0.6525051537151564
AUC según el mejor F1-score 0.8551862948655476
Confusion Matrix:
 [[14108  2357]
 [ 1520  3640]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8207
Precision:  0.6070
Recall:     0.7054
F1-score:   0.6525

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6008, Test Loss: 0.5574, F1: 0.5842, AUC: 0.8297
Epoch [10/30] Train Loss: 0.4920, Test Loss: 0.4097, F1: 0.6431, AUC: 0.8512
Epoch [20/30] Train Loss: 0.4880, Test Loss: 0.4980, F1: 0.6370, AUC: 0.8539
Mejores resultados en la época:  21
f1-score 0.6544131367792498
AUC según el mejor F1-score 0.8541046782345451
Confusion Matrix:
 [[14414  2051]
 [ 1653  3507]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8287
Precision:  0.6310
Recall:     0.6797
F1-score:   0.6544

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6104, Test Loss: 0.4374, F1: 0.6115, AUC: 0.8278
Epoch [10/30] Train Loss: 0.4959, Test Loss: 0.3995, F1: 0.6246, AUC: 0.8513
Epoch [20/30] Train Loss: 0.4839, Test Loss: 0.4575, F1: 0.6503, AUC: 0.8540
Mejores resultados en la época:  24
f1-score 0.6549505425909921
AUC según el mejor F1-score 0.8549125523485335
Confusion Matrix:
 [[14622  1843]
 [ 1750  3410]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8338
Precision:  0.6492
Recall:     0.6609
F1-score:   0.6550

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.6186, Test Loss: 0.4362, F1: 0.5887, AUC: 0.8264
Epoch [10/30] Train Loss: 0.4894, Test Loss: 0.4587, F1: 0.6434, AUC: 0.8510
Epoch [20/30] Train Loss: 0.4840, Test Loss: 0.4544, F1: 0.6486, AUC: 0.8537
Mejores resultados en la época:  27
f1-score 0.656059204440333
AUC según el mejor F1-score 0.8550342634246475
Confusion Matrix:
 [[14361  2104]
 [ 1614  3546]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8281
Precision:  0.6276
Recall:     0.6872
F1-score:   0.6561
Tiempo total para red 3: 990.51 segundos

Entrenando red 4 con capas [300, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.5988, Test Loss: 0.5347, F1: 0.5974, AUC: 0.8309
Epoch [10/30] Train Loss: 0.4912, Test Loss: 0.5332, F1: 0.6199, AUC: 0.8513
Epoch [20/30] Train Loss: 0.4856, Test Loss: 0.3993, F1: 0.5646, AUC: 0.8540
Mejores resultados en la época:  23
f1-score 0.653790993402811
AUC según el mejor F1-score 0.8543957643297857
Confusion Matrix:
 [[14585  1880]
 [ 1741  3419]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8326
Precision:  0.6452
Recall:     0.6626
F1-score:   0.6538

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6158, Test Loss: 0.4413, F1: 0.6000, AUC: 0.8275
Epoch [10/30] Train Loss: 0.4904, Test Loss: 0.4721, F1: 0.6433, AUC: 0.8512
Epoch [20/30] Train Loss: 0.4805, Test Loss: 0.6690, F1: 0.5527, AUC: 0.8546
Mejores resultados en la época:  29
f1-score 0.6556541438550123
AUC según el mejor F1-score 0.8561305399991056
Confusion Matrix:
 [[14504  1961]
 [ 1687  3473]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8313
Precision:  0.6391
Recall:     0.6731
F1-score:   0.6557

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6053, Test Loss: 0.5844, F1: 0.5754, AUC: 0.8311
Epoch [10/30] Train Loss: 0.4941, Test Loss: 0.4224, F1: 0.6520, AUC: 0.8516
Epoch [20/30] Train Loss: 0.4838, Test Loss: 0.4449, F1: 0.6488, AUC: 0.8548
Mejores resultados en la época:  28
f1-score 0.6546242774566474
AUC según el mejor F1-score 0.8554014741158718
Confusion Matrix:
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
 [[14446  2019]
 [ 1644  3516]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8306
Precision:  0.6352
Recall:     0.6814
F1-score:   0.6575

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6197, Test Loss: 0.4407, F1: 0.6044, AUC: 0.8296
Epoch [10/30] Train Loss: 0.4893, Test Loss: 0.6111, F1: 0.5772, AUC: 0.8517
Epoch [20/30] Train Loss: 0.4868, Test Loss: 0.4715, F1: 0.6432, AUC: 0.8538
Mejores resultados en la época:  22
f1-score 0.6558996769903097
AUC según el mejor F1-score 0.8549067848878404
Confusion Matrix:
 [[14551  1914]
 [ 1708  3452]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8325
Precision:  0.6433
Recall:     0.6690
F1-score:   0.6559
Tiempo total para red 4: 1016.51 segundos

Entrenando red 5 con capas [300, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.5934, Test Loss: 0.4740, F1: 0.6237, AUC: 0.8306
Epoch [10/30] Train Loss: 0.4913, Test Loss: 0.4812, F1: 0.6403, AUC: 0.8517
Epoch [20/30] Train Loss: 0.4864, Test Loss: 0.7510, F1: 0.5256, AUC: 0.8545
Mejores resultados en la época:  29
f1-score 0.6549054199745173
AUC según el mejor F1-score 0.8563722377982895
Confusion Matrix:
 [[14763  1702]
 [ 1819  3341]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8372
Precision:  0.6625
Recall:     0.6475
F1-score:   0.6549

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.5863, Test Loss: 0.6257, F1: 0.5598, AUC: 0.8334
Epoch [10/30] Train Loss: 0.5017, Test Loss: 0.4114, F1: 0.6186, AUC: 0.8509
Epoch [20/30] Train Loss: 0.4821, Test Loss: 0.4257, F1: 0.6550, AUC: 0.8552
Mejores resultados en la época:  20
f1-score 0.655045871559633
AUC según el mejor F1-score 0.8551954521806886
Confusion Matrix:
 [[14295  2170]
 [ 1590  3570]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8261
Precision:  0.6220
Recall:     0.6919
F1-score:   0.6550

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.5871, Test Loss: 0.4136, F1: 0.5148, AUC: 0.8340
Epoch [10/30] Train Loss: 0.4928, Test Loss: 0.5370, F1: 0.6128, AUC: 0.8516
Epoch [20/30] Train Loss: 0.4869, Test Loss: 0.6817, F1: 0.5402, AUC: 0.8546
Mejores resultados en la época:  19
f1-score 0.6533987116623402
AUC según el mejor F1-score 0.8538359969585472
Confusion Matrix:
 [[14622  1843]
 [ 1762  3398]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8333
Precision:  0.6483
Recall:     0.6585
F1-score:   0.6534

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.5921, Test Loss: 0.6123, F1: 0.5611, AUC: 0.8329
Epoch [10/30] Train Loss: 0.4875, Test Loss: 0.4822, F1: 0.6381, AUC: 0.8516
Epoch [20/30] Train Loss: 0.4868, Test Loss: 0.5571, F1: 0.6070, AUC: 0.8548
Mejores resultados en la época:  22
f1-score 0.6534580810860393
AUC según el mejor F1-score 0.8558209568334992
Confusion Matrix:
 [[14285  2180]
 [ 1598  3562]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8253
Precision:  0.6203
Recall:     0.6903
F1-score:   0.6535
Tiempo total para red 5: 1073.48 segundos

Entrenando red 6 con capas [300, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6928, Test Loss: 0.6888, F1: 0.0004, AUC: 0.8092
Epoch [10/30] Train Loss: 0.4929, Test Loss: 0.5519, F1: 0.5971, AUC: 0.8495
Epoch [20/30] Train Loss: 0.4859, Test Loss: 0.5101, F1: 0.6209, AUC: 0.8534
Mejores resultados en la época:  29
f1-score 0.6550169109357384
AUC según el mejor F1-score 0.8548824732754704
Confusion Matrix:
 [[14467  1998]
 [ 1674  3486]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8302
Precision:  0.6357
Recall:     0.6756
F1-score:   0.6550

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6493, Test Loss: 0.5739, F1: 0.5716, AUC: 0.8245
Epoch [10/30] Train Loss: 0.4928, Test Loss: 0.5693, F1: 0.5990, AUC: 0.8512
Epoch [20/30] Train Loss: 0.4857, Test Loss: 0.4876, F1: 0.6383, AUC: 0.8541
Mejores resultados en la época:  22
f1-score 0.6525649896214293
AUC según el mejor F1-score 0.8538825603758972
Confusion Matrix:
 [[14809  1656]
 [ 1859  3301]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8375
Precision:  0.6659
Recall:     0.6397
F1-score:   0.6526

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6162, Test Loss: 0.6211, F1: 0.5443, AUC: 0.8303
Epoch [10/30] Train Loss: 0.4924, Test Loss: 0.4792, F1: 0.6459, AUC: 0.8518
Epoch [20/30] Train Loss: 0.4813, Test Loss: 0.5284, F1: 0.6245, AUC: 0.8544
Mejores resultados en la época:  17
f1-score 0.6535205848403232
AUC según el mejor F1-score 0.8539429421582544
Confusion Matrix:
 [[14626  1839]
 [ 1763  3397]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8334
Precision:  0.6488
Recall:     0.6583
F1-score:   0.6535

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6387, Test Loss: 0.5310, F1: 0.5979, AUC: 0.8252
Epoch [10/30] Train Loss: 0.4936, Test Loss: 0.4245, F1: 0.6497, AUC: 0.8511
Epoch [20/30] Train Loss: 0.4884, Test Loss: 0.4381, F1: 0.6499, AUC: 0.8537
Mejores resultados en la época:  27
f1-score 0.650866361282613
AUC según el mejor F1-score 0.8555422472380926
Confusion Matrix:
 [[14851  1614]
 [ 1892  3268]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8379
Precision:  0.6694
Recall:     0.6333
F1-score:   0.6509
Tiempo total para red 6: 1209.66 segundos
Saved on: outputs_only_text/0/lyrics_bert

==============================
Model: Logistic Regression
Accuracy:  0.8054
Precision: 0.5696
Recall:    0.7548
F1-score:  0.6493
              precision    recall  f1-score   support

           0       0.91      0.82      0.87     16465
           1       0.57      0.75      0.65      5160

    accuracy                           0.81     21625
   macro avg       0.74      0.79      0.76     21625
weighted avg       0.83      0.81      0.81     21625

[[13522  2943]
 [ 1265  3895]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.3293
Precision: 0.2559
Recall:    0.9492
F1-score:  0.4031
              precision    recall  f1-score   support

           0       0.89      0.14      0.23     16465
           1       0.26      0.95      0.40      5160

    accuracy                           0.33     21625
   macro avg       0.58      0.54      0.32     21625
weighted avg       0.74      0.33      0.27     21625

[[ 2223 14242]
 [  262  4898]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_svm.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6610
Precision: 0.3885
Recall:    0.7331
F1-score:  0.5079
              precision    recall  f1-score   support

           0       0.88      0.64      0.74     16465
           1       0.39      0.73      0.51      5160

    accuracy                           0.66     21625
   macro avg       0.64      0.69      0.62     21625
weighted avg       0.77      0.66      0.69     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
 [[14177  2288]
 [ 1536  3624]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8232
Precision:  0.6130
Recall:     0.7023
F1-score:   0.6546

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.6094, Test Loss: 0.5879, F1: 0.5678, AUC: 0.8296
Epoch [10/30] Train Loss: 0.4921, Test Loss: 0.4109, F1: 0.6469, AUC: 0.8514
Epoch [20/30] Train Loss: 0.4844, Test Loss: 0.4345, F1: 0.6528, AUC: 0.8548
Mejores resultados en la época:  27
f1-score 0.6570102135561745
AUC según el mejor F1-score 0.8557072790062077
Confusion Matrix:
 [[14393  2072]
 [ 1622  3538]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8292
Precision:  0.6307
Recall:     0.6857
F1-score:   0.6570
Tiempo total para red 4: 1013.31 segundos

Entrenando red 5 con capas [300, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6013, Test Loss: 0.4734, F1: 0.6230, AUC: 0.8306
Epoch [10/30] Train Loss: 0.4913, Test Loss: 0.4922, F1: 0.6359, AUC: 0.8512
Epoch [20/30] Train Loss: 0.4836, Test Loss: 0.4888, F1: 0.6380, AUC: 0.8540
Mejores resultados en la época:  29
f1-score 0.6561445123620108
AUC según el mejor F1-score 0.8562376911795516
Confusion Matrix:
 [[14260  2205]
 [ 1564  3596]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8257
Precision:  0.6199
Recall:     0.6969
F1-score:   0.6561

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.5992, Test Loss: 0.4883, F1: 0.6223, AUC: 0.8326
Epoch [10/30] Train Loss: 0.4938, Test Loss: 0.4850, F1: 0.6334, AUC: 0.8520
Epoch [20/30] Train Loss: 0.4848, Test Loss: 0.4599, F1: 0.6483, AUC: 0.8542
Mejores resultados en la época:  25
f1-score 0.6527680798004988
AUC según el mejor F1-score 0.8549714451844057
Confusion Matrix:
 [[14872  1593]
 [ 1888  3272]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8390
Precision:  0.6726
Recall:     0.6341
F1-score:   0.6528

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6167, Test Loss: 0.4237, F1: 0.5994, AUC: 0.8296
Epoch [10/30] Train Loss: 0.4908, Test Loss: 0.4347, F1: 0.6469, AUC: 0.8515
Epoch [20/30] Train Loss: 0.4833, Test Loss: 0.4069, F1: 0.6458, AUC: 0.8545
Mejores resultados en la época:  28
f1-score 0.6545752825373679
AUC según el mejor F1-score 0.8554816123936845
Confusion Matrix:
 [[14244  2221]
 [ 1569  3591]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8247
Precision:  0.6179
Recall:     0.6959
F1-score:   0.6546

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.6040, Test Loss: 0.6800, F1: 0.5273, AUC: 0.8281
Epoch [10/30] Train Loss: 0.4944, Test Loss: 0.4283, F1: 0.6506, AUC: 0.8511
Epoch [20/30] Train Loss: 0.4847, Test Loss: 0.4246, F1: 0.6554, AUC: 0.8541
Mejores resultados en la época:  20
f1-score 0.6554034922133082
AUC según el mejor F1-score 0.8540794956179069
Confusion Matrix:
 [[14502  1963]
 [ 1688  3472]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8312
Precision:  0.6388
Recall:     0.6729
F1-score:   0.6554
Tiempo total para red 5: 1083.26 segundos

Entrenando red 6 con capas [300, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6127, Test Loss: 0.5202, F1: 0.6044, AUC: 0.8274
Epoch [10/30] Train Loss: 0.4925, Test Loss: 0.4229, F1: 0.6489, AUC: 0.8511
Epoch [20/30] Train Loss: 0.4862, Test Loss: 0.4982, F1: 0.6306, AUC: 0.8542
Mejores resultados en la época:  24
f1-score 0.657244477723699
AUC según el mejor F1-score 0.855025576922624
Confusion Matrix:
 [[14452  2013]
 [ 1649  3511]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8307
Precision:  0.6356
Recall:     0.6804
F1-score:   0.6572

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6378, Test Loss: 0.5554, F1: 0.5948, AUC: 0.8283
Epoch [10/30] Train Loss: 0.4988, Test Loss: 0.6355, F1: 0.5743, AUC: 0.8511
Epoch [20/30] Train Loss: 0.4918, Test Loss: 0.4188, F1: 0.6543, AUC: 0.8536
Mejores resultados en la época:  26
f1-score 0.6545736734891842
AUC según el mejor F1-score 0.8546303057695794
Confusion Matrix:
 [[14690  1775]
 [ 1786  3374]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8353
Precision:  0.6553
Recall:     0.6539
F1-score:   0.6546

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6859, Test Loss: 0.6573, F1: 0.0452, AUC: 0.8177
Epoch [10/30] Train Loss: 0.4949, Test Loss: 0.4216, F1: 0.6518, AUC: 0.8507
Epoch [20/30] Train Loss: 0.4864, Test Loss: 0.4228, F1: 0.6550, AUC: 0.8544
Mejores resultados en la época:  29
f1-score 0.6567749160134378
AUC según el mejor F1-score 0.8560974065259406
Confusion Matrix:
 [[14428  2037]
 [ 1641  3519]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8299
Precision:  0.6334
Recall:     0.6820
F1-score:   0.6568

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6492, Test Loss: 0.5114, F1: 0.6057, AUC: 0.8254
Epoch [10/30] Train Loss: 0.4928, Test Loss: 0.5025, F1: 0.6351, AUC: 0.8509
Epoch [20/30] Train Loss: 0.4889, Test Loss: 0.5620, F1: 0.6099, AUC: 0.8542
Mejores resultados en la época:  25
f1-score 0.6565761022844787
AUC según el mejor F1-score 0.8549917843110945
Confusion Matrix:
 [[14480  1985]
 [ 1668  3492]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8311
Precision:  0.6376
Recall:     0.6767
F1-score:   0.6566
Tiempo total para red 6: 1194.44 segundos
Saved on: outputs_only_text/0/lyrics_bert

==============================
Model: Logistic Regression
Accuracy:  0.8054
Precision: 0.5696
Recall:    0.7548
F1-score:  0.6493
              precision    recall  f1-score   support

           0       0.91      0.82      0.87     16465
           1       0.57      0.75      0.65      5160

    accuracy                           0.81     21625
   macro avg       0.74      0.79      0.76     21625
weighted avg       0.83      0.81      0.81     21625

[[13522  2943]
 [ 1265  3895]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.3293
Precision: 0.2559
Recall:    0.9492
F1-score:  0.4031
              precision    recall  f1-score   support

           0       0.89      0.14      0.23     16465
           1       0.26      0.95      0.40      5160

    accuracy                           0.33     21625
   macro avg       0.58      0.54      0.32     21625
weighted avg       0.74      0.33      0.27     21625

[[ 2223 14242]
 [  262  4898]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_svm.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6610
Precision: 0.3885
Recall:    0.7331
F1-score:  0.5079
              precision    recall  f1-score   support

           0       0.88      0.64      0.74     16465
           1       0.39      0.73      0.51      5160

    accuracy                           0.66     21625
   macro avg       0.64      0.69      0.62     21625
weighted avg       0.77      0.66      0.69     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:24:59] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [05:25:57] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[10511  5954]
 [ 1377  3783]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8063
Precision: 0.5772
Recall:    0.7031
F1-score:  0.6340
              precision    recall  f1-score   support

           0       0.90      0.84      0.87     16465
           1       0.58      0.70      0.63      5160

    accuracy                           0.81     21625
   macro avg       0.74      0.77      0.75     21625
weighted avg       0.82      0.81      0.81     21625

[[13808  2657]
 [ 1532  3628]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8068
Precision: 0.5741
Recall:    0.7364
F1-score:  0.6452
              precision    recall  f1-score   support

           0       0.91      0.83      0.87     16465
           1       0.57      0.74      0.65      5160

    accuracy                           0.81     21625
   macro avg       0.74      0.78      0.76     21625
weighted avg       0.83      0.81      0.81     21625

[[13646  2819]
 [ 1360  3800]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6437
Precision: 0.3792
Recall:    0.7742
F1-score:  0.5090
              precision    recall  f1-score   support

           0       0.89      0.60      0.72     16465
           1       0.38      0.77      0.51      5160

    accuracy                           0.64     21625
   macro avg       0.64      0.69      0.61     21625
weighted avg       0.77      0.64      0.67     21625

[[9924 6541]
 [1165 3995]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.8054, 'precision': 0.5696, 'recall': 0.7548, 'f1_score': 0.6493}
XGBoost: {'accuracy': 0.8068, 'precision': 0.5741, 'recall': 0.7364, 'f1_score': 0.6452}
Random Forest: {'accuracy': 0.8063, 'precision': 0.5772, 'recall': 0.7031, 'f1_score': 0.634}
Naive Bayes: {'accuracy': 0.6437, 'precision': 0.3792, 'recall': 0.7742, 'f1_score': 0.509}
Decision Tree: {'accuracy': 0.661, 'precision': 0.3885, 'recall': 0.7331, 'f1_score': 0.5079}
SVM: {'accuracy': 0.3293, 'precision': 0.2559, 'recall': 0.9492, 'f1_score': 0.4031}

##################################################
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1536)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1536)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1536, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5529, Test Loss: 0.5799, F1: 0.6679, AUC: 0.8760
Epoch [10/30] Train Loss: 0.4133, Test Loss: 0.3631, F1: 0.7107, AUC: 0.8946
Epoch [20/30] Train Loss: 0.4065, Test Loss: 0.3697, F1: 0.7243, AUC: 0.8965
Mejores resultados en la época:  28
f1-score 0.7265356481889487
AUC según el mejor F1-score 0.8977467119588888
Confusion Matrix:
 [[14400  2065]
 [ 1038  4122]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8565
Precision:  0.6662
Recall:     0.7988
F1-score:   0.7265

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5114, Test Loss: 0.4817, F1: 0.6729, AUC: 0.8762
Epoch [10/30] Train Loss: 0.3936, Test Loss: 0.4073, F1: 0.7149, AUC: 0.9008
Epoch [20/30] Train Loss: 0.3850, Test Loss: 0.3616, F1: 0.7275, AUC: 0.9052
Mejores resultados en la época:  28
f1-score 0.7304052847705766
AUC según el mejor F1-score 0.9059774786545101
Confusion Matrix:
 [[14514  1951]
 [ 1069  4091]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8603
Precision:  0.6771
Recall:     0.7928
F1-score:   0.7304

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5062, Test Loss: 0.3964, F1: 0.6898, AUC: 0.8772
Epoch [10/30] Train Loss: 0.3926, Test Loss: 0.6935, F1: 0.5446, AUC: 0.8969
Epoch [20/30] Train Loss: 0.3837, Test Loss: 0.3718, F1: 0.7264, AUC: 0.9054
Mejores resultados en la época:  28
f1-score 0.7325167231343932
AUC según el mejor F1-score 0.9055775641070912
Confusion Matrix:
 [[14330  2135]
 [  944  4216]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8576
Precision:  0.6638
Recall:     0.8171
F1-score:   0.7325

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4968, Test Loss: 0.4117, F1: 0.6917, AUC: 0.8789
Epoch [10/30] Train Loss: 0.3930, Test Loss: 0.3760, F1: 0.7252, AUC: 0.9018
Epoch [20/30] Train Loss: 0.3836, Test Loss: 0.4151, F1: 0.7098, AUC: 0.9051
Mejores resultados en la época:  29
f1-score 0.7311199036642009
AUC según el mejor F1-score 0.9062842663672295
Confusion Matrix:
 [[14249  2216]
 [  910  4250]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8554
Precision:  0.6573
Recall:     0.8236
F1-score:   0.7311
Tiempo total para red 1: 844.71 segundos

Entrenando red 2 con capas [1536, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4721, Test Loss: 0.4248, F1: 0.6971, AUC: 0.8862
Epoch [10/30] Train Loss: 0.3901, Test Loss: 0.3446, F1: 0.7291, AUC: 0.9044
Epoch [20/30] Train Loss: 0.3832, Test Loss: 0.3534, F1: 0.7329, AUC: 0.9072
Mejores resultados en la época:  26

[[10511  5954]
 [ 1377  3783]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8063
Precision: 0.5772
Recall:    0.7031
F1-score:  0.6340
              precision    recall  f1-score   support

           0       0.90      0.84      0.87     16465
           1       0.58      0.70      0.63      5160

    accuracy                           0.81     21625
   macro avg       0.74      0.77      0.75     21625
weighted avg       0.82      0.81      0.81     21625

[[13808  2657]
 [ 1532  3628]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8068
Precision: 0.5741
Recall:    0.7364
F1-score:  0.6452
              precision    recall  f1-score   support

           0       0.91      0.83      0.87     16465
           1       0.57      0.74      0.65      5160

    accuracy                           0.81     21625
   macro avg       0.74      0.78      0.76     21625
weighted avg       0.83      0.81      0.81     21625

[[13646  2819]
 [ 1360  3800]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6437
Precision: 0.3792
Recall:    0.7742
F1-score:  0.5090
              precision    recall  f1-score   support

           0       0.89      0.60      0.72     16465
           1       0.38      0.77      0.51      5160

    accuracy                           0.64     21625
   macro avg       0.64      0.69      0.61     21625
weighted avg       0.77      0.64      0.67     21625

[[9924 6541]
 [1165 3995]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.8054, 'precision': 0.5696, 'recall': 0.7548, 'f1_score': 0.6493}
XGBoost: {'accuracy': 0.8068, 'precision': 0.5741, 'recall': 0.7364, 'f1_score': 0.6452}
Random Forest: {'accuracy': 0.8063, 'precision': 0.5772, 'recall': 0.7031, 'f1_score': 0.634}
Naive Bayes: {'accuracy': 0.6437, 'precision': 0.3792, 'recall': 0.7742, 'f1_score': 0.509}
Decision Tree: {'accuracy': 0.661, 'precision': 0.3885, 'recall': 0.7331, 'f1_score': 0.5079}
SVM: {'accuracy': 0.3293, 'precision': 0.2559, 'recall': 0.9492, 'f1_score': 0.4031}

##################################################
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1536)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1536)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1536, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4726, Test Loss: 0.4226, F1: 0.7048, AUC: 0.8851
Epoch [10/30] Train Loss: 0.4110, Test Loss: 0.3859, F1: 0.7250, AUC: 0.8958
Epoch [20/30] Train Loss: 0.4061, Test Loss: 0.4441, F1: 0.7142, AUC: 0.8971
Mejores resultados en la época:  25
f1-score 0.7275344067644148
AUC según el mejor F1-score 0.8986703943295268
Confusion Matrix:
 [[14552  1913]
 [ 1116  4044]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8599
Precision:  0.6789
Recall:     0.7837
F1-score:   0.7275

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4711, Test Loss: 0.4372, F1: 0.6957, AUC: 0.8846
Epoch [10/30] Train Loss: 0.3943, Test Loss: 0.3488, F1: 0.7284, AUC: 0.9018
Epoch [20/30] Train Loss: 0.3855, Test Loss: 0.4336, F1: 0.7053, AUC: 0.9058
Mejores resultados en la época:  29
f1-score 0.7332749562171629
AUC según el mejor F1-score 0.905750587927881
Confusion Matrix:
 [[14392  2073]
 [  973  4187]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8591
Precision:  0.6688
Recall:     0.8114
F1-score:   0.7333

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4805, Test Loss: 0.5264, F1: 0.6539, AUC: 0.8834
Epoch [10/30] Train Loss: 0.3955, Test Loss: 0.4516, F1: 0.6884, AUC: 0.9011
Epoch [20/30] Train Loss: 0.3845, Test Loss: 0.3831, F1: 0.7285, AUC: 0.9040
Mejores resultados en la época:  23
f1-score 0.7302998605299861
AUC según el mejor F1-score 0.9046367500241292
Confusion Matrix:
 [[14342  2123]
 [  971  4189]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8569
Precision:  0.6637
Recall:     0.8118
F1-score:   0.7303

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4834, Test Loss: 0.4033, F1: 0.7043, AUC: 0.8841
Epoch [10/30] Train Loss: 0.3931, Test Loss: 0.3913, F1: 0.7225, AUC: 0.9026
Epoch [20/30] Train Loss: 0.3846, Test Loss: 0.3343, F1: 0.7252, AUC: 0.9049
Mejores resultados en la época:  28
f1-score 0.7339581831290555
AUC según el mejor F1-score 0.9063045819532624
Confusion Matrix:
 [[14601  1864]
 [ 1088  4072]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8635
Precision:  0.6860
Recall:     0.7891
F1-score:   0.7340
Tiempo total para red 1: 842.26 segundos

Entrenando red 2 con capas [1536, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4670, Test Loss: 0.4548, F1: 0.6900, AUC: 0.8865
Epoch [10/30] Train Loss: 0.3936, Test Loss: 0.3932, F1: 0.7195, AUC: 0.9050
Epoch [20/30] Train Loss: 0.3820, Test Loss: 0.3509, F1: 0.7315, AUC: 0.9061
Mejores resultados en la época:  18
f1-score 0.7346867343367168
AUC según el mejor F1-score 0.9077700113230555
Confusion Matrix:
 [[14395  2070]
 [  962  4198]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8598
Precision:  0.6698
Recall:     0.8136
F1-score:   0.7347

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4664, Test Loss: 0.3833, F1: 0.7087, AUC: 0.8874
Epoch [10/30] Train Loss: 0.3893, Test Loss: 0.3331, F1: 0.7186, AUC: 0.9036
Epoch [20/30] Train Loss: 0.3815, Test Loss: 0.4314, F1: 0.6990, AUC: 0.9062
Mejores resultados en la época:  29
f1-score 0.7349581796924184
AUC según el mejor F1-score 0.9072197779174522
Confusion Matrix:
 [[14592  1873]
 [ 1074  4086]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8637
Precision:  0.6857
Recall:     0.7919
F1-score:   0.7350

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4688, Test Loss: 0.3601, F1: 0.6988, AUC: 0.8869
Epoch [10/30] Train Loss: 0.3929, Test Loss: 0.3967, F1: 0.7127, AUC: 0.9047
Epoch [20/30] Train Loss: 0.3832, Test Loss: 0.3821, F1: 0.7280, AUC: 0.9069
Mejores resultados en la época:  23
f1-score 0.7340851218624954
AUC según el mejor F1-score 0.9068671271218959
Confusion Matrix:
 [[14665  1800]
 [ 1124  4036]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8648
Precision:  0.6916
Recall:     0.7822
F1-score:   0.7341

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4666, Test Loss: 0.3731, F1: 0.7072, AUC: 0.8873
Epoch [10/30] Train Loss: 0.3913, Test Loss: 0.3359, F1: 0.7290, AUC: 0.9049
Epoch [20/30] Train Loss: 0.3805, Test Loss: 0.4169, F1: 0.7094, AUC: 0.9072
Mejores resultados en la época:  19
f1-score 0.7326469021835612
AUC según el mejor F1-score 0.9069625374002172
Confusion Matrix:
 [[14486  1979]
 [ 1033  4127]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8607
Precision:  0.6759
Recall:     0.7998
F1-score:   0.7326
Tiempo total para red 2: 797.71 segundos

Entrenando red 3 con capas [1536, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4581, Test Loss: 0.3563, F1: 0.7065, AUC: 0.8891
Epoch [10/30] Train Loss: 0.3961, Test Loss: 0.5685, F1: 0.6336, AUC: 0.9016
Epoch [20/30] Train Loss: 0.3855, Test Loss: 0.3385, F1: 0.7317, AUC: 0.9065
Mejores resultados en la época:  14
f1-score 0.7326226764161747
AUC según el mejor F1-score 0.9063816128644977
Confusion Matrix:
 [[14432  2033]
 [ 1002  4158]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8597
Precision:  0.6716
Recall:     0.8058
F1-score:   0.7326

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4658, Test Loss: 0.4277, F1: 0.6998, AUC: 0.8879
Epoch [10/30] Train Loss: 0.3965, Test Loss: 0.4104, F1: 0.7127, AUC: 0.9046
Epoch [20/30] Train Loss: 0.3855, Test Loss: 0.4333, F1: 0.7096, AUC: 0.9062
Mejores resultados en la época:  24
f1-score 0.7328033183546492
AUC según el mejor F1-score 0.9071576658968874
Confusion Matrix:
 [[14293  2172]
 [  920  4240]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8570
Precision:  0.6613
Recall:     0.8217
F1-score:   0.7328

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4681, Test Loss: 0.4365, F1: 0.6970, AUC: 0.8870
Epoch [10/30] Train Loss: 0.3929, Test Loss: 0.3393, F1: 0.7284, AUC: 0.9048
Epoch [20/30] Train Loss: 0.3800, Test Loss: 0.3471, F1: 0.7247, AUC: 0.9062
Mejores resultados en la época:  29
f1-score 0.7363741442162355
AUC según el mejor F1-score 0.9080887576889668
Confusion Matrix:
 [[14519  1946]
 [ 1019  4141]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8629
Precision:  0.6803
Recall:     0.8025
F1-score:   0.7364

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4626, Test Loss: 0.3512, F1: 0.6944, AUC: 0.8891
Epoch [10/30] Train Loss: 0.3925, Test Loss: 0.3825, F1: 0.7221, AUC: 0.9050
Epoch [20/30] Train Loss: 0.3831, Test Loss: 0.3508, F1: 0.7333, AUC: 0.9068
Mejores resultados en la época:  25
f1-score 0.7358277628266342
AUC según el mejor F1-score 0.9078899156538299
Confusion Matrix:
 [[14344  2121]
 [  922  4238]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8593
Precision:  0.6665
Recall:     0.8213
F1-score:   0.7358
Tiempo total para red 3: 806.51 segundos

Entrenando red 4 con capas [1536, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4635, Test Loss: 0.4811, F1: 0.6730, AUC: 0.8889
Epoch [10/30] Train Loss: 0.3932, Test Loss: 0.4666, F1: 0.6922, AUC: 0.9037
Epoch [20/30] Train Loss: 0.3828, Test Loss: 0.3893, F1: 0.7295, AUC: 0.9065
Mejores resultados en la época:  25
f1-score 0.7352167046850324
AUC según el mejor F1-score 0.9076258660018787
Confusion Matrix:
 [[14417  2048]
 [  970  4190]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8604
Precision:  0.6717
Recall:     0.8120
F1-score:   0.7352

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4701, Test Loss: 0.4088, F1: 0.7093, AUC: 0.8872
Epoch [10/30] Train Loss: 0.3909, Test Loss: 0.3573, F1: 0.7297, AUC: 0.9054
Epoch [20/30] Train Loss: 0.3834, Test Loss: 0.3415, F1: 0.7319, AUC: 0.9066
Mejores resultados en la época:  22
f1-score 0.7338869832099657
AUC según el mejor F1-score 0.9066406777825643
Confusion Matrix:
 [[14612  1853]
 [ 1095  4065]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8637
Precision:  0.6869
Recall:     0.7878
F1-score:   0.7339

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4655, Test Loss: 0.4087, F1: 0.7082, AUC: 0.8882
Epoch [10/30] Train Loss: 0.3951, Test Loss: 0.3594, F1: 0.7288, AUC: 0.9046
Epoch [20/30] Train Loss: 0.3879, Test Loss: 0.3575, F1: 0.7313, AUC: 0.9064
Mejores resultados en la época:  28
f1-score 0.7342381034186566
AUC según el mejor F1-score 0.9078518151022724
Confusion Matrix:
 [[14257  2208]
 [  886  4274]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8569
Precision:  0.6594
Recall:     0.8283
F1-score:   0.7342

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4693, Test Loss: 0.3632, F1: 0.7104, AUC: 0.8882
Epoch [10/30] Train Loss: 0.3916, Test Loss: 0.4243, F1: 0.6940, AUC: 0.9047
Epoch [20/30] Train Loss: 0.3818, Test Loss: 0.4631, F1: 0.7055, AUC: 0.9066
Mejores resultados en la época:  27
f1-score 0.7348725431293451
AUC según el mejor F1-score 0.907821794880849
Confusion Matrix:
 [[14255  2210]
 [  879  4281]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8572
Precision:  0.6595
Recall:     0.8297
F1-score:   0.7349
Tiempo total para red 4: 836.46 segundos

Entrenando red 5 con capas [1536, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4702, Test Loss: 0.3630, F1: 0.7078, AUC: 0.8876
Epoch [10/30] Train Loss: 0.3951, Test Loss: 0.3334, F1: 0.7181, AUC: 0.9037
Epoch [20/30] Train Loss: 0.3826, Test Loss: 0.4081, F1: 0.7015, AUC: 0.9062
Mejores resultados en la época:  29
f1-score 0.7343848580441641
AUC según el mejor F1-score 0.9072263575307735
Confusion Matrix:
 [[14604  1861]
 [ 1086  4074]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8637
Precision:  0.6864
Recall:     0.7895
F1-score:   0.7344

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4749, Test Loss: 0.4095, F1: 0.7003, AUC: 0.8871
Epoch [10/30] Train Loss: 0.3958, Test Loss: 0.3376, F1: 0.7164, AUC: 0.9031
Epoch [20/30] Train Loss: 0.3874, Test Loss: 0.3970, F1: 0.7114, AUC: 0.9060
Mejores resultados en la época:  23
f1-score 0.7358256555634302
AUC según el mejor F1-score 0.9070262737260386
Confusion Matrix:
 [[14490  1975]
f1-score 0.7322146477888481
AUC según el mejor F1-score 0.9067147543414855
Confusion Matrix:
 [[14372  2093]
 [  971  4189]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8583
Precision:  0.6668
Recall:     0.8118
F1-score:   0.7322

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4576, Test Loss: 0.4142, F1: 0.7036, AUC: 0.8885
Epoch [10/30] Train Loss: 0.3936, Test Loss: 0.4244, F1: 0.7007, AUC: 0.9048
Epoch [20/30] Train Loss: 0.3855, Test Loss: 0.5017, F1: 0.6510, AUC: 0.9028
Mejores resultados en la época:  17
f1-score 0.7332679796809554
AUC según el mejor F1-score 0.906094328585183
Confusion Matrix:
 [[14518  1947]
 [ 1046  4114]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8616
Precision:  0.6788
Recall:     0.7973
F1-score:   0.7333

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4701, Test Loss: 0.5045, F1: 0.6612, AUC: 0.8870
Epoch [10/30] Train Loss: 0.3938, Test Loss: 0.4286, F1: 0.7068, AUC: 0.9040
Epoch [20/30] Train Loss: 0.3850, Test Loss: 0.4227, F1: 0.7125, AUC: 0.9070
Mejores resultados en la época:  21
f1-score 0.7351099982114112
AUC según el mejor F1-score 0.9067953222362681
Confusion Matrix:
 [[14553  1912]
 [ 1050  4110]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8630
Precision:  0.6825
Recall:     0.7965
F1-score:   0.7351

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4692, Test Loss: 0.3700, F1: 0.7065, AUC: 0.8862
Epoch [10/30] Train Loss: 0.3887, Test Loss: 0.3466, F1: 0.7303, AUC: 0.9047
Epoch [20/30] Train Loss: 0.3832, Test Loss: 0.3974, F1: 0.7139, AUC: 0.9068
Mejores resultados en la época:  28
f1-score 0.7383334809173825
AUC según el mejor F1-score 0.9075219045803053
Confusion Matrix:
 [[14501  1964]
 [  991  4169]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8634
Precision:  0.6798
Recall:     0.8079
F1-score:   0.7383
Tiempo total para red 2: 804.07 segundos

Entrenando red 3 con capas [1536, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4608, Test Loss: 0.3831, F1: 0.7132, AUC: 0.8889
Epoch [10/30] Train Loss: 0.3931, Test Loss: 0.3965, F1: 0.7188, AUC: 0.9044
Epoch [20/30] Train Loss: 0.3839, Test Loss: 0.3701, F1: 0.7323, AUC: 0.9069
Mejores resultados en la época:  25
f1-score 0.7356801390699695
AUC según el mejor F1-score 0.907431473150705
Confusion Matrix:
 [[14352  2113]
 [  928  4232]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8594
Precision:  0.6670
Recall:     0.8202
F1-score:   0.7357

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4692, Test Loss: 0.3700, F1: 0.7074, AUC: 0.8869
Epoch [10/30] Train Loss: 0.3955, Test Loss: 0.3666, F1: 0.7276, AUC: 0.9046
Epoch [20/30] Train Loss: 0.3824, Test Loss: 0.3568, F1: 0.7325, AUC: 0.9069
Mejores resultados en la época:  27
f1-score 0.7334616730836542
AUC según el mejor F1-score 0.9074147004333835
Confusion Matrix:
 [[14388  2077]
 [  969  4191]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8591
Precision:  0.6686
Recall:     0.8122
F1-score:   0.7335

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4681, Test Loss: 0.4523, F1: 0.6843, AUC: 0.8875
Epoch [10/30] Train Loss: 0.3940, Test Loss: 0.4357, F1: 0.6962, AUC: 0.9048
Epoch [20/30] Train Loss: 0.3830, Test Loss: 0.3788, F1: 0.7292, AUC: 0.9071
Mejores resultados en la época:  27
f1-score 0.7345775012960083
AUC según el mejor F1-score 0.9075078213829194
Confusion Matrix:
 [[14302  2163]
 [  909  4251]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8579
Precision:  0.6628
Recall:     0.8238
F1-score:   0.7346

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4669, Test Loss: 0.3949, F1: 0.7087, AUC: 0.8877
Epoch [10/30] Train Loss: 0.3987, Test Loss: 0.4395, F1: 0.6891, AUC: 0.9039
Epoch [20/30] Train Loss: 0.3840, Test Loss: 0.4128, F1: 0.7136, AUC: 0.9071
Mejores resultados en la época:  22
f1-score 0.7333091436865021
AUC según el mejor F1-score 0.9071186708004058
Confusion Matrix:
 [[14643  1822]
 [ 1118  4042]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8640
Precision:  0.6893
Recall:     0.7833
F1-score:   0.7333
Tiempo total para red 3: 804.75 segundos

Entrenando red 4 con capas [1536, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4631, Test Loss: 0.4571, F1: 0.6918, AUC: 0.8887
Epoch [10/30] Train Loss: 0.3928, Test Loss: 0.3394, F1: 0.7266, AUC: 0.9043
Epoch [20/30] Train Loss: 0.3895, Test Loss: 0.4276, F1: 0.6990, AUC: 0.9062
Mejores resultados en la época:  25
f1-score 0.7334828359496292
AUC según el mejor F1-score 0.9074354927176981
Confusion Matrix:
 [[14283  2182]
 [  908  4252]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8571
Precision:  0.6609
Recall:     0.8240
F1-score:   0.7335

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4641, Test Loss: 0.3932, F1: 0.7122, AUC: 0.8891
Epoch [10/30] Train Loss: 0.3918, Test Loss: 0.3730, F1: 0.7296, AUC: 0.9049
Epoch [20/30] Train Loss: 0.3808, Test Loss: 0.3567, F1: 0.7300, AUC: 0.9067
Mejores resultados en la época:  24
f1-score 0.7349786343420249
AUC según el mejor F1-score 0.9075631360390963
Confusion Matrix:
 [[14372  2093]
 [  946  4214]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8595
Precision:  0.6681
Recall:     0.8167
F1-score:   0.7350

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4690, Test Loss: 0.4347, F1: 0.6982, AUC: 0.8885
Epoch [10/30] Train Loss: 0.3940, Test Loss: 0.3533, F1: 0.7301, AUC: 0.9041
Epoch [20/30] Train Loss: 0.3866, Test Loss: 0.3473, F1: 0.7356, AUC: 0.9063
Mejores resultados en la época:  29
f1-score 0.736163165693367
AUC según el mejor F1-score 0.907853851369007
Confusion Matrix:
 [[14375  2090]
 [  937  4223]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8600
Precision:  0.6689
Recall:     0.8184
F1-score:   0.7362

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4691, Test Loss: 0.3613, F1: 0.7009, AUC: 0.8878
Epoch [10/30] Train Loss: 0.3942, Test Loss: 0.4207, F1: 0.7140, AUC: 0.9048
Epoch [20/30] Train Loss: 0.3824, Test Loss: 0.4205, F1: 0.7007, AUC: 0.9071
Mejores resultados en la época:  25
f1-score 0.734022805621851
AUC según el mejor F1-score 0.9070476662970784
Confusion Matrix:
 [[14464  2001]
 [ 1008  4152]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8609
Precision:  0.6748
Recall:     0.8047
F1-score:   0.7340
Tiempo total para red 4: 831.83 segundos

Entrenando red 5 con capas [1536, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4704, Test Loss: 0.3789, F1: 0.7118, AUC: 0.8885
Epoch [10/30] Train Loss: 0.3982, Test Loss: 0.5293, F1: 0.6440, AUC: 0.9024
Epoch [20/30] Train Loss: 0.3874, Test Loss: 0.3817, F1: 0.7219, AUC: 0.9069
Mejores resultados en la época:  26
f1-score 0.7352171165478018
AUC según el mejor F1-score 0.9069949411130493
Confusion Matrix:
 [[14620  1845]
 [ 1088  4072]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8644
Precision:  0.6882
Recall:     0.7891
F1-score:   0.7352

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4688, Test Loss: 0.5392, F1: 0.6466, AUC: 0.8882
Epoch [10/30] Train Loss: 0.3958, Test Loss: 0.4570, F1: 0.6969, AUC: 0.9046
Epoch [20/30] Train Loss: 0.3836, Test Loss: 0.3943, F1: 0.7222, AUC: 0.9071
Mejores resultados en la época:  27
f1-score 0.7353507084135036
AUC según el mejor F1-score 0.9073456615748227
Confusion Matrix:
 [[14395  2070]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [07:20:56] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [07:21:23] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
 [ 1007  4153]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8621
Precision:  0.6777
Recall:     0.8048
F1-score:   0.7358

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4639, Test Loss: 0.3791, F1: 0.7121, AUC: 0.8886
Epoch [10/30] Train Loss: 0.3963, Test Loss: 0.3951, F1: 0.7255, AUC: 0.9038
Epoch [20/30] Train Loss: 0.3867, Test Loss: 0.3538, F1: 0.7344, AUC: 0.9070
Mejores resultados en la época:  20
f1-score 0.7344135258893977
AUC según el mejor F1-score 0.9069621901755426
Confusion Matrix:
 [[14439  2026]
 [  990  4170]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8605
Precision:  0.6730
Recall:     0.8081
F1-score:   0.7344

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4766, Test Loss: 0.3795, F1: 0.7075, AUC: 0.8872
Epoch [10/30] Train Loss: 0.3925, Test Loss: 0.3448, F1: 0.7311, AUC: 0.9042
Epoch [20/30] Train Loss: 0.3871, Test Loss: 0.3602, F1: 0.7334, AUC: 0.9057
Mejores resultados en la época:  19
f1-score 0.7350086183434636
AUC según el mejor F1-score 0.9063573777592591
Confusion Matrix:
 [[14653  1812]
 [ 1109  4051]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8649
Precision:  0.6909
Recall:     0.7851
F1-score:   0.7350
Tiempo total para red 5: 1142.95 segundos

Entrenando red 6 con capas [1536, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4697, Test Loss: 0.4139, F1: 0.7097, AUC: 0.8892
Epoch [10/30] Train Loss: 0.3950, Test Loss: 0.3412, F1: 0.7279, AUC: 0.9041
Epoch [20/30] Train Loss: 0.3848, Test Loss: 0.3667, F1: 0.7283, AUC: 0.9070
Mejores resultados en la época:  16
f1-score 0.7346286931062017
AUC según el mejor F1-score 0.9056479506682016
Confusion Matrix:
 [[14494  1971]
 [ 1020  4140]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8617
Precision:  0.6775
Recall:     0.8023
F1-score:   0.7346

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4814, Test Loss: 0.3849, F1: 0.7100, AUC: 0.8878
Epoch [10/30] Train Loss: 0.3966, Test Loss: 0.4242, F1: 0.7206, AUC: 0.9025
Epoch [20/30] Train Loss: 0.3859, Test Loss: 0.4056, F1: 0.7127, AUC: 0.9063
Mejores resultados en la época:  19
f1-score 0.7365221278497989
AUC según el mejor F1-score 0.9064904589721678
Confusion Matrix:
 [[14559  1906]
 [ 1041  4119]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8637
Precision:  0.6837
Recall:     0.7983
F1-score:   0.7365

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4706, Test Loss: 0.3570, F1: 0.6971, AUC: 0.8892
Epoch [10/30] Train Loss: 0.3959, Test Loss: 0.3980, F1: 0.7188, AUC: 0.9040
Epoch [20/30] Train Loss: 0.3884, Test Loss: 0.3566, F1: 0.7320, AUC: 0.9066
Mejores resultados en la época:  29
f1-score 0.7344751866490997
AUC según el mejor F1-score 0.9068945931821552
Confusion Matrix:
 [[14421  2044]
 [  979  4181]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8602
Precision:  0.6716
Recall:     0.8103
F1-score:   0.7345

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4802, Test Loss: 0.4268, F1: 0.7041, AUC: 0.8871
Epoch [10/30] Train Loss: 0.3956, Test Loss: 0.4638, F1: 0.6829, AUC: 0.9033
Epoch [20/30] Train Loss: 0.3856, Test Loss: 0.3471, F1: 0.7312, AUC: 0.9062
Mejores resultados en la época:  26
f1-score 0.7337205209433298
AUC según el mejor F1-score 0.9068812691709216
Confusion Matrix:
 [[14430  2035]
 [  991  4169]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8601
Precision:  0.6720
Recall:     0.8079
F1-score:   0.7337
Tiempo total para red 6: 1371.10 segundos
Saved on: outputs_only_text/0/gpt

==============================
Model: Logistic Regression
Accuracy:  0.8486
Precision: 0.6432
Recall:    0.8203
F1-score:  0.7211
              precision    recall  f1-score   support

           0       0.94      0.86      0.90     16465
           1       0.64      0.82      0.72      5160

    accuracy                           0.85     21625
   macro avg       0.79      0.84      0.81     21625
weighted avg       0.87      0.85      0.85     21625

[[14117  2348]
 [  927  4233]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text/0/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7494
Precision: 0.4824
Recall:    0.6891
F1-score:  0.5675
              precision    recall  f1-score   support

           0       0.89      0.77      0.82     16465
           1       0.48      0.69      0.57      5160

    accuracy                           0.75     21625
   macro avg       0.68      0.73      0.70     21625
weighted avg       0.79      0.75      0.76     21625

[[12649  3816]
 [ 1604  3556]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_svm.png
Modelo guardado como: outputs_only_text/0/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7428
Precision: 0.4750
Recall:    0.7399
F1-score:  0.5786
              precision    recall  f1-score   support

           0       0.90      0.74      0.81     16465
           1       0.47      0.74      0.58      5160

    accuracy                           0.74     21625
   macro avg       0.69      0.74      0.70     21625
weighted avg       0.80      0.74      0.76     21625

[[12245  4220]
 [ 1342  3818]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text/0/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8384
Precision: 0.6376
Recall:    0.7477
F1-score:  0.6883
              precision    recall  f1-score   support

           0       0.92      0.87      0.89     16465
           1       0.64      0.75      0.69      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.81      0.79     21625
weighted avg       0.85      0.84      0.84     21625

[[14272  2193]
 [ 1302  3858]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text/0/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8400
Precision: 0.6292
Recall:    0.8021
F1-score:  0.7052
              precision    recall  f1-score   support

           0       0.93      0.85      0.89     16465
           1       0.63      0.80      0.71      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.83      0.80     21625
weighted avg       0.86      0.84      0.85     21625

[[14026  2439]
 [ 1021  4139]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text/0/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8308
Precision: 0.6256
Recall:    0.7238
F1-score:  0.6712
              precision    recall  f1-score   support

           0       0.91      0.86      0.89     16465
           1       0.63      0.72      0.67      5160

    accuracy                           0.83     21625
   macro avg       0.77      0.79      0.78     21625
weighted avg       0.84      0.83      0.83     21625

[[14230  2235]
 [ 1425  3735]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text/0/gpt/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.8486, 'precision': 0.6432, 'recall': 0.8203, 'f1_score': 0.7211}
XGBoost: {'accuracy': 0.84, 'precision': 0.6292, 'recall': 0.8021, 'f1_score': 0.7052}
Random Forest: {'accuracy': 0.8384, 'precision': 0.6376, 'recall': 0.7477, 'f1_score': 0.6883}
Naive Bayes: {'accuracy': 0.8308, 'precision': 0.6256, 'recall': 0.7238, 'f1_score': 0.6712}
Decision Tree: {'accuracy': 0.7428, 'precision': 0.475, 'recall': 0.7399, 'f1_score': 0.5786}
SVM: {'accuracy': 0.7494, 'precision': 0.4824, 'recall': 0.6891, 'f1_score': 0.5675}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
XGBoost: {'accuracy': 0.867, 'precision': 0.6775, 'recall': 0.8444, 'f1_score': 0.7518}
Decision Tree: {'accuracy': 0.8637, 'precision': 0.6782, 'recall': 0.8157, 'f1_score': 0.7406}
Random Forest: {'accuracy': 0.853, 'precision': 0.6721, 'recall': 0.75, 'f1_score': 0.7089}
MLP_2733057: {'accuracy': 0.8390751445086705, 'precision': 0.6277760876178886, 'recall': 0.7998062015503876, 'f1_score': 0.7034259417078575, 'f1_score_avg': 0.7004145163638206}
MLP_5820417: {'accuracy': 0.8366242774566474, 'precision': 0.6266936614234543, 'recall': 0.7798449612403101, 'f1_score': 0.7034259417078575, 'f1_score_avg': 0.6994722924905517}
MLP_1323521: {'accuracy': 0.8353294797687861, 'precision': 0.6204610516799759, 'recall': 0.798062015503876, 'f1_score': 0.7032887603903144, 'f1_score_avg': 0.7006287574907137}
MLP_650497: {'accuracy': 0.8344046242774567, 'precision': 0.6171538803976852, 'recall': 0.8060077519379845, 'f1_score': 0.7016920473773266, 'f1_score_avg': 0.6977646818212084}
MLP_322177: {'accuracy': 0.840092485549133, 'precision': 0.6338471217363951, 'recall': 0.7810077519379846, 'f1_score': 0.6997742663656885, 'f1_score_avg': 0.697564717564414}
MLP_160065: {'accuracy': 0.8352832369942197, 'precision': 0.6254317111459968, 'recall': 0.772093023255814, 'f1_score': 0.6930763109028207, 'f1_score_avg': 0.6918278826176383}
Logistic Regression: {'accuracy': 0.825, 'precision': 0.607, 'recall': 0.7566, 'f1_score': 0.6736}
Naive Bayes: {'accuracy': 0.8214, 'precision': 0.6084, 'recall': 0.7054, 'f1_score': 0.6533}
SVM: {'accuracy': 0.6767, 'precision': 0.3854, 'recall': 0.5969, 'f1_score': 0.4684}


EMBEDDINGS TYPE: LYRICS_BERT
MLP_120321: {'accuracy': 0.8325086705202313, 'precision': 0.6433097279165114, 'recall': 0.6689922480620155, 'f1_score': 0.6575035063113605, 'f1_score_avg': 0.655843921646582}
MLP_326657: {'accuracy': 0.8252947976878613, 'precision': 0.6203413444792755, 'recall': 0.6903100775193799, 'f1_score': 0.6575035063113605, 'f1_score_avg': 0.6542020210706325}
MLP_1007617: {'accuracy': 0.8378728323699421, 'precision': 0.6693977877918885, 'recall': 0.6333333333333333, 'f1_score': 0.6575035063113605, 'f1_score_avg': 0.652992211670026}
MLP_21377: {'accuracy': 0.8270520231213873, 'precision': 0.6264245014245015, 'recall': 0.681782945736434, 'f1_score': 0.6565551237084084, 'f1_score_avg': 0.6542206624061386}
MLP_48897: {'accuracy': 0.8173872832369942, 'precision': 0.59780326280084, 'recall': 0.7172480620155038, 'f1_score': 0.6565551237084084, 'f1_score_avg': 0.653844445791119}
MLP_9665: {'accuracy': 0.23861271676300577, 'precision': 0.23861271676300577, 'recall': 1.0, 'f1_score': 0.6526375928394591, 'f1_score_avg': 0.5837919927082572}
Logistic Regression: {'accuracy': 0.8054, 'precision': 0.5696, 'recall': 0.7548, 'f1_score': 0.6493}
XGBoost: {'accuracy': 0.8068, 'precision': 0.5741, 'recall': 0.7364, 'f1_score': 0.6452}
Random Forest: {'accuracy': 0.8063, 'precision': 0.5772, 'recall': 0.7031, 'f1_score': 0.634}
Naive Bayes: {'accuracy': 0.6437, 'precision': 0.3792, 'recall': 0.7742, 'f1_score': 0.509}
Decision Tree: {'accuracy': 0.661, 'precision': 0.3885, 'recall': 0.7331, 'f1_score': 0.5079}
SVM: {'accuracy': 0.3293, 'precision': 0.2559, 'recall': 0.9492, 'f1_score': 0.4031}


EMBEDDINGS TYPE: GPT
MLP_2273281: {'accuracy': 0.8600693641618498, 'precision': 0.6719858156028369, 'recall': 0.8079457364341085, 'f1_score': 0.7365221278497989, 'f1_score_avg': 0.7348366321371075}
MLP_207105: {'accuracy': 0.8592832369942196, 'precision': 0.6664569900927819, 'recall': 0.8213178294573643, 'f1_score': 0.7363741442162355, 'f1_score_avg': 0.7344069754534235}
MLP_436737: {'accuracy': 0.8571560693641619, 'precision': 0.65952857803112, 'recall': 0.8296511627906977, 'f1_score': 0.7363741442162355, 'f1_score_avg': 0.73455358361075}
MLP_959489: {'accuracy': 0.8649248554913295, 'precision': 0.6909432031383251, 'recall': 0.7850775193798449, 'f1_score': 0.7363741442162355, 'f1_score_avg': 0.7349081644601139}
MLP_100481: {'accuracy': 0.8607167630057804, 'precision': 0.6758925646904684, 'recall': 0.7998062015503876, 'f1_score': 0.7349581796924184, 'f1_score_avg': 0.734094234518798}
MLP_49217: {'accuracy': 0.8554450867052024, 'precision': 0.6572842561088772, 'recall': 0.8236434108527132, 'f1_score': 0.7325167231343932, 'f1_score_avg': 0.7301443899395299}
Logistic Regression: {'accuracy': 0.8486, 'precision': 0.6432, 'recall': 0.8203, 'f1_score': 0.7211}
XGBoost: {'accuracy': 0.84, 'precision': 0.6292, 'recall': 0.8021, 'f1_score': 0.7052}
Random Forest: {'accuracy': 0.8384, 'precision': 0.6376, 'recall': 0.7477, 'f1_score': 0.6883}
Naive Bayes: {'accuracy': 0.8308, 'precision': 0.6256, 'recall': 0.7238, 'f1_score': 0.6712}
Decision Tree: {'accuracy': 0.7428, 'precision': 0.475, 'recall': 0.7399, 'f1_score': 0.5786}
SVM: {'accuracy': 0.7494, 'precision': 0.4824, 'recall': 0.6891, 'f1_score': 0.5675}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: False
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: Not used
====================================

 [  956  4204]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8601
Precision:  0.6701
Recall:     0.8147
F1-score:   0.7354

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4718, Test Loss: 0.5435, F1: 0.6400, AUC: 0.8877
Epoch [10/30] Train Loss: 0.3944, Test Loss: 0.3608, F1: 0.7310, AUC: 0.9047
Epoch [20/30] Train Loss: 0.3835, Test Loss: 0.4473, F1: 0.6976, AUC: 0.9058
Mejores resultados en la época:  29
f1-score 0.7389153700872381
AUC según el mejor F1-score 0.9067547558010061
Confusion Matrix:
 [[14614  1851]
 [ 1052  4108]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8658
Precision:  0.6894
Recall:     0.7961
F1-score:   0.7389

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4674, Test Loss: 0.4094, F1: 0.7082, AUC: 0.8892
Epoch [10/30] Train Loss: 0.3947, Test Loss: 0.4211, F1: 0.7051, AUC: 0.9046
Epoch [20/30] Train Loss: 0.3868, Test Loss: 0.3905, F1: 0.7229, AUC: 0.9071
Mejores resultados en la época:  29
f1-score 0.733292024431265
AUC según el mejor F1-score 0.9072081370631149
Confusion Matrix:
 [[14470  1995]
 [ 1018  4142]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8607
Precision:  0.6749
Recall:     0.8027
F1-score:   0.7333
Tiempo total para red 5: 1162.17 segundos

Entrenando red 6 con capas [1536, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4745, Test Loss: 0.5645, F1: 0.6601, AUC: 0.8877
Epoch [10/30] Train Loss: 0.3934, Test Loss: 0.3470, F1: 0.7263, AUC: 0.9036
Epoch [20/30] Train Loss: 0.3840, Test Loss: 0.3467, F1: 0.7314, AUC: 0.9062
Mejores resultados en la época:  19
f1-score 0.7317927170868347
AUC según el mejor F1-score 0.9063809713816248
Confusion Matrix:
 [[14381  2084]
 [  980  4180]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8583
Precision:  0.6673
Recall:     0.8101
F1-score:   0.7318

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4847, Test Loss: 0.6803, F1: 0.5956, AUC: 0.8871
Epoch [10/30] Train Loss: 0.3911, Test Loss: 0.3896, F1: 0.7225, AUC: 0.9043
Epoch [20/30] Train Loss: 0.3890, Test Loss: 0.3691, F1: 0.7274, AUC: 0.9064
Mejores resultados en la época:  28
f1-score 0.7342415985467756
AUC según el mejor F1-score 0.9067451924095509
Confusion Matrix:
 [[14657  1808]
 [ 1118  4042]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8647
Precision:  0.6909
Recall:     0.7833
F1-score:   0.7342

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4705, Test Loss: 0.3778, F1: 0.7125, AUC: 0.8892
Epoch [10/30] Train Loss: 0.3995, Test Loss: 0.3587, F1: 0.7295, AUC: 0.9027
Epoch [20/30] Train Loss: 0.3873, Test Loss: 0.4022, F1: 0.7252, AUC: 0.9073
Mejores resultados en la época:  27
f1-score 0.7345155936053115
AUC según el mejor F1-score 0.9071577777150027
Confusion Matrix:
 [[14382  2083]
 [  956  4204]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8595
Precision:  0.6687
Recall:     0.8147
F1-score:   0.7345

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4812, Test Loss: 0.6106, F1: 0.5722, AUC: 0.8883
Epoch [10/30] Train Loss: 0.3957, Test Loss: 0.5503, F1: 0.6438, AUC: 0.9026
Epoch [20/30] Train Loss: 0.3848, Test Loss: 0.3790, F1: 0.7198, AUC: 0.9062
Mejores resultados en la época:  21
f1-score 0.7369931826336562
AUC según el mejor F1-score 0.9063025633420199
Confusion Matrix:
 [[14585  1880]
 [ 1052  4108]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8644
Precision:  0.6860
Recall:     0.7961
F1-score:   0.7370
Tiempo total para red 6: 1369.32 segundos
Saved on: outputs_only_text/0/gpt

==============================
Model: Logistic Regression
Accuracy:  0.8486
Precision: 0.6432
Recall:    0.8203
F1-score:  0.7211
              precision    recall  f1-score   support

           0       0.94      0.86      0.90     16465
           1       0.64      0.82      0.72      5160

    accuracy                           0.85     21625
   macro avg       0.79      0.84      0.81     21625
weighted avg       0.87      0.85      0.85     21625

[[14117  2348]
 [  927  4233]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text/0/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7494
Precision: 0.4824
Recall:    0.6891
F1-score:  0.5675
              precision    recall  f1-score   support

           0       0.89      0.77      0.82     16465
           1       0.48      0.69      0.57      5160

    accuracy                           0.75     21625
   macro avg       0.68      0.73      0.70     21625
weighted avg       0.79      0.75      0.76     21625

[[12649  3816]
 [ 1604  3556]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_svm.png
Modelo guardado como: outputs_only_text/0/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7428
Precision: 0.4750
Recall:    0.7399
F1-score:  0.5786
              precision    recall  f1-score   support

           0       0.90      0.74      0.81     16465
           1       0.47      0.74      0.58      5160

    accuracy                           0.74     21625
   macro avg       0.69      0.74      0.70     21625
weighted avg       0.80      0.74      0.76     21625

[[12245  4220]
 [ 1342  3818]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text/0/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8384
Precision: 0.6376
Recall:    0.7477
F1-score:  0.6883
              precision    recall  f1-score   support

           0       0.92      0.87      0.89     16465
           1       0.64      0.75      0.69      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.81      0.79     21625
weighted avg       0.85      0.84      0.84     21625

[[14272  2193]
 [ 1302  3858]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text/0/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8400
Precision: 0.6292
Recall:    0.8021
F1-score:  0.7052
              precision    recall  f1-score   support

           0       0.93      0.85      0.89     16465
           1       0.63      0.80      0.71      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.83      0.80     21625
weighted avg       0.86      0.84      0.85     21625

[[14026  2439]
 [ 1021  4139]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text/0/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8308
Precision: 0.6256
Recall:    0.7238
F1-score:  0.6712
              precision    recall  f1-score   support

           0       0.91      0.86      0.89     16465
           1       0.63      0.72      0.67      5160

    accuracy                           0.83     21625
   macro avg       0.77      0.79      0.78     21625
weighted avg       0.84      0.83      0.83     21625

[[14230  2235]
 [ 1425  3735]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text/0/gpt/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.8486, 'precision': 0.6432, 'recall': 0.8203, 'f1_score': 0.7211}
XGBoost: {'accuracy': 0.84, 'precision': 0.6292, 'recall': 0.8021, 'f1_score': 0.7052}
Random Forest: {'accuracy': 0.8384, 'precision': 0.6376, 'recall': 0.7477, 'f1_score': 0.6883}
Naive Bayes: {'accuracy': 0.8308, 'precision': 0.6256, 'recall': 0.7238, 'f1_score': 0.6712}
Decision Tree: {'accuracy': 0.7428, 'precision': 0.475, 'recall': 0.7399, 'f1_score': 0.5786}
SVM: {'accuracy': 0.7494, 'precision': 0.4824, 'recall': 0.6891, 'f1_score': 0.5675}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
XGBoost: {'accuracy': 0.867, 'precision': 0.6775, 'recall': 0.8444, 'f1_score': 0.7518}
Decision Tree: {'accuracy': 0.8637, 'precision': 0.6782, 'recall': 0.8157, 'f1_score': 0.7406}
Random Forest: {'accuracy': 0.853, 'precision': 0.6721, 'recall': 0.75, 'f1_score': 0.7089}
MLP_650497: {'accuracy': 0.8363468208092486, 'precision': 0.6244052187260168, 'recall': 0.7883720930232558, 'f1_score': 0.7069831918211749, 'f1_score_avg': 0.6981915626282931}
MLP_1323521: {'accuracy': 0.8387514450867052, 'precision': 0.6281599509728819, 'recall': 0.7945736434108527, 'f1_score': 0.7069831918211749, 'f1_score_avg': 0.6973473395712912}
MLP_2733057: {'accuracy': 0.8401387283236994, 'precision': 0.6304980842911877, 'recall': 0.7972868217054263, 'f1_score': 0.7069831918211749, 'f1_score_avg': 0.7012803429462174}
MLP_5820417: {'accuracy': 0.8463352601156069, 'precision': 0.6539299480475951, 'recall': 0.756201550387597, 'f1_score': 0.7069831918211749, 'f1_score_avg': 0.6991395427415297}
MLP_322177: {'accuracy': 0.8384277456647399, 'precision': 0.6300749531542785, 'recall': 0.7819767441860465, 'f1_score': 0.6990241832838354, 'f1_score_avg': 0.6961394662744136}
MLP_160065: {'accuracy': 0.828485549132948, 'precision': 0.6061448427212874, 'recall': 0.8029069767441861, 'f1_score': 0.6951343994555972, 'f1_score_avg': 0.6931635298604031}
Logistic Regression: {'accuracy': 0.825, 'precision': 0.607, 'recall': 0.7566, 'f1_score': 0.6736}
Naive Bayes: {'accuracy': 0.8214, 'precision': 0.6084, 'recall': 0.7054, 'f1_score': 0.6533}
SVM: {'accuracy': 0.6767, 'precision': 0.3854, 'recall': 0.5969, 'f1_score': 0.4684}


EMBEDDINGS TYPE: LYRICS_BERT
MLP_1007617: {'accuracy': 0.8310751445086705, 'precision': 0.6375753149534417, 'recall': 0.6767441860465117, 'f1_score': 0.657244477723699, 'f1_score_avg': 0.6562922923776999}
MLP_120321: {'accuracy': 0.8291791907514451, 'precision': 0.6306595365418894, 'recall': 0.6856589147286821, 'f1_score': 0.6570102135561745, 'f1_score_avg': 0.6552699070676613}
MLP_326657: {'accuracy': 0.8311676300578035, 'precision': 0.6388224471021159, 'recall': 0.6728682170542636, 'f1_score': 0.6570102135561745, 'f1_score_avg': 0.6547228417282964}
MLP_48897: {'accuracy': 0.8280693641618497, 'precision': 0.6276106194690265, 'recall': 0.6872093023255814, 'f1_score': 0.656059204440333, 'f1_score_avg': 0.6544820093814329}
MLP_21377: {'accuracy': 0.8235375722543352, 'precision': 0.6151868357901954, 'recall': 0.6955426356589147, 'f1_score': 0.6544278512318082, 'f1_score_avg': 0.6535714015913195}
MLP_9665: {'accuracy': 0.8184971098265896, 'precision': 0.6020155294895093, 'recall': 0.7062015503875969, 'f1_score': 0.652349245052987, 'f1_score_avg': 0.584370155656261}
Logistic Regression: {'accuracy': 0.8054, 'precision': 0.5696, 'recall': 0.7548, 'f1_score': 0.6493}
XGBoost: {'accuracy': 0.8068, 'precision': 0.5741, 'recall': 0.7364, 'f1_score': 0.6452}
Random Forest: {'accuracy': 0.8063, 'precision': 0.5772, 'recall': 0.7031, 'f1_score': 0.634}
Naive Bayes: {'accuracy': 0.6437, 'precision': 0.3792, 'recall': 0.7742, 'f1_score': 0.509}
Decision Tree: {'accuracy': 0.661, 'precision': 0.3885, 'recall': 0.7331, 'f1_score': 0.5079}
SVM: {'accuracy': 0.3293, 'precision': 0.2559, 'recall': 0.9492, 'f1_score': 0.4031}


EMBEDDINGS TYPE: GPT
MLP_959489: {'accuracy': 0.8606705202312138, 'precision': 0.6749226006191951, 'recall': 0.8027131782945737, 'f1_score': 0.7389153700872381, 'f1_score_avg': 0.7356938048699522}
MLP_2273281: {'accuracy': 0.8644161849710983, 'precision': 0.6860387441549766, 'recall': 0.796124031007752, 'f1_score': 0.7389153700872381, 'f1_score_avg': 0.7343857729681446}
MLP_100481: {'accuracy': 0.8633526011560694, 'precision': 0.6797652046306865, 'recall': 0.8079457364341085, 'f1_score': 0.7383334809173825, 'f1_score_avg': 0.7347315266496492}
MLP_207105: {'accuracy': 0.8640462427745664, 'precision': 0.6892905866302865, 'recall': 0.7833333333333333, 'f1_score': 0.7383334809173825, 'f1_score_avg': 0.7342571142840335}
MLP_436737: {'accuracy': 0.8608554913294798, 'precision': 0.6747927840078011, 'recall': 0.8046511627906977, 'f1_score': 0.7383334809173825, 'f1_score_avg': 0.734661860401718}
MLP_49217: {'accuracy': 0.8634913294797688, 'precision': 0.6859838274932615, 'recall': 0.7891472868217054, 'f1_score': 0.7339581831290555, 'f1_score_avg': 0.7312668516601548}
Logistic Regression: {'accuracy': 0.8486, 'precision': 0.6432, 'recall': 0.8203, 'f1_score': 0.7211}
XGBoost: {'accuracy': 0.84, 'precision': 0.6292, 'recall': 0.8021, 'f1_score': 0.7052}
Random Forest: {'accuracy': 0.8384, 'precision': 0.6376, 'recall': 0.7477, 'f1_score': 0.6883}
Naive Bayes: {'accuracy': 0.8308, 'precision': 0.6256, 'recall': 0.7238, 'f1_score': 0.6712}
Decision Tree: {'accuracy': 0.7428, 'precision': 0.475, 'recall': 0.7399, 'f1_score': 0.5786}
SVM: {'accuracy': 0.7494, 'precision': 0.4824, 'recall': 0.6891, 'f1_score': 0.5675}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: False
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: Not used
====================================

