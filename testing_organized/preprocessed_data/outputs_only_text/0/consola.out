2025-09-26 20:20:25.941278: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
Your are not adding cols numercis
You are executing with [ALL] dataset
--> PaTH:  ../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy

##################################################
Running experiment with TFIDF embeddings
Preprocessing text...
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5000)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5000)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5000, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4887, Test Loss: 0.4476, F1: 0.6879, AUC: 0.8791
Epoch [10/30] Train Loss: 0.2915, Test Loss: 0.4492, F1: 0.6847, AUC: 0.8810
Epoch [20/30] Train Loss: 0.2347, Test Loss: 0.5264, F1: 0.6738, AUC: 0.8760
Mejores resultados en la época:  2
f1-score 0.7005601034037053
AUC según el mejor F1-score 0.8857559787380795
Confusion Matrix:
 [[14085  2380]
 [ 1095  4065]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_160065.png
Accuracy:   0.8393
Precision:  0.6307
Recall:     0.7878
F1-score:   0.7006

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4900, Test Loss: 0.4381, F1: 0.6914, AUC: 0.8793
Epoch [10/30] Train Loss: 0.2935, Test Loss: 0.4614, F1: 0.6840, AUC: 0.8818
Epoch [20/30] Train Loss: 0.2438, Test Loss: 0.5154, F1: 0.6747, AUC: 0.8772
Mejores resultados en la época:  1
f1-score 0.7005884389061959
AUC según el mejor F1-score 0.8850024070320648
Confusion Matrix:
 [[14117  2348]
 [ 1112  4048]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_160065.png
Accuracy:   0.8400
Precision:  0.6329
Recall:     0.7845
F1-score:   0.7006

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4903, Test Loss: 0.4325, F1: 0.6911, AUC: 0.8784
Epoch [10/30] Train Loss: 0.2917, Test Loss: 0.4608, F1: 0.6840, AUC: 0.8821
Epoch [20/30] Train Loss: 0.2375, Test Loss: 0.5356, F1: 0.6712, AUC: 0.8763
Mejores resultados en la época:  2
f1-score 0.6960514233241506
AUC según el mejor F1-score 0.8855160700287431
Confusion Matrix:
 [[13815  2650]
 [  991  4169]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_160065.png
Accuracy:   0.8316
Precision:  0.6114
Recall:     0.8079
F1-score:   0.6961

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4893, Test Loss: 0.4420, F1: 0.6908, AUC: 0.8788
Epoch [10/30] Train Loss: 0.2969, Test Loss: 0.4432, F1: 0.6925, AUC: 0.8819
Epoch [20/30] Train Loss: 0.2470, Test Loss: 0.5098, F1: 0.6818, AUC: 0.8770
Mejores resultados en la época:  1
f1-score 0.6928755364806867
AUC según el mejor F1-score 0.8834668853593599
Confusion Matrix:
 [[14011  2454]
 [ 1124  4036]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_160065.png
Accuracy:   0.8345
Precision:  0.6219
Recall:     0.7822
F1-score:   0.6929
Tiempo total para red 1: 418.97 segundos

Entrenando red 2 con capas [5000, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4661, Test Loss: 0.4279, F1: 0.6923, AUC: 0.8847
Epoch [10/30] Train Loss: 0.0611, Test Loss: 0.9514, F1: 0.6413, AUC: 0.8485
Epoch [20/30] Train Loss: 0.0057, Test Loss: 1.7454, F1: 0.6403, AUC: 0.8504
Mejores resultados en la época:  0
f1-score 0.6922694094226941
AUC según el mejor F1-score 0.884670925171317
Confusion Matrix:
 [[13742  2723]
 [  987  4173]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_322177.png
Accuracy:   0.8284
Precision:  0.6051
Recall:     0.8087
F1-score:   0.6923

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4661, Test Loss: 0.4098, F1: 0.6964, AUC: 0.8840
Epoch [10/30] Train Loss: 0.0814, Test Loss: 0.8643, F1: 0.6458, AUC: 0.8508
Epoch [20/30] Train Loss: 0.0057, Test Loss: 1.8415, F1: 0.6370, AUC: 0.8465
Mejores resultados en la época:  2
f1-score 0.6991238221193585
AUC según el mejor F1-score 0.8889354738851734
Confusion Matrix:
 [[13756  2709]
 [  931  4229]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_322177.png
Accuracy:   0.8317
Precision:  0.6095
Recall:     0.8196
F1-score:   0.6991

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4671, Test Loss: 0.4070, F1: 0.6973, AUC: 0.8839
Epoch [10/30] Train Loss: 0.0720, Test Loss: 0.9138, F1: 0.6450, AUC: 0.8558
Epoch [20/30] Train Loss: 0.0053, Test Loss: 1.7643, F1: 0.6279, AUC: 0.8490
Mejores resultados en la época:  3
f1-score 0.7070193285859614
AUC según el mejor F1-score 0.8863209073981219
Confusion Matrix:
 [[13999  2466]
 [  990  4170]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_322177.png
Accuracy:   0.8402
Precision:  0.6284
Recall:     0.8081
F1-score:   0.7070

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4631, Test Loss: 0.4353, F1: 0.6871, AUC: 0.8852
Epoch [10/30] Train Loss: 0.0831, Test Loss: 0.9726, F1: 0.6619, AUC: 0.8591
Epoch [20/30] Train Loss: 0.0088, Test Loss: 1.9010, F1: 0.6360, AUC: 0.8444
Mejores resultados en la época:  4
f1-score 0.69162706865476
AUC según el mejor F1-score 0.8822336492489353
Confusion Matrix:
 [[13640  2825]
 [  939  4221]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_322177.png
Accuracy:   0.8259
Precision:  0.5991
Recall:     0.8180
F1-score:   0.6916
Tiempo total para red 2: 390.01 segundos

Entrenando red 3 con capas [5000, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4561, Test Loss: 0.4220, F1: 0.6946, AUC: 0.8867
Epoch [10/30] Train Loss: 0.0179, Test Loss: 1.6282, F1: 0.6413, AUC: 0.8479
Epoch [20/30] Train Loss: 0.0037, Test Loss: 1.9294, F1: 0.6442, AUC: 0.8502
Mejores resultados en la época:  1
f1-score 0.7017426273458445
AUC según el mejor F1-score 0.8898401942574924
Confusion Matrix:
 [[13877  2588]
 [  972  4188]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_650497.png
Accuracy:   0.8354
Precision:  0.6181
Recall:     0.8116
F1-score:   0.7017

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4584, Test Loss: 0.4254, F1: 0.6951, AUC: 0.8854
Epoch [10/30] Train Loss: 0.0174, Test Loss: 1.5320, F1: 0.6243, AUC: 0.8427
Epoch [20/30] Train Loss: 0.0062, Test Loss: 1.9545, F1: 0.6370, AUC: 0.8543
Mejores resultados en la época:  2
f1-score 0.7076346923142395
AUC según el mejor F1-score 0.8875812799996233
Confusion Matrix:
 [[14033  2432]
 [ 1003  4157]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_650497.png
Accuracy:   0.8412
Precision:  0.6309
Recall:     0.8056
F1-score:   0.7076

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4556, Test Loss: 0.3990, F1: 0.7015, AUC: 0.8859
Epoch [10/30] Train Loss: 0.0265, Test Loss: 1.9442, F1: 0.6135, AUC: 0.8377
Epoch [20/30] Train Loss: 0.0054, Test Loss: 2.0510, F1: 0.6393, AUC: 0.8526
Mejores resultados en la época:  0
f1-score 0.7014512785072564
AUC según el mejor F1-score 0.8858993001363004
Confusion Matrix:
 [[14109  2356]
 [ 1100  4060]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_650497.png
Accuracy:   0.8402
Precision:  0.6328
Recall:     0.7868
F1-score:   0.7015

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4588, Test Loss: 0.4420, F1: 0.6874, AUC: 0.8854
Epoch [10/30] Train Loss: 0.0096, Test Loss: 1.8214, F1: 0.6311, AUC: 0.8479
Epoch [20/30] Train Loss: 0.0185, Test Loss: 1.6793, F1: 0.6499, AUC: 0.8567
Mejores resultados en la época:  3
f1-score 0.7069199457259159
AUC según el mejor F1-score 0.8840306193311158
Confusion Matrix:
 [[14001  2464]
 [  992  4168]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_650497.png
Accuracy:   0.8402
Precision:  0.6285
Recall:     0.8078
F1-score:   0.7069
Tiempo total para red 3: 403.15 segundos

Entrenando red 4 con capas [5000, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4547, Test Loss: 0.3694, F1: 0.7057, AUC: 0.8872
Epoch [10/30] Train Loss: 0.0090, Test Loss: 1.4595, F1: 0.6621, AUC: 0.8651
Epoch [20/30] Train Loss: 0.0041, Test Loss: 1.8734, F1: 0.6381, AUC: 0.8560
Mejores resultados en la época:  0
f1-score 0.705696496608122
AUC según el mejor F1-score 0.8871606791008411
Confusion Matrix:
 [[14661  1804]
 [ 1363  3797]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_1323521.png
Accuracy:   0.8535
Precision:  0.6779
Recall:     0.7359
F1-score:   0.7057

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4543, Test Loss: 0.4257, F1: 0.6933, AUC: 0.8852
Epoch [10/30] Train Loss: 0.0087, Test Loss: 1.5093, F1: 0.6246, AUC: 0.8471
Epoch [20/30] Train Loss: 0.0041, Test Loss: 2.0978, F1: 0.6121, AUC: 0.8423
Mejores resultados en la época:  0
f1-score 0.6933355465183833
AUC según el mejor F1-score 0.8852004016035896
Confusion Matrix:
 [[13753  2712]
 [  983  4177]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_1323521.png
Accuracy:   0.8291
Precision:  0.6063
Recall:     0.8095
F1-score:   0.6933

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4546, Test Loss: 0.4523, F1: 0.6796, AUC: 0.8851
Epoch [10/30] Train Loss: 0.0076, Test Loss: 1.6485, F1: 0.6202, AUC: 0.8508
Epoch [20/30] Train Loss: 0.0063, Test Loss: 2.2527, F1: 0.6162, AUC: 0.8473
Mejores resultados en la época:  2
f1-score 0.7061491495856956
AUC según el mejor F1-score 0.8846290051483415
Confusion Matrix:
 [[14208  2257]
 [ 1112  4048]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_1323521.png
Accuracy:   0.8442
Precision:  0.6420
Recall:     0.7845
F1-score:   0.7061

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4537, Test Loss: 0.4153, F1: 0.6960, AUC: 0.8867
Epoch [10/30] Train Loss: 0.0112, Test Loss: 1.6675, F1: 0.6227, AUC: 0.8506
Epoch [20/30] Train Loss: 0.0048, Test Loss: 1.6171, F1: 0.6553, AUC: 0.8647
Mejores resultados en la época:  1
f1-score 0.7025481456563788
AUC según el mejor F1-score 0.8885210170975784
Confusion Matrix:
 [[13911  2554]
 [  983  4177]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_1323521.png
Accuracy:   0.8364
Precision:  0.6206
Recall:     0.8095
F1-score:   0.7025
Tiempo total para red 4: 428.70 segundos

Entrenando red 5 con capas [5000, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4547, Test Loss: 0.4240, F1: 0.6963, AUC: 0.8858
Epoch [10/30] Train Loss: 0.0068, Test Loss: 1.4236, F1: 0.6600, AUC: 0.8645
Epoch [20/30] Train Loss: 0.0053, Test Loss: 1.6371, F1: 0.6486, AUC: 0.8636
Mejores resultados en la época:  0
f1-score 0.6962962962962963
AUC según el mejor F1-score 0.8858383828040217
Confusion Matrix:
 [[13969  2496]
 [ 1071  4089]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_2733057.png
Accuracy:   0.8351
Precision:  0.6210
Recall:     0.7924
F1-score:   0.6963

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4484, Test Loss: 0.4693, F1: 0.6826, AUC: 0.8886
Epoch [10/30] Train Loss: 0.0127, Test Loss: 1.4292, F1: 0.6526, AUC: 0.8627
Epoch [20/30] Train Loss: 0.0035, Test Loss: 1.5715, F1: 0.6391, AUC: 0.8584
Mejores resultados en la época:  2
f1-score 0.7036029911624745
AUC según el mejor F1-score 0.8795098658888834
Confusion Matrix:
 [[13997  2468]
 [ 1020  4140]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_2733057.png
Accuracy:   0.8387
Precision:  0.6265
Recall:     0.8023
F1-score:   0.7036

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4522, Test Loss: 0.4363, F1: 0.6921, AUC: 0.8862
Epoch [10/30] Train Loss: 0.0100, Test Loss: 1.6917, F1: 0.6309, AUC: 0.8559
Epoch [20/30] Train Loss: 0.0064, Test Loss: 1.3881, F1: 0.6455, AUC: 0.8592
Mejores resultados en la época:  2
f1-score 0.6968325791855203
AUC según el mejor F1-score 0.8790893591527247
Confusion Matrix:
 [[13705  2760]
 [  925  4235]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_2733057.png
Accuracy:   0.8296
Precision:  0.6054
Recall:     0.8207
F1-score:   0.6968

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4503, Test Loss: 0.4164, F1: 0.6996, AUC: 0.8890
Epoch [10/30] Train Loss: 0.0061, Test Loss: 1.7666, F1: 0.6304, AUC: 0.8584
Epoch [20/30] Train Loss: 0.0019, Test Loss: 2.2586, F1: 0.6538, AUC: 0.8661
Mejores resultados en la época:  1
f1-score 0.7027884454833863
AUC según el mejor F1-score 0.8888823838209781
Confusion Matrix:
 [[13856  2609]
 [  951  4209]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_2733057.png
Accuracy:   0.8354
Precision:  0.6173
Recall:     0.8157
F1-score:   0.7028
Tiempo total para red 5: 433.63 segundos

Entrenando red 6 con capas [5000, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4567, Test Loss: 0.4248, F1: 0.7042, AUC: 0.8869
Epoch [10/30] Train Loss: 0.0062, Test Loss: 1.8173, F1: 0.6648, AUC: 0.8688
Epoch [20/30] Train Loss: 0.0040, Test Loss: 1.9805, F1: 0.6642, AUC: 0.8680
Mejores resultados en la época:  0
f1-score 0.7042203147353362
AUC según el mejor F1-score 0.8868988246150513
Confusion Matrix:
 [[14379  2086]
 [ 1222  3938]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_5820417.png
Accuracy:   0.8470
Precision:  0.6537
Recall:     0.7632
F1-score:   0.7042

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4592, Test Loss: 0.3874, F1: 0.7027, AUC: 0.8875
Epoch [10/30] Train Loss: 0.0062, Test Loss: 1.3769, F1: 0.6624, AUC: 0.8710
Epoch [20/30] Train Loss: 0.0065, Test Loss: 1.6852, F1: 0.6545, AUC: 0.8685
Mejores resultados en la época:  0
f1-score 0.7026700077767217
AUC según el mejor F1-score 0.8875279898398528
Confusion Matrix:
 [[14118  2347]
 [ 1094  4066]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_5820417.png
Accuracy:   0.8409
Precision:  0.6340
Recall:     0.7880
F1-score:   0.7027

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4571, Test Loss: 0.4047, F1: 0.6998, AUC: 0.8869
Epoch [10/30] Train Loss: 0.0065, Test Loss: 0.7965, F1: 0.6629, AUC: 0.8683
Epoch [20/30] Train Loss: 0.0056, Test Loss: 1.8855, F1: 0.6154, AUC: 0.8678
Mejores resultados en la época:  1
f1-score 0.702125871775481
AUC según el mejor F1-score 0.8869072168588762
Confusion Matrix:
 [[13902  2563]
 [  982  4178]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_5820417.png
Accuracy:   0.8361
Precision:  0.6198
Recall:     0.8097
F1-score:   0.7021

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4542, Test Loss: 0.4037, F1: 0.7019, AUC: 0.8868
Epoch [10/30] Train Loss: 0.0071, Test Loss: 1.5624, F1: 0.6453, AUC: 0.8627
Epoch [20/30] Train Loss: 0.0042, Test Loss: 2.0882, F1: 0.6428, AUC: 0.8674
Mejores resultados en la época:  0
f1-score 0.7018520128684462
AUC según el mejor F1-score 0.8867955105615153
Confusion Matrix:
 [[14160  2305]
 [ 1124  4036]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_5820417.png
Accuracy:   0.8414
Precision:  0.6365
Recall:     0.7822
F1-score:   0.7019
Tiempo total para red 6: 476.61 segundos
Saved on: outputs_only_text/0/tfidf

==============================
Model: Logistic Regression
Accuracy:  0.8247
Precision: 0.6063
Recall:    0.7570
F1-score:  0.6733
              precision    recall  f1-score   support

           0       0.92      0.85      0.88     16465
           1       0.61      0.76      0.67      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.80      0.78     21625
weighted avg       0.84      0.82      0.83     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [21:15:31] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[13929  2536]
 [ 1254  3906]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text/0/tfidf/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.6144
Precision: 0.3332
Recall:    0.6155
F1-score:  0.4324
              precision    recall  f1-score   support

           0       0.84      0.61      0.71     16465
           1       0.33      0.62      0.43      5160

    accuracy                           0.61     21625
   macro avg       0.58      0.61      0.57     21625
weighted avg       0.72      0.61      0.64     21625

[[10110  6355]
 [ 1984  3176]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_svm.png
Modelo guardado como: outputs_only_text/0/tfidf/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8635
Precision: 0.6777
Recall:    0.8159
F1-score:  0.7404
              precision    recall  f1-score   support

           0       0.94      0.88      0.91     16465
           1       0.68      0.82      0.74      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.85      0.82     21625
weighted avg       0.88      0.86      0.87     21625

[[14463  2002]
 [  950  4210]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text/0/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8540
Precision: 0.6743
Recall:    0.7504
F1-score:  0.7103
              precision    recall  f1-score   support

           0       0.92      0.89      0.90     16465
           1       0.67      0.75      0.71      5160

    accuracy                           0.85     21625
   macro avg       0.80      0.82      0.81     21625
weighted avg       0.86      0.85      0.86     21625

[[14595  1870]
 [ 1288  3872]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text/0/tfidf/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8667
Precision: 0.6768
Recall:    0.8448
F1-score:  0.7515
              precision    recall  f1-score   support

           0       0.95      0.87      0.91     16465
           1       0.68      0.84      0.75      5160

    accuracy                           0.87     21625
   macro avg       0.81      0.86      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14383  2082]
 [  801  4359]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text/0/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8212
Precision: 0.6080
Recall:    0.7052
F1-score:  0.6530
              precision    recall  f1-score   support

           0       0.90      0.86      0.88     16465
           1       0.61      0.71      0.65      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.78      0.77     21625
weighted avg       0.83      0.82      0.83     21625

[[14119  2346]
 [ 1521  3639]]
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text/0/tfidf/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.8667, 'precision': 0.6768, 'recall': 0.8448, 'f1_score': 0.7515}
Decision Tree: {'accuracy': 0.8635, 'precision': 0.6777, 'recall': 0.8159, 'f1_score': 0.7404}
Random Forest: {'accuracy': 0.854, 'precision': 0.6743, 'recall': 0.7504, 'f1_score': 0.7103}
Logistic Regression: {'accuracy': 0.8247, 'precision': 0.6063, 'recall': 0.757, 'f1_score': 0.6733}
Naive Bayes: {'accuracy': 0.8212, 'precision': 0.608, 'recall': 0.7052, 'f1_score': 0.653}
SVM: {'accuracy': 0.6144, 'precision': 0.3332, 'recall': 0.6155, 'f1_score': 0.4324}

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 300)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 300)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [300, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5006, Test Loss: 0.4543, F1: 0.6448, AUC: 0.8552
Epoch [10/30] Train Loss: 0.4348, Test Loss: 0.4419, F1: 0.6657, AUC: 0.8662
Epoch [20/30] Train Loss: 0.4048, Test Loss: 0.4700, F1: 0.6450, AUC: 0.8624
Mejores resultados en la época:  14
f1-score 0.6666110183639399
AUC según el mejor F1-score 0.8665407123873283
Confusion Matrix:
 [[13638  2827]
 [ 1167  3993]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8153
Precision:  0.5855
Recall:     0.7738
F1-score:   0.6666

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4995, Test Loss: 0.4882, F1: 0.6329, AUC: 0.8547
Epoch [10/30] Train Loss: 0.4344, Test Loss: 0.4522, F1: 0.6594, AUC: 0.8667
Epoch [20/30] Train Loss: 0.4057, Test Loss: 0.4339, F1: 0.6640, AUC: 0.8643
Mejores resultados en la época:  8
f1-score 0.671421206392851
AUC según el mejor F1-score 0.8673082966687619
Confusion Matrix:
 [[13894  2571]
 [ 1253  3907]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8232
Precision:  0.6031
Recall:     0.7572
F1-score:   0.6714

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5015, Test Loss: 0.4597, F1: 0.6458, AUC: 0.8544
Epoch [10/30] Train Loss: 0.4327, Test Loss: 0.4542, F1: 0.6576, AUC: 0.8650
Epoch [20/30] Train Loss: 0.4038, Test Loss: 0.4396, F1: 0.6636, AUC: 0.8637
Mejores resultados en la época:  11
f1-score 0.6704074585635359
AUC según el mejor F1-score 0.8657713154753918
Confusion Matrix:
 [[13924  2541]
 [ 1277  3883]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8234
Precision:  0.6045
Recall:     0.7525
F1-score:   0.6704

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5019, Test Loss: 0.4420, F1: 0.6497, AUC: 0.8542
Epoch [10/30] Train Loss: 0.4317, Test Loss: 0.4453, F1: 0.6626, AUC: 0.8673
Epoch [20/30] Train Loss: 0.4028, Test Loss: 0.4626, F1: 0.6491, AUC: 0.8630
Mejores resultados en la época:  13
f1-score 0.6694625128380691
AUC según el mejor F1-score 0.8665047128393091
Confusion Matrix:
 [[13852  2613]
 [ 1249  3911]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8214
Precision:  0.5995
Recall:     0.7579
F1-score:   0.6695
Tiempo total para red 1: 156.30 segundos

Entrenando red 2 con capas [300, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4916, Test Loss: 0.4447, F1: 0.6523, AUC: 0.8586
Epoch [10/30] Train Loss: 0.3809, Test Loss: 0.4663, F1: 0.6525, AUC: 0.8614
Epoch [20/30] Train Loss: 0.3026, Test Loss: 0.5575, F1: 0.6223, AUC: 0.8410
Mejores resultados en la época:  7
f1-score 0.6645010827919373
AUC según el mejor F1-score 0.8655895521861031
Confusion Matrix:
 [[13608  2857]
 [ 1171  3989]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8137
Precision:  0.5827
Recall:     0.7731
F1-score:   0.6645

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4897, Test Loss: 0.4628, F1: 0.6513, AUC: 0.8601
Epoch [10/30] Train Loss: 0.3848, Test Loss: 0.4793, F1: 0.6473, AUC: 0.8594
Epoch [20/30] Train Loss: 0.3173, Test Loss: 0.5470, F1: 0.6269, AUC: 0.8436
Mejores resultados en la época:  7
f1-score 0.6738598241000997
AUC según el mejor F1-score 0.8632108924968868
Confusion Matrix:
 [[14312  2153]
 [ 1444  3716]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8337
Precision:  0.6332
Recall:     0.7202
F1-score:   0.6739

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4919, Test Loss: 0.4475, F1: 0.6560, AUC: 0.8592
Epoch [10/30] Train Loss: 0.3773, Test Loss: 0.4734, F1: 0.6467, AUC: 0.8600
Epoch [20/30] Train Loss: 0.2977, Test Loss: 0.5736, F1: 0.6193, AUC: 0.8379
Mejores resultados en la época:  6
f1-score 0.6734676083355131
AUC según el mejor F1-score 0.8660576698987987
Confusion Matrix:
 [[14018  2447]
 [ 1298  3862]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8268
Precision:  0.6121
Recall:     0.7484
F1-score:   0.6735

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4929, Test Loss: 0.4575, F1: 0.6501, AUC: 0.8608
Epoch [10/30] Train Loss: 0.3847, Test Loss: 0.4708, F1: 0.6543, AUC: 0.8623
Epoch [20/30] Train Loss: 0.3040, Test Loss: 0.5592, F1: 0.6208, AUC: 0.8387
Mejores resultados en la época:  4
f1-score 0.6722222222222223
AUC según el mejor F1-score 0.8677619015671015
Confusion Matrix:
 [[13977  2488]
 [ 1288  3872]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8254
Precision:  0.6088
Recall:     0.7504
F1-score:   0.6722
Tiempo total para red 2: 170.99 segundos

Entrenando red 3 con capas [300, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4854, Test Loss: 0.4475, F1: 0.6601, AUC: 0.8622
Epoch [10/30] Train Loss: 0.3141, Test Loss: 0.5152, F1: 0.6378, AUC: 0.8410
Epoch [20/30] Train Loss: 0.1678, Test Loss: 0.8558, F1: 0.6145, AUC: 0.8067
Mejores resultados en la época:  1
f1-score 0.6746861924686193
AUC según el mejor F1-score 0.8662103310522438
Confusion Matrix:
 [[14023  2442]
 [ 1290  3870]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8274
Precision:  0.6131
Recall:     0.7500
F1-score:   0.6747

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4861, Test Loss: 0.4361, F1: 0.6663, AUC: 0.8623
Epoch [10/30] Train Loss: 0.3173, Test Loss: 0.5290, F1: 0.6259, AUC: 0.8345
Epoch [20/30] Train Loss: 0.1684, Test Loss: 0.8872, F1: 0.6040, AUC: 0.8065
Mejores resultados en la época:  6
f1-score 0.6702228340359632
AUC según el mejor F1-score 0.8641707803962834
Confusion Matrix:
 [[13897  2568]
 [ 1265  3895]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8228
Precision:  0.6027
Recall:     0.7548
F1-score:   0.6702

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4866, Test Loss: 0.4463, F1: 0.6592, AUC: 0.8617
Epoch [10/30] Train Loss: 0.3155, Test Loss: 0.5074, F1: 0.6453, AUC: 0.8419
Epoch [20/30] Train Loss: 0.1646, Test Loss: 0.8676, F1: 0.6045, AUC: 0.8102
Mejores resultados en la época:  5
f1-score 0.6661978021978022
AUC según el mejor F1-score 0.8621234083574036
Confusion Matrix:
 [[14039  2426]
 [ 1371  3789]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8244
Precision:  0.6097
Recall:     0.7343
F1-score:   0.6662

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4847, Test Loss: 0.4297, F1: 0.6691, AUC: 0.8630
Epoch [10/30] Train Loss: 0.3105, Test Loss: 0.5617, F1: 0.6227, AUC: 0.8374
Epoch [20/30] Train Loss: 0.1626, Test Loss: 0.8718, F1: 0.6087, AUC: 0.8063
Mejores resultados en la época:  5
f1-score 0.6746053724729991
AUC según el mejor F1-score 0.8632062079063647
Confusion Matrix:
 [[14446  2019]
 [ 1506  3654]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8370
Precision:  0.6441
Recall:     0.7081
F1-score:   0.6746
Tiempo total para red 3: 185.75 segundos

Entrenando red 4 con capas [300, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4817, Test Loss: 0.4577, F1: 0.6602, AUC: 0.8615
Epoch [10/30] Train Loss: 0.2481, Test Loss: 0.6017, F1: 0.6239, AUC: 0.8264
Epoch [20/30] Train Loss: 0.0993, Test Loss: 1.2560, F1: 0.5904, AUC: 0.7988
Mejores resultados en la época:  4
f1-score 0.6745541232562247
AUC según el mejor F1-score 0.8676507072790063
Confusion Matrix:
 [[14119  2346]
 [ 1340  3820]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8295
Precision:  0.6195
Recall:     0.7403
F1-score:   0.6746

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4849, Test Loss: 0.4477, F1: 0.6642, AUC: 0.8615
Epoch [10/30] Train Loss: 0.2566, Test Loss: 0.6382, F1: 0.6207, AUC: 0.8318
Epoch [20/30] Train Loss: 0.0927, Test Loss: 1.3283, F1: 0.5954, AUC: 0.8075
Mejores resultados en la época:  2
f1-score 0.6784230449635306
AUC según el mejor F1-score 0.8673854805942603
Confusion Matrix:
 [[14468  1997]
 [ 1486  3674]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8389
Precision:  0.6479
Recall:     0.7120
F1-score:   0.6784

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4856, Test Loss: 0.4275, F1: 0.6713, AUC: 0.8605
Epoch [10/30] Train Loss: 0.2505, Test Loss: 0.6014, F1: 0.6133, AUC: 0.8174
Epoch [20/30] Train Loss: 0.0955, Test Loss: 1.2092, F1: 0.5952, AUC: 0.8004
Mejores resultados en la época:  0
f1-score 0.6713467578052835
AUC según el mejor F1-score 0.8604678999616289
Confusion Matrix:
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
 [[14433  2032]
 [ 1526  3634]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8355
Precision:  0.6414
Recall:     0.7043
F1-score:   0.6713

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4858, Test Loss: 0.4595, F1: 0.6577, AUC: 0.8629
Epoch [10/30] Train Loss: 0.2499, Test Loss: 0.5189, F1: 0.6325, AUC: 0.8305
Epoch [20/30] Train Loss: 0.0853, Test Loss: 1.0619, F1: 0.5964, AUC: 0.8004
Mejores resultados en la época:  2
f1-score 0.6732078140192699
AUC según el mejor F1-score 0.8668028434758248
Confusion Matrix:
 [[14120  2345]
 [ 1352  3808]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8290
Precision:  0.6189
Recall:     0.7380
F1-score:   0.6732
Tiempo total para red 4: 202.86 segundos

Entrenando red 5 con capas [300, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4825, Test Loss: 0.4449, F1: 0.6641, AUC: 0.8629
Epoch [10/30] Train Loss: 0.1704, Test Loss: 0.8156, F1: 0.6128, AUC: 0.8134
Epoch [20/30] Train Loss: 0.0507, Test Loss: 1.5381, F1: 0.5891, AUC: 0.8131
Mejores resultados en la época:  1
f1-score 0.6753315183981734
AUC según el mejor F1-score 0.8666086919163742
Confusion Matrix:
 [[14083  2382]
 [ 1315  3845]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8290
Precision:  0.6175
Recall:     0.7452
F1-score:   0.6753

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4822, Test Loss: 0.4542, F1: 0.6605, AUC: 0.8633
Epoch [10/30] Train Loss: 0.1657, Test Loss: 0.8813, F1: 0.5972, AUC: 0.8129
Epoch [20/30] Train Loss: 0.0516, Test Loss: 1.4549, F1: 0.5951, AUC: 0.8121
Mejores resultados en la época:  3
f1-score 0.6679010267631712
AUC según el mejor F1-score 0.8662812472781116
Confusion Matrix:
 [[13711  2754]
 [ 1192  3968]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8175
Precision:  0.5903
Recall:     0.7690
F1-score:   0.6679

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4835, Test Loss: 0.4439, F1: 0.6563, AUC: 0.8624
Epoch [10/30] Train Loss: 0.1659, Test Loss: 0.8729, F1: 0.6159, AUC: 0.8198
Epoch [20/30] Train Loss: 0.0517, Test Loss: 1.2928, F1: 0.5854, AUC: 0.8035
Mejores resultados en la época:  2
f1-score 0.6800651819663226
AUC según el mejor F1-score 0.868507228158391
Confusion Matrix:
 [[14335  2130]
 [ 1404  3756]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8366
Precision:  0.6381
Recall:     0.7279
F1-score:   0.6801

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4823, Test Loss: 0.4254, F1: 0.6663, AUC: 0.8631
Epoch [10/30] Train Loss: 0.1686, Test Loss: 0.7753, F1: 0.6182, AUC: 0.8211
Epoch [20/30] Train Loss: 0.0490, Test Loss: 1.6589, F1: 0.5915, AUC: 0.8099
Mejores resultados en la época:  2
f1-score 0.6677449561590193
AUC según el mejor F1-score 0.8661813760454993
Confusion Matrix:
 [[13800  2665]
 [ 1238  3922]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8195
Precision:  0.5954
Recall:     0.7601
F1-score:   0.6677
Tiempo total para red 5: 206.73 segundos

Entrenando red 6 con capas [300, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4840, Test Loss: 0.4009, F1: 0.6720, AUC: 0.8637
Epoch [10/30] Train Loss: 0.1288, Test Loss: 0.8924, F1: 0.6108, AUC: 0.8185
Epoch [20/30] Train Loss: 0.0416, Test Loss: 1.8095, F1: 0.5876, AUC: 0.8093
Mejores resultados en la época:  1
f1-score 0.6779598600626036
AUC según el mejor F1-score 0.8671918940105509
Confusion Matrix:
 [[14445  2020]
 [ 1478  3682]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8382
Precision:  0.6457
Recall:     0.7136
F1-score:   0.6780

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4857, Test Loss: 0.4538, F1: 0.6651, AUC: 0.8617
Epoch [10/30] Train Loss: 0.1288, Test Loss: 1.0247, F1: 0.6098, AUC: 0.8240
Epoch [20/30] Train Loss: 0.0450, Test Loss: 1.2199, F1: 0.6026, AUC: 0.8217
Mejores resultados en la época:  0
f1-score 0.6650569161779925
AUC según el mejor F1-score 0.8617070565470094
Confusion Matrix:
 [[13885  2580]
 [ 1304  3856]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8204
Precision:  0.5991
Recall:     0.7473
F1-score:   0.6651

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4873, Test Loss: 0.4621, F1: 0.6678, AUC: 0.8618
Epoch [10/30] Train Loss: 0.1293, Test Loss: 0.9677, F1: 0.6120, AUC: 0.8216
Epoch [20/30] Train Loss: 0.0458, Test Loss: 1.4301, F1: 0.5821, AUC: 0.8149
Mejores resultados en la época:  1
f1-score 0.6686616791354946
AUC según el mejor F1-score 0.8683000056497575
Confusion Matrix:
 [[13617  2848]
 [ 1138  4022]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8157
Precision:  0.5854
Recall:     0.7795
F1-score:   0.6687

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4868, Test Loss: 0.4459, F1: 0.6673, AUC: 0.8620
Epoch [10/30] Train Loss: 0.1334, Test Loss: 0.8787, F1: 0.5981, AUC: 0.8156
Epoch [20/30] Train Loss: 0.0472, Test Loss: 1.3788, F1: 0.6092, AUC: 0.8239
Mejores resultados en la época:  1
f1-score 0.6719745222929936
AUC según el mejor F1-score 0.8661987078533983
Confusion Matrix:
 [[14119  2346]
 [ 1362  3798]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8285
Precision:  0.6182
Recall:     0.7360
F1-score:   0.6720
Tiempo total para red 6: 241.62 segundos
Saved on: outputs_only_text/0/lyrics_bert

==============================
Model: Logistic Regression
Accuracy:  0.8051
Precision: 0.5689
Recall:    0.7560
F1-score:  0.6492
              precision    recall  f1-score   support

           0       0.91      0.82      0.87     16465
           1       0.57      0.76      0.65      5160

    accuracy                           0.81     21625
   macro avg       0.74      0.79      0.76     21625
weighted avg       0.83      0.81      0.81     21625

[[13509  2956]
 [ 1259  3901]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.3293
Precision: 0.2559
Recall:    0.9492
F1-score:  0.4031
              precision    recall  f1-score   support

           0       0.89      0.14      0.23     16465
           1       0.26      0.95      0.40      5160

    accuracy                           0.33     21625
   macro avg       0.58      0.54      0.32     21625
weighted avg       0.74      0.33      0.27     21625

[[ 2223 14242]
 [  262  4898]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_svm.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6610
Precision: 0.3885
Recall:    0.7331
F1-score:  0.5079
              precision    recall  f1-score   support

           0       0.88      0.64      0.74     16465
           1       0.39      0.73      0.51      5160

    accuracy                           0.66     21625
   macro avg       0.64      0.69      0.62     21625
weighted avg       0.77      0.66      0.69     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [21:44:40] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[10511  5954]
 [ 1377  3783]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8063
Precision: 0.5772
Recall:    0.7031
F1-score:  0.6340
              precision    recall  f1-score   support

           0       0.90      0.84      0.87     16465
           1       0.58      0.70      0.63      5160

    accuracy                           0.81     21625
   macro avg       0.74      0.77      0.75     21625
weighted avg       0.82      0.81      0.81     21625

[[13808  2657]
 [ 1532  3628]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8068
Precision: 0.5741
Recall:    0.7364
F1-score:  0.6452
              precision    recall  f1-score   support

           0       0.91      0.83      0.87     16465
           1       0.57      0.74      0.65      5160

    accuracy                           0.81     21625
   macro avg       0.74      0.78      0.76     21625
weighted avg       0.83      0.81      0.81     21625

[[13646  2819]
 [ 1360  3800]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6437
Precision: 0.3792
Recall:    0.7742
F1-score:  0.5090
              precision    recall  f1-score   support

           0       0.89      0.60      0.72     16465
           1       0.38      0.77      0.51      5160

    accuracy                           0.64     21625
   macro avg       0.64      0.69      0.61     21625
weighted avg       0.77      0.64      0.67     21625

[[9924 6541]
 [1165 3995]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.8051, 'precision': 0.5689, 'recall': 0.756, 'f1_score': 0.6492}
XGBoost: {'accuracy': 0.8068, 'precision': 0.5741, 'recall': 0.7364, 'f1_score': 0.6452}
Random Forest: {'accuracy': 0.8063, 'precision': 0.5772, 'recall': 0.7031, 'f1_score': 0.634}
Naive Bayes: {'accuracy': 0.6437, 'precision': 0.3792, 'recall': 0.7742, 'f1_score': 0.509}
Decision Tree: {'accuracy': 0.661, 'precision': 0.3885, 'recall': 0.7331, 'f1_score': 0.5079}
SVM: {'accuracy': 0.3293, 'precision': 0.2559, 'recall': 0.9492, 'f1_score': 0.4031}

##################################################
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1536)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1536)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1536, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4478, Test Loss: 0.3972, F1: 0.7134, AUC: 0.8928
Epoch [10/30] Train Loss: 0.3663, Test Loss: 0.4152, F1: 0.7156, AUC: 0.9070
Epoch [20/30] Train Loss: 0.3527, Test Loss: 0.3614, F1: 0.7323, AUC: 0.9066
Mejores resultados en la época:  24
f1-score 0.7350060147791717
AUC según el mejor F1-score 0.9071812595192528
Confusion Matrix:
 [[14264  2201]
 [  883  4277]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8574
Precision:  0.6602
Recall:     0.8289
F1-score:   0.7350

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4441, Test Loss: 0.4158, F1: 0.7036, AUC: 0.8937
Epoch [10/30] Train Loss: 0.3645, Test Loss: 0.3722, F1: 0.7277, AUC: 0.9074
Epoch [20/30] Train Loss: 0.3512, Test Loss: 0.3723, F1: 0.7293, AUC: 0.9069
Mejores resultados en la época:  17
f1-score 0.7338822704975473
AUC según el mejor F1-score 0.9071031928191582
Confusion Matrix:
 [[14398  2067]
 [  971  4189]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8595
Precision:  0.6696
Recall:     0.8118
F1-score:   0.7339

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4462, Test Loss: 0.4117, F1: 0.7112, AUC: 0.8935
Epoch [10/30] Train Loss: 0.3651, Test Loss: 0.3660, F1: 0.7312, AUC: 0.9076
Epoch [20/30] Train Loss: 0.3524, Test Loss: 0.3905, F1: 0.7237, AUC: 0.9067
Mejores resultados en la época:  7
f1-score 0.7340241796200345
AUC según el mejor F1-score 0.9071521044169333
Confusion Matrix:
 [[14295  2170]
 [  910  4250]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8576
Precision:  0.6620
Recall:     0.8236
F1-score:   0.7340

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4480, Test Loss: 0.4335, F1: 0.6995, AUC: 0.8929
Epoch [10/30] Train Loss: 0.3654, Test Loss: 0.4002, F1: 0.7226, AUC: 0.9064
Epoch [20/30] Train Loss: 0.3511, Test Loss: 0.3863, F1: 0.7241, AUC: 0.9065
Mejores resultados en la época:  29
f1-score 0.7339846113944843
AUC según el mejor F1-score 0.906742373416008
Confusion Matrix:
 [[14303  2162]
 [  915  4245]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8577
Precision:  0.6626
Recall:     0.8227
F1-score:   0.7340
Tiempo total para red 1: 207.92 segundos

Entrenando red 2 con capas [1536, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4311, Test Loss: 0.4031, F1: 0.7155, AUC: 0.8983
Epoch [10/30] Train Loss: 0.3404, Test Loss: 0.3945, F1: 0.7236, AUC: 0.9067
Epoch [20/30] Train Loss: 0.2235, Test Loss: 0.5006, F1: 0.6732, AUC: 0.8787
Mejores resultados en la época:  3
f1-score 0.7359764542936288
AUC según el mejor F1-score 0.9071941421431884
Confusion Matrix:
 [[14324  2141]
 [  909  4251]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8590
Precision:  0.6651
Recall:     0.8238
F1-score:   0.7360

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4294, Test Loss: 0.3865, F1: 0.7209, AUC: 0.8992
Epoch [10/30] Train Loss: 0.3446, Test Loss: 0.3530, F1: 0.7353, AUC: 0.9068
Epoch [20/30] Train Loss: 0.2501, Test Loss: 0.4815, F1: 0.7031, AUC: 0.8919
Mejores resultados en la época:  11
f1-score 0.7357062048559743
AUC según el mejor F1-score 0.9059418322163293
Confusion Matrix:
 [[14361  2104]
 [  933  4227]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8596
Precision:  0.6677
Recall:     0.8192
F1-score:   0.7357

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4304, Test Loss: 0.3653, F1: 0.7263, AUC: 0.8981
Epoch [10/30] Train Loss: 0.3405, Test Loss: 0.3846, F1: 0.7236, AUC: 0.9067
Epoch [20/30] Train Loss: 0.2327, Test Loss: 0.4252, F1: 0.7110, AUC: 0.8922
Mejores resultados en la época:  5
f1-score 0.737579952685534
AUC según el mejor F1-score 0.90827854245675
Confusion Matrix:
 [[14421  2044]
 [  951  4209]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8615
Precision:  0.6731
Recall:     0.8157
F1-score:   0.7376

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4306, Test Loss: 0.3678, F1: 0.7237, AUC: 0.8990
Epoch [10/30] Train Loss: 0.3427, Test Loss: 0.3810, F1: 0.7256, AUC: 0.9075
Epoch [20/30] Train Loss: 0.2427, Test Loss: 0.4195, F1: 0.7115, AUC: 0.8894
Mejores resultados en la época:  2
f1-score 0.7354455791219342
AUC según el mejor F1-score 0.9062433762479489
Confusion Matrix:
 [[14381  2084]
 [  947  4213]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8598
Precision:  0.6690
Recall:     0.8165
F1-score:   0.7354
Tiempo total para red 2: 223.11 segundos

Entrenando red 3 con capas [1536, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4270, Test Loss: 0.3896, F1: 0.7222, AUC: 0.9004
Epoch [10/30] Train Loss: 0.3250, Test Loss: 0.3898, F1: 0.7254, AUC: 0.9007
Epoch [20/30] Train Loss: 0.1773, Test Loss: 0.6022, F1: 0.6469, AUC: 0.8579
Mejores resultados en la época:  4
f1-score 0.7380806040916674
AUC según el mejor F1-score 0.9078484252478242
Confusion Matrix:
 [[14439  2026]
 [  957  4203]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8621
Precision:  0.6747
Recall:     0.8145
F1-score:   0.7381

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4233, Test Loss: 0.3698, F1: 0.7260, AUC: 0.9016
Epoch [10/30] Train Loss: 0.3246, Test Loss: 0.4305, F1: 0.7002, AUC: 0.9030
Epoch [20/30] Train Loss: 0.1710, Test Loss: 0.6713, F1: 0.6631, AUC: 0.8672
Mejores resultados en la época:  7
f1-score 0.7379053694843168
AUC según el mejor F1-score 0.9065823616927616
Confusion Matrix:
 [[14503  1962]
 [  996  4164]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8632
Precision:  0.6797
Recall:     0.8070
F1-score:   0.7379

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4276, Test Loss: 0.3813, F1: 0.7228, AUC: 0.9017
Epoch [10/30] Train Loss: 0.3156, Test Loss: 0.4300, F1: 0.7042, AUC: 0.8982
Epoch [20/30] Train Loss: 0.1572, Test Loss: 0.7855, F1: 0.6639, AUC: 0.8607
Mejores resultados en la época:  3
f1-score 0.7357015985790408
AUC según el mejor F1-score 0.9072054122321958
Confusion Matrix:
 [[14507  1958]
 [ 1018  4142]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8624
Precision:  0.6790
Recall:     0.8027
F1-score:   0.7357

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4253, Test Loss: 0.4267, F1: 0.7027, AUC: 0.9001
Epoch [10/30] Train Loss: 0.3312, Test Loss: 0.3754, F1: 0.7311, AUC: 0.9060
Epoch [20/30] Train Loss: 0.1797, Test Loss: 0.5964, F1: 0.6647, AUC: 0.8726
Mejores resultados en la época:  6
f1-score 0.7355442176870748
AUC según el mejor F1-score 0.9076655732032005
Confusion Matrix:
 [[14190  2275]
 [  835  4325]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8562
Precision:  0.6553
Recall:     0.8382
F1-score:   0.7355
Tiempo total para red 3: 236.74 segundos

Entrenando red 4 con capas [1536, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4259, Test Loss: 0.4229, F1: 0.7009, AUC: 0.9014
Epoch [10/30] Train Loss: 0.2937, Test Loss: 0.4420, F1: 0.6977, AUC: 0.8954
Epoch [20/30] Train Loss: 0.1207, Test Loss: 0.7525, F1: 0.6740, AUC: 0.8633
Mejores resultados en la época:  5
f1-score 0.7360711483128434
AUC según el mejor F1-score 0.9069319110069044
Confusion Matrix:
 [[14377  2088]
 [  939  4221]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8600
Precision:  0.6690
Recall:     0.8180
F1-score:   0.7361

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4226, Test Loss: 0.3934, F1: 0.7157, AUC: 0.9021
Epoch [10/30] Train Loss: 0.2947, Test Loss: 0.4223, F1: 0.6943, AUC: 0.8939
Epoch [20/30] Train Loss: 0.1147, Test Loss: 0.8126, F1: 0.6721, AUC: 0.8587
Mejores resultados en la época:  5
f1-score 0.736220814165437
AUC según el mejor F1-score 0.9060459113411818
Confusion Matrix:
 [[14345  2120]
 [  919  4241]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8595
Precision:  0.6667
Recall:     0.8219
F1-score:   0.7362

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4210, Test Loss: 0.4058, F1: 0.7222, AUC: 0.9032
Epoch [10/30] Train Loss: 0.2807, Test Loss: 0.4098, F1: 0.7021, AUC: 0.8889
Epoch [20/30] Train Loss: 0.1065, Test Loss: 0.8703, F1: 0.6447, AUC: 0.8537
Mejores resultados en la época:  5
f1-score 0.735657029631631
AUC según el mejor F1-score 0.9063415525533373
Confusion Matrix:
 [[14606  1859]
 [ 1076  4084]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8643
Precision:  0.6872
Recall:     0.7915
F1-score:   0.7357

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4231, Test Loss: 0.4123, F1: 0.7130, AUC: 0.9024
Epoch [10/30] Train Loss: 0.2945, Test Loss: 0.4239, F1: 0.7081, AUC: 0.8964
Epoch [20/30] Train Loss: 0.1239, Test Loss: 0.7916, F1: 0.6995, AUC: 0.8764
Mejores resultados en la época:  2
f1-score 0.7316989835141369
AUC según el mejor F1-score 0.9079672761342477
Confusion Matrix:
 [[14201  2264]
 [  877  4283]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8548
Precision:  0.6542
Recall:     0.8300
F1-score:   0.7317
Tiempo total para red 4: 253.34 segundos

Entrenando red 5 con capas [1536, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4183, Test Loss: 0.4193, F1: 0.7030, AUC: 0.9031
Epoch [10/30] Train Loss: 0.2588, Test Loss: 0.5047, F1: 0.6847, AUC: 0.8864
Epoch [20/30] Train Loss: 0.0798, Test Loss: 1.0061, F1: 0.6680, AUC: 0.8632
Mejores resultados en la época:  1
f1-score 0.7296636601196999
AUC según el mejor F1-score 0.906174425666848
Confusion Matrix:
 [[14090  2375]
 [  832  4328]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8517
Precision:  0.6457
Recall:     0.8388
F1-score:   0.7297

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4212, Test Loss: 0.3614, F1: 0.7301, AUC: 0.9033
Epoch [10/30] Train Loss: 0.2529, Test Loss: 0.4882, F1: 0.6701, AUC: 0.8820
Epoch [20/30] Train Loss: 0.0764, Test Loss: 1.0325, F1: 0.6645, AUC: 0.8530
Mejores resultados en la época:  4
f1-score 0.7347764871524112
AUC según el mejor F1-score 0.9078653980607209
Confusion Matrix:
 [[14436  2029]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [22:21:43] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
 [  985  4175]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8606
Precision:  0.6730
Recall:     0.8091
F1-score:   0.7348

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4215, Test Loss: 0.3376, F1: 0.7225, AUC: 0.9028
Epoch [10/30] Train Loss: 0.2500, Test Loss: 0.5200, F1: 0.6875, AUC: 0.8907
Epoch [20/30] Train Loss: 0.0894, Test Loss: 0.8400, F1: 0.6662, AUC: 0.8602
Mejores resultados en la época:  6
f1-score 0.7303323517038284
AUC según el mejor F1-score 0.9054596018804276
Confusion Matrix:
 [[14080  2385]
 [  820  4340]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8518
Precision:  0.6454
Recall:     0.8411
F1-score:   0.7303

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4164, Test Loss: 0.3685, F1: 0.7274, AUC: 0.9032
Epoch [10/30] Train Loss: 0.2383, Test Loss: 0.5002, F1: 0.6958, AUC: 0.8856
Epoch [20/30] Train Loss: 0.0752, Test Loss: 0.9707, F1: 0.6680, AUC: 0.8654
Mejores resultados en la época:  2
f1-score 0.7336561743341404
AUC según el mejor F1-score 0.9072236268146904
Confusion Matrix:
 [[14303  2162]
 [  918  4242]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8576
Precision:  0.6624
Recall:     0.8221
F1-score:   0.7337
Tiempo total para red 5: 261.58 segundos

Entrenando red 6 con capas [1536, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4236, Test Loss: 0.4225, F1: 0.6961, AUC: 0.9022
Epoch [10/30] Train Loss: 0.2354, Test Loss: 0.4376, F1: 0.7054, AUC: 0.8853
Epoch [20/30] Train Loss: 0.0678, Test Loss: 1.0507, F1: 0.6902, AUC: 0.8761
Mejores resultados en la época:  6
f1-score 0.7317325071004389
AUC según el mejor F1-score 0.9053933172786061
Confusion Matrix:
 [[14257  2208]
 [  909  4251]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8559
Precision:  0.6582
Recall:     0.8238
F1-score:   0.7317

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4212, Test Loss: 0.3628, F1: 0.7239, AUC: 0.9032
Epoch [10/30] Train Loss: 0.2295, Test Loss: 0.4908, F1: 0.6980, AUC: 0.8861
Epoch [20/30] Train Loss: 0.0618, Test Loss: 1.1676, F1: 0.6588, AUC: 0.8572
Mejores resultados en la época:  4
f1-score 0.7330526677549618
AUC según el mejor F1-score 0.9079688710136842
Confusion Matrix:
 [[14252  2213]
 [  894  4266]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8563
Precision:  0.6584
Recall:     0.8267
F1-score:   0.7331

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4189, Test Loss: 0.3494, F1: 0.7279, AUC: 0.9032
Epoch [10/30] Train Loss: 0.2357, Test Loss: 0.5671, F1: 0.6560, AUC: 0.8816
Epoch [20/30] Train Loss: 0.0662, Test Loss: 0.9860, F1: 0.6521, AUC: 0.8562
Mejores resultados en la época:  5
f1-score 0.7355623100303952
AUC según el mejor F1-score 0.9070322883636184
Confusion Matrix:
 [[14553  1912]
 [ 1046  4114]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8632
Precision:  0.6827
Recall:     0.7973
F1-score:   0.7356

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4241, Test Loss: 0.3889, F1: 0.7221, AUC: 0.9029
Epoch [10/30] Train Loss: 0.2274, Test Loss: 0.5294, F1: 0.6974, AUC: 0.8892
Epoch [20/30] Train Loss: 0.0669, Test Loss: 1.1015, F1: 0.6743, AUC: 0.8702
Mejores resultados en la época:  2
f1-score 0.733713283647523
AUC según el mejor F1-score 0.9065379522454254
Confusion Matrix:
 [[14404  2061]
 [  976  4184]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8596
Precision:  0.6700
Recall:     0.8109
F1-score:   0.7337
Tiempo total para red 6: 294.13 segundos
Saved on: outputs_only_text/0/gpt

==============================
Model: Logistic Regression
Accuracy:  0.8477
Precision: 0.6414
Recall:    0.8202
F1-score:  0.7199
              precision    recall  f1-score   support

           0       0.94      0.86      0.90     16465
           1       0.64      0.82      0.72      5160

    accuracy                           0.85     21625
   macro avg       0.79      0.84      0.81     21625
weighted avg       0.87      0.85      0.85     21625

[[14099  2366]
 [  928  4232]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text/0/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7494
Precision: 0.4824
Recall:    0.6891
F1-score:  0.5675
              precision    recall  f1-score   support

           0       0.89      0.77      0.82     16465
           1       0.48      0.69      0.57      5160

    accuracy                           0.75     21625
   macro avg       0.68      0.73      0.70     21625
weighted avg       0.79      0.75      0.76     21625

[[12649  3816]
 [ 1604  3556]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_svm.png
Modelo guardado como: outputs_only_text/0/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7428
Precision: 0.4750
Recall:    0.7399
F1-score:  0.5786
              precision    recall  f1-score   support

           0       0.90      0.74      0.81     16465
           1       0.47      0.74      0.58      5160

    accuracy                           0.74     21625
   macro avg       0.69      0.74      0.70     21625
weighted avg       0.80      0.74      0.76     21625

[[12245  4220]
 [ 1342  3818]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text/0/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8384
Precision: 0.6376
Recall:    0.7477
F1-score:  0.6883
              precision    recall  f1-score   support

           0       0.92      0.87      0.89     16465
           1       0.64      0.75      0.69      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.81      0.79     21625
weighted avg       0.85      0.84      0.84     21625

[[14272  2193]
 [ 1302  3858]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text/0/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8400
Precision: 0.6292
Recall:    0.8021
F1-score:  0.7052
              precision    recall  f1-score   support

           0       0.93      0.85      0.89     16465
           1       0.63      0.80      0.71      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.83      0.80     21625
weighted avg       0.86      0.84      0.85     21625

[[14026  2439]
 [ 1021  4139]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text/0/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8308
Precision: 0.6256
Recall:    0.7238
F1-score:  0.6712
              precision    recall  f1-score   support

           0       0.91      0.86      0.89     16465
           1       0.63      0.72      0.67      5160

    accuracy                           0.83     21625
   macro avg       0.77      0.79      0.78     21625
weighted avg       0.84      0.83      0.83     21625

[[14230  2235]
 [ 1425  3735]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text/0/gpt/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.8477, 'precision': 0.6414, 'recall': 0.8202, 'f1_score': 0.7199}
XGBoost: {'accuracy': 0.84, 'precision': 0.6292, 'recall': 0.8021, 'f1_score': 0.7052}
Random Forest: {'accuracy': 0.8384, 'precision': 0.6376, 'recall': 0.7477, 'f1_score': 0.6883}
Naive Bayes: {'accuracy': 0.8308, 'precision': 0.6256, 'recall': 0.7238, 'f1_score': 0.6712}
Decision Tree: {'accuracy': 0.7428, 'precision': 0.475, 'recall': 0.7399, 'f1_score': 0.5786}
SVM: {'accuracy': 0.7494, 'precision': 0.4824, 'recall': 0.6891, 'f1_score': 0.5675}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
XGBoost: {'accuracy': 0.8667, 'precision': 0.6768, 'recall': 0.8448, 'f1_score': 0.7515}
Decision Tree: {'accuracy': 0.8635, 'precision': 0.6777, 'recall': 0.8159, 'f1_score': 0.7404}
Random Forest: {'accuracy': 0.854, 'precision': 0.6743, 'recall': 0.7504, 'f1_score': 0.7103}
MLP_650497: {'accuracy': 0.8401849710982658, 'precision': 0.6284680337756333, 'recall': 0.8077519379844961, 'f1_score': 0.7076346923142395, 'f1_score_avg': 0.704437135973314}
MLP_1323521: {'accuracy': 0.8364393063583815, 'precision': 0.620561580745803, 'recall': 0.8094961240310078, 'f1_score': 0.7076346923142395, 'f1_score_avg': 0.7019323345921449}
MLP_2733057: {'accuracy': 0.8353757225433526, 'precision': 0.6173364623056615, 'recall': 0.8156976744186046, 'f1_score': 0.7076346923142395, 'f1_score_avg': 0.6998800780319193}
MLP_5820417: {'accuracy': 0.8414335260115607, 'precision': 0.6364926667718026, 'recall': 0.7821705426356589, 'f1_score': 0.7076346923142395, 'f1_score_avg': 0.7027170517889962}
MLP_322177: {'accuracy': 0.8259421965317919, 'precision': 0.5990632983252909, 'recall': 0.8180232558139535, 'f1_score': 0.7070193285859614, 'f1_score_avg': 0.6975099071956935}
MLP_160065: {'accuracy': 0.8345433526011561, 'precision': 0.6218798151001541, 'recall': 0.7821705426356589, 'f1_score': 0.7005884389061959, 'f1_score_avg': 0.6975188755286845}
Logistic Regression: {'accuracy': 0.8247, 'precision': 0.6063, 'recall': 0.757, 'f1_score': 0.6733}
Naive Bayes: {'accuracy': 0.8212, 'precision': 0.608, 'recall': 0.7052, 'f1_score': 0.653}
SVM: {'accuracy': 0.6144, 'precision': 0.3332, 'recall': 0.6155, 'f1_score': 0.4324}


EMBEDDINGS TYPE: LYRICS_BERT
MLP_326657: {'accuracy': 0.819514450867052, 'precision': 0.5954152117807803, 'recall': 0.7600775193798449, 'f1_score': 0.6800651819663226, 'f1_score_avg': 0.6727606708216716}
MLP_1007617: {'accuracy': 0.8285317919075145, 'precision': 0.6181640625, 'recall': 0.736046511627907, 'f1_score': 0.6800651819663226, 'f1_score_avg': 0.670913244417271}
MLP_120321: {'accuracy': 0.8290404624277457, 'precision': 0.6188850967007964, 'recall': 0.737984496124031, 'f1_score': 0.6784230449635306, 'f1_score_avg': 0.6743829350110772}
MLP_48897: {'accuracy': 0.8369942196531792, 'precision': 0.6441036488630354, 'recall': 0.708139534883721, 'f1_score': 0.6746861924686193, 'f1_score_avg': 0.671428050293846}
MLP_21377: {'accuracy': 0.8253872832369942, 'precision': 0.6088050314465409, 'recall': 0.7503875968992249, 'f1_score': 0.6738598241000997, 'f1_score_avg': 0.6710126843624431}
MLP_9665: {'accuracy': 0.8214104046242775, 'precision': 0.5994788473329246, 'recall': 0.7579457364341086, 'f1_score': 0.671421206392851, 'f1_score_avg': 0.6694755490395989}
Logistic Regression: {'accuracy': 0.8051, 'precision': 0.5689, 'recall': 0.756, 'f1_score': 0.6492}
XGBoost: {'accuracy': 0.8068, 'precision': 0.5741, 'recall': 0.7364, 'f1_score': 0.6452}
Random Forest: {'accuracy': 0.8063, 'precision': 0.5772, 'recall': 0.7031, 'f1_score': 0.634}
Naive Bayes: {'accuracy': 0.6437, 'precision': 0.3792, 'recall': 0.7742, 'f1_score': 0.509}
Decision Tree: {'accuracy': 0.661, 'precision': 0.3885, 'recall': 0.7331, 'f1_score': 0.5079}
SVM: {'accuracy': 0.3293, 'precision': 0.2559, 'recall': 0.9492, 'f1_score': 0.4031}


EMBEDDINGS TYPE: GPT
MLP_207105: {'accuracy': 0.8561849710982659, 'precision': 0.6553030303030303, 'recall': 0.8381782945736435, 'f1_score': 0.7380806040916674, 'f1_score_avg': 0.7368079474605249}
MLP_436737: {'accuracy': 0.8547514450867052, 'precision': 0.6541927600427677, 'recall': 0.8300387596899225, 'f1_score': 0.7380806040916674, 'f1_score_avg': 0.7349119939060121}
MLP_959489: {'accuracy': 0.8575722543352601, 'precision': 0.6623985009369144, 'recall': 0.8220930232558139, 'f1_score': 0.7380806040916674, 'f1_score_avg': 0.73210716832752}
MLP_2273281: {'accuracy': 0.8595606936416185, 'precision': 0.6699759807846277, 'recall': 0.8108527131782945, 'f1_score': 0.7380806040916674, 'f1_score_avg': 0.7335151921333298}
MLP_100481: {'accuracy': 0.8598381502890173, 'precision': 0.6690487533746229, 'recall': 0.8164728682170542, 'f1_score': 0.737579952685534, 'f1_score_avg': 0.7361770477392678}
MLP_49217: {'accuracy': 0.8577109826589595, 'precision': 0.6625565787420009, 'recall': 0.8226744186046512, 'f1_score': 0.7350060147791717, 'f1_score_avg': 0.7342242690728095}
Logistic Regression: {'accuracy': 0.8477, 'precision': 0.6414, 'recall': 0.8202, 'f1_score': 0.7199}
XGBoost: {'accuracy': 0.84, 'precision': 0.6292, 'recall': 0.8021, 'f1_score': 0.7052}
Random Forest: {'accuracy': 0.8384, 'precision': 0.6376, 'recall': 0.7477, 'f1_score': 0.6883}
Naive Bayes: {'accuracy': 0.8308, 'precision': 0.6256, 'recall': 0.7238, 'f1_score': 0.6712}
Decision Tree: {'accuracy': 0.7428, 'precision': 0.475, 'recall': 0.7399, 'f1_score': 0.5786}
SVM: {'accuracy': 0.7494, 'precision': 0.4824, 'recall': 0.6891, 'f1_score': 0.5675}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: False
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: Not used
====================================

