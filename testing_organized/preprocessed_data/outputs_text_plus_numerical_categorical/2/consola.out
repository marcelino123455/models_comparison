2025-10-12 18:06:23.399288: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text_plus_numerical_categorical.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 17 ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Length', 'Genre', 'Album', 'Loudness (db)', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']

CAT_LOW 3 ['emotion', 'Key', 'Time signature']
CAT_HIGH 14 ['Artist(s)', 'song', 'Length', 'Genre', 'Album', 'Loudness (db)', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
--> PaTH:  ../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy

##################################################
Running experiment with TFIDF embeddings
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Contaning the categorical cols
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_text:  (86500, 5000)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 10023)
Shape of X_test after concatenation:  (21625, 10023)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 10023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 10023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [10023, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.2804, Test Loss: 0.1866, F1: 0.8654, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0191, Test Loss: 0.4084, F1: 0.8390, AUC: 0.9747
Epoch [20/30] Train Loss: 0.0037, Test Loss: 0.7095, F1: 0.8370, AUC: 0.9698
Mejores resultados en la época:  2
f1-score 0.8684018929741536
AUC según el mejor F1-score 0.9813842317624655
Confusion Matrix:
 [[15408  1057]
 [  389  4771]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_320801.png
Accuracy:   0.9331
Precision:  0.8186
Recall:     0.9246
F1-score:   0.8684

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.2782, Test Loss: 0.1740, F1: 0.8709, AUC: 0.9794
Epoch [10/30] Train Loss: 0.0196, Test Loss: 0.4344, F1: 0.8356, AUC: 0.9745
Epoch [20/30] Train Loss: 0.0042, Test Loss: 0.8035, F1: 0.8255, AUC: 0.9689
Mejores resultados en la época:  2
f1-score 0.8754158964879852
AUC según el mejor F1-score 0.9816377175450861
Confusion Matrix:
 [[15541   924]
 [  424  4736]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_320801.png
Accuracy:   0.9377
Precision:  0.8367
Recall:     0.9178
F1-score:   0.8754

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.2853, Test Loss: 0.1947, F1: 0.8613, AUC: 0.9792
Epoch [10/30] Train Loss: 0.0121, Test Loss: 0.3610, F1: 0.8494, AUC: 0.9760
Epoch [20/30] Train Loss: 0.0043, Test Loss: 0.6916, F1: 0.8320, AUC: 0.9729
Mejores resultados en la época:  3
f1-score 0.8739134498551266
AUC según el mejor F1-score 0.980509278549519
Confusion Matrix:
 [[15601   864]
 [  485  4675]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_320801.png
Accuracy:   0.9376
Precision:  0.8440
Recall:     0.9060
F1-score:   0.8739

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.2808, Test Loss: 0.2029, F1: 0.8546, AUC: 0.9798
Epoch [10/30] Train Loss: 0.0153, Test Loss: 0.3815, F1: 0.8473, AUC: 0.9756
Epoch [20/30] Train Loss: 0.0032, Test Loss: 0.7295, F1: 0.8331, AUC: 0.9703
Mejores resultados en la época:  1
f1-score 0.8810531264692054
AUC según el mejor F1-score 0.9823906065720803
Confusion Matrix:
 [[15675   790]
 [  475  4685]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_320801.png
Accuracy:   0.9415
Precision:  0.8557
Recall:     0.9079
F1-score:   0.8811
Tiempo total para red 1: 569.86 segundos

Entrenando red 2 con capas [10023, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.2378, Test Loss: 0.1640, F1: 0.8738, AUC: 0.9811
Epoch [10/30] Train Loss: 0.0050, Test Loss: 0.5286, F1: 0.8540, AUC: 0.9769
Epoch [20/30] Train Loss: 0.0007, Test Loss: 0.6321, F1: 0.8650, AUC: 0.9767
Mejores resultados en la época:  0
f1-score 0.8737720111214087
AUC según el mejor F1-score 0.981109288672001
Confusion Matrix:
 [[15549   916]
 [  446  4714]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_643649.png
Accuracy:   0.9370
Precision:  0.8373
Recall:     0.9136
F1-score:   0.8738

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.2418, Test Loss: 0.1731, F1: 0.8680, AUC: 0.9807
Epoch [10/30] Train Loss: 0.0054, Test Loss: 0.4162, F1: 0.8647, AUC: 0.9779
Epoch [20/30] Train Loss: 0.0001, Test Loss: 0.7161, F1: 0.8525, AUC: 0.9757
Mejores resultados en la época:  2
f1-score 0.8692901665271188
AUC según el mejor F1-score 0.9793476413439831
Confusion Matrix:
 [[15548   917]
 [  488  4672]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_643649.png
Accuracy:   0.9350
Precision:  0.8359
Recall:     0.9054
F1-score:   0.8693

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.2388, Test Loss: 0.1803, F1: 0.8640, AUC: 0.9810
Epoch [10/30] Train Loss: 0.0003, Test Loss: 0.5495, F1: 0.8567, AUC: 0.9773
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.9862, F1: 0.8470, AUC: 0.9703
Mejores resultados en la época:  3
f1-score 0.8649046873561101
AUC según el mejor F1-score 0.9779487437528982
Confusion Matrix:
 [[15462  1003]
 [  464  4696]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_643649.png
Accuracy:   0.9322
Precision:  0.8240
Recall:     0.9101
F1-score:   0.8649

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.2415, Test Loss: 0.1655, F1: 0.8754, AUC: 0.9811
Epoch [10/30] Train Loss: 0.0045, Test Loss: 0.5136, F1: 0.8544, AUC: 0.9768
Epoch [20/30] Train Loss: 0.0001, Test Loss: 0.7145, F1: 0.8540, AUC: 0.9752
Mejores resultados en la época:  0
f1-score 0.8754165124028138
AUC según el mejor F1-score 0.9811364545888978
Confusion Matrix:
 [[15550   915]
 [  431  4729]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_643649.png
Accuracy:   0.9378
Precision:  0.8379
Recall:     0.9165
F1-score:   0.8754
Tiempo total para red 2: 590.92 segundos

Entrenando red 3 con capas [10023, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.2357, Test Loss: 0.1883, F1: 0.8577, AUC: 0.9813
Epoch [10/30] Train Loss: 0.0019, Test Loss: 0.4234, F1: 0.8774, AUC: 0.9805
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7650, F1: 0.8631, AUC: 0.9774
Mejores resultados en la época:  10
f1-score 0.8773978315262719
AUC según el mejor F1-score 0.9805281463852146
Confusion Matrix:
 [[15568   897]
 [  426  4734]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_1293441.png
Accuracy:   0.9388
Precision:  0.8407
Recall:     0.9174
F1-score:   0.8774

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.2368, Test Loss: 0.1755, F1: 0.8658, AUC: 0.9813
Epoch [10/30] Train Loss: 0.0029, Test Loss: 0.4690, F1: 0.8646, AUC: 0.9776
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8184, F1: 0.8595, AUC: 0.9746
Mejores resultados en la época:  14
f1-score 0.8723859094711405
AUC según el mejor F1-score 0.9781296831192311
Confusion Matrix:
 [[15559   906]
 [  467  4693]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_1293441.png
Accuracy:   0.9365
Precision:  0.8382
Recall:     0.9095
F1-score:   0.8724

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.2366, Test Loss: 0.1976, F1: 0.8535, AUC: 0.9809
Epoch [10/30] Train Loss: 0.0031, Test Loss: 0.5436, F1: 0.8532, AUC: 0.9786
Epoch [20/30] Train Loss: 0.0005, Test Loss: 0.5295, F1: 0.8732, AUC: 0.9791
Mejores resultados en la época:  15
f1-score 0.8849951597289448
AUC según el mejor F1-score 0.9785238478614491
Confusion Matrix:
 [[15866   599]
 [  589  4571]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_1293441.png
Accuracy:   0.9451
Precision:  0.8841
Recall:     0.8859
F1-score:   0.8850

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.2338, Test Loss: 0.1506, F1: 0.8807, AUC: 0.9799
Epoch [10/30] Train Loss: 0.0049, Test Loss: 0.4569, F1: 0.8598, AUC: 0.9803
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7367, F1: 0.8643, AUC: 0.9772
Mejores resultados en la época:  0
f1-score 0.8806608357628766
AUC según el mejor F1-score 0.979905372448487
Confusion Matrix:
 [[15866   599]
 [  629  4531]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_1293441.png
Accuracy:   0.9432
Precision:  0.8832
Recall:     0.8781
F1-score:   0.8807
Tiempo total para red 3: 653.39 segundos

Entrenando red 4 con capas [10023, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.2327, Test Loss: 0.1649, F1: 0.8707, AUC: 0.9814
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.4654, F1: 0.8775, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7103, F1: 0.8775, AUC: 0.9792
Mejores resultados en la época:  11
f1-score 0.8845759074367827
AUC según el mejor F1-score 0.9817753421045815
Confusion Matrix:
 [[15648   817]
 [  420  4740]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_2609409.png
Accuracy:   0.9428
Precision:  0.8530
Recall:     0.9186
F1-score:   0.8846

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.2302, Test Loss: 0.1775, F1: 0.8663, AUC: 0.9808
Epoch [10/30] Train Loss: 0.0007, Test Loss: 0.6088, F1: 0.8655, AUC: 0.9798
Epoch [20/30] Train Loss: 0.0001, Test Loss: 0.7607, F1: 0.8636, AUC: 0.9785
Mejores resultados en la época:  12
f1-score 0.8862700009581297
AUC según el mejor F1-score 0.9810392257949091
Confusion Matrix:
 [[15813   652]
 [  535  4625]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_2609409.png
Accuracy:   0.9451
Precision:  0.8764
Recall:     0.8963
F1-score:   0.8863

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.2327, Test Loss: 0.1872, F1: 0.8571, AUC: 0.9814
Epoch [10/30] Train Loss: 0.0025, Test Loss: 0.5203, F1: 0.8631, AUC: 0.9803
Epoch [20/30] Train Loss: 0.0003, Test Loss: 0.4416, F1: 0.8851, AUC: 0.9809
Mejores resultados en la época:  20
f1-score 0.8851220457187137
AUC según el mejor F1-score 0.9809388896343427
Confusion Matrix:
 [[15870   595]
 [  591  4569]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_2609409.png
Accuracy:   0.9452
Precision:  0.8848
Recall:     0.8855
F1-score:   0.8851

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.2323, Test Loss: 0.1874, F1: 0.8570, AUC: 0.9810
Epoch [10/30] Train Loss: 0.0012, Test Loss: 0.6168, F1: 0.8687, AUC: 0.9799
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7503, F1: 0.8701, AUC: 0.9783
Mejores resultados en la época:  14
f1-score 0.8848974985894301
AUC según el mejor F1-score 0.980229974552551
Confusion Matrix:
 [[15696   769]
 [  455  4705]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_2609409.png
Accuracy:   0.9434
Precision:  0.8595
Recall:     0.9118
F1-score:   0.8849
Tiempo total para red 4: 760.69 segundos

Entrenando red 5 con capas [10023, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.2254, Test Loss: 0.1819, F1: 0.8649, AUC: 0.9817
Epoch [10/30] Train Loss: 0.0014, Test Loss: 0.6415, F1: 0.8647, AUC: 0.9813
Epoch [20/30] Train Loss: 0.0008, Test Loss: 0.4862, F1: 0.8888, AUC: 0.9817
Mejores resultados en la época:  20
f1-score 0.8887851339994397
AUC según el mejor F1-score 0.98173772413647
Confusion Matrix:
 [[15675   790]
 [  401  4759]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_5304833.png
Accuracy:   0.9449
Precision:  0.8576
Recall:     0.9223
F1-score:   0.8888

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.2275, Test Loss: 0.1713, F1: 0.8661, AUC: 0.9815
Epoch [10/30] Train Loss: 0.0018, Test Loss: 0.5407, F1: 0.8875, AUC: 0.9818
Epoch [20/30] Train Loss: 0.0009, Test Loss: 0.3898, F1: 0.8916, AUC: 0.9823
Mejores resultados en la época:  20
f1-score 0.8916275430359938
AUC según el mejor F1-score 0.9823379166990353
Confusion Matrix:
 [[15959   506]
 [  602  4558]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_5304833.png
Accuracy:   0.9488
Precision:  0.9001
Recall:     0.8833
F1-score:   0.8916

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.2274, Test Loss: 0.1593, F1: 0.8712, AUC: 0.9815
Epoch [10/30] Train Loss: 0.0034, Test Loss: 0.5135, F1: 0.8585, AUC: 0.9814
Epoch [20/30] Train Loss: 0.0010, Test Loss: 0.7588, F1: 0.8362, AUC: 0.9797
Mejores resultados en la época:  1
f1-score 0.8830153559795254
AUC según el mejor F1-score 0.982632604514627
Confusion Matrix:
 [[15624   841]
 [  416  4744]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_5304833.png
Accuracy:   0.9419
Precision:  0.8494
Recall:     0.9194
F1-score:   0.8830

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.2259, Test Loss: 0.1543, F1: 0.8799, AUC: 0.9809
Epoch [10/30] Train Loss: 0.0023, Test Loss: 0.4622, F1: 0.8821, AUC: 0.9826
Epoch [20/30] Train Loss: 0.0008, Test Loss: 0.6360, F1: 0.8661, AUC: 0.9807
Mejores resultados en la época:  24
f1-score 0.895239934343922
AUC según el mejor F1-score 0.9814098322257455
Confusion Matrix:
 [[15904   561]
 [  524  4636]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_5304833.png
Accuracy:   0.9498
Precision:  0.8921
Recall:     0.8984
F1-score:   0.8952
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [19:37:13] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Tiempo total para red 5: 767.69 segundos

Entrenando red 6 con capas [10023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.2271, Test Loss: 0.1523, F1: 0.8753, AUC: 0.9820
Epoch [10/30] Train Loss: 0.0051, Test Loss: 0.4754, F1: 0.8752, AUC: 0.9819
Epoch [20/30] Train Loss: 0.0000, Test Loss: 1.1455, F1: 0.8803, AUC: 0.9743
Mejores resultados en la época:  2
f1-score 0.8896655960702815
AUC según el mejor F1-score 0.9832116163720553
Confusion Matrix:
 [[15748   717]
 [  451  4709]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_10963969.png
Accuracy:   0.9460
Precision:  0.8679
Recall:     0.9126
F1-score:   0.8897

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.2271, Test Loss: 0.1787, F1: 0.8610, AUC: 0.9815
Epoch [10/30] Train Loss: 0.0007, Test Loss: 0.9355, F1: 0.8802, AUC: 0.9761
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.9375, F1: 0.8823, AUC: 0.9754
Mejores resultados en la época:  3
f1-score 0.8927177534507378
AUC según el mejor F1-score 0.9829348665362514
Confusion Matrix:
 [[15809   656]
 [  471  4689]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_10963969.png
Accuracy:   0.9479
Precision:  0.8773
Recall:     0.9087
F1-score:   0.8927

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.2290, Test Loss: 0.1807, F1: 0.8605, AUC: 0.9811
Epoch [10/30] Train Loss: 0.0046, Test Loss: 0.7746, F1: 0.8777, AUC: 0.9783
Epoch [20/30] Train Loss: 0.0000, Test Loss: 1.0848, F1: 0.8677, AUC: 0.9737
Mejores resultados en la época:  11
f1-score 0.8912466843501327
AUC según el mejor F1-score 0.9834804918584642
Confusion Matrix:
 [[15773   692]
 [  456  4704]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_10963969.png
Accuracy:   0.9469
Precision:  0.8718
Recall:     0.9116
F1-score:   0.8912

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.2308, Test Loss: 0.1545, F1: 0.8781, AUC: 0.9817
Epoch [10/30] Train Loss: 0.0039, Test Loss: 0.4905, F1: 0.8747, AUC: 0.9829
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8580, F1: 0.8775, AUC: 0.9792
Mejores resultados en la época:  19
f1-score 0.8890736122656322
AUC según el mejor F1-score 0.9812538224140002
Confusion Matrix:
 [[15611   854]
 [  347  4813]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_10963969.png
Accuracy:   0.9445
Precision:  0.8493
Recall:     0.9328
F1-score:   0.8891
Tiempo total para red 6: 890.98 segundos
Saved on: outputs_text_plus_numerical_categorical/2/tfidf

==============================
Model: Logistic Regression
Accuracy:  0.9353
Precision: 0.8251
Recall:    0.9246
F1-score:  0.8721
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15454  1011]
 [  389  4771]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/tfidf/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/tfidf/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7564
Precision: 0.4944
Recall:    0.9291
F1-score:  0.6454
              precision    recall  f1-score   support

           0       0.97      0.70      0.81     16465
           1       0.49      0.93      0.65      5160

    accuracy                           0.76     21625
   macro avg       0.73      0.82      0.73     21625
weighted avg       0.86      0.76      0.77     21625

[[11563  4902]
 [  366  4794]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/tfidf/conf_matrix_svm.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/tfidf/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8997
Precision: 0.7507
Recall:    0.8676
F1-score:  0.8049
              precision    recall  f1-score   support

           0       0.96      0.91      0.93     16465
           1       0.75      0.87      0.80      5160

    accuracy                           0.90     21625
   macro avg       0.85      0.89      0.87     21625
weighted avg       0.91      0.90      0.90     21625

[[14978  1487]
 [  683  4477]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/tfidf/conf_matrix_decision_tree.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8846
Precision: 0.7423
Recall:    0.7911
F1-score:  0.7659
              precision    recall  f1-score   support

           0       0.93      0.91      0.92     16465
           1       0.74      0.79      0.77      5160

    accuracy                           0.88     21625
   macro avg       0.84      0.85      0.84     21625
weighted avg       0.89      0.88      0.89     21625

[[15048  1417]
 [ 1078  4082]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/tfidf/conf_matrix_random_forest.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/tfidf/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9366
Precision: 0.8272
Recall:    0.9279
F1-score:  0.8747
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15465  1000]
 [  372  4788]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text_plus_numerical_categorical.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/tfidf/conf_matrix_xgboost.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.9163
Precision: 0.8097
Recall:    0.8486
F1-score:  0.8287
              precision    recall  f1-score   support

           0       0.95      0.94      0.94     16465
           1       0.81      0.85      0.83      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.89      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15436  1029]
 [  781  4379]]
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/tfidf/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/tfidf/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.9366, 'precision': 0.8272, 'recall': 0.9279, 'f1_score': 0.8747}
Logistic Regression: {'accuracy': 0.9353, 'precision': 0.8251, 'recall': 0.9246, 'f1_score': 0.8721}
Naive Bayes: {'accuracy': 0.9163, 'precision': 0.8097, 'recall': 0.8486, 'f1_score': 0.8287}
Decision Tree: {'accuracy': 0.8997, 'precision': 0.7507, 'recall': 0.8676, 'f1_score': 0.8049}
Random Forest: {'accuracy': 0.8846, 'precision': 0.7423, 'recall': 0.7911, 'f1_score': 0.7659}
SVM: {'accuracy': 0.7564, 'precision': 0.4944, 'recall': 0.9291, 'f1_score': 0.6454}

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_text:  (86500, 300)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 623)
Shape of X_test after concatenation:  (21625, 623)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 623)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 623)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [623, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.6220, Test Loss: 0.5594, F1: 0.6729, AUC: 0.8815
Epoch [10/30] Train Loss: 0.3432, Test Loss: 0.3373, F1: 0.7535, AUC: 0.9306
Epoch [20/30] Train Loss: 0.3285, Test Loss: 0.3427, F1: 0.7493, AUC: 0.9358
Mejores resultados en la época:  19
f1-score 0.7798946864528482
AUC según el mejor F1-score 0.935515675722757
Confusion Matrix:
 [[15253  1212]
 [ 1087  4073]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_20001.png
Accuracy:   0.8937
Precision:  0.7707
Recall:     0.7893
F1-score:   0.7799

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5450, Test Loss: 0.4210, F1: 0.6915, AUC: 0.8984
Epoch [10/30] Train Loss: 0.3357, Test Loss: 0.3001, F1: 0.7653, AUC: 0.9334
Epoch [20/30] Train Loss: 0.3247, Test Loss: 0.3280, F1: 0.7536, AUC: 0.9371
Mejores resultados en la época:  27
f1-score 0.7862927118309303
AUC según el mejor F1-score 0.9395766919257904
Confusion Matrix:
 [[15480   985]
 [ 1179  3981]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_20001.png
Accuracy:   0.8999
Precision:  0.8017
Recall:     0.7715
F1-score:   0.7863

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5614, Test Loss: 0.5195, F1: 0.6188, AUC: 0.8948
Epoch [10/30] Train Loss: 0.3344, Test Loss: 0.3523, F1: 0.7393, AUC: 0.9335
Epoch [20/30] Train Loss: 0.3251, Test Loss: 0.2876, F1: 0.7744, AUC: 0.9376
Mejores resultados en la época:  23
f1-score 0.7805372529143436
AUC según el mejor F1-score 0.9383543080577312
Confusion Matrix:
 [[15610   855]
 [ 1310  3850]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_20001.png
Accuracy:   0.8999
Precision:  0.8183
Recall:     0.7461
F1-score:   0.7805

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5520, Test Loss: 0.3692, F1: 0.7137, AUC: 0.8954
Epoch [10/30] Train Loss: 0.3364, Test Loss: 0.3026, F1: 0.7639, AUC: 0.9341
Epoch [20/30] Train Loss: 0.3220, Test Loss: 0.3445, F1: 0.7473, AUC: 0.9382
Mejores resultados en la época:  25
f1-score 0.7818292931350581
AUC según el mejor F1-score 0.9390698969154678
Confusion Matrix:
 [[15645   820]
 [ 1322  3838]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_20001.png
Accuracy:   0.9009
Precision:  0.8240
Recall:     0.7438
F1-score:   0.7818
Tiempo total para red 1: 241.02 segundos

Entrenando red 2 con capas [623, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.5032, Test Loss: 0.3722, F1: 0.7122, AUC: 0.9104
Epoch [10/30] Train Loss: 0.3397, Test Loss: 0.3150, F1: 0.7628, AUC: 0.9361
Epoch [20/30] Train Loss: 0.3210, Test Loss: 0.2795, F1: 0.7829, AUC: 0.9401
Mejores resultados en la época:  27
f1-score 0.7868243573453231
AUC según el mejor F1-score 0.9415604924234398
Confusion Matrix:
 [[15419  1046]
 [ 1135  4025]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_42049.png
Accuracy:   0.8991
Precision:  0.7937
Recall:     0.7800
F1-score:   0.7868

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.5369, Test Loss: 0.4178, F1: 0.6855, AUC: 0.9038
Epoch [10/30] Train Loss: 0.3368, Test Loss: 0.3192, F1: 0.7560, AUC: 0.9351
Epoch [20/30] Train Loss: 0.3240, Test Loss: 0.2646, F1: 0.7825, AUC: 0.9380
Mejores resultados en la época:  20
f1-score 0.7825061425061425
AUC según el mejor F1-score 0.9379637273803723
Confusion Matrix:
 [[15431  1034]
 [ 1179  3981]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_42049.png
Accuracy:   0.8977
Precision:  0.7938
Recall:     0.7715
F1-score:   0.7825

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4987, Test Loss: 0.3301, F1: 0.7353, AUC: 0.9101
Epoch [10/30] Train Loss: 0.3362, Test Loss: 0.3884, F1: 0.7214, AUC: 0.9358
Epoch [20/30] Train Loss: 0.3267, Test Loss: 0.2543, F1: 0.7803, AUC: 0.9402
Mejores resultados en la época:  24
f1-score 0.7882132650044348
AUC según el mejor F1-score 0.9417423263346965
Confusion Matrix:
 [[15477   988]
 [ 1161  3999]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_42049.png
Accuracy:   0.9006
Precision:  0.8019
Recall:     0.7750
F1-score:   0.7882

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4821, Test Loss: 0.3591, F1: 0.7188, AUC: 0.9129
Epoch [10/30] Train Loss: 0.3386, Test Loss: 0.3952, F1: 0.7215, AUC: 0.9360
Epoch [20/30] Train Loss: 0.3263, Test Loss: 0.2576, F1: 0.7825, AUC: 0.9397
Mejores resultados en la época:  26
f1-score 0.7843653105327445
AUC según el mejor F1-score 0.9414302890557137
Confusion Matrix:
 [[15156  1309]
 [  986  4174]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_42049.png
Accuracy:   0.8939
Precision:  0.7613
Recall:     0.8089
F1-score:   0.7844
Tiempo total para red 2: 214.56 segundos

Entrenando red 3 con capas [623, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4745, Test Loss: 0.3249, F1: 0.7452, AUC: 0.9151
Epoch [10/30] Train Loss: 0.3359, Test Loss: 0.2808, F1: 0.7792, AUC: 0.9373
Epoch [20/30] Train Loss: 0.3244, Test Loss: 0.2614, F1: 0.7786, AUC: 0.9405
Mejores resultados en la época:  19
f1-score 0.7814468410001909
AUC según el mejor F1-score 0.9401960701229057
Confusion Matrix:
 [[15241  1224]
 [ 1066  4094]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_90241.png
Accuracy:   0.8941
Precision:  0.7698
Recall:     0.7934
F1-score:   0.7814

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4817, Test Loss: 0.3059, F1: 0.7442, AUC: 0.9137
Epoch [10/30] Train Loss: 0.3419, Test Loss: 0.2861, F1: 0.7736, AUC: 0.9364
Epoch [20/30] Train Loss: 0.3214, Test Loss: 0.3707, F1: 0.7331, AUC: 0.9408
Mejores resultados en la época:  28
f1-score 0.7874557609123083
AUC según el mejor F1-score 0.9419659213695012
Confusion Matrix:
 [[15458  1007]
 [ 1155  4005]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_90241.png
Accuracy:   0.9000
Precision:  0.7991
Recall:     0.7762
F1-score:   0.7875

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4679, Test Loss: 0.4029, F1: 0.7106, AUC: 0.9151
Epoch [10/30] Train Loss: 0.3357, Test Loss: 0.2675, F1: 0.7802, AUC: 0.9371
Epoch [20/30] Train Loss: 0.3237, Test Loss: 0.3134, F1: 0.7672, AUC: 0.9409
Mejores resultados en la época:  27
f1-score 0.7872485178345806
AUC según el mejor F1-score 0.942111602718475
Confusion Matrix:
 [[15386  1079]
 [ 1110  4050]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_90241.png
Accuracy:   0.8988
Precision:  0.7896
Recall:     0.7849
F1-score:   0.7872

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4924, Test Loss: 0.3066, F1: 0.7433, AUC: 0.9132
Epoch [10/30] Train Loss: 0.3402, Test Loss: 0.2901, F1: 0.7759, AUC: 0.9364
Epoch [20/30] Train Loss: 0.3231, Test Loss: 0.3104, F1: 0.7629, AUC: 0.9413
Mejores resultados en la época:  29
f1-score 0.7891827872088433
AUC según el mejor F1-score 0.9418647142046672
Confusion Matrix:
 [[15491   974]
 [ 1162  3998]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_90241.png
Accuracy:   0.9012
Precision:  0.8041
Recall:     0.7748
F1-score:   0.7892
Tiempo total para red 3: 230.65 segundos

Entrenando red 4 con capas [623, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4816, Test Loss: 0.3134, F1: 0.7471, AUC: 0.9136
Epoch [10/30] Train Loss: 0.3381, Test Loss: 0.3314, F1: 0.7533, AUC: 0.9377
Epoch [20/30] Train Loss: 0.3218, Test Loss: 0.2788, F1: 0.7768, AUC: 0.9410
Mejores resultados en la época:  24
f1-score 0.786095717884131
AUC según el mejor F1-score 0.9417337575359526
Confusion Matrix:
 [[15601   864]
 [ 1259  3901]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_203009.png
Accuracy:   0.9018
Precision:  0.8187
Recall:     0.7560
F1-score:   0.7861

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4702, Test Loss: 0.4301, F1: 0.6970, AUC: 0.9170
Epoch [10/30] Train Loss: 0.3473, Test Loss: 0.2780, F1: 0.7793, AUC: 0.9369
Epoch [20/30] Train Loss: 0.3233, Test Loss: 0.3625, F1: 0.7410, AUC: 0.9404
Mejores resultados en la época:  17
f1-score 0.7832774600670479
AUC según el mejor F1-score 0.9400437856199549
Confusion Matrix:
 [[15455  1010]
 [ 1188  3972]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_203009.png
Accuracy:   0.8984
Precision:  0.7973
Recall:     0.7698
F1-score:   0.7833

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4693, Test Loss: 0.3534, F1: 0.7410, AUC: 0.9172
Epoch [10/30] Train Loss: 0.3455, Test Loss: 0.2612, F1: 0.7647, AUC: 0.9365
Epoch [20/30] Train Loss: 0.3224, Test Loss: 0.3936, F1: 0.7191, AUC: 0.9412
Mejores resultados en la época:  19
f1-score 0.7851940536765439
AUC según el mejor F1-score 0.9403693823167301
Confusion Matrix:
 [[15537   928]
 [ 1225  3935]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_203009.png
Accuracy:   0.9004
Precision:  0.8092
Recall:     0.7626
F1-score:   0.7852

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4760, Test Loss: 0.3142, F1: 0.7386, AUC: 0.9147
Epoch [10/30] Train Loss: 0.3363, Test Loss: 0.3326, F1: 0.7519, AUC: 0.9372
Epoch [20/30] Train Loss: 0.3224, Test Loss: 0.2802, F1: 0.7724, AUC: 0.9406
Mejores resultados en la época:  11
f1-score 0.7806356968215159
AUC según el mejor F1-score 0.9377942993947698
Confusion Matrix:
 [[15391  1074]
 [ 1169  3991]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_203009.png
Accuracy:   0.8963
Precision:  0.7880
Recall:     0.7734
F1-score:   0.7806
Tiempo total para red 4: 246.39 segundos

Entrenando red 5 con capas [623, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4602, Test Loss: 0.3751, F1: 0.7232, AUC: 0.9182
Epoch [10/30] Train Loss: 0.3337, Test Loss: 0.2808, F1: 0.7721, AUC: 0.9381
Epoch [20/30] Train Loss: 0.3260, Test Loss: 0.3067, F1: 0.7590, AUC: 0.9403
Mejores resultados en la época:  25
f1-score 0.7864664586583463
AUC según el mejor F1-score 0.9416479341897426
Confusion Matrix:
 [[15402  1063]
 [ 1127  4033]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_492033.png
Accuracy:   0.8987
Precision:  0.7914
Recall:     0.7816
F1-score:   0.7865

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4716, Test Loss: 0.3798, F1: 0.7319, AUC: 0.9171
Epoch [10/30] Train Loss: 0.3338, Test Loss: 0.2615, F1: 0.7784, AUC: 0.9382
Epoch [20/30] Train Loss: 0.3264, Test Loss: 0.3838, F1: 0.7205, AUC: 0.9403
Mejores resultados en la época:  29
f1-score 0.7860501567398119
AUC según el mejor F1-score 0.9423188840787482
Confusion Matrix:
 [[15429  1036]
 [ 1148  4012]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_492033.png
Accuracy:   0.8990
Precision:  0.7948
Recall:     0.7775
F1-score:   0.7861

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4702, Test Loss: 0.2979, F1: 0.7497, AUC: 0.9168
Epoch [10/30] Train Loss: 0.3443, Test Loss: 0.2608, F1: 0.7676, AUC: 0.9375
Epoch [20/30] Train Loss: 0.3277, Test Loss: 0.3255, F1: 0.7475, AUC: 0.9410
Mejores resultados en la época:  27
f1-score 0.7854910502888531
AUC según el mejor F1-score 0.9425704159869301
Confusion Matrix:
 [[15213  1252]
 [ 1013  4147]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [20:18:50] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_492033.png
Accuracy:   0.8953
Precision:  0.7681
Recall:     0.8037
F1-score:   0.7855

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4668, Test Loss: 0.2974, F1: 0.7249, AUC: 0.9172
Epoch [10/30] Train Loss: 0.3385, Test Loss: 0.2939, F1: 0.7664, AUC: 0.9370
Epoch [20/30] Train Loss: 0.3215, Test Loss: 0.3953, F1: 0.7252, AUC: 0.9411
Mejores resultados en la época:  27
f1-score 0.7838508084515425
AUC según el mejor F1-score 0.9411234071803709
Confusion Matrix:
 [[15495   970]
 [ 1209  3951]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_492033.png
Accuracy:   0.8992
Precision:  0.8029
Recall:     0.7657
F1-score:   0.7839
Tiempo total para red 5: 253.33 segundos

Entrenando red 6 con capas [623, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.6912, Test Loss: 0.6844, F1: 0.0000, AUC: 0.8518
Epoch [10/30] Train Loss: 0.3363, Test Loss: 0.4028, F1: 0.7229, AUC: 0.9366
Epoch [20/30] Train Loss: 0.3272, Test Loss: 0.2863, F1: 0.7761, AUC: 0.9409
Mejores resultados en la época:  23
f1-score 0.7824872843323027
AUC según el mejor F1-score 0.9410550274601752
Confusion Matrix:
 [[15521   944]
 [ 1237  3923]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_1338369.png
Accuracy:   0.8991
Precision:  0.8060
Recall:     0.7603
F1-score:   0.7825

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.5356, Test Loss: 0.4413, F1: 0.6900, AUC: 0.9128
Epoch [10/30] Train Loss: 0.3401, Test Loss: 0.2774, F1: 0.7716, AUC: 0.9375
Epoch [20/30] Train Loss: 0.3253, Test Loss: 0.3043, F1: 0.7627, AUC: 0.9406
Mejores resultados en la época:  16
f1-score 0.7821141400274564
AUC según el mejor F1-score 0.9396362615555194
Confusion Matrix:
 [[15415  1050]
 [ 1172  3988]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_1338369.png
Accuracy:   0.8972
Precision:  0.7916
Recall:     0.7729
F1-score:   0.7821

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4766, Test Loss: 0.3186, F1: 0.7156, AUC: 0.9171
Epoch [10/30] Train Loss: 0.3382, Test Loss: 0.3952, F1: 0.7170, AUC: 0.9375
Epoch [20/30] Train Loss: 0.3260, Test Loss: 0.2848, F1: 0.7808, AUC: 0.9412
Mejores resultados en la época:  23
f1-score 0.7838131987731275
AUC según el mejor F1-score 0.940470454122793
Confusion Matrix:
 [[15479   986]
 [ 1199  3961]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_1338369.png
Accuracy:   0.8990
Precision:  0.8007
Recall:     0.7676
F1-score:   0.7838

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4730, Test Loss: 0.3379, F1: 0.6193, AUC: 0.9169
Epoch [10/30] Train Loss: 0.3356, Test Loss: 0.4127, F1: 0.7039, AUC: 0.9370
Epoch [20/30] Train Loss: 0.3298, Test Loss: 0.2912, F1: 0.7749, AUC: 0.9408
Mejores resultados en la época:  24
f1-score 0.7877491492464754
AUC según el mejor F1-score 0.942283726109177
Confusion Matrix:
 [[15391  1074]
 [ 1109  4051]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_1338369.png
Accuracy:   0.8991
Precision:  0.7904
Recall:     0.7851
F1-score:   0.7877
Tiempo total para red 6: 283.86 segundos
Saved on: outputs_text_plus_numerical_categorical/2/lyrics_bert

==============================
Model: Logistic Regression
Accuracy:  0.8731
Precision: 0.6902
Recall:    0.8496
F1-score:  0.7616
              precision    recall  f1-score   support

           0       0.95      0.88      0.91     16465
           1       0.69      0.85      0.76      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.87      0.84     21625
weighted avg       0.89      0.87      0.88     21625

[[14497  1968]
 [  776  4384]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.5012
Precision: 0.3165
Recall:    0.9403
F1-score:  0.4736
              precision    recall  f1-score   support

           0       0.95      0.36      0.53     16465
           1       0.32      0.94      0.47      5160

    accuracy                           0.50     21625
   macro avg       0.63      0.65      0.50     21625
weighted avg       0.80      0.50      0.51     21625

[[ 5986 10479]
 [  308  4852]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/lyrics_bert/conf_matrix_svm.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8082
Precision: 0.5718
Recall:    0.7814
F1-score:  0.6604
              precision    recall  f1-score   support

           0       0.92      0.82      0.87     16465
           1       0.57      0.78      0.66      5160

    accuracy                           0.81     21625
   macro avg       0.75      0.80      0.76     21625
weighted avg       0.84      0.81      0.82     21625

[[13446  3019]
 [ 1128  4032]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8695
Precision: 0.7004
Recall:    0.7921
F1-score:  0.7434
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.70      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14717  1748]
 [ 1073  4087]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/lyrics_bert/conf_matrix_random_forest.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8966
Precision: 0.7468
Recall:    0.8570
F1-score:  0.7981
              precision    recall  f1-score   support

           0       0.95      0.91      0.93     16465
           1       0.75      0.86      0.80      5160

    accuracy                           0.90     21625
   macro avg       0.85      0.88      0.86     21625
weighted avg       0.90      0.90      0.90     21625

[[14966  1499]
 [  738  4422]]
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text_plus_numerical_categorical.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/lyrics_bert/conf_matrix_xgboost.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7207
Precision: 0.4518
Recall:    0.7990
F1-score:  0.5772
              precision    recall  f1-score   support

           0       0.92      0.70      0.79     16465
           1       0.45      0.80      0.58      5160

    accuracy                           0.72     21625
   macro avg       0.68      0.75      0.68     21625
weighted avg       0.81      0.72      0.74     21625

[[11462  5003]
 [ 1037  4123]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/lyrics_bert/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/lyrics_bert/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.8966, 'precision': 0.7468, 'recall': 0.857, 'f1_score': 0.7981}
Logistic Regression: {'accuracy': 0.8731, 'precision': 0.6902, 'recall': 0.8496, 'f1_score': 0.7616}
Random Forest: {'accuracy': 0.8695, 'precision': 0.7004, 'recall': 0.7921, 'f1_score': 0.7434}
Decision Tree: {'accuracy': 0.8082, 'precision': 0.5718, 'recall': 0.7814, 'f1_score': 0.6604}
Naive Bayes: {'accuracy': 0.7207, 'precision': 0.4518, 'recall': 0.799, 'f1_score': 0.5772}
SVM: {'accuracy': 0.5012, 'precision': 0.3165, 'recall': 0.9403, 'f1_score': 0.4736}

##################################################
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_text:  (86500, 1536)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 3095)
Shape of X_test after concatenation:  (21625, 3095)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 3095)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 3095)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [3095, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.3800, Test Loss: 0.3215, F1: 0.7627, AUC: 0.9484
Epoch [10/30] Train Loss: 0.2340, Test Loss: 0.2719, F1: 0.8026, AUC: 0.9687
Epoch [20/30] Train Loss: 0.2194, Test Loss: 0.2305, F1: 0.8275, AUC: 0.9708
Mejores resultados en la época:  22
f1-score 0.8547848300255453
AUC según el mejor F1-score 0.9710464233504473
Confusion Matrix:
 [[15797   668]
 [  810  4350]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_99105.png
Accuracy:   0.9317
Precision:  0.8669
Recall:     0.8430
F1-score:   0.8548

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4033, Test Loss: 0.3510, F1: 0.7456, AUC: 0.9460
Epoch [10/30] Train Loss: 0.2326, Test Loss: 0.1874, F1: 0.8475, AUC: 0.9687
Epoch [20/30] Train Loss: 0.2222, Test Loss: 0.2724, F1: 0.8040, AUC: 0.9707
Mejores resultados en la época:  26
f1-score 0.8549960967993755
AUC según el mejor F1-score 0.9713198892647547
Confusion Matrix:
 [[15758   707]
 [  779  4381]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_99105.png
Accuracy:   0.9313
Precision:  0.8610
Recall:     0.8490
F1-score:   0.8550

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4128, Test Loss: 0.3115, F1: 0.7634, AUC: 0.9435
Epoch [10/30] Train Loss: 0.2318, Test Loss: 0.2373, F1: 0.8253, AUC: 0.9683
Epoch [20/30] Train Loss: 0.2194, Test Loss: 0.1819, F1: 0.8521, AUC: 0.9705
Mejores resultados en la época:  23
f1-score 0.8535676759957971
AUC según el mejor F1-score 0.9708317266835689
Confusion Matrix:
 [[15624   841]
 [  692  4468]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_99105.png
Accuracy:   0.9291
Precision:  0.8416
Recall:     0.8659
F1-score:   0.8536

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.3807, Test Loss: 0.2519, F1: 0.7977, AUC: 0.9505
Epoch [10/30] Train Loss: 0.2380, Test Loss: 0.2120, F1: 0.8393, AUC: 0.9690
Epoch [20/30] Train Loss: 0.2227, Test Loss: 0.1799, F1: 0.8479, AUC: 0.9709
Mejores resultados en la época:  25
f1-score 0.8566097033981536
AUC según el mejor F1-score 0.9718590938730733
Confusion Matrix:
 [[15804   661]
 [  799  4361]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_99105.png
Accuracy:   0.9325
Precision:  0.8684
Recall:     0.8452
F1-score:   0.8566
Tiempo total para red 1: 302.89 segundos

Entrenando red 2 con capas [3095, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3583, Test Loss: 0.3592, F1: 0.7448, AUC: 0.9559
Epoch [10/30] Train Loss: 0.2440, Test Loss: 0.2735, F1: 0.7986, AUC: 0.9688
Epoch [20/30] Train Loss: 0.2193, Test Loss: 0.3075, F1: 0.7819, AUC: 0.9712
Mejores resultados en la época:  25
f1-score 0.8524947921833151
AUC según el mejor F1-score 0.9719410330110617
Confusion Matrix:
 [[15841   624]
 [  863  4297]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_200257.png
Accuracy:   0.9312
Precision:  0.8732
Recall:     0.8328
F1-score:   0.8525

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3603, Test Loss: 0.2619, F1: 0.7977, AUC: 0.9546
Epoch [10/30] Train Loss: 0.2363, Test Loss: 0.2870, F1: 0.7908, AUC: 0.9693
Epoch [20/30] Train Loss: 0.2259, Test Loss: 0.2011, F1: 0.8474, AUC: 0.9712
Mejores resultados en la época:  28
f1-score 0.8558860093717128
AUC según el mejor F1-score 0.9726272902115599
Confusion Matrix:
 [[15643   822]
 [  685  4475]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_200257.png
Accuracy:   0.9303
Precision:  0.8448
Recall:     0.8672
F1-score:   0.8559

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3579, Test Loss: 0.2992, F1: 0.7762, AUC: 0.9545
Epoch [10/30] Train Loss: 0.2405, Test Loss: 0.2113, F1: 0.8396, AUC: 0.9692
Epoch [20/30] Train Loss: 0.2214, Test Loss: 0.1925, F1: 0.8512, AUC: 0.9718
Mejores resultados en la época:  23
f1-score 0.8526574616985058
AUC según el mejor F1-score 0.972353730134629
Confusion Matrix:
 [[15559   906]
 [  652  4508]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_200257.png
Accuracy:   0.9280
Precision:  0.8327
Recall:     0.8736
F1-score:   0.8527

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3621, Test Loss: 0.2306, F1: 0.8095, AUC: 0.9546
Epoch [10/30] Train Loss: 0.2390, Test Loss: 0.2242, F1: 0.8325, AUC: 0.9692
Epoch [20/30] Train Loss: 0.2215, Test Loss: 0.2007, F1: 0.8411, AUC: 0.9713
Mejores resultados en la época:  28
f1-score 0.8560258039292347
AUC según el mejor F1-score 0.9724594688757218
Confusion Matrix:
 [[15773   692]
 [  781  4379]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_200257.png
Accuracy:   0.9319
Precision:  0.8635
Recall:     0.8486
F1-score:   0.8560
Tiempo total para red 2: 313.88 segundos

Entrenando red 3 con capas [3095, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3670, Test Loss: 0.2326, F1: 0.8110, AUC: 0.9550
Epoch [10/30] Train Loss: 0.2414, Test Loss: 0.2448, F1: 0.8162, AUC: 0.9691
Epoch [20/30] Train Loss: 0.2161, Test Loss: 0.4486, F1: 0.7063, AUC: 0.9715
Mejores resultados en la época:  21
f1-score 0.853008377760853
AUC según el mejor F1-score 0.9716222925303143
Confusion Matrix:
 [[15601   864]
 [  680  4480]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_406657.png
Accuracy:   0.9286
Precision:  0.8383
Recall:     0.8682
F1-score:   0.8530

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3713, Test Loss: 0.2460, F1: 0.8022, AUC: 0.9528
Epoch [10/30] Train Loss: 0.2385, Test Loss: 0.2613, F1: 0.8041, AUC: 0.9691
Epoch [20/30] Train Loss: 0.2223, Test Loss: 0.2105, F1: 0.8347, AUC: 0.9719
Mejores resultados en la época:  29
f1-score 0.8561261781111752
AUC según el mejor F1-score 0.9732402123837974
Confusion Matrix:
 [[15678   787]
 [  709  4451]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_406657.png
Accuracy:   0.9308
Precision:  0.8498
Recall:     0.8626
F1-score:   0.8561

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3613, Test Loss: 0.2926, F1: 0.7788, AUC: 0.9543
Epoch [10/30] Train Loss: 0.2353, Test Loss: 0.2299, F1: 0.8305, AUC: 0.9697
Epoch [20/30] Train Loss: 0.2253, Test Loss: 0.2930, F1: 0.7899, AUC: 0.9713
Mejores resultados en la época:  25
f1-score 0.8547929665343165
AUC según el mejor F1-score 0.9725863883219514
Confusion Matrix:
 [[15568   897]
 [  639  4521]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_406657.png
Accuracy:   0.9290
Precision:  0.8344
Recall:     0.8762
F1-score:   0.8548

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3761, Test Loss: 0.2312, F1: 0.8002, AUC: 0.9532
Epoch [10/30] Train Loss: 0.2391, Test Loss: 0.1959, F1: 0.8467, AUC: 0.9692
Epoch [20/30] Train Loss: 0.2296, Test Loss: 0.2092, F1: 0.8388, AUC: 0.9710
Mejores resultados en la época:  22
f1-score 0.8516628873771731
AUC según el mejor F1-score 0.9717434092048671
Confusion Matrix:
 [[15548   917]
 [  653  4507]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_406657.png
Accuracy:   0.9274
Precision:  0.8309
Recall:     0.8734
F1-score:   0.8517
Tiempo total para red 3: 331.97 segundos

Entrenando red 4 con capas [3095, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3732, Test Loss: 0.4099, F1: 0.7185, AUC: 0.9540
Epoch [10/30] Train Loss: 0.2371, Test Loss: 0.2058, F1: 0.8381, AUC: 0.9692
Epoch [20/30] Train Loss: 0.2202, Test Loss: 0.1954, F1: 0.8447, AUC: 0.9717
Mejores resultados en la época:  26
f1-score 0.8543503257953239
AUC según el mejor F1-score 0.9726854415167716
Confusion Matrix:
 [[15647   818]
 [  702  4458]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_835841.png
Accuracy:   0.9297
Precision:  0.8450
Recall:     0.8640
F1-score:   0.8544

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3682, Test Loss: 0.2663, F1: 0.7977, AUC: 0.9545
Epoch [10/30] Train Loss: 0.2420, Test Loss: 0.3224, F1: 0.7754, AUC: 0.9690
Epoch [20/30] Train Loss: 0.2185, Test Loss: 0.1898, F1: 0.8485, AUC: 0.9719
Mejores resultados en la época:  29
f1-score 0.855245165071303
AUC según el mejor F1-score 0.9729882743993012
Confusion Matrix:
 [[15765   700]
 [  782  4378]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_835841.png
Accuracy:   0.9315
Precision:  0.8622
Recall:     0.8484
F1-score:   0.8552

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3780, Test Loss: 0.2615, F1: 0.7970, AUC: 0.9528
Epoch [10/30] Train Loss: 0.2354, Test Loss: 0.2054, F1: 0.8418, AUC: 0.9693
Epoch [20/30] Train Loss: 0.2214, Test Loss: 0.2678, F1: 0.8088, AUC: 0.9718
Mejores resultados en la época:  26
f1-score 0.8529273564514613
AUC según el mejor F1-score 0.9727373957443202
Confusion Matrix:
 [[15522   943]
 [  622  4538]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_835841.png
Accuracy:   0.9276
Precision:  0.8280
Recall:     0.8795
F1-score:   0.8529

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3618, Test Loss: 0.2703, F1: 0.7875, AUC: 0.9546
Epoch [10/30] Train Loss: 0.2359, Test Loss: 0.2688, F1: 0.8083, AUC: 0.9693
Epoch [20/30] Train Loss: 0.2253, Test Loss: 0.1789, F1: 0.8520, AUC: 0.9717
Mejores resultados en la época:  29
f1-score 0.8547024952015355
AUC según el mejor F1-score 0.9733103105718731
Confusion Matrix:
 [[15658   807]
 [  707  4453]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_835841.png
Accuracy:   0.9300
Precision:  0.8466
Recall:     0.8630
F1-score:   0.8547
Tiempo total para red 4: 348.77 segundos

Entrenando red 5 con capas [3095, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3719, Test Loss: 0.2673, F1: 0.7944, AUC: 0.9548
Epoch [10/30] Train Loss: 0.2386, Test Loss: 0.3602, F1: 0.7547, AUC: 0.9693
Epoch [20/30] Train Loss: 0.2211, Test Loss: 0.2355, F1: 0.8179, AUC: 0.9717
Mejores resultados en la época:  29
f1-score 0.8557878435927216
AUC según el mejor F1-score 0.9728881559898022
Confusion Matrix:
 [[15714   751]
 [  739  4421]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_1757697.png
Accuracy:   0.9311
Precision:  0.8548
Recall:     0.8568
F1-score:   0.8558

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3694, Test Loss: 0.2485, F1: 0.7666, AUC: 0.9555
Epoch [10/30] Train Loss: 0.2412, Test Loss: 0.1869, F1: 0.8463, AUC: 0.9690
Epoch [20/30] Train Loss: 0.2225, Test Loss: 0.2010, F1: 0.8443, AUC: 0.9718
Mejores resultados en la época:  27
f1-score 0.8567318757192175
AUC según el mejor F1-score 0.9731885053331356
Confusion Matrix:
 [[15664   801]
 [  693  4467]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_1757697.png
Accuracy:   0.9309
Precision:  0.8479
Recall:     0.8657
F1-score:   0.8567

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3787, Test Loss: 0.2773, F1: 0.7829, AUC: 0.9539
Epoch [10/30] Train Loss: 0.2435, Test Loss: 0.2155, F1: 0.8357, AUC: 0.9690
Epoch [20/30] Train Loss: 0.2280, Test Loss: 0.2707, F1: 0.7982, AUC: 0.9716
Mejores resultados en la época:  27
f1-score 0.854479046520569
AUC según el mejor F1-score 0.9728056401057448
Confusion Matrix:
 [[15666   799]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [21:20:29] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
 [  715  4445]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_1757697.png
Accuracy:   0.9300
Precision:  0.8476
Recall:     0.8614
F1-score:   0.8545

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3619, Test Loss: 0.4356, F1: 0.6989, AUC: 0.9571
Epoch [10/30] Train Loss: 0.2386, Test Loss: 0.1870, F1: 0.8467, AUC: 0.9693
Epoch [20/30] Train Loss: 0.2232, Test Loss: 0.1908, F1: 0.8507, AUC: 0.9717
Mejores resultados en la época:  19
f1-score 0.851004851004851
AUC según el mejor F1-score 0.9713485029319887
Confusion Matrix:
 [[15822   643]
 [  862  4298]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_1757697.png
Accuracy:   0.9304
Precision:  0.8699
Recall:     0.8329
F1-score:   0.8510
Tiempo total para red 5: 493.72 segundos

Entrenando red 6 con capas [3095, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3854, Test Loss: 0.3755, F1: 0.7859, AUC: 0.9533
Epoch [10/30] Train Loss: 0.2430, Test Loss: 0.1841, F1: 0.8466, AUC: 0.9692
Epoch [20/30] Train Loss: 0.2292, Test Loss: 0.2472, F1: 0.8105, AUC: 0.9714
Mejores resultados en la época:  21
f1-score 0.8510516798324926
AUC según el mejor F1-score 0.9715487986026266
Confusion Matrix:
 [[15589   876]
 [  689  4471]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_3869697.png
Accuracy:   0.9276
Precision:  0.8362
Recall:     0.8665
F1-score:   0.8511

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3799, Test Loss: 0.2644, F1: 0.6951, AUC: 0.9550
Epoch [10/30] Train Loss: 0.2428, Test Loss: 0.1861, F1: 0.8460, AUC: 0.9689
Epoch [20/30] Train Loss: 0.2209, Test Loss: 0.2516, F1: 0.8030, AUC: 0.9721
Mejores resultados en la época:  28
f1-score 0.8529911304019626
AUC según el mejor F1-score 0.9729412990204732
Confusion Matrix:
 [[15547   918]
 [  640  4520]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_3869697.png
Accuracy:   0.9280
Precision:  0.8312
Recall:     0.8760
F1-score:   0.8530

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3793, Test Loss: 0.2791, F1: 0.7913, AUC: 0.9551
Epoch [10/30] Train Loss: 0.2457, Test Loss: 0.2599, F1: 0.8066, AUC: 0.9689
Epoch [20/30] Train Loss: 0.2216, Test Loss: 0.1792, F1: 0.8536, AUC: 0.9718
Mejores resultados en la época:  20
f1-score 0.8536110570371812
AUC según el mejor F1-score 0.9718323458028189
Confusion Matrix:
 [[15736   729]
 [  775  4385]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_3869697.png
Accuracy:   0.9305
Precision:  0.8575
Recall:     0.8498
F1-score:   0.8536

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3725, Test Loss: 0.4625, F1: 0.6953, AUC: 0.9558
Epoch [10/30] Train Loss: 0.2439, Test Loss: 0.1851, F1: 0.8431, AUC: 0.9688
Epoch [20/30] Train Loss: 0.2308, Test Loss: 0.3630, F1: 0.7792, AUC: 0.9714
Mejores resultados en la época:  28
f1-score 0.8554099002152221
AUC según el mejor F1-score 0.9730977737601725
Confusion Matrix:
 [[15775   690]
 [  788  4372]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_3869697.png
Accuracy:   0.9317
Precision:  0.8637
Recall:     0.8473
F1-score:   0.8554
Tiempo total para red 6: 539.71 segundos
Saved on: outputs_text_plus_numerical_categorical/2/gpt

==============================
Model: Logistic Regression
Accuracy:  0.9169
Precision: 0.7778
Recall:    0.9124
F1-score:  0.8397
              precision    recall  f1-score   support

           0       0.97      0.92      0.94     16465
           1       0.78      0.91      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.87      0.92      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15120  1345]
 [  452  4708]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8193
Precision: 0.5765
Recall:    0.9138
F1-score:  0.7070
              precision    recall  f1-score   support

           0       0.97      0.79      0.87     16465
           1       0.58      0.91      0.71      5160

    accuracy                           0.82     21625
   macro avg       0.77      0.85      0.79     21625
weighted avg       0.87      0.82      0.83     21625

[[13002  3463]
 [  445  4715]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/gpt/conf_matrix_svm.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8209
Precision: 0.5959
Recall:    0.7748
F1-score:  0.6737
              precision    recall  f1-score   support

           0       0.92      0.84      0.88     16465
           1       0.60      0.77      0.67      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.81      0.78     21625
weighted avg       0.84      0.82      0.83     21625

[[13754  2711]
 [ 1162  3998]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8851
Precision: 0.7361
Recall:    0.8083
F1-score:  0.7706
              precision    recall  f1-score   support

           0       0.94      0.91      0.92     16465
           1       0.74      0.81      0.77      5160

    accuracy                           0.89     21625
   macro avg       0.84      0.86      0.85     21625
weighted avg       0.89      0.89      0.89     21625

[[14970  1495]
 [  989  4171]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9217
Precision: 0.8027
Recall:    0.8909
F1-score:  0.8445
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15335  1130]
 [  563  4597]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8612
Precision: 0.6897
Recall:    0.7607
F1-score:  0.7234
              precision    recall  f1-score   support

           0       0.92      0.89      0.91     16465
           1       0.69      0.76      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.83      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14699  1766]
 [ 1235  3925]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/gpt/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.9217, 'precision': 0.8027, 'recall': 0.8909, 'f1_score': 0.8445}
Logistic Regression: {'accuracy': 0.9169, 'precision': 0.7778, 'recall': 0.9124, 'f1_score': 0.8397}
Random Forest: {'accuracy': 0.8851, 'precision': 0.7361, 'recall': 0.8083, 'f1_score': 0.7706}
Naive Bayes: {'accuracy': 0.8612, 'precision': 0.6897, 'recall': 0.7607, 'f1_score': 0.7234}
SVM: {'accuracy': 0.8193, 'precision': 0.5765, 'recall': 0.9138, 'f1_score': 0.707}
Decision Tree: {'accuracy': 0.8209, 'precision': 0.5959, 'recall': 0.7748, 'f1_score': 0.6737}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
MLP_5304833: {'accuracy': 0.9498265895953757, 'precision': 0.892053107562055, 'recall': 0.8984496124031007, 'f1_score': 0.895239934343922, 'f1_score_avg': 0.8896669918397202}
MLP_10963969: {'accuracy': 0.9444624277456647, 'precision': 0.849302982177519, 'recall': 0.9327519379844961, 'f1_score': 0.895239934343922, 'f1_score_avg': 0.890675911534196}
MLP_2609409: {'accuracy': 0.9433988439306359, 'precision': 0.8595177201315308, 'recall': 0.9118217054263565, 'f1_score': 0.8862700009581297, 'f1_score_avg': 0.885216363175764}
MLP_1293441: {'accuracy': 0.9432138728323699, 'precision': 0.8832358674463937, 'recall': 0.8781007751937985, 'f1_score': 0.8849951597289448, 'f1_score_avg': 0.8788599341223085}
MLP_320801: {'accuracy': 0.9415028901734104, 'precision': 0.8557077625570776, 'recall': 0.9079457364341085, 'f1_score': 0.8810531264692054, 'f1_score_avg': 0.8746960914466178}
MLP_643649: {'accuracy': 0.937757225433526, 'precision': 0.8378809355067328, 'recall': 0.9164728682170543, 'f1_score': 0.8810531264692054, 'f1_score_avg': 0.870845844351863}
XGBoost: {'accuracy': 0.9366, 'precision': 0.8272, 'recall': 0.9279, 'f1_score': 0.8747}
Logistic Regression: {'accuracy': 0.9353, 'precision': 0.8251, 'recall': 0.9246, 'f1_score': 0.8721}
Naive Bayes: {'accuracy': 0.9163, 'precision': 0.8097, 'recall': 0.8486, 'f1_score': 0.8287}
Decision Tree: {'accuracy': 0.8997, 'precision': 0.7507, 'recall': 0.8676, 'f1_score': 0.8049}
Random Forest: {'accuracy': 0.8846, 'precision': 0.7423, 'recall': 0.7911, 'f1_score': 0.7659}
SVM: {'accuracy': 0.7564, 'precision': 0.4944, 'recall': 0.9291, 'f1_score': 0.6454}


EMBEDDINGS TYPE: LYRICS_BERT
XGBoost: {'accuracy': 0.8966, 'precision': 0.7468, 'recall': 0.857, 'f1_score': 0.7981}
MLP_90241: {'accuracy': 0.9012254335260116, 'precision': 0.8041029766693484, 'recall': 0.7748062015503876, 'f1_score': 0.7891827872088433, 'f1_score_avg': 0.7863334767389808}
MLP_203009: {'accuracy': 0.8962774566473989, 'precision': 0.7879565646594274, 'recall': 0.7734496124031007, 'f1_score': 0.7891827872088433, 'f1_score_avg': 0.7838007321123096}
MLP_492033: {'accuracy': 0.8992369942196532, 'precision': 0.8028855923592766, 'recall': 0.7656976744186047, 'f1_score': 0.7891827872088433, 'f1_score_avg': 0.7854646185346386}
MLP_1338369: {'accuracy': 0.8990520231213873, 'precision': 0.7904390243902439, 'recall': 0.7850775193798449, 'f1_score': 0.7891827872088433, 'f1_score_avg': 0.7840409430948405}
MLP_42049: {'accuracy': 0.8938728323699422, 'precision': 0.7612620828013861, 'recall': 0.8089147286821705, 'f1_score': 0.7882132650044348, 'f1_score_avg': 0.7854772688471612}
MLP_20001: {'accuracy': 0.9009479768786127, 'precision': 0.823958780592529, 'recall': 0.743798449612403, 'f1_score': 0.7862927118309303, 'f1_score_avg': 0.7821384860832951}
Logistic Regression: {'accuracy': 0.8731, 'precision': 0.6902, 'recall': 0.8496, 'f1_score': 0.7616}
Random Forest: {'accuracy': 0.8695, 'precision': 0.7004, 'recall': 0.7921, 'f1_score': 0.7434}
Decision Tree: {'accuracy': 0.8082, 'precision': 0.5718, 'recall': 0.7814, 'f1_score': 0.6604}
Naive Bayes: {'accuracy': 0.7207, 'precision': 0.4518, 'recall': 0.799, 'f1_score': 0.5772}
SVM: {'accuracy': 0.5012, 'precision': 0.3165, 'recall': 0.9403, 'f1_score': 0.4736}


EMBEDDINGS TYPE: GPT
MLP_1757697: {'accuracy': 0.9304046242774566, 'precision': 0.8698643999190447, 'recall': 0.8329457364341085, 'f1_score': 0.8567318757192175, 'f1_score_avg': 0.8545009042093399}
MLP_3869697: {'accuracy': 0.9316531791907514, 'precision': 0.863690241011458, 'recall': 0.8472868217054264, 'f1_score': 0.8567318757192175, 'f1_score_avg': 0.8532659418717147}
MLP_99105: {'accuracy': 0.932485549132948, 'precision': 0.868379131819992, 'recall': 0.8451550387596899, 'f1_score': 0.8566097033981536, 'f1_score_avg': 0.8549895765547179}
MLP_200257: {'accuracy': 0.9318843930635838, 'precision': 0.8635377637546835, 'recall': 0.8486434108527132, 'f1_score': 0.8566097033981536, 'f1_score_avg': 0.854266016795692}
MLP_406657: {'accuracy': 0.9273988439306359, 'precision': 0.8309365781710915, 'recall': 0.8734496124031008, 'f1_score': 0.8566097033981536, 'f1_score_avg': 0.8538976024458794}
MLP_835841: {'accuracy': 0.9299884393063583, 'precision': 0.8465779467680609, 'recall': 0.862984496124031, 'f1_score': 0.8566097033981536, 'f1_score_avg': 0.8543063356299059}
XGBoost: {'accuracy': 0.9217, 'precision': 0.8027, 'recall': 0.8909, 'f1_score': 0.8445}
Logistic Regression: {'accuracy': 0.9169, 'precision': 0.7778, 'recall': 0.9124, 'f1_score': 0.8397}
Random Forest: {'accuracy': 0.8851, 'precision': 0.7361, 'recall': 0.8083, 'f1_score': 0.7706}
Naive Bayes: {'accuracy': 0.8612, 'precision': 0.6897, 'recall': 0.7607, 'f1_score': 0.7234}
SVM: {'accuracy': 0.8193, 'precision': 0.5765, 'recall': 0.9138, 'f1_score': 0.707}
Decision Tree: {'accuracy': 0.8209, 'precision': 0.5959, 'recall': 0.7748, 'f1_score': 0.6737}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
====================================

