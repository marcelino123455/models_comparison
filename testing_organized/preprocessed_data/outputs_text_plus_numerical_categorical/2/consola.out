2025-10-23 01:35:30.699857: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text_plus_numerical_categorical.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
For both embbedings your are adding this categorical columns: 
--> CAT_ALL 15 ['emotion', 'Key', 'Time signature', 'Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']

CAT_LOW 3 ['emotion', 'Key', 'Time signature']
CAT_HIGH 12 ['Artist(s)', 'song', 'Genre', 'Album', 'Similar Artist 1', 'Similar Song 1', 'Similar Artist 2', 'Similar Song 2', 'Similar Artist 3', 'Similar Song 3', 'song_normalized', 'artist_normalized']
For both embbedings your are adding this columns: 
--> ['Release Date', 'Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine']
You are executing with [ALL] dataset
--> PaTH:  ../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T.npy

##################################################
Running experiment with TFIDF embeddings
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Contaning the categorical cols
Preprocessing text...
Label distribution: {0: 82326, 1: 25799}
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_text:  (86500, 5000)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 10023)
Shape of X_test after concatenation:  (21625, 10023)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 10023)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 10023)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [10023, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.2802, Test Loss: 0.1953, F1: 0.8619, AUC: 0.9795
Epoch [10/30] Train Loss: 0.0147, Test Loss: 0.3811, F1: 0.8484, AUC: 0.9758
Epoch [20/30] Train Loss: 0.0033, Test Loss: 0.7977, F1: 0.8231, AUC: 0.9708
Mejores resultados en la época:  2
f1-score 0.8697077301283802
AUC según el mejor F1-score 0.9816399833332157
Confusion Matrix:
 [[15418  1047]
 [  384  4776]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_320801.png
Accuracy:   0.9338
Precision:  0.8202
Recall:     0.9256
F1-score:   0.8697

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.2803, Test Loss: 0.1758, F1: 0.8681, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0176, Test Loss: 0.3838, F1: 0.8451, AUC: 0.9748
Epoch [20/30] Train Loss: 0.0045, Test Loss: 0.6865, F1: 0.8329, AUC: 0.9720
Mejores resultados en la época:  1
f1-score 0.8724242146716732
AUC según el mejor F1-score 0.982136185048388
Confusion Matrix:
 [[15469   996]
 [  397  4763]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_320801.png
Accuracy:   0.9356
Precision:  0.8271
Recall:     0.9231
F1-score:   0.8724

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.2780, Test Loss: 0.1756, F1: 0.8690, AUC: 0.9796
Epoch [10/30] Train Loss: 0.0158, Test Loss: 0.4034, F1: 0.8374, AUC: 0.9753
Epoch [20/30] Train Loss: 0.0042, Test Loss: 0.6656, F1: 0.8296, AUC: 0.9734
Mejores resultados en la época:  1
f1-score 0.8781299085281346
AUC según el mejor F1-score 0.9822443425918733
Confusion Matrix:
 [[15554   911]
 [  408  4752]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_320801.png
Accuracy:   0.9390
Precision:  0.8391
Recall:     0.9209
F1-score:   0.8781

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.2753, Test Loss: 0.1782, F1: 0.8687, AUC: 0.9802
Epoch [10/30] Train Loss: 0.0194, Test Loss: 0.4136, F1: 0.8391, AUC: 0.9743
Epoch [20/30] Train Loss: 0.0064, Test Loss: 0.7457, F1: 0.8341, AUC: 0.9686
Mejores resultados en la época:  0
f1-score 0.868737289702348
AUC según el mejor F1-score 0.9801508897190894
Confusion Matrix:
 [[15506   959]
 [  461  4699]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_320801.png
Accuracy:   0.9343
Precision:  0.8305
Recall:     0.9107
F1-score:   0.8687
Tiempo total para red 1: 657.06 segundos

Entrenando red 2 con capas [10023, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.2462, Test Loss: 0.1661, F1: 0.8704, AUC: 0.9816
Epoch [10/30] Train Loss: 0.0056, Test Loss: 0.5286, F1: 0.8418, AUC: 0.9770
Epoch [20/30] Train Loss: 0.0003, Test Loss: 0.6476, F1: 0.8527, AUC: 0.9778
Mejores resultados en la época:  0
f1-score 0.8703720716582453
AUC según el mejor F1-score 0.9816042191917552
Confusion Matrix:
 [[15477   988]
 [  423  4737]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_643649.png
Accuracy:   0.9348
Precision:  0.8274
Recall:     0.9180
F1-score:   0.8704

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.2475, Test Loss: 0.1649, F1: 0.8756, AUC: 0.9814
Epoch [10/30] Train Loss: 0.0043, Test Loss: 0.6385, F1: 0.8388, AUC: 0.9745
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7645, F1: 0.8531, AUC: 0.9732
Mejores resultados en la época:  0
f1-score 0.8756126884305928
AUC según el mejor F1-score 0.9814203548989283
Confusion Matrix:
 [[15546   919]
 [  426  4734]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_643649.png
Accuracy:   0.9378
Precision:  0.8374
Recall:     0.9174
F1-score:   0.8756

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.2409, Test Loss: 0.1732, F1: 0.8709, AUC: 0.9811
Epoch [10/30] Train Loss: 0.0001, Test Loss: 0.5217, F1: 0.8594, AUC: 0.9792
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6998, F1: 0.8574, AUC: 0.9771
Mejores resultados en la época:  1
f1-score 0.8781456953642384
AUC según el mejor F1-score 0.9806599034362298
Confusion Matrix:
 [[15696   769]
 [  519  4641]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_643649.png
Accuracy:   0.9404
Precision:  0.8579
Recall:     0.8994
F1-score:   0.8781

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.2444, Test Loss: 0.1900, F1: 0.8580, AUC: 0.9809
Epoch [10/30] Train Loss: 0.0044, Test Loss: 0.5547, F1: 0.8440, AUC: 0.9770
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7090, F1: 0.8563, AUC: 0.9761
Mejores resultados en la época:  5
f1-score 0.8637873754152824
AUC según el mejor F1-score 0.9772782293660266
Confusion Matrix:
 [[15469   996]
 [  480  4680]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_643649.png
Accuracy:   0.9317
Precision:  0.8245
Recall:     0.9070
F1-score:   0.8638
Tiempo total para red 2: 696.75 segundos

Entrenando red 3 con capas [10023, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.2381, Test Loss: 0.2140, F1: 0.8417, AUC: 0.9811
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.5440, F1: 0.8479, AUC: 0.9780
Epoch [20/30] Train Loss: 0.0010, Test Loss: 0.6176, F1: 0.8680, AUC: 0.9779
Mejores resultados en la época:  21
f1-score 0.8797450557690505
AUC según el mejor F1-score 0.9787254264978331
Confusion Matrix:
 [[15649   816]
 [  467  4693]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_1293441.png
Accuracy:   0.9407
Precision:  0.8519
Recall:     0.9095
F1-score:   0.8797

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.2350, Test Loss: 0.2280, F1: 0.8320, AUC: 0.9812
Epoch [10/30] Train Loss: 0.0030, Test Loss: 0.4181, F1: 0.8764, AUC: 0.9797
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8749, F1: 0.8570, AUC: 0.9754
Mejores resultados en la época:  10
f1-score 0.876391772032459
AUC según el mejor F1-score 0.9797078957713921
Confusion Matrix:
 [[15671   794]
 [  516  4644]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_1293441.png
Accuracy:   0.9394
Precision:  0.8540
Recall:     0.9000
F1-score:   0.8764

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.2354, Test Loss: 0.1924, F1: 0.8567, AUC: 0.9813
Epoch [10/30] Train Loss: 0.0019, Test Loss: 0.8339, F1: 0.8268, AUC: 0.9764
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7781, F1: 0.8600, AUC: 0.9760
Mejores resultados en la época:  14
f1-score 0.8720117690327326
AUC según el mejor F1-score 0.9791175373178247
Confusion Matrix:
 [[15491   974]
 [  418  4742]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_1293441.png
Accuracy:   0.9356
Precision:  0.8296
Recall:     0.9190
F1-score:   0.8720

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.2342, Test Loss: 0.2006, F1: 0.8479, AUC: 0.9811
Epoch [10/30] Train Loss: 0.0022, Test Loss: 0.4868, F1: 0.8672, AUC: 0.9802
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.6718, F1: 0.8662, AUC: 0.9793
Mejores resultados en la época:  8
f1-score 0.8766768433712647
AUC según el mejor F1-score 0.9803893624484167
Confusion Matrix:
 [[15554   911]
 [  422  4738]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_1293441.png
Accuracy:   0.9384
Precision:  0.8387
Recall:     0.9182
F1-score:   0.8767
Tiempo total para red 3: 671.69 segundos

Entrenando red 4 con capas [10023, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.2313, Test Loss: 0.1735, F1: 0.8692, AUC: 0.9814
Epoch [10/30] Train Loss: 0.0016, Test Loss: 0.5085, F1: 0.8759, AUC: 0.9817
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8317, F1: 0.8786, AUC: 0.9769
Mejores resultados en la época:  12
f1-score 0.8813907841854379
AUC según el mejor F1-score 0.9817042375534667
Confusion Matrix:
 [[15641   824]
 [  445  4715]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_2609409.png
Accuracy:   0.9413
Precision:  0.8512
Recall:     0.9138
F1-score:   0.8814

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.2341, Test Loss: 0.1668, F1: 0.8695, AUC: 0.9811
Epoch [10/30] Train Loss: 0.0029, Test Loss: 0.5071, F1: 0.8640, AUC: 0.9813
Epoch [20/30] Train Loss: 0.0000, Test Loss: 1.1091, F1: 0.8724, AUC: 0.9722
Mejores resultados en la época:  11
f1-score 0.8771005540921064
AUC según el mejor F1-score 0.98078495728548
Confusion Matrix:
 [[15444  1021]
 [  332  4828]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_2609409.png
Accuracy:   0.9374
Precision:  0.8254
Recall:     0.9357
F1-score:   0.8771

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.2347, Test Loss: 0.2021, F1: 0.8509, AUC: 0.9811
Epoch [10/30] Train Loss: 0.0019, Test Loss: 0.5667, F1: 0.8676, AUC: 0.9797
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7189, F1: 0.8792, AUC: 0.9789
Mejores resultados en la época:  11
f1-score 0.886175807663411
AUC según el mejor F1-score 0.9811307989463203
Confusion Matrix:
 [[15695   770]
 [  442  4718]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_2609409.png
Accuracy:   0.9440
Precision:  0.8597
Recall:     0.9143
F1-score:   0.8862

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.2283, Test Loss: 0.1609, F1: 0.8721, AUC: 0.9807
Epoch [10/30] Train Loss: 0.0019, Test Loss: 0.5966, F1: 0.8606, AUC: 0.9813
Epoch [20/30] Train Loss: 0.0011, Test Loss: 0.4685, F1: 0.8847, AUC: 0.9826
Mejores resultados en la época:  18
f1-score 0.8941676322904596
AUC según el mejor F1-score 0.9831945082003877
Confusion Matrix:
 [[15899   566]
 [  530  4630]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_2609409.png
Accuracy:   0.9493
Precision:  0.8911
Recall:     0.8973
F1-score:   0.8942
Tiempo total para red 4: 690.72 segundos

Entrenando red 5 con capas [10023, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.2236, Test Loss: 0.1587, F1: 0.8756, AUC: 0.9809
Epoch [10/30] Train Loss: 0.0019, Test Loss: 0.4444, F1: 0.8774, AUC: 0.9824
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8174, F1: 0.8633, AUC: 0.9786
Mejores resultados en la época:  12
f1-score 0.8841480382153789
AUC según el mejor F1-score 0.9826422973796894
Confusion Matrix:
 [[15610   855]
 [  394  4766]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_5304833.png
Accuracy:   0.9422
Precision:  0.8479
Recall:     0.9236
F1-score:   0.8841

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.2273, Test Loss: 0.2275, F1: 0.8381, AUC: 0.9815
Epoch [10/30] Train Loss: 0.0012, Test Loss: 0.4704, F1: 0.8699, AUC: 0.9831
Epoch [20/30] Train Loss: 0.0013, Test Loss: 0.6871, F1: 0.8545, AUC: 0.9802
Mejores resultados en la época:  9
f1-score 0.8844452635458901
AUC según el mejor F1-score 0.9829537873384228
Confusion Matrix:
 [[15572   893]
 [  361  4799]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_5304833.png
Accuracy:   0.9420
Precision:  0.8431
Recall:     0.9300
F1-score:   0.8844

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.2250, Test Loss: 0.1627, F1: 0.8721, AUC: 0.9809
Epoch [10/30] Train Loss: 0.0007, Test Loss: 0.5562, F1: 0.8832, AUC: 0.9817
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.7556, F1: 0.8768, AUC: 0.9798
Mejores resultados en la época:  4
f1-score 0.8898918664786084
AUC según el mejor F1-score 0.9827031028938529
Confusion Matrix:
 [[15722   743]
 [  428  4732]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_5304833.png
Accuracy:   0.9458
Precision:  0.8643
Recall:     0.9171
F1-score:   0.8899

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.2278, Test Loss: 0.2207, F1: 0.8401, AUC: 0.9819
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.3993, F1: 0.8690, AUC: 0.9831
Epoch [20/30] Train Loss: 0.0003, Test Loss: 0.5740, F1: 0.8808, AUC: 0.9819
Mejores resultados en la época:  13
f1-score 0.8898168359115498
AUC según el mejor F1-score 0.9817091398950558
Confusion Matrix:
 [[15776   689]
 [  472  4688]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_5304833.png
Accuracy:   0.9463
Precision:  0.8719
Recall:     0.9085
F1-score:   0.8898
Tiempo total para red 5: 701.49 segundos

/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:03:58] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Entrenando red 6 con capas [10023, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.2294, Test Loss: 0.1944, F1: 0.8604, AUC: 0.9819
Epoch [10/30] Train Loss: 0.0043, Test Loss: 0.3897, F1: 0.8776, AUC: 0.9821
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8123, F1: 0.8725, AUC: 0.9805
Mejores resultados en la época:  12
f1-score 0.8937960042060988
AUC según el mejor F1-score 0.9823570199412898
Confusion Matrix:
 [[15839   626]
 [  485  4675]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_10963969.png
Accuracy:   0.9486
Precision:  0.8819
Recall:     0.9060
F1-score:   0.8938

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.2310, Test Loss: 0.1704, F1: 0.8720, AUC: 0.9819
Epoch [10/30] Train Loss: 0.0019, Test Loss: 0.6756, F1: 0.8823, AUC: 0.9816
Epoch [20/30] Train Loss: 0.0027, Test Loss: 0.6959, F1: 0.8808, AUC: 0.9790
Mejores resultados en la época:  28
f1-score 0.8965247950019524
AUC según el mejor F1-score 0.983625055026283
Confusion Matrix:
 [[15973   492]
 [  568  4592]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_10963969.png
Accuracy:   0.9510
Precision:  0.9032
Recall:     0.8899
F1-score:   0.8965

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.2295, Test Loss: 0.1881, F1: 0.8684, AUC: 0.9805
Epoch [10/30] Train Loss: 0.0014, Test Loss: 0.4911, F1: 0.8865, AUC: 0.9830
Epoch [20/30] Train Loss: 0.0020, Test Loss: 0.6219, F1: 0.8819, AUC: 0.9810
Mejores resultados en la época:  11
f1-score 0.8945863515059093
AUC según el mejor F1-score 0.981024701210225
Confusion Matrix:
 [[15826   639]
 [  467  4693]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_10963969.png
Accuracy:   0.9489
Precision:  0.8802
Recall:     0.9095
F1-score:   0.8946

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.2289, Test Loss: 0.1550, F1: 0.8759, AUC: 0.9821
Epoch [10/30] Train Loss: 0.0032, Test Loss: 0.7199, F1: 0.8720, AUC: 0.9807
Epoch [20/30] Train Loss: 0.0000, Test Loss: 0.8153, F1: 0.8686, AUC: 0.9801
Mejores resultados en la época:  11
f1-score 0.8936010508538187
AUC según el mejor F1-score 0.9824209622478501
Confusion Matrix:
 [[15729   736]
 [  398  4762]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/tfidf/confusion_matrix_param_10963969.png
Accuracy:   0.9476
Precision:  0.8661
Recall:     0.9229
F1-score:   0.8936
Tiempo total para red 6: 747.19 segundos
Saved on: outputs_text_plus_numerical_categorical/2/tfidf

==============================
Model: Logistic Regression
Accuracy:  0.9353
Precision: 0.8251
Recall:    0.9246
F1-score:  0.8721
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.92      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.91     21625
weighted avg       0.94      0.94      0.94     21625

[[15454  1011]
 [  389  4771]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/tfidf/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/tfidf/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.7564
Precision: 0.4944
Recall:    0.9291
F1-score:  0.6454
              precision    recall  f1-score   support

           0       0.97      0.70      0.81     16465
           1       0.49      0.93      0.65      5160

    accuracy                           0.76     21625
   macro avg       0.73      0.82      0.73     21625
weighted avg       0.86      0.76      0.77     21625

[[11563  4902]
 [  366  4794]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/tfidf/conf_matrix_svm.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/tfidf/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8997
Precision: 0.7507
Recall:    0.8676
F1-score:  0.8049
              precision    recall  f1-score   support

           0       0.96      0.91      0.93     16465
           1       0.75      0.87      0.80      5160

    accuracy                           0.90     21625
   macro avg       0.85      0.89      0.87     21625
weighted avg       0.91      0.90      0.90     21625

[[14978  1487]
 [  683  4477]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/tfidf/conf_matrix_decision_tree.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8846
Precision: 0.7423
Recall:    0.7911
F1-score:  0.7659
              precision    recall  f1-score   support

           0       0.93      0.91      0.92     16465
           1       0.74      0.79      0.77      5160

    accuracy                           0.88     21625
   macro avg       0.84      0.85      0.84     21625
weighted avg       0.89      0.88      0.89     21625

[[15048  1417]
 [ 1078  4082]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/tfidf/conf_matrix_random_forest.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/tfidf/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9366
Precision: 0.8272
Recall:    0.9279
F1-score:  0.8747
              precision    recall  f1-score   support

           0       0.98      0.94      0.96     16465
           1       0.83      0.93      0.87      5160

    accuracy                           0.94     21625
   macro avg       0.90      0.93      0.92     21625
weighted avg       0.94      0.94      0.94     21625

[[15465  1000]
 [  372  4788]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text_plus_numerical_categorical.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/tfidf/conf_matrix_xgboost.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.9163
Precision: 0.8097
Recall:    0.8486
F1-score:  0.8287
              precision    recall  f1-score   support

           0       0.95      0.94      0.94     16465
           1       0.81      0.85      0.83      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.89      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15436  1029]
 [  781  4379]]
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/tfidf/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/tfidf/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.9366, 'precision': 0.8272, 'recall': 0.9279, 'f1_score': 0.8747}
Logistic Regression: {'accuracy': 0.9353, 'precision': 0.8251, 'recall': 0.9246, 'f1_score': 0.8721}
Naive Bayes: {'accuracy': 0.9163, 'precision': 0.8097, 'recall': 0.8486, 'f1_score': 0.8287}
Decision Tree: {'accuracy': 0.8997, 'precision': 0.7507, 'recall': 0.8676, 'f1_score': 0.8049}
Random Forest: {'accuracy': 0.8846, 'precision': 0.7423, 'recall': 0.7911, 'f1_score': 0.7659}
SVM: {'accuracy': 0.7564, 'precision': 0.4944, 'recall': 0.9291, 'f1_score': 0.6454}

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_text:  (86500, 300)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 623)
Shape of X_test after concatenation:  (21625, 623)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 623)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 623)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [623, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5728, Test Loss: 0.4667, F1: 0.6636, AUC: 0.8899
Epoch [10/30] Train Loss: 0.3380, Test Loss: 0.3780, F1: 0.7249, AUC: 0.9325
Epoch [20/30] Train Loss: 0.3250, Test Loss: 0.2810, F1: 0.7737, AUC: 0.9366
Mejores resultados en la época:  16
f1-score 0.7787035155456208
AUC según el mejor F1-score 0.9354184822397522
Confusion Matrix:
 [[15615   850]
 [ 1328  3832]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_20001.png
Accuracy:   0.8993
Precision:  0.8185
Recall:     0.7426
F1-score:   0.7787

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5246, Test Loss: 0.4264, F1: 0.6858, AUC: 0.9029
Epoch [10/30] Train Loss: 0.3352, Test Loss: 0.2692, F1: 0.7779, AUC: 0.9352
Epoch [20/30] Train Loss: 0.3232, Test Loss: 0.3178, F1: 0.7560, AUC: 0.9395
Mejores resultados en la época:  27
f1-score 0.7869206598586017
AUC según el mejor F1-score 0.9412324239577963
Confusion Matrix:
 [[15448  1017]
 [ 1153  4007]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_20001.png
Accuracy:   0.8997
Precision:  0.7976
Recall:     0.7766
F1-score:   0.7869

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5337, Test Loss: 0.3741, F1: 0.7183, AUC: 0.9003
Epoch [10/30] Train Loss: 0.3384, Test Loss: 0.3489, F1: 0.7409, AUC: 0.9338
Epoch [20/30] Train Loss: 0.3268, Test Loss: 0.2855, F1: 0.7727, AUC: 0.9373
Mejores resultados en la época:  28
f1-score 0.7816732302372491
AUC según el mejor F1-score 0.9391741055139278
Confusion Matrix:
 [[15283  1182]
 [ 1091  4069]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_20001.png
Accuracy:   0.8949
Precision:  0.7749
Recall:     0.7886
F1-score:   0.7817

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5276, Test Loss: 0.3917, F1: 0.7078, AUC: 0.9011
Epoch [10/30] Train Loss: 0.3368, Test Loss: 0.3308, F1: 0.7517, AUC: 0.9348
Epoch [20/30] Train Loss: 0.3251, Test Loss: 0.4587, F1: 0.6868, AUC: 0.9384
Mejores resultados en la época:  27
f1-score 0.7858951879992208
AUC según el mejor F1-score 0.9402301040261584
Confusion Matrix:
 [[15393  1072]
 [ 1126  4034]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_20001.png
Accuracy:   0.8984
Precision:  0.7901
Recall:     0.7818
F1-score:   0.7859
Tiempo total para red 1: 254.99 segundos

Entrenando red 2 con capas [623, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4876, Test Loss: 0.3833, F1: 0.7071, AUC: 0.9117
Epoch [10/30] Train Loss: 0.3335, Test Loss: 0.3604, F1: 0.7367, AUC: 0.9365
Epoch [20/30] Train Loss: 0.3283, Test Loss: 0.2767, F1: 0.7818, AUC: 0.9399
Mejores resultados en la época:  20
f1-score 0.781758345086977
AUC según el mejor F1-score 0.9398980513045055
Confusion Matrix:
 [[15147  1318]
 [ 1003  4157]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_42049.png
Accuracy:   0.8927
Precision:  0.7593
Recall:     0.8056
F1-score:   0.7818

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4955, Test Loss: 0.3420, F1: 0.7280, AUC: 0.9108
Epoch [10/30] Train Loss: 0.3388, Test Loss: 0.3605, F1: 0.7386, AUC: 0.9357
Epoch [20/30] Train Loss: 0.3227, Test Loss: 0.2689, F1: 0.7829, AUC: 0.9403
Mejores resultados en la época:  20
f1-score 0.782888325862405
AUC según el mejor F1-score 0.9402915157122109
Confusion Matrix:
 [[15380  1085]
 [ 1143  4017]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_42049.png
Accuracy:   0.8970
Precision:  0.7873
Recall:     0.7785
F1-score:   0.7829

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4974, Test Loss: 0.4034, F1: 0.6965, AUC: 0.9094
Epoch [10/30] Train Loss: 0.3418, Test Loss: 0.3173, F1: 0.7624, AUC: 0.9355
Epoch [20/30] Train Loss: 0.3253, Test Loss: 0.3853, F1: 0.7218, AUC: 0.9388
Mejores resultados en la época:  21
f1-score 0.7833907386146192
AUC según el mejor F1-score 0.9396946423821262
Confusion Matrix:
 [[15267  1198]
 [ 1066  4094]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_42049.png
Accuracy:   0.8953
Precision:  0.7736
Recall:     0.7934
F1-score:   0.7834

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4994, Test Loss: 0.3854, F1: 0.7101, AUC: 0.9098
Epoch [10/30] Train Loss: 0.3381, Test Loss: 0.4199, F1: 0.7047, AUC: 0.9360
Epoch [20/30] Train Loss: 0.3284, Test Loss: 0.3077, F1: 0.7641, AUC: 0.9391
Mejores resultados en la época:  26
f1-score 0.788027477919529
AUC según el mejor F1-score 0.941013642987121
Confusion Matrix:
 [[15450  1015]
 [ 1145  4015]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_42049.png
Accuracy:   0.9001
Precision:  0.7982
Recall:     0.7781
F1-score:   0.7880
Tiempo total para red 2: 260.96 segundos

Entrenando red 3 con capas [623, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4905, Test Loss: 0.3026, F1: 0.7436, AUC: 0.9127
Epoch [10/30] Train Loss: 0.3372, Test Loss: 0.3636, F1: 0.7401, AUC: 0.9363
Epoch [20/30] Train Loss: 0.3273, Test Loss: 0.2928, F1: 0.7773, AUC: 0.9408
Mejores resultados en la época:  26
f1-score 0.7865346534653466
AUC según el mejor F1-score 0.9415606572080311
Confusion Matrix:
 [[15497   968]
 [ 1188  3972]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_90241.png
Accuracy:   0.9003
Precision:  0.8040
Recall:     0.7698
F1-score:   0.7865

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4838, Test Loss: 0.3371, F1: 0.7392, AUC: 0.9138
Epoch [10/30] Train Loss: 0.3405, Test Loss: 0.2761, F1: 0.7795, AUC: 0.9359
Epoch [20/30] Train Loss: 0.3237, Test Loss: 0.2928, F1: 0.7726, AUC: 0.9403
Mejores resultados en la época:  27
f1-score 0.7843327825833414
AUC según el mejor F1-score 0.9421395807879999
Confusion Matrix:
 [[15371  1094]
 [ 1125  4035]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_90241.png
Accuracy:   0.8974
Precision:  0.7867
Recall:     0.7820
F1-score:   0.7843

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4905, Test Loss: 0.3496, F1: 0.7278, AUC: 0.9122
Epoch [10/30] Train Loss: 0.3380, Test Loss: 0.4234, F1: 0.7086, AUC: 0.9361
Epoch [20/30] Train Loss: 0.3274, Test Loss: 0.2536, F1: 0.7788, AUC: 0.9397
Mejores resultados en la época:  19
f1-score 0.7837039952765203
AUC según el mejor F1-score 0.9389311247489978
Confusion Matrix:
 [[15445  1020]
 [ 1178  3982]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_90241.png
Accuracy:   0.8984
Precision:  0.7961
Recall:     0.7717
F1-score:   0.7837

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4681, Test Loss: 0.3909, F1: 0.7120, AUC: 0.9162
Epoch [10/30] Train Loss: 0.3343, Test Loss: 0.3179, F1: 0.7588, AUC: 0.9374
Epoch [20/30] Train Loss: 0.3346, Test Loss: 0.3167, F1: 0.7703, AUC: 0.9405
Mejores resultados en la época:  25
f1-score 0.7853174603174603
AUC según el mejor F1-score 0.9419204231668302
Confusion Matrix:
 [[15503   962]
 [ 1202  3958]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_90241.png
Accuracy:   0.8999
Precision:  0.8045
Recall:     0.7671
F1-score:   0.7853
Tiempo total para red 3: 327.88 segundos

Entrenando red 4 con capas [623, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4994, Test Loss: 0.3426, F1: 0.7374, AUC: 0.9131
Epoch [10/30] Train Loss: 0.3358, Test Loss: 0.2627, F1: 0.7804, AUC: 0.9377
Epoch [20/30] Train Loss: 0.3227, Test Loss: 0.3342, F1: 0.7449, AUC: 0.9406
Mejores resultados en la época:  24
f1-score 0.783014376844711
AUC según el mejor F1-score 0.9417452748018464
Confusion Matrix:
 [[15234  1231]
 [ 1048  4112]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_203009.png
Accuracy:   0.8946
Precision:  0.7696
Recall:     0.7969
F1-score:   0.7830

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4668, Test Loss: 0.3074, F1: 0.7417, AUC: 0.9177
Epoch [10/30] Train Loss: 0.3438, Test Loss: 0.3458, F1: 0.7565, AUC: 0.9369
Epoch [20/30] Train Loss: 0.3299, Test Loss: 0.3148, F1: 0.7622, AUC: 0.9406
Mejores resultados en la época:  23
f1-score 0.7849175797058533
AUC según el mejor F1-score 0.9413198951499185
Confusion Matrix:
 [[15470   995]
 [ 1184  3976]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_203009.png
Accuracy:   0.8992
Precision:  0.7998
Recall:     0.7705
F1-score:   0.7849

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4678, Test Loss: 0.3608, F1: 0.7355, AUC: 0.9168
Epoch [10/30] Train Loss: 0.3392, Test Loss: 0.3790, F1: 0.7342, AUC: 0.9368
Epoch [20/30] Train Loss: 0.3223, Test Loss: 0.3100, F1: 0.7652, AUC: 0.9411
Mejores resultados en la época:  29
f1-score 0.7887766113999803
AUC según el mejor F1-score 0.9434464285293915
Confusion Matrix:
 [[15452  1013]
 [ 1140  4020]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_203009.png
Accuracy:   0.9004
Precision:  0.7987
Recall:     0.7791
F1-score:   0.7888

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4684, Test Loss: 0.2952, F1: 0.7178, AUC: 0.9166
Epoch [10/30] Train Loss: 0.3383, Test Loss: 0.3221, F1: 0.7594, AUC: 0.9373
Epoch [20/30] Train Loss: 0.3178, Test Loss: 0.3666, F1: 0.7322, AUC: 0.9411
Mejores resultados en la época:  24
f1-score 0.7840109460516028
AUC según el mejor F1-score 0.9409194862487259
Confusion Matrix:
 [[15404  1061]
 [ 1149  4011]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_203009.png
Accuracy:   0.8978
Precision:  0.7908
Recall:     0.7773
F1-score:   0.7840
Tiempo total para red 4: 336.33 segundos

Entrenando red 5 con capas [623, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4939, Test Loss: 0.4758, F1: 0.6715, AUC: 0.9124
Epoch [10/30] Train Loss: 0.3391, Test Loss: 0.2763, F1: 0.7789, AUC: 0.9373
Epoch [20/30] Train Loss: 0.3246, Test Loss: 0.5329, F1: 0.6420, AUC: 0.9398
Mejores resultados en la época:  23
f1-score 0.7847086429409464
AUC según el mejor F1-score 0.9417022660235359
Confusion Matrix:
 [[15410  1055]
 [ 1147  4013]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_492033.png
Accuracy:   0.8982
Precision:  0.7918
Recall:     0.7777
F1-score:   0.7847

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4665, Test Loss: 0.5584, F1: 0.6247, AUC: 0.9175
Epoch [10/30] Train Loss: 0.3373, Test Loss: 0.4092, F1: 0.7134, AUC: 0.9374
Epoch [20/30] Train Loss: 0.3236, Test Loss: 0.2916, F1: 0.7813, AUC: 0.9412
Mejores resultados en la época:  19
f1-score 0.7841315916787615
AUC según el mejor F1-score 0.9407645416516596
Confusion Matrix:
 [[15342  1123]
 [ 1108  4052]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_492033.png
Accuracy:   0.8968
Precision:  0.7830
Recall:     0.7853
F1-score:   0.7841

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4710, Test Loss: 0.4114, F1: 0.7031, AUC: 0.9162
Epoch [10/30] Train Loss: 0.3374, Test Loss: 0.3318, F1: 0.7549, AUC: 0.9368
Epoch [20/30] Train Loss: 0.3216, Test Loss: 0.2608, F1: 0.7769, AUC: 0.9404
Mejores resultados en la época:  25
f1-score 0.7838575907757661
AUC según el mejor F1-score 0.9417243118477767
Confusion Matrix:
 [[15613   852]
 [ 1285  3875]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:51:39] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_492033.png
Accuracy:   0.9012
Precision:  0.8198
Recall:     0.7510
F1-score:   0.7839

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4550, Test Loss: 0.3113, F1: 0.7521, AUC: 0.9184
Epoch [10/30] Train Loss: 0.3363, Test Loss: 0.2950, F1: 0.7775, AUC: 0.9378
Epoch [20/30] Train Loss: 0.3266, Test Loss: 0.3357, F1: 0.7543, AUC: 0.9405
Mejores resultados en la época:  23
f1-score 0.7846507444788648
AUC según el mejor F1-score 0.9414992396368147
Confusion Matrix:
 [[15544   921]
 [ 1234  3926]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_492033.png
Accuracy:   0.9003
Precision:  0.8100
Recall:     0.7609
F1-score:   0.7847
Tiempo total para red 5: 342.17 segundos

Entrenando red 6 con capas [623, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.5071, Test Loss: 0.5095, F1: 0.6545, AUC: 0.9131
Epoch [10/30] Train Loss: 0.3370, Test Loss: 0.2640, F1: 0.7801, AUC: 0.9370
Epoch [20/30] Train Loss: 0.3229, Test Loss: 0.2790, F1: 0.7772, AUC: 0.9409
Mejores resultados en la época:  21
f1-score 0.7830256044068142
AUC según el mejor F1-score 0.9399092566567091
Confusion Matrix:
 [[15660   805]
 [ 1322  3838]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_1338369.png
Accuracy:   0.9016
Precision:  0.8266
Recall:     0.7438
F1-score:   0.7830

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4899, Test Loss: 0.6038, F1: 0.6254, AUC: 0.9138
Epoch [10/30] Train Loss: 0.3362, Test Loss: 0.4234, F1: 0.6960, AUC: 0.9381
Epoch [20/30] Train Loss: 0.3263, Test Loss: 0.3603, F1: 0.7384, AUC: 0.9413
Mejores resultados en la época:  25
f1-score 0.7856650585802895
AUC según el mejor F1-score 0.9419834297323191
Confusion Matrix:
 [[15458  1007]
 [ 1170  3990]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_1338369.png
Accuracy:   0.8993
Precision:  0.7985
Recall:     0.7733
F1-score:   0.7857

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4809, Test Loss: 0.3974, F1: 0.7186, AUC: 0.9166
Epoch [10/30] Train Loss: 0.3383, Test Loss: 0.3338, F1: 0.7519, AUC: 0.9383
Epoch [20/30] Train Loss: 0.3232, Test Loss: 0.2672, F1: 0.7826, AUC: 0.9410
Mejores resultados en la época:  24
f1-score 0.7867146858743498
AUC según el mejor F1-score 0.9420976136837125
Confusion Matrix:
 [[15561   904]
 [ 1228  3932]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_1338369.png
Accuracy:   0.9014
Precision:  0.8131
Recall:     0.7620
F1-score:   0.7867

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4810, Test Loss: 0.4533, F1: 0.6919, AUC: 0.9157
Epoch [10/30] Train Loss: 0.3405, Test Loss: 0.3671, F1: 0.7482, AUC: 0.9372
Epoch [20/30] Train Loss: 0.3252, Test Loss: 0.3206, F1: 0.7583, AUC: 0.9407
Mejores resultados en la época:  27
f1-score 0.7896440129449838
AUC según el mejor F1-score 0.9431080492564683
Confusion Matrix:
 [[15641   824]
 [ 1256  3904]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/lyrics_bert/confusion_matrix_param_1338369.png
Accuracy:   0.9038
Precision:  0.8257
Recall:     0.7566
F1-score:   0.7896
Tiempo total para red 6: 354.18 segundos
Saved on: outputs_text_plus_numerical_categorical/2/lyrics_bert

==============================
Model: Logistic Regression
Accuracy:  0.8731
Precision: 0.6902
Recall:    0.8496
F1-score:  0.7616
              precision    recall  f1-score   support

           0       0.95      0.88      0.91     16465
           1       0.69      0.85      0.76      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.87      0.84     21625
weighted avg       0.89      0.87      0.88     21625

[[14497  1968]
 [  776  4384]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.5012
Precision: 0.3165
Recall:    0.9403
F1-score:  0.4736
              precision    recall  f1-score   support

           0       0.95      0.36      0.53     16465
           1       0.32      0.94      0.47      5160

    accuracy                           0.50     21625
   macro avg       0.63      0.65      0.50     21625
weighted avg       0.80      0.50      0.51     21625

[[ 5986 10479]
 [  308  4852]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/lyrics_bert/conf_matrix_svm.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8082
Precision: 0.5718
Recall:    0.7814
F1-score:  0.6604
              precision    recall  f1-score   support

           0       0.92      0.82      0.87     16465
           1       0.57      0.78      0.66      5160

    accuracy                           0.81     21625
   macro avg       0.75      0.80      0.76     21625
weighted avg       0.84      0.81      0.82     21625

[[13446  3019]
 [ 1128  4032]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8695
Precision: 0.7004
Recall:    0.7921
F1-score:  0.7434
              precision    recall  f1-score   support

           0       0.93      0.89      0.91     16465
           1       0.70      0.79      0.74      5160

    accuracy                           0.87     21625
   macro avg       0.82      0.84      0.83     21625
weighted avg       0.88      0.87      0.87     21625

[[14717  1748]
 [ 1073  4087]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/lyrics_bert/conf_matrix_random_forest.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8966
Precision: 0.7468
Recall:    0.8570
F1-score:  0.7981
              precision    recall  f1-score   support

           0       0.95      0.91      0.93     16465
           1       0.75      0.86      0.80      5160

    accuracy                           0.90     21625
   macro avg       0.85      0.88      0.86     21625
weighted avg       0.90      0.90      0.90     21625

[[14966  1499]
 [  738  4422]]
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/preprocessed_data/main_only_text_plus_numerical_categorical.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/lyrics_bert/conf_matrix_xgboost.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.7207
Precision: 0.4518
Recall:    0.7990
F1-score:  0.5772
              precision    recall  f1-score   support

           0       0.92      0.70      0.79     16465
           1       0.45      0.80      0.58      5160

    accuracy                           0.72     21625
   macro avg       0.68      0.75      0.68     21625
weighted avg       0.81      0.72      0.74     21625

[[11462  5003]
 [ 1037  4123]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/lyrics_bert/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/lyrics_bert/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.8966, 'precision': 0.7468, 'recall': 0.857, 'f1_score': 0.7981}
Logistic Regression: {'accuracy': 0.8731, 'precision': 0.6902, 'recall': 0.8496, 'f1_score': 0.7616}
Random Forest: {'accuracy': 0.8695, 'precision': 0.7004, 'recall': 0.7921, 'f1_score': 0.7434}
Decision Tree: {'accuracy': 0.8082, 'precision': 0.5718, 'recall': 0.7814, 'f1_score': 0.6604}
Naive Bayes: {'accuracy': 0.7207, 'precision': 0.4518, 'recall': 0.799, 'f1_score': 0.5772}
SVM: {'accuracy': 0.5012, 'precision': 0.3165, 'recall': 0.9403, 'f1_score': 0.4736}

##################################################
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 1536)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 1536), y: (108138,)
Shape filtrado  X: (108125, 1536), y: (108125,)
Tamanio:  (108125, 43)
Total de columnas:  43
Columnas numericas usadas:  ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
Columnas del df:  ['Artist(s)', 'song', 'text', 'Length', 'emotion', 'Genre', 'Album', 'Key', 'Tempo', 'Loudness (db)', 'Time signature', 'Explicit', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Similar Artist 1', 'Similar Song 1', 'Similarity Score 1', 'Similar Artist 2', 'Similar Song 2', 'Similarity Score 2', 'Similar Artist 3', 'Similar Song 3', 'Similarity Score 3', 'song_normalized', 'artist_normalized', 'Release_Year', 'Release_Month', 'Release_Day']

Splitting data (index-based split to avoid leakage)...
X_train_text:  (86500, 1536)
X_train_Numeric:  (86500, 23)
Concatenating TF-IDF and numeric features...
Shape of X_train after concatenation:  (86500, 3095)
Shape of X_test after concatenation:  (21625, 3095)
Shape of y_train:  (86500,)
Shape of y_test:  (21625,)

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 3095)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 3095)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [3095, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5091, Test Loss: 0.4739, F1: 0.7590, AUC: 0.9277
Epoch [10/30] Train Loss: 0.2425, Test Loss: 0.2111, F1: 0.8449, AUC: 0.9655
Epoch [20/30] Train Loss: 0.2250, Test Loss: 0.2652, F1: 0.8125, AUC: 0.9696
Mejores resultados en la época:  26
f1-score 0.8544408696499951
AUC según el mejor F1-score 0.9700342339988276
Confusion Matrix:
 [[15750   715]
 [  778  4382]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_99105.png
Accuracy:   0.9310
Precision:  0.8597
Recall:     0.8492
F1-score:   0.8544

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4194, Test Loss: 0.2971, F1: 0.7645, AUC: 0.9398
Epoch [10/30] Train Loss: 0.2361, Test Loss: 0.2309, F1: 0.8273, AUC: 0.9674
Epoch [20/30] Train Loss: 0.2220, Test Loss: 0.1973, F1: 0.8477, AUC: 0.9700
Mejores resultados en la época:  19
f1-score 0.8507064519316273
AUC según el mejor F1-score 0.9698298363689009
Confusion Matrix:
 [[15809   656]
 [  855  4305]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_99105.png
Accuracy:   0.9301
Precision:  0.8678
Recall:     0.8343
F1-score:   0.8507

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.3943, Test Loss: 0.2588, F1: 0.7857, AUC: 0.9453
Epoch [10/30] Train Loss: 0.2371, Test Loss: 0.2867, F1: 0.7943, AUC: 0.9682
Epoch [20/30] Train Loss: 0.2216, Test Loss: 0.2152, F1: 0.8370, AUC: 0.9704
Mejores resultados en la época:  22
f1-score 0.853448275862069
AUC según el mejor F1-score 0.9707168482828268
Confusion Matrix:
 [[15773   692]
 [  804  4356]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_99105.png
Accuracy:   0.9308
Precision:  0.8629
Recall:     0.8442
F1-score:   0.8534

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.3671, Test Loss: 0.2461, F1: 0.8051, AUC: 0.9534
Epoch [10/30] Train Loss: 0.2307, Test Loss: 0.1841, F1: 0.8497, AUC: 0.9694
Epoch [20/30] Train Loss: 0.2237, Test Loss: 0.3372, F1: 0.7662, AUC: 0.9713
Mejores resultados en la época:  28
f1-score 0.8560613606932962
AUC según el mejor F1-score 0.9730186065344153
Confusion Matrix:
 [[15883   582]
 [  863  4297]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_99105.png
Accuracy:   0.9332
Precision:  0.8807
Recall:     0.8328
F1-score:   0.8561
Tiempo total para red 1: 433.76 segundos

Entrenando red 2 con capas [3095, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3702, Test Loss: 0.4565, F1: 0.6862, AUC: 0.9528
Epoch [10/30] Train Loss: 0.2343, Test Loss: 0.2381, F1: 0.8220, AUC: 0.9690
Epoch [20/30] Train Loss: 0.2178, Test Loss: 0.1966, F1: 0.8483, AUC: 0.9713
Mejores resultados en la época:  27
f1-score 0.8565660449913478
AUC según el mejor F1-score 0.9724581329435
Confusion Matrix:
 [[15678   787]
 [  705  4455]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_200257.png
Accuracy:   0.9310
Precision:  0.8499
Recall:     0.8634
F1-score:   0.8566

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3721, Test Loss: 0.3014, F1: 0.7740, AUC: 0.9517
Epoch [10/30] Train Loss: 0.2358, Test Loss: 0.2081, F1: 0.8391, AUC: 0.9692
Epoch [20/30] Train Loss: 0.2227, Test Loss: 0.2383, F1: 0.8250, AUC: 0.9711
Mejores resultados en la época:  24
f1-score 0.854336833916764
AUC según el mejor F1-score 0.9716883005294293
Confusion Matrix:
 [[15734   731]
 [  767  4393]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_200257.png
Accuracy:   0.9307
Precision:  0.8573
Recall:     0.8514
F1-score:   0.8543

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3636, Test Loss: 0.3157, F1: 0.7699, AUC: 0.9549
Epoch [10/30] Train Loss: 0.2337, Test Loss: 0.1939, F1: 0.8461, AUC: 0.9689
Epoch [20/30] Train Loss: 0.2247, Test Loss: 0.1802, F1: 0.8523, AUC: 0.9713
Mejores resultados en la época:  23
f1-score 0.8549072642967542
AUC según el mejor F1-score 0.9720523332321085
Confusion Matrix:
 [[15698   767]
 [  735  4425]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_200257.png
Accuracy:   0.9305
Precision:  0.8523
Recall:     0.8576
F1-score:   0.8549

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.3726, Test Loss: 0.3015, F1: 0.7744, AUC: 0.9525
Epoch [10/30] Train Loss: 0.2370, Test Loss: 0.2894, F1: 0.7923, AUC: 0.9688
Epoch [20/30] Train Loss: 0.2237, Test Loss: 0.2280, F1: 0.8271, AUC: 0.9709
Mejores resultados en la época:  25
f1-score 0.8542478565861262
AUC según el mejor F1-score 0.9715321553589126
Confusion Matrix:
 [[15745   720]
 [  776  4384]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_200257.png
Accuracy:   0.9308
Precision:  0.8589
Recall:     0.8496
F1-score:   0.8542
Tiempo total para red 2: 435.36 segundos

Entrenando red 3 con capas [3095, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3603, Test Loss: 0.2221, F1: 0.8059, AUC: 0.9549
Epoch [10/30] Train Loss: 0.2393, Test Loss: 0.1879, F1: 0.8470, AUC: 0.9691
Epoch [20/30] Train Loss: 0.2249, Test Loss: 0.1938, F1: 0.8458, AUC: 0.9715
Mejores resultados en la época:  26
f1-score 0.8561478185160104
AUC según el mejor F1-score 0.9724610814106504
Confusion Matrix:
 [[15713   752]
 [  735  4425]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_406657.png
Accuracy:   0.9312
Precision:  0.8547
Recall:     0.8576
F1-score:   0.8561

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3730, Test Loss: 0.2690, F1: 0.7942, AUC: 0.9525
Epoch [10/30] Train Loss: 0.2332, Test Loss: 0.1839, F1: 0.8475, AUC: 0.9694
Epoch [20/30] Train Loss: 0.2188, Test Loss: 0.1875, F1: 0.8513, AUC: 0.9718
Mejores resultados en la época:  28
f1-score 0.8544461003477397
AUC según el mejor F1-score 0.9733813445010205
Confusion Matrix:
 [[15860   605]
 [  860  4300]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_406657.png
Accuracy:   0.9323
Precision:  0.8767
Recall:     0.8333
F1-score:   0.8544

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3519, Test Loss: 0.2799, F1: 0.7889, AUC: 0.9562
Epoch [10/30] Train Loss: 0.2416, Test Loss: 0.3657, F1: 0.7490, AUC: 0.9690
Epoch [20/30] Train Loss: 0.2206, Test Loss: 0.2589, F1: 0.8193, AUC: 0.9714
Mejores resultados en la época:  26
f1-score 0.8554924613275896
AUC según el mejor F1-score 0.972355024870703
Confusion Matrix:
 [[15780   685]
 [  791  4369]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_406657.png
Accuracy:   0.9317
Precision:  0.8645
Recall:     0.8467
F1-score:   0.8555

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.3655, Test Loss: 0.2214, F1: 0.8080, AUC: 0.9548
Epoch [10/30] Train Loss: 0.2407, Test Loss: 0.2601, F1: 0.8117, AUC: 0.9689
Epoch [20/30] Train Loss: 0.2257, Test Loss: 0.1797, F1: 0.8384, AUC: 0.9715
Mejores resultados en la época:  24
f1-score 0.851911583341239
AUC según el mejor F1-score 0.9719334293792092
Confusion Matrix:
 [[15574   891]
 [  670  4490]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_406657.png
Accuracy:   0.9278
Precision:  0.8344
Recall:     0.8702
F1-score:   0.8519
Tiempo total para red 3: 435.16 segundos

Entrenando red 4 con capas [3095, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3742, Test Loss: 0.3337, F1: 0.7629, AUC: 0.9532
Epoch [10/30] Train Loss: 0.2326, Test Loss: 0.1911, F1: 0.8461, AUC: 0.9695
Epoch [20/30] Train Loss: 0.2219, Test Loss: 0.2029, F1: 0.8377, AUC: 0.9718
Mejores resultados en la época:  27
f1-score 0.8553154375423975
AUC según el mejor F1-score 0.9728921049348278
Confusion Matrix:
 [[15719   746]
 [  747  4413]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_835841.png
Accuracy:   0.9310
Precision:  0.8554
Recall:     0.8552
F1-score:   0.8553

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3664, Test Loss: 0.2319, F1: 0.8055, AUC: 0.9544
Epoch [10/30] Train Loss: 0.2362, Test Loss: 0.1943, F1: 0.8474, AUC: 0.9694
Epoch [20/30] Train Loss: 0.2205, Test Loss: 0.2504, F1: 0.8068, AUC: 0.9719
Mejores resultados en la época:  28
f1-score 0.8554561035993145
AUC según el mejor F1-score 0.973006694962535
Confusion Matrix:
 [[15615   850]
 [  668  4492]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_835841.png
Accuracy:   0.9298
Precision:  0.8409
Recall:     0.8705
F1-score:   0.8555

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3838, Test Loss: 0.3627, F1: 0.7394, AUC: 0.9528
Epoch [10/30] Train Loss: 0.2369, Test Loss: 0.2601, F1: 0.8064, AUC: 0.9695
Epoch [20/30] Train Loss: 0.2208, Test Loss: 0.1775, F1: 0.8534, AUC: 0.9717
Mejores resultados en la época:  24
f1-score 0.8559639190096919
AUC según el mejor F1-score 0.972572181536122
Confusion Matrix:
 [[15664   801]
 [  700  4460]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_835841.png
Accuracy:   0.9306
Precision:  0.8477
Recall:     0.8643
F1-score:   0.8560

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.3796, Test Loss: 0.2387, F1: 0.8018, AUC: 0.9525
Epoch [10/30] Train Loss: 0.2422, Test Loss: 0.2430, F1: 0.8187, AUC: 0.9692
Epoch [20/30] Train Loss: 0.2143, Test Loss: 0.1858, F1: 0.8319, AUC: 0.9718
Mejores resultados en la época:  29
f1-score 0.8559977097051246
AUC según el mejor F1-score 0.9733584159021842
Confusion Matrix:
 [[15631   834]
 [  675  4485]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_835841.png
Accuracy:   0.9302
Precision:  0.8432
Recall:     0.8692
F1-score:   0.8560
Tiempo total para red 4: 437.09 segundos

Entrenando red 5 con capas [3095, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3684, Test Loss: 0.2714, F1: 0.7896, AUC: 0.9553
Epoch [10/30] Train Loss: 0.2407, Test Loss: 0.1844, F1: 0.8452, AUC: 0.9692
Epoch [20/30] Train Loss: 0.2221, Test Loss: 0.1929, F1: 0.8487, AUC: 0.9716
Mejores resultados en la época:  29
f1-score 0.8560736902705814
AUC según el mejor F1-score 0.9731525057851163
Confusion Matrix:
 [[15664   801]
 [  699  4461]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_1757697.png
Accuracy:   0.9306
Precision:  0.8478
Recall:     0.8645
F1-score:   0.8561

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3751, Test Loss: 0.2249, F1: 0.7998, AUC: 0.9540
Epoch [10/30] Train Loss: 0.2403, Test Loss: 0.2322, F1: 0.8282, AUC: 0.9689
Epoch [20/30] Train Loss: 0.2273, Test Loss: 0.1844, F1: 0.8513, AUC: 0.9719
Mejores resultados en la época:  25
f1-score 0.8545627376425855
AUC según el mejor F1-score 0.9729215837211657
Confusion Matrix:
 [[15600   865]
 [  665  4495]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_1757697.png
Accuracy:   0.9292
Precision:  0.8386
Recall:     0.8711
F1-score:   0.8546

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3670, Test Loss: 0.2594, F1: 0.7362, AUC: 0.9550
Epoch [10/30] Train Loss: 0.2349, Test Loss: 0.2830, F1: 0.7906, AUC: 0.9693
Epoch [20/30] Train Loss: 0.2201, Test Loss: 0.3040, F1: 0.7788, AUC: 0.9717
Mejores resultados en la época:  22
f1-score 0.8537441497659907
AUC según el mejor F1-score 0.9717190328556934
Confusion Matrix:
 [[15747   718]
 [  782  4378]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [04:55:48] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_1757697.png
Accuracy:   0.9306
Precision:  0.8591
Recall:     0.8484
F1-score:   0.8537

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.3830, Test Loss: 0.2231, F1: 0.8085, AUC: 0.9545
Epoch [10/30] Train Loss: 0.2370, Test Loss: 0.2436, F1: 0.8143, AUC: 0.9695
Epoch [20/30] Train Loss: 0.2183, Test Loss: 0.3671, F1: 0.7382, AUC: 0.9720
Mejores resultados en la época:  25
f1-score 0.851998491704374
AUC según el mejor F1-score 0.9728627085407853
Confusion Matrix:
 [[15536   929]
 [  641  4519]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_1757697.png
Accuracy:   0.9274
Precision:  0.8295
Recall:     0.8758
F1-score:   0.8520
Tiempo total para red 5: 430.48 segundos

Entrenando red 6 con capas [3095, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3766, Test Loss: 0.3588, F1: 0.7454, AUC: 0.9563
Epoch [10/30] Train Loss: 0.2372, Test Loss: 0.3033, F1: 0.7855, AUC: 0.9695
Epoch [20/30] Train Loss: 0.2213, Test Loss: 0.1866, F1: 0.8492, AUC: 0.9717
Mejores resultados en la época:  29
f1-score 0.8493840985442329
AUC según el mejor F1-score 0.9729657459916148
Confusion Matrix:
 [[15460  1005]
 [  609  4551]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_3869697.png
Accuracy:   0.9254
Precision:  0.8191
Recall:     0.8820
F1-score:   0.8494

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3703, Test Loss: 0.2256, F1: 0.7958, AUC: 0.9566
Epoch [10/30] Train Loss: 0.2400, Test Loss: 0.3809, F1: 0.7668, AUC: 0.9693
Epoch [20/30] Train Loss: 0.2226, Test Loss: 0.1857, F1: 0.8482, AUC: 0.9716
Mejores resultados en la época:  23
f1-score 0.8546203649205415
AUC según el mejor F1-score 0.9720314644406621
Confusion Matrix:
 [[15787   678]
 [  804  4356]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_3869697.png
Accuracy:   0.9315
Precision:  0.8653
Recall:     0.8442
F1-score:   0.8546

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3763, Test Loss: 0.2963, F1: 0.7854, AUC: 0.9541
Epoch [10/30] Train Loss: 0.2376, Test Loss: 0.3659, F1: 0.7578, AUC: 0.9694
Epoch [20/30] Train Loss: 0.2173, Test Loss: 0.1858, F1: 0.8529, AUC: 0.9717
Mejores resultados en la época:  23
f1-score 0.8548797183649521
AUC según el mejor F1-score 0.9725909140130462
Confusion Matrix:
 [[15770   695]
 [  789  4371]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_3869697.png
Accuracy:   0.9314
Precision:  0.8628
Recall:     0.8471
F1-score:   0.8549

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.3960, Test Loss: 0.2333, F1: 0.8092, AUC: 0.9545
Epoch [10/30] Train Loss: 0.2390, Test Loss: 0.4044, F1: 0.6848, AUC: 0.9693
Epoch [20/30] Train Loss: 0.2225, Test Loss: 0.1903, F1: 0.8526, AUC: 0.9715
Mejores resultados en la época:  20
f1-score 0.8525561395126613
AUC según el mejor F1-score 0.9714801717055441
Confusion Matrix:
 [[15621   844]
 [  699  4461]]
Matriz de confusión guardada en: outputs_text_plus_numerical_categorical/2/gpt/confusion_matrix_param_3869697.png
Accuracy:   0.9286
Precision:  0.8409
Recall:     0.8645
F1-score:   0.8526
Tiempo total para red 6: 454.43 segundos
Saved on: outputs_text_plus_numerical_categorical/2/gpt

==============================
Model: Logistic Regression
Accuracy:  0.9169
Precision: 0.7778
Recall:    0.9124
F1-score:  0.8397
              precision    recall  f1-score   support

           0       0.97      0.92      0.94     16465
           1       0.78      0.91      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.87      0.92      0.89     21625
weighted avg       0.92      0.92      0.92     21625

[[15120  1345]
 [  452  4708]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.8193
Precision: 0.5765
Recall:    0.9138
F1-score:  0.7070
              precision    recall  f1-score   support

           0       0.97      0.79      0.87     16465
           1       0.58      0.91      0.71      5160

    accuracy                           0.82     21625
   macro avg       0.77      0.85      0.79     21625
weighted avg       0.87      0.82      0.83     21625

[[13002  3463]
 [  445  4715]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/gpt/conf_matrix_svm.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8209
Precision: 0.5959
Recall:    0.7748
F1-score:  0.6737
              precision    recall  f1-score   support

           0       0.92      0.84      0.88     16465
           1       0.60      0.77      0.67      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.81      0.78     21625
weighted avg       0.84      0.82      0.83     21625

[[13754  2711]
 [ 1162  3998]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8851
Precision: 0.7361
Recall:    0.8083
F1-score:  0.7706
              precision    recall  f1-score   support

           0       0.94      0.91      0.92     16465
           1       0.74      0.81      0.77      5160

    accuracy                           0.89     21625
   macro avg       0.84      0.86      0.85     21625
weighted avg       0.89      0.89      0.89     21625

[[14970  1495]
 [  989  4171]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.9217
Precision: 0.8027
Recall:    0.8909
F1-score:  0.8445
              precision    recall  f1-score   support

           0       0.96      0.93      0.95     16465
           1       0.80      0.89      0.84      5160

    accuracy                           0.92     21625
   macro avg       0.88      0.91      0.90     21625
weighted avg       0.93      0.92      0.92     21625

[[15335  1130]
 [  563  4597]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8612
Precision: 0.6897
Recall:    0.7607
F1-score:  0.7234
              precision    recall  f1-score   support

           0       0.92      0.89      0.91     16465
           1       0.69      0.76      0.72      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.83      0.82     21625
weighted avg       0.87      0.86      0.86     21625

[[14699  1766]
 [ 1235  3925]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_text_plus_numerical_categorical/2/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_text_plus_numerical_categorical/2/gpt/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.9217, 'precision': 0.8027, 'recall': 0.8909, 'f1_score': 0.8445}
Logistic Regression: {'accuracy': 0.9169, 'precision': 0.7778, 'recall': 0.9124, 'f1_score': 0.8397}
Random Forest: {'accuracy': 0.8851, 'precision': 0.7361, 'recall': 0.8083, 'f1_score': 0.7706}
Naive Bayes: {'accuracy': 0.8612, 'precision': 0.6897, 'recall': 0.7607, 'f1_score': 0.7234}
SVM: {'accuracy': 0.8193, 'precision': 0.5765, 'recall': 0.9138, 'f1_score': 0.707}
Decision Tree: {'accuracy': 0.8209, 'precision': 0.5959, 'recall': 0.7748, 'f1_score': 0.6737}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
MLP_10963969: {'accuracy': 0.9475606936416185, 'precision': 0.8661331393233903, 'recall': 0.9228682170542636, 'f1_score': 0.8965247950019524, 'f1_score_avg': 0.8946270503919448}
MLP_2609409: {'accuracy': 0.9493179190751445, 'precision': 0.8910700538876059, 'recall': 0.8972868217054264, 'f1_score': 0.8941676322904596, 'f1_score_avg': 0.8847086945578537}
MLP_5304833: {'accuracy': 0.9463121387283236, 'precision': 0.8718616328807886, 'recall': 0.9085271317829458, 'f1_score': 0.8941676322904596, 'f1_score_avg': 0.8870755010378568}
MLP_1293441: {'accuracy': 0.9383583815028902, 'precision': 0.8387325190299167, 'recall': 0.9182170542635659, 'f1_score': 0.8797450557690505, 'f1_score_avg': 0.8762063600513766}
MLP_643649: {'accuracy': 0.9317456647398844, 'precision': 0.8245243128964059, 'recall': 0.9069767441860465, 'f1_score': 0.8781456953642384, 'f1_score_avg': 0.8719794577170896}
MLP_320801: {'accuracy': 0.934335260115607, 'precision': 0.8305054789678331, 'recall': 0.9106589147286822, 'f1_score': 0.8781299085281346, 'f1_score_avg': 0.8722497857576339}
XGBoost: {'accuracy': 0.9366, 'precision': 0.8272, 'recall': 0.9279, 'f1_score': 0.8747}
Logistic Regression: {'accuracy': 0.9353, 'precision': 0.8251, 'recall': 0.9246, 'f1_score': 0.8721}
Naive Bayes: {'accuracy': 0.9163, 'precision': 0.8097, 'recall': 0.8486, 'f1_score': 0.8287}
Decision Tree: {'accuracy': 0.8997, 'precision': 0.7507, 'recall': 0.8676, 'f1_score': 0.8049}
Random Forest: {'accuracy': 0.8846, 'precision': 0.7423, 'recall': 0.7911, 'f1_score': 0.7659}
SVM: {'accuracy': 0.7564, 'precision': 0.4944, 'recall': 0.9291, 'f1_score': 0.6454}


EMBEDDINGS TYPE: LYRICS_BERT
XGBoost: {'accuracy': 0.8966, 'precision': 0.7468, 'recall': 0.857, 'f1_score': 0.7981}
MLP_1338369: {'accuracy': 0.9038150289017342, 'precision': 0.8257191201353637, 'recall': 0.7565891472868217, 'f1_score': 0.7896440129449838, 'f1_score_avg': 0.7862623404516093}
MLP_203009: {'accuracy': 0.8978034682080925, 'precision': 0.7908123028391167, 'recall': 0.7773255813953488, 'f1_score': 0.7887766113999803, 'f1_score_avg': 0.7851798785005368}
MLP_492033: {'accuracy': 0.9003468208092485, 'precision': 0.8099855580771611, 'recall': 0.7608527131782946, 'f1_score': 0.7887766113999803, 'f1_score_avg': 0.7843371424685847}
MLP_42049: {'accuracy': 0.9001156069364162, 'precision': 0.7982107355864811, 'recall': 0.7781007751937985, 'f1_score': 0.788027477919529, 'f1_score_avg': 0.7840162218708825}
MLP_90241: {'accuracy': 0.8999306358381503, 'precision': 0.8044715447154471, 'recall': 0.7670542635658915, 'f1_score': 0.788027477919529, 'f1_score_avg': 0.7849722229106672}
MLP_20001: {'accuracy': 0.8983583815028902, 'precision': 0.7900509204857031, 'recall': 0.7817829457364341, 'f1_score': 0.7869206598586017, 'f1_score_avg': 0.7832981484101731}
Logistic Regression: {'accuracy': 0.8731, 'precision': 0.6902, 'recall': 0.8496, 'f1_score': 0.7616}
Random Forest: {'accuracy': 0.8695, 'precision': 0.7004, 'recall': 0.7921, 'f1_score': 0.7434}
Decision Tree: {'accuracy': 0.8082, 'precision': 0.5718, 'recall': 0.7814, 'f1_score': 0.6604}
Naive Bayes: {'accuracy': 0.7207, 'precision': 0.4518, 'recall': 0.799, 'f1_score': 0.5772}
SVM: {'accuracy': 0.5012, 'precision': 0.3165, 'recall': 0.9403, 'f1_score': 0.4736}


EMBEDDINGS TYPE: GPT
MLP_200257: {'accuracy': 0.9308208092485549, 'precision': 0.8589341692789969, 'recall': 0.8496124031007752, 'f1_score': 0.8565660449913478, 'f1_score_avg': 0.855014499947748}
MLP_406657: {'accuracy': 0.9278150289017341, 'precision': 0.8344173945363316, 'recall': 0.8701550387596899, 'f1_score': 0.8565660449913478, 'f1_score_avg': 0.8544994908831447}
MLP_835841: {'accuracy': 0.9302196531791908, 'precision': 0.8432036097010717, 'recall': 0.8691860465116279, 'f1_score': 0.8565660449913478, 'f1_score_avg': 0.855683292464132}
MLP_1757697: {'accuracy': 0.9273988439306359, 'precision': 0.8294787077826725, 'recall': 0.8757751937984496, 'f1_score': 0.8565660449913478, 'f1_score_avg': 0.854094767345883}
MLP_3869697: {'accuracy': 0.9286473988439307, 'precision': 0.8409048067860508, 'recall': 0.8645348837209302, 'f1_score': 0.8565660449913478, 'f1_score_avg': 0.8528600803355969}
MLP_99105: {'accuracy': 0.9331791907514451, 'precision': 0.8807132609141217, 'recall': 0.8327519379844961, 'f1_score': 0.8560613606932962, 'f1_score_avg': 0.8536642395342469}
XGBoost: {'accuracy': 0.9217, 'precision': 0.8027, 'recall': 0.8909, 'f1_score': 0.8445}
Logistic Regression: {'accuracy': 0.9169, 'precision': 0.7778, 'recall': 0.9124, 'f1_score': 0.8397}
Random Forest: {'accuracy': 0.8851, 'precision': 0.7361, 'recall': 0.8083, 'f1_score': 0.7706}
Naive Bayes: {'accuracy': 0.8612, 'precision': 0.6897, 'recall': 0.7607, 'f1_score': 0.7234}
SVM: {'accuracy': 0.8193, 'precision': 0.5765, 'recall': 0.9138, 'f1_score': 0.707}
Decision Tree: {'accuracy': 0.8209, 'precision': 0.5959, 'recall': 0.7748, 'f1_score': 0.6737}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: True
REMOVE_STOPWORDS: True
NUMERIC_COLS: True
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: ['Tempo', 'Length', 'Loudness (db)', 'Popularity', 'Energy', 'Danceability', 'Positiveness', 'Speechiness', 'Liveness', 'Acousticness', 'Instrumentalness', 'Good for Party', 'Good for Work/Study', 'Good for Relaxation/Meditation', 'Good for Exercise', 'Good for Running', 'Good for Yoga/Stretching', 'Good for Driving', 'Good for Social Gatherings', 'Good for Morning Routine', 'Release_Year', 'Release_Month', 'Release_Day']
====================================

