2025-09-27 01:38:59.325305: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/no_preprocessed_data/main_only_text.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))
For TF-IDF embbedings you are selecteing this columns:
--> ['text']
Your are not adding cols numercis
You are executing with [ALL] dataset
--> PaTH:  ../../data/new_embbedings_khipu/LB_fuss/lb_khipu_T_NP.npy

##################################################
Running experiment with TFIDF embeddings
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 5000)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 5000), y: (108138,)
Shape filtrado  X: (108125, 5000), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 5000)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 5000)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [5000, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4903, Test Loss: 0.4457, F1: 0.6910, AUC: 0.8765
Epoch [10/30] Train Loss: 0.2964, Test Loss: 0.4775, F1: 0.6729, AUC: 0.8768
Epoch [20/30] Train Loss: 0.2304, Test Loss: 0.5511, F1: 0.6626, AUC: 0.8688
Mejores resultados en la época:  1
f1-score 0.691364376983106
AUC según el mejor F1-score 0.8812981200432206
Confusion Matrix:
 [[13995  2470]
 [ 1129  4031]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_160065.png
Accuracy:   0.8336
Precision:  0.6201
Recall:     0.7812
F1-score:   0.6914

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4916, Test Loss: 0.4455, F1: 0.6867, AUC: 0.8764
Epoch [10/30] Train Loss: 0.2961, Test Loss: 0.4557, F1: 0.6787, AUC: 0.8768
Epoch [20/30] Train Loss: 0.2302, Test Loss: 0.5301, F1: 0.6650, AUC: 0.8688
Mejores resultados en la época:  2
f1-score 0.6927715960248025
AUC según el mejor F1-score 0.8830369093943696
Confusion Matrix:
 [[13930  2535]
 [ 1082  4078]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_160065.png
Accuracy:   0.8327
Precision:  0.6167
Recall:     0.7903
F1-score:   0.6928

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4926, Test Loss: 0.4434, F1: 0.6911, AUC: 0.8762
Epoch [10/30] Train Loss: 0.2725, Test Loss: 0.4908, F1: 0.6708, AUC: 0.8765
Epoch [20/30] Train Loss: 0.1871, Test Loss: 0.5629, F1: 0.6600, AUC: 0.8662
Mejores resultados en la época:  1
f1-score 0.6930556750709922
AUC según el mejor F1-score 0.8814824963453132
Confusion Matrix:
 [[14031  2434]
 [ 1133  4027]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_160065.png
Accuracy:   0.8351
Precision:  0.6233
Recall:     0.7804
F1-score:   0.6931

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4894, Test Loss: 0.4552, F1: 0.6844, AUC: 0.8767
Epoch [10/30] Train Loss: 0.2966, Test Loss: 0.4561, F1: 0.6768, AUC: 0.8768
Epoch [20/30] Train Loss: 0.2340, Test Loss: 0.5478, F1: 0.6600, AUC: 0.8686
Mejores resultados en la época:  2
f1-score 0.6924189568620778
AUC según el mejor F1-score 0.8829800645955598
Confusion Matrix:
 [[13941  2524]
 [ 1091  4069]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_160065.png
Accuracy:   0.8328
Precision:  0.6172
Recall:     0.7886
F1-score:   0.6924
Tiempo total para red 1: 379.89 segundos

Entrenando red 2 con capas [5000, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4690, Test Loss: 0.4049, F1: 0.6977, AUC: 0.8813
Epoch [10/30] Train Loss: 0.1046, Test Loss: 0.8460, F1: 0.6556, AUC: 0.8596
Epoch [20/30] Train Loss: 0.0098, Test Loss: 1.8298, F1: 0.6348, AUC: 0.8489
Mejores resultados en la época:  0
f1-score 0.6976744186046512
AUC según el mejor F1-score 0.8812855140219916
Confusion Matrix:
 [[14205  2260]
 [ 1185  3975]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_322177.png
Accuracy:   0.8407
Precision:  0.6375
Recall:     0.7703
F1-score:   0.6977

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4669, Test Loss: 0.4555, F1: 0.6790, AUC: 0.8813
Epoch [10/30] Train Loss: 0.0890, Test Loss: 0.8271, F1: 0.6476, AUC: 0.8532
Epoch [20/30] Train Loss: 0.0128, Test Loss: 1.6714, F1: 0.6405, AUC: 0.8489
Mejores resultados en la época:  1
f1-score 0.6942524860947245
AUC según el mejor F1-score 0.8842659670383737
Confusion Matrix:
 [[13878  2587]
 [ 1041  4119]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_322177.png
Accuracy:   0.8322
Precision:  0.6142
Recall:     0.7983
F1-score:   0.6943

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4672, Test Loss: 0.4241, F1: 0.6903, AUC: 0.8807
Epoch [10/30] Train Loss: 0.1047, Test Loss: 0.8523, F1: 0.6421, AUC: 0.8524
Epoch [20/30] Train Loss: 0.0220, Test Loss: 1.7138, F1: 0.6382, AUC: 0.8523
Mejores resultados en la época:  0
f1-score 0.6903013421119271
AUC según el mejor F1-score 0.8807405949194556
Confusion Matrix:
 [[13867  2598]
 [ 1071  4089]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_322177.png
Accuracy:   0.8303
Precision:  0.6115
Recall:     0.7924
F1-score:   0.6903

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4700, Test Loss: 0.4041, F1: 0.6948, AUC: 0.8798
Epoch [10/30] Train Loss: 0.0811, Test Loss: 0.9889, F1: 0.6422, AUC: 0.8480
Epoch [20/30] Train Loss: 0.0123, Test Loss: 1.8738, F1: 0.6339, AUC: 0.8489
Mejores resultados en la época:  0
f1-score 0.694781233526621
AUC según el mejor F1-score 0.8798464148758114
Confusion Matrix:
 [[14197  2268]
 [ 1206  3954]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_322177.png
Accuracy:   0.8394
Precision:  0.6355
Recall:     0.7663
F1-score:   0.6948
Tiempo total para red 2: 391.38 segundos

Entrenando red 3 con capas [5000, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4620, Test Loss: 0.4333, F1: 0.6843, AUC: 0.8824
Epoch [10/30] Train Loss: 0.0194, Test Loss: 1.7232, F1: 0.6435, AUC: 0.8553
Epoch [20/30] Train Loss: 0.0073, Test Loss: 1.9150, F1: 0.6468, AUC: 0.8562
Mejores resultados en la época:  1
f1-score 0.6924810147709255
AUC según el mejor F1-score 0.8858287841015826
Confusion Matrix:
 [[13791  2674]
 [ 1011  4149]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_650497.png
Accuracy:   0.8296
Precision:  0.6081
Recall:     0.8041
F1-score:   0.6925

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4612, Test Loss: 0.4483, F1: 0.6782, AUC: 0.8819
Epoch [10/30] Train Loss: 0.0248, Test Loss: 1.5053, F1: 0.6343, AUC: 0.8501
Epoch [20/30] Train Loss: 0.0090, Test Loss: 1.9483, F1: 0.6366, AUC: 0.8469
Mejores resultados en la época:  1
f1-score 0.6924738328626018
AUC según el mejor F1-score 0.8857838273339972
Confusion Matrix:
 [[13755  2710]
 [  992  4168]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_650497.png
Accuracy:   0.8288
Precision:  0.6060
Recall:     0.8078
F1-score:   0.6925

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4622, Test Loss: 0.4566, F1: 0.6724, AUC: 0.8807
Epoch [10/30] Train Loss: 0.0173, Test Loss: 1.5971, F1: 0.6325, AUC: 0.8507
Epoch [20/30] Train Loss: 0.0074, Test Loss: 1.8007, F1: 0.6382, AUC: 0.8528
Mejores resultados en la época:  2
f1-score 0.7020436803789142
AUC según el mejor F1-score 0.8824932026356118
Confusion Matrix:
 [[14226  2239]
 [ 1158  4002]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_650497.png
Accuracy:   0.8429
Precision:  0.6412
Recall:     0.7756
F1-score:   0.7020

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4600, Test Loss: 0.4076, F1: 0.6928, AUC: 0.8824
Epoch [10/30] Train Loss: 0.0175, Test Loss: 1.4127, F1: 0.6524, AUC: 0.8568
Epoch [20/30] Train Loss: 0.0065, Test Loss: 1.6466, F1: 0.6445, AUC: 0.8546
Mejores resultados en la época:  0
f1-score 0.6927556088371296
AUC según el mejor F1-score 0.8824123875639422
Confusion Matrix:
 [[13992  2473]
 [ 1115  4045]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_650497.png
Accuracy:   0.8341
Precision:  0.6206
Recall:     0.7839
F1-score:   0.6928
Tiempo total para red 3: 403.94 segundos

Entrenando red 4 con capas [5000, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4594, Test Loss: 0.4006, F1: 0.6964, AUC: 0.8824
Epoch [10/30] Train Loss: 0.0125, Test Loss: 1.4211, F1: 0.6390, AUC: 0.8528
Epoch [20/30] Train Loss: 0.0056, Test Loss: 1.7604, F1: 0.6432, AUC: 0.8563
Mejores resultados en la época:  1
f1-score 0.6992706992706993
AUC según el mejor F1-score 0.8837380796003738
Confusion Matrix:
 [[14045  2420]
 [ 1085  4075]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_1323521.png
Accuracy:   0.8379
Precision:  0.6274
Recall:     0.7897
F1-score:   0.6993

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4582, Test Loss: 0.5027, F1: 0.6476, AUC: 0.8815
Epoch [10/30] Train Loss: 0.0127, Test Loss: 1.4245, F1: 0.6209, AUC: 0.8450
Epoch [20/30] Train Loss: 0.0090, Test Loss: 1.7270, F1: 0.6299, AUC: 0.8553
Mejores resultados en la época:  1
f1-score 0.6892933267748811
AUC según el mejor F1-score 0.8844680106027115
Confusion Matrix:
 [[13631  2834]
 [  956  4204]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_1323521.png
Accuracy:   0.8247
Precision:  0.5973
Recall:     0.8147
F1-score:   0.6893

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4600, Test Loss: 0.4294, F1: 0.6870, AUC: 0.8821
Epoch [10/30] Train Loss: 0.0098, Test Loss: 1.6364, F1: 0.6430, AUC: 0.8568
Epoch [20/30] Train Loss: 0.0045, Test Loss: 1.9072, F1: 0.6371, AUC: 0.8547
Mejores resultados en la época:  1
f1-score 0.6904114106686454
AUC según el mejor F1-score 0.8828216418665857
Confusion Matrix:
 [[13683  2782]
 [  973  4187]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_1323521.png
Accuracy:   0.8264
Precision:  0.6008
Recall:     0.8114
F1-score:   0.6904

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4577, Test Loss: 0.4650, F1: 0.6724, AUC: 0.8825
Epoch [10/30] Train Loss: 0.0127, Test Loss: 1.3759, F1: 0.6347, AUC: 0.8499
Epoch [20/30] Train Loss: 0.0052, Test Loss: 1.8331, F1: 0.6354, AUC: 0.8545
Mejores resultados en la época:  1
f1-score 0.6956741289345229
AUC según el mejor F1-score 0.8843345586244724
Confusion Matrix:
 [[13876  2589]
 [ 1027  4133]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_1323521.png
Accuracy:   0.8328
Precision:  0.6148
Recall:     0.8010
F1-score:   0.6957
Tiempo total para red 4: 427.62 segundos

Entrenando red 5 con capas [5000, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4587, Test Loss: 0.4224, F1: 0.6880, AUC: 0.8822
Epoch [10/30] Train Loss: 0.0111, Test Loss: 1.5556, F1: 0.6351, AUC: 0.8555
Epoch [20/30] Train Loss: 0.0075, Test Loss: 1.8107, F1: 0.6180, AUC: 0.8464
Mejores resultados en la época:  0
f1-score 0.6880161806843081
AUC según el mejor F1-score 0.8822239975800207
Confusion Matrix:
 [[13841  2624]
 [ 1078  4082]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_2733057.png
Accuracy:   0.8288
Precision:  0.6087
Recall:     0.7911
F1-score:   0.6880

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4587, Test Loss: 0.4318, F1: 0.6832, AUC: 0.8812
Epoch [10/30] Train Loss: 0.0096, Test Loss: 1.6165, F1: 0.6513, AUC: 0.8630
Epoch [20/30] Train Loss: 0.0056, Test Loss: 1.7765, F1: 0.6463, AUC: 0.8568
Mejores resultados en la época:  1
f1-score 0.6952992898207643
AUC según el mejor F1-score 0.885521237202711
Confusion Matrix:
 [[13909  2556]
 [ 1048  4112]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_2733057.png
Accuracy:   0.8333
Precision:  0.6167
Recall:     0.7969
F1-score:   0.6953

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4563, Test Loss: 0.4047, F1: 0.6948, AUC: 0.8821
Epoch [10/30] Train Loss: 0.0094, Test Loss: 1.3111, F1: 0.6468, AUC: 0.8553
Epoch [20/30] Train Loss: 0.0055, Test Loss: 1.7521, F1: 0.6239, AUC: 0.8466
Mejores resultados en la época:  0
f1-score 0.6947656986207488
AUC según el mejor F1-score 0.8821257094565168
Confusion Matrix:
 [[14007  2458]
 [ 1105  4055]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_2733057.png
Accuracy:   0.8352
Precision:  0.6226
Recall:     0.7859
F1-score:   0.6948

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4579, Test Loss: 0.4259, F1: 0.6935, AUC: 0.8831
Epoch [10/30] Train Loss: 0.0113, Test Loss: 1.7937, F1: 0.6314, AUC: 0.8608
Epoch [20/30] Train Loss: 0.0051, Test Loss: 1.7841, F1: 0.6484, AUC: 0.8588
Mejores resultados en la época:  0
f1-score 0.6935096153846154
AUC según el mejor F1-score 0.8830702547334374
Confusion Matrix:
 [[14016  2449]
 [ 1121  4039]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_2733057.png
Accuracy:   0.8349
Precision:  0.6225
Recall:     0.7828
F1-score:   0.6935
Tiempo total para red 5: 433.61 segundos

Entrenando red 6 con capas [5000, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4586, Test Loss: 0.4159, F1: 0.6886, AUC: 0.8826
Epoch [10/30] Train Loss: 0.0068, Test Loss: 1.7962, F1: 0.6476, AUC: 0.8578
Epoch [20/30] Train Loss: 0.0082, Test Loss: 1.9018, F1: 0.6384, AUC: 0.8578
Mejores resultados en la época:  2
f1-score 0.6953436039466852
AUC según el mejor F1-score 0.8763883278365902
Confusion Matrix:
 [[14088  2377]
 [ 1143  4017]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_5820417.png
Accuracy:   0.8372
Precision:  0.6282
Recall:     0.7785
F1-score:   0.6953

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4659, Test Loss: 0.4230, F1: 0.6835, AUC: 0.8812
Epoch [10/30] Train Loss: 0.0105, Test Loss: 2.0366, F1: 0.6243, AUC: 0.8623
Epoch [20/30] Train Loss: 0.0039, Test Loss: 2.3231, F1: 0.6538, AUC: 0.8649
Mejores resultados en la época:  0
f1-score 0.6834922722788557
AUC según el mejor F1-score 0.8811587417048613
Confusion Matrix:
 [[13618  2847]
 [ 1003  4157]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_5820417.png
Accuracy:   0.8220
Precision:  0.5935
Recall:     0.8056
F1-score:   0.6835

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4645, Test Loss: 0.4233, F1: 0.6730, AUC: 0.8806
Epoch [10/30] Train Loss: 0.0130, Test Loss: 1.4393, F1: 0.6352, AUC: 0.8506
Epoch [20/30] Train Loss: 0.0048, Test Loss: 2.4945, F1: 0.6442, AUC: 0.8585
Mejores resultados en la época:  2
f1-score 0.6859717933739464
AUC según el mejor F1-score 0.8765596096488442
Confusion Matrix:
 [[13752  2713]
 [ 1050  4110]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_5820417.png
Accuracy:   0.8260
Precision:  0.6024
Recall:     0.7965
F1-score:   0.6860

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4602, Test Loss: 0.4297, F1: 0.6973, AUC: 0.8833
Epoch [10/30] Train Loss: 0.0086, Test Loss: 2.0527, F1: 0.6287, AUC: 0.8477
Epoch [20/30] Train Loss: 0.0068, Test Loss: 1.7624, F1: 0.6199, AUC: 0.8535
Mejores resultados en la época:  1
f1-score 0.7037003363942177
AUC según el mejor F1-score 0.8839443604827717
Confusion Matrix:
 [[14496  1969]
 [ 1290  3870]]
Matriz de confusión guardada en: outputs_only_text/0/tfidf/confusion_matrix_param_5820417.png
Accuracy:   0.8493
Precision:  0.6628
Recall:     0.7500
F1-score:   0.7037
Tiempo total para red 6: 478.29 segundos
Saved on: outputs_only_text/0/tfidf

==============================
Model: Logistic Regression
Accuracy:  0.8198
Precision: 0.5973
Recall:    0.7514
F1-score:  0.6655
              precision    recall  f1-score   support

           0       0.92      0.84      0.88     16465
           1       0.60      0.75      0.67      5160

    accuracy                           0.82     21625
   macro avg       0.76      0.80      0.77     21625
weighted avg       0.84      0.82      0.83     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [02:28:46] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/no_preprocessed_data/main_only_text.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[13851  2614]
 [ 1283  3877]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text/0/tfidf/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.6250
Precision: 0.3522
Recall:    0.6812
F1-score:  0.4643
              precision    recall  f1-score   support

           0       0.86      0.61      0.71     16465
           1       0.35      0.68      0.46      5160

    accuracy                           0.62     21625
   macro avg       0.61      0.64      0.59     21625
weighted avg       0.74      0.62      0.65     21625

[[10000  6465]
 [ 1645  3515]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_svm.png
Modelo guardado como: outputs_only_text/0/tfidf/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.8617
Precision: 0.6782
Recall:    0.8004
F1-score:  0.7342
              precision    recall  f1-score   support

           0       0.93      0.88      0.91     16465
           1       0.68      0.80      0.73      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.84      0.82     21625
weighted avg       0.87      0.86      0.87     21625

[[14505  1960]
 [ 1030  4130]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text/0/tfidf/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8535
Precision: 0.6740
Recall:    0.7473
F1-score:  0.7088
              precision    recall  f1-score   support

           0       0.92      0.89      0.90     16465
           1       0.67      0.75      0.71      5160

    accuracy                           0.85     21625
   macro avg       0.80      0.82      0.81     21625
weighted avg       0.86      0.85      0.86     21625

[[14600  1865]
 [ 1304  3856]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text/0/tfidf/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8650
Precision: 0.6762
Recall:    0.8329
F1-score:  0.7464
              precision    recall  f1-score   support

           0       0.94      0.88      0.91     16465
           1       0.68      0.83      0.75      5160

    accuracy                           0.86     21625
   macro avg       0.81      0.85      0.83     21625
weighted avg       0.88      0.86      0.87     21625

[[14407  2058]
 [  862  4298]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text/0/tfidf/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8204
Precision: 0.6063
Recall:    0.7052
F1-score:  0.6520
              precision    recall  f1-score   support

           0       0.90      0.86      0.88     16465
           1       0.61      0.71      0.65      5160

    accuracy                           0.82     21625
   macro avg       0.75      0.78      0.77     21625
weighted avg       0.83      0.82      0.82     21625

[[14102  2363]
 [ 1521  3639]]
Hiperparámetros: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': True}
Confusion matrix saved as: outputs_only_text/0/tfidf/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text/0/tfidf/naive_bayes_model.pkl


Resumen de métricas:
XGBoost: {'accuracy': 0.865, 'precision': 0.6762, 'recall': 0.8329, 'f1_score': 0.7464}
Decision Tree: {'accuracy': 0.8617, 'precision': 0.6782, 'recall': 0.8004, 'f1_score': 0.7342}
Random Forest: {'accuracy': 0.8535, 'precision': 0.674, 'recall': 0.7473, 'f1_score': 0.7088}
Logistic Regression: {'accuracy': 0.8198, 'precision': 0.5973, 'recall': 0.7514, 'f1_score': 0.6655}
Naive Bayes: {'accuracy': 0.8204, 'precision': 0.6063, 'recall': 0.7052, 'f1_score': 0.652}
SVM: {'accuracy': 0.625, 'precision': 0.3522, 'recall': 0.6812, 'f1_score': 0.4643}

##################################################
Running experiment with LYRICS_BERT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Label distribution: {0: 82336, 1: 25802}
X shape: (108138, 300)
y shape: (108138,)
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original X: (108138, 300), y: (108138,)
Shape filtrado  X: (108125, 300), y: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 300)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 300)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [300, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5436, Test Loss: 0.4654, F1: 0.6279, AUC: 0.8405
Epoch [10/30] Train Loss: 0.4466, Test Loss: 0.4297, F1: 0.6633, AUC: 0.8621
Epoch [20/30] Train Loss: 0.4233, Test Loss: 0.4875, F1: 0.6365, AUC: 0.8599
Mejores resultados en la época:  18
f1-score 0.6646282868873941
AUC según el mejor F1-score 0.8616404776869894
Confusion Matrix:
 [[13982  2483]
 [ 1356  3804]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8225
Precision:  0.6051
Recall:     0.7372
F1-score:   0.6646

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5496, Test Loss: 0.4968, F1: 0.6116, AUC: 0.8377
Epoch [10/30] Train Loss: 0.4458, Test Loss: 0.4851, F1: 0.6408, AUC: 0.8619
Epoch [20/30] Train Loss: 0.4227, Test Loss: 0.4926, F1: 0.6361, AUC: 0.8616
Mejores resultados en la época:  19
f1-score 0.6627350427350427
AUC según el mejor F1-score 0.8628099245051164
Confusion Matrix:
 [[13802  2663]
 [ 1283  3877]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8175
Precision:  0.5928
Recall:     0.7514
F1-score:   0.6627

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5373, Test Loss: 0.5285, F1: 0.6027, AUC: 0.8402
Epoch [10/30] Train Loss: 0.4429, Test Loss: 0.4954, F1: 0.6332, AUC: 0.8628
Epoch [20/30] Train Loss: 0.4189, Test Loss: 0.4412, F1: 0.6567, AUC: 0.8631
Mejores resultados en la época:  15
f1-score 0.669817563096701
AUC según el mejor F1-score 0.8620239078901216
Confusion Matrix:
 [[14589  1876]
 [ 1617  3543]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8385
Precision:  0.6538
Recall:     0.6866
F1-score:   0.6698

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.5396, Test Loss: 0.5195, F1: 0.6036, AUC: 0.8405
Epoch [10/30] Train Loss: 0.4454, Test Loss: 0.4325, F1: 0.6646, AUC: 0.8625
Epoch [20/30] Train Loss: 0.4240, Test Loss: 0.4838, F1: 0.6384, AUC: 0.8618
Mejores resultados en la época:  10
f1-score 0.6645569620253164
AUC según el mejor F1-score 0.8624841630237503
Confusion Matrix:
 [[13818  2647]
 [ 1275  3885]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_9665.png
Accuracy:   0.8186
Precision:  0.5948
Recall:     0.7529
F1-score:   0.6646
Tiempo total para red 1: 159.69 segundos

Entrenando red 2 con capas [300, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.5227, Test Loss: 0.4245, F1: 0.6569, AUC: 0.8515
Epoch [10/30] Train Loss: 0.4161, Test Loss: 0.4885, F1: 0.6386, AUC: 0.8639
Epoch [20/30] Train Loss: 0.3644, Test Loss: 0.4830, F1: 0.6357, AUC: 0.8502
Mejores resultados en la época:  7
f1-score 0.6664299032229424
AUC según el mejor F1-score 0.8628569410800925
Confusion Matrix:
 [[14115  2350]
 [ 1407  3753]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8263
Precision:  0.6149
Recall:     0.7273
F1-score:   0.6664

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.5214, Test Loss: 0.4457, F1: 0.6474, AUC: 0.8511
Epoch [10/30] Train Loss: 0.4111, Test Loss: 0.4629, F1: 0.6481, AUC: 0.8623
Epoch [20/30] Train Loss: 0.3588, Test Loss: 0.5027, F1: 0.6315, AUC: 0.8533
Mejores resultados en la época:  13
f1-score 0.6617163108598431
AUC según el mejor F1-score 0.8619233539784885
Confusion Matrix:
 [[13695  2770]
 [ 1239  3921]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8146
Precision:  0.5860
Recall:     0.7599
F1-score:   0.6617

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.5188, Test Loss: 0.4384, F1: 0.6490, AUC: 0.8512
Epoch [10/30] Train Loss: 0.4108, Test Loss: 0.4703, F1: 0.6444, AUC: 0.8610
Epoch [20/30] Train Loss: 0.3598, Test Loss: 0.5087, F1: 0.6294, AUC: 0.8509
Mejores resultados en la época:  7
f1-score 0.6683636363636364
AUC según el mejor F1-score 0.860674528068701
Confusion Matrix:
 [[14301  2164]
 [ 1484  3676]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8313
Precision:  0.6295
Recall:     0.7124
F1-score:   0.6684

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.5171, Test Loss: 0.4592, F1: 0.6412, AUC: 0.8516
Epoch [10/30] Train Loss: 0.4113, Test Loss: 0.4872, F1: 0.6422, AUC: 0.8615
Epoch [20/30] Train Loss: 0.3579, Test Loss: 0.5281, F1: 0.6218, AUC: 0.8524
Mejores resultados en la época:  9
f1-score 0.6656221198156682
AUC según el mejor F1-score 0.8581173066193971
Confusion Matrix:
 [[14386  2079]
 [ 1549  3611]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_21377.png
Accuracy:   0.8322
Precision:  0.6346
Recall:     0.6998
F1-score:   0.6656
Tiempo total para red 2: 174.29 segundos

Entrenando red 3 con capas [300, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.5125, Test Loss: 0.4015, F1: 0.6524, AUC: 0.8536
Epoch [10/30] Train Loss: 0.3809, Test Loss: 0.4809, F1: 0.6420, AUC: 0.8499
Epoch [20/30] Train Loss: 0.2790, Test Loss: 0.6243, F1: 0.6166, AUC: 0.8232
Mejores resultados en la época:  3
f1-score 0.6657456019158147
AUC según el mejor F1-score 0.8603602544274088
Confusion Matrix:
 [[14382  2083]
 [ 1546  3614]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8322
Precision:  0.6344
Recall:     0.7004
F1-score:   0.6657

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.5087, Test Loss: 0.4248, F1: 0.6559, AUC: 0.8537
Epoch [10/30] Train Loss: 0.3807, Test Loss: 0.4817, F1: 0.6411, AUC: 0.8517
Epoch [20/30] Train Loss: 0.2773, Test Loss: 0.6123, F1: 0.6163, AUC: 0.8238
Mejores resultados en la época:  5
f1-score 0.6706809528026211
AUC según el mejor F1-score 0.8639089435659856
Confusion Matrix:
 [[14119  2346]
 [ 1373  3787]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8280
Precision:  0.6175
Recall:     0.7339
F1-score:   0.6707

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.5087, Test Loss: 0.4192, F1: 0.6603, AUC: 0.8549
Epoch [10/30] Train Loss: 0.3851, Test Loss: 0.4815, F1: 0.6374, AUC: 0.8557
Epoch [20/30] Train Loss: 0.2751, Test Loss: 0.6847, F1: 0.5960, AUC: 0.8175
Mejores resultados en la época:  6
f1-score 0.668680417709502
AUC según el mejor F1-score 0.8634455810657796
Confusion Matrix:
 [[13912  2553]
 [ 1286  3874]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8225
Precision:  0.6028
Recall:     0.7508
F1-score:   0.6687

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.5122, Test Loss: 0.4386, F1: 0.6531, AUC: 0.8529
Epoch [10/30] Train Loss: 0.3700, Test Loss: 0.5187, F1: 0.6292, AUC: 0.8489
Epoch [20/30] Train Loss: 0.2708, Test Loss: 0.6701, F1: 0.6182, AUC: 0.8269
Mejores resultados en la época:  2
f1-score 0.6678638771662028
AUC según el mejor F1-score 0.8627796394513143
Confusion Matrix:
 [[14207  2258]
 [ 1441  3719]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_48897.png
Accuracy:   0.8289
Precision:  0.6222
Recall:     0.7207
F1-score:   0.6679
Tiempo total para red 3: 189.05 segundos

Entrenando red 4 con capas [300, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.5051, Test Loss: 0.4628, F1: 0.6527, AUC: 0.8557
Epoch [10/30] Train Loss: 0.3539, Test Loss: 0.5159, F1: 0.6381, AUC: 0.8449
Epoch [20/30] Train Loss: 0.2310, Test Loss: 0.7735, F1: 0.6159, AUC: 0.8182
Mejores resultados en la época:  2
f1-score 0.6658163265306123
AUC según el mejor F1-score 0.8624424842924974
Confusion Matrix:
 [[14303  2162]
 [ 1506  3654]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8304
Precision:  0.6283
Recall:     0.7081
F1-score:   0.6658

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.5079, Test Loss: 0.4211, F1: 0.6585, AUC: 0.8524
Epoch [10/30] Train Loss: 0.3476, Test Loss: 0.5603, F1: 0.6271, AUC: 0.8409
Epoch [20/30] Train Loss: 0.2324, Test Loss: 0.7379, F1: 0.6279, AUC: 0.8242
Mejores resultados en la época:  2
f1-score 0.668450678231983
AUC según el mejor F1-score 0.8609217638071832
Confusion Matrix:
 [[14410  2055]
 [ 1538  3622]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8338
Precision:  0.6380
Recall:     0.7019
F1-score:   0.6685

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.5067, Test Loss: 0.4635, F1: 0.6489, AUC: 0.8546
Epoch [10/30] Train Loss: 0.3536, Test Loss: 0.6373, F1: 0.5905, AUC: 0.8442
Epoch [20/30] Train Loss: 0.2354, Test Loss: 0.8266, F1: 0.6085, AUC: 0.8170
Mejores resultados en la época:  2
f1-score 0.6692592241297544
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
AUC según el mejor F1-score 0.8615983340277827
Confusion Matrix:
 [[14610  1855]
 [ 1632  3528]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8388
Precision:  0.6554
Recall:     0.6837
F1-score:   0.6693

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.5044, Test Loss: 0.4772, F1: 0.6497, AUC: 0.8572
Epoch [10/30] Train Loss: 0.3456, Test Loss: 0.5110, F1: 0.6329, AUC: 0.8380
Epoch [20/30] Train Loss: 0.2213, Test Loss: 0.9010, F1: 0.5888, AUC: 0.8082
Mejores resultados en la época:  6
f1-score 0.6576837796349991
AUC según el mejor F1-score 0.8581519584648668
Confusion Matrix:
 [[13755  2710]
 [ 1304  3856]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_120321.png
Accuracy:   0.8144
Precision:  0.5873
Recall:     0.7473
F1-score:   0.6577
Tiempo total para red 4: 206.67 segundos

Entrenando red 5 con capas [300, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4988, Test Loss: 0.4640, F1: 0.6599, AUC: 0.8560
Epoch [10/30] Train Loss: 0.3198, Test Loss: 0.5822, F1: 0.6327, AUC: 0.8358
Epoch [20/30] Train Loss: 0.1818, Test Loss: 1.0132, F1: 0.5971, AUC: 0.8047
Mejores resultados en la época:  3
f1-score 0.667138059041895
AUC según el mejor F1-score 0.8606940432724336
Confusion Matrix:
 [[14085  2380]
 [ 1386  3774]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8258
Precision:  0.6133
Recall:     0.7314
F1-score:   0.6671

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.5042, Test Loss: 0.4622, F1: 0.6484, AUC: 0.8556
Epoch [10/30] Train Loss: 0.3140, Test Loss: 0.6304, F1: 0.6406, AUC: 0.8426
Epoch [20/30] Train Loss: 0.1851, Test Loss: 1.1229, F1: 0.5876, AUC: 0.8091
Mejores resultados en la época:  3
f1-score 0.6651262302096705
AUC según el mejor F1-score 0.8616856463204777
Confusion Matrix:
 [[13826  2639]
 [ 1274  3886]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8191
Precision:  0.5956
Recall:     0.7531
F1-score:   0.6651

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4967, Test Loss: 0.4403, F1: 0.6644, AUC: 0.8582
Epoch [10/30] Train Loss: 0.3171, Test Loss: 0.5710, F1: 0.6452, AUC: 0.8453
Epoch [20/30] Train Loss: 0.1812, Test Loss: 1.0018, F1: 0.6115, AUC: 0.8144
Mejores resultados en la época:  4
f1-score 0.6709525534226729
AUC según el mejor F1-score 0.861016002937874
Confusion Matrix:
 [[14286  2179]
 [ 1455  3705]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8320
Precision:  0.6297
Recall:     0.7180
F1-score:   0.6710

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4983, Test Loss: 0.4390, F1: 0.6543, AUC: 0.8561
Epoch [10/30] Train Loss: 0.3152, Test Loss: 0.6035, F1: 0.6337, AUC: 0.8379
Epoch [20/30] Train Loss: 0.1823, Test Loss: 0.9535, F1: 0.6132, AUC: 0.8166
Mejores resultados en la época:  3
f1-score 0.6702049780380673
AUC según el mejor F1-score 0.8606234507305843
Confusion Matrix:
 [[14359  2106]
 [ 1498  3662]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_326657.png
Accuracy:   0.8333
Precision:  0.6349
Recall:     0.7097
F1-score:   0.6702
Tiempo total para red 5: 210.93 segundos

Entrenando red 6 con capas [300, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.5084, Test Loss: 0.5442, F1: 0.6216, AUC: 0.8551
Epoch [10/30] Train Loss: 0.3028, Test Loss: 0.7303, F1: 0.6015, AUC: 0.8323
Epoch [20/30] Train Loss: 0.1731, Test Loss: 1.0863, F1: 0.5908, AUC: 0.8076
Mejores resultados en la época:  3
f1-score 0.6600127030215044
AUC según el mejor F1-score 0.8591836747905469
Confusion Matrix:
 [[14241  2224]
 [ 1523  3637]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8267
Precision:  0.6205
Recall:     0.7048
F1-score:   0.6600

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.5054, Test Loss: 0.4442, F1: 0.6551, AUC: 0.8561
Epoch [10/30] Train Loss: 0.3010, Test Loss: 0.5922, F1: 0.6419, AUC: 0.8391
Epoch [20/30] Train Loss: 0.1673, Test Loss: 1.3005, F1: 0.6072, AUC: 0.8183
Mejores resultados en la época:  5
f1-score 0.6595066897242196
AUC según el mejor F1-score 0.8566383119466475
Confusion Matrix:
 [[14261  2204]
 [ 1537  3623]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8270
Precision:  0.6218
Recall:     0.7021
F1-score:   0.6595

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.5023, Test Loss: 0.4588, F1: 0.6554, AUC: 0.8579
Epoch [10/30] Train Loss: 0.2959, Test Loss: 0.6521, F1: 0.6292, AUC: 0.8317
Epoch [20/30] Train Loss: 0.1660, Test Loss: 1.1466, F1: 0.6160, AUC: 0.8153
Mejores resultados en la época:  3
f1-score 0.6663733169057467
AUC según el mejor F1-score 0.8606670421401281
Confusion Matrix:
 [[14048  2417]
 [ 1374  3786]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8247
Precision:  0.6103
Recall:     0.7337
F1-score:   0.6664

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.5080, Test Loss: 0.4216, F1: 0.6637, AUC: 0.8560
Epoch [10/30] Train Loss: 0.3014, Test Loss: 0.6343, F1: 0.6204, AUC: 0.8330
Epoch [20/30] Train Loss: 0.1640, Test Loss: 1.2885, F1: 0.5976, AUC: 0.8112
Mejores resultados en la época:  4
f1-score 0.6677527378043262
AUC según el mejor F1-score 0.8571328775862352
Confusion Matrix:
 [[14265  2200]
 [ 1471  3689]]
Matriz de confusión guardada en: outputs_only_text/0/lyrics_bert/confusion_matrix_param_1007617.png
Accuracy:   0.8302
Precision:  0.6264
Recall:     0.7149
F1-score:   0.6678
Tiempo total para red 6: 247.73 segundos
Saved on: outputs_only_text/0/lyrics_bert

==============================
Model: Logistic Regression
Accuracy:  0.7997
Precision: 0.5589
Recall:    0.7609
F1-score:  0.6445
              precision    recall  f1-score   support

           0       0.92      0.81      0.86     16465
           1       0.56      0.76      0.64      5160

    accuracy                           0.80     21625
   macro avg       0.74      0.79      0.75     21625
weighted avg       0.83      0.80      0.81     21625

[[13367  3098]
 [ 1234  3926]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.3120
Precision: 0.2525
Recall:    0.9609
F1-score:  0.4000
              precision    recall  f1-score   support

           0       0.90      0.11      0.19     16465
           1       0.25      0.96      0.40      5160

    accuracy                           0.31     21625
   macro avg       0.58      0.53      0.30     21625
weighted avg       0.74      0.31      0.24     21625

[[ 1790 14675]
 [  202  4958]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_svm.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.6364
Precision: 0.3664
Recall:    0.7178
F1-score:  0.4851
              precision    recall  f1-score   support

           0       0.87      0.61      0.72     16465
           1       0.37      0.72      0.49      5160

    accuracy                           0.64     21625
   macro avg       0.62      0.66      0.60     21625
weighted avg       0.75      0.64      0.66     21625
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [02:59:26] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/marcelino.maita/musica_ia_workshop/models_comparison/testing_organized/no_preprocessed_data/main_only_text.py:280: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  .apply(lambda grp: grp.sample(n=n_min, random_state=RANDOM_STATE))

[[10059  6406]
 [ 1456  3704]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.7723
Precision: 0.5167
Recall:    0.7064
F1-score:  0.5968
              precision    recall  f1-score   support

           0       0.90      0.79      0.84     16465
           1       0.52      0.71      0.60      5160

    accuracy                           0.77     21625
   macro avg       0.71      0.75      0.72     21625
weighted avg       0.81      0.77      0.78     21625

[[13055  3410]
 [ 1515  3645]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8007
Precision: 0.5633
Recall:    0.7328
F1-score:  0.6370
              precision    recall  f1-score   support

           0       0.91      0.82      0.86     16465
           1       0.56      0.73      0.64      5160

    accuracy                           0.80     21625
   macro avg       0.74      0.78      0.75     21625
weighted avg       0.83      0.80      0.81     21625

[[13534  2931]
 [ 1379  3781]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.6441
Precision: 0.3788
Recall:    0.7682
F1-score:  0.5074
              precision    recall  f1-score   support

           0       0.89      0.61      0.72     16465
           1       0.38      0.77      0.51      5160

    accuracy                           0.64     21625
   macro avg       0.64      0.69      0.61     21625
weighted avg       0.77      0.64      0.67     21625

[[9964 6501]
 [1196 3964]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_only_text/0/lyrics_bert/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text/0/lyrics_bert/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.7997, 'precision': 0.5589, 'recall': 0.7609, 'f1_score': 0.6445}
XGBoost: {'accuracy': 0.8007, 'precision': 0.5633, 'recall': 0.7328, 'f1_score': 0.637}
Random Forest: {'accuracy': 0.7723, 'precision': 0.5167, 'recall': 0.7064, 'f1_score': 0.5968}
Naive Bayes: {'accuracy': 0.6441, 'precision': 0.3788, 'recall': 0.7682, 'f1_score': 0.5074}
Decision Tree: {'accuracy': 0.6364, 'precision': 0.3664, 'recall': 0.7178, 'f1_score': 0.4851}
SVM: {'accuracy': 0.312, 'precision': 0.2525, 'recall': 0.9609, 'f1_score': 0.4}

##################################################
Running experiment with GPT embeddings
Loading Lb vectors from:  ../../data/spotify_dataset_sin_duplicados_4.csv
Se eliminarán los siguientes indices [9681, 13382, 27808, 38450, 46753, 53015, 53018, 53102, 53137, 57281, 70666, 75102, 98849]
Shape original y: (108138,)
Shape filtrado  y: (108125,)
Label distribution: {0: 82336, 1: 25802}
X shape: (108125, 1536)
y shape: (108125,)

Splitting data...

Data con el spliting...
Label distribution en TRAIN: {0: 65861, 1: 20639}
Label distribution en TEST: {0: 16465, 1: 5160}


==================================================
Data antes del undersampling ...
X: (86500, 1536)
y: (86500,)
Apliying UNDERSAMPLE
20639
Label distribution: {0: 20639, 1: 20639}
X shape: (41278, 1536)
y shape: (41278,)
Resultados con MLP

Entrenando red 1 con capas [1536, 32, 1]

--- Iteración 1 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4340, Test Loss: 0.4092, F1: 0.7016, AUC: 0.8983
Epoch [10/30] Train Loss: 0.3540, Test Loss: 0.3881, F1: 0.7166, AUC: 0.9080
Epoch [20/30] Train Loss: 0.3305, Test Loss: 0.3972, F1: 0.7173, AUC: 0.9077
Mejores resultados en la época:  18
f1-score 0.7271484708696395
AUC según el mejor F1-score 0.9080824252525324
Confusion Matrix:
 [[14175  2290]
 [  904  4256]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8523
Precision:  0.6502
Recall:     0.8248
F1-score:   0.7271

--- Iteración 2 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4323, Test Loss: 0.3986, F1: 0.7026, AUC: 0.8987
Epoch [10/30] Train Loss: 0.3558, Test Loss: 0.3778, F1: 0.7231, AUC: 0.9085
Epoch [20/30] Train Loss: 0.3366, Test Loss: 0.3694, F1: 0.7226, AUC: 0.9081
Mejores resultados en la época:  28
f1-score 0.7303955586398334
AUC según el mejor F1-score 0.906851507896713
Confusion Matrix:
 [[14307  2158]
 [  950  4210]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8563
Precision:  0.6611
Recall:     0.8159
F1-score:   0.7304

--- Iteración 3 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4339, Test Loss: 0.3879, F1: 0.7091, AUC: 0.8975
Epoch [10/30] Train Loss: 0.3571, Test Loss: 0.3797, F1: 0.7189, AUC: 0.9083
Epoch [20/30] Train Loss: 0.3375, Test Loss: 0.4252, F1: 0.7019, AUC: 0.9060
Mejores resultados en la época:  15
f1-score 0.7254819429244886
AUC según el mejor F1-score 0.9088221020864085
Confusion Matrix:
 [[14055  2410]
 [  851  4309]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8492
Precision:  0.6413
Recall:     0.8351
F1-score:   0.7255

--- Iteración 4 de 4 para la red 1 ---
Epoch [0/30] Train Loss: 0.4389, Test Loss: 0.3926, F1: 0.7049, AUC: 0.8972
Epoch [10/30] Train Loss: 0.3556, Test Loss: 0.3860, F1: 0.7177, AUC: 0.9087
Epoch [20/30] Train Loss: 0.3363, Test Loss: 0.3746, F1: 0.7205, AUC: 0.9070
Mejores resultados en la época:  12
f1-score 0.7281620044619873
AUC según el mejor F1-score 0.9087764685249661
Confusion Matrix:
 [[14214  2251]
 [  917  4243]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_49217.png
Accuracy:   0.8535
Precision:  0.6534
Recall:     0.8223
F1-score:   0.7282
Tiempo total para red 1: 208.77 segundos

Entrenando red 2 con capas [1536, 64, 32, 1]

--- Iteración 1 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4187, Test Loss: 0.3693, F1: 0.7166, AUC: 0.9032
Epoch [10/30] Train Loss: 0.3280, Test Loss: 0.3805, F1: 0.7185, AUC: 0.9074
Epoch [20/30] Train Loss: 0.2114, Test Loss: 0.5039, F1: 0.6680, AUC: 0.8823
Mejores resultados en la época:  4
f1-score 0.725
AUC según el mejor F1-score 0.909869325819156
Confusion Matrix:
 [[14077  2388]
 [  868  4292]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8494
Precision:  0.6425
Recall:     0.8318
F1-score:   0.7250

--- Iteración 2 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4217, Test Loss: 0.3638, F1: 0.7148, AUC: 0.9021
Epoch [10/30] Train Loss: 0.3393, Test Loss: 0.3896, F1: 0.7161, AUC: 0.9081
Epoch [20/30] Train Loss: 0.2419, Test Loss: 0.4551, F1: 0.6880, AUC: 0.8872
Mejores resultados en la época:  5
f1-score 0.726016751575857
AUC según el mejor F1-score 0.9095429463955724
Confusion Matrix:
 [[14248  2217]
 [  956  4204]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8533
Precision:  0.6547
Recall:     0.8147
F1-score:   0.7260

--- Iteración 3 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4245, Test Loss: 0.4093, F1: 0.7014, AUC: 0.9003
Epoch [10/30] Train Loss: 0.3365, Test Loss: 0.4048, F1: 0.7098, AUC: 0.9080
Epoch [20/30] Train Loss: 0.2460, Test Loss: 0.4665, F1: 0.6738, AUC: 0.8883
Mejores resultados en la época:  9
f1-score 0.7224939888898101
AUC según el mejor F1-score 0.9097869217532139
Confusion Matrix:
 [[13921  2544]
 [  803  4357]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8452
Precision:  0.6314
Recall:     0.8444
F1-score:   0.7225

--- Iteración 4 de 4 para la red 2 ---
Epoch [0/30] Train Loss: 0.4193, Test Loss: 0.3987, F1: 0.7094, AUC: 0.9030
Epoch [10/30] Train Loss: 0.3228, Test Loss: 0.3948, F1: 0.7124, AUC: 0.9057
Epoch [20/30] Train Loss: 0.1986, Test Loss: 0.5520, F1: 0.6830, AUC: 0.8816
Mejores resultados en la época:  11
f1-score 0.7241939621996066
AUC según el mejor F1-score 0.9050115820026978
Confusion Matrix:
 [[14166  2299]
 [  926  4234]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_100481.png
Accuracy:   0.8509
Precision:  0.6481
Recall:     0.8205
F1-score:   0.7242
Tiempo total para red 2: 225.31 segundos

Entrenando red 3 con capas [1536, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4160, Test Loss: 0.3738, F1: 0.7154, AUC: 0.9037
Epoch [10/30] Train Loss: 0.3101, Test Loss: 0.4318, F1: 0.6970, AUC: 0.8992
Epoch [20/30] Train Loss: 0.1348, Test Loss: 0.7096, F1: 0.6740, AUC: 0.8651
Mejores resultados en la época:  5
f1-score 0.728278723956082
AUC según el mejor F1-score 0.9086167275192621
Confusion Matrix:
 [[14270  2195]
 [  948  4212]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8547
Precision:  0.6574
Recall:     0.8163
F1-score:   0.7283

--- Iteración 2 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4182, Test Loss: 0.3581, F1: 0.7173, AUC: 0.9041
Epoch [10/30] Train Loss: 0.2981, Test Loss: 0.4437, F1: 0.6937, AUC: 0.8957
Epoch [20/30] Train Loss: 0.1310, Test Loss: 0.8164, F1: 0.6793, AUC: 0.8642
Mejores resultados en la época:  3
f1-score 0.7277167277167277
AUC según el mejor F1-score 0.9095931939255691
Confusion Matrix:
 [[14331  2134]
 [  988  4172]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8556
Precision:  0.6616
Recall:     0.8085
F1-score:   0.7277

--- Iteración 3 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4156, Test Loss: 0.3551, F1: 0.7186, AUC: 0.9044
Epoch [10/30] Train Loss: 0.2942, Test Loss: 0.4405, F1: 0.6999, AUC: 0.8964
Epoch [20/30] Train Loss: 0.1295, Test Loss: 0.7725, F1: 0.6757, AUC: 0.8613
Mejores resultados en la época:  4
f1-score 0.7293486722601851
AUC según el mejor F1-score 0.9096644456057834
Confusion Matrix:
 [[14280  2185]
 [  944  4216]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8553
Precision:  0.6586
Recall:     0.8171
F1-score:   0.7293

--- Iteración 4 de 4 para la red 3 ---
Epoch [0/30] Train Loss: 0.4174, Test Loss: 0.3484, F1: 0.7202, AUC: 0.9051
Epoch [10/30] Train Loss: 0.3032, Test Loss: 0.4092, F1: 0.7033, AUC: 0.8965
Epoch [20/30] Train Loss: 0.1275, Test Loss: 0.6847, F1: 0.6896, AUC: 0.8714
Mejores resultados en la época:  4
f1-score 0.727131381186836
AUC según el mejor F1-score 0.9101738830547297
Confusion Matrix:
 [[14257  2208]
 [  951  4209]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_207105.png
Accuracy:   0.8539
Precision:  0.6559
Recall:     0.8157
F1-score:   0.7271
Tiempo total para red 3: 238.49 segundos

Entrenando red 4 con capas [1536, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4141, Test Loss: 0.3709, F1: 0.7146, AUC: 0.9045
Epoch [10/30] Train Loss: 0.2737, Test Loss: 0.4388, F1: 0.6938, AUC: 0.8847
Epoch [20/30] Train Loss: 0.1000, Test Loss: 0.9315, F1: 0.6658, AUC: 0.8542
Mejores resultados en la época:  6
f1-score 0.7217529480638957
AUC según el mejor F1-score 0.9037941887536871
Confusion Matrix:
 [[13983  2482]
 [  845  4315]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8462
Precision:  0.6348
Recall:     0.8362
F1-score:   0.7218

--- Iteración 2 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4142, Test Loss: 0.4307, F1: 0.6964, AUC: 0.9049
Epoch [10/30] Train Loss: 0.2656, Test Loss: 0.4382, F1: 0.6989, AUC: 0.8924
Epoch [20/30] Train Loss: 0.0898, Test Loss: 0.8985, F1: 0.6792, AUC: 0.8662
Mejores resultados en la época:  5
f1-score 0.7300479720889664
AUC según el mejor F1-score 0.9083178200411022
Confusion Matrix:
 [[14345  2120]
 [  975  4185]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8569
Precision:  0.6638
Recall:     0.8110
F1-score:   0.7300

--- Iteración 3 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4162, Test Loss: 0.3725, F1: 0.7132, AUC: 0.9052
Epoch [10/30] Train Loss: 0.2741, Test Loss: 0.4276, F1: 0.7047, AUC: 0.8938
Epoch [20/30] Train Loss: 0.0916, Test Loss: 0.9479, F1: 0.6648, AUC: 0.8600
Mejores resultados en la época:  6
f1-score 0.7279187817258883
AUC según el mejor F1-score 0.908741681320725
Confusion Matrix:
 [[14107  2358]
 [  858  4302]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8513
Precision:  0.6459
Recall:     0.8337
F1-score:   0.7279

--- Iteración 4 de 4 para la red 4 ---
Epoch [0/30] Train Loss: 0.4158, Test Loss: 0.4140, F1: 0.6961, AUC: 0.9044
Epoch [10/30] Train Loss: 0.2688, Test Loss: 0.4752, F1: 0.6858, AUC: 0.8862
Epoch [20/30] Train Loss: 0.0879, Test Loss: 0.8610, F1: 0.6853, AUC: 0.8688
Mejores resultados en la época:  3
f1-score 0.728598715054697
AUC según el mejor F1-score 0.9094405857385999
Confusion Matrix:
 [[14303  2162]
 [  964  4196]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_436737.png
Accuracy:   0.8554
Precision:  0.6600
Recall:     0.8132
F1-score:   0.7286
Tiempo total para red 4: 255.25 segundos

Entrenando red 5 con capas [1536, 512, 256, 128, 64, 1]

--- Iteración 1 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4098, Test Loss: 0.3925, F1: 0.7118, AUC: 0.9062
Epoch [10/30] Train Loss: 0.2142, Test Loss: 0.5161, F1: 0.6956, AUC: 0.8805
Epoch [20/30] Train Loss: 0.0645, Test Loss: 1.0317, F1: 0.6941, AUC: 0.8739
Mejores resultados en la época:  3
f1-score 0.7251739942284842
AUC según el mejor F1-score 0.9098776180151931
Confusion Matrix:
 [[14115  2350]
 [  888  4272]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8503
Precision:  0.6451
Recall:     0.8279
F1-score:   0.7252

--- Iteración 2 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4113, Test Loss: 0.3753, F1: 0.7158, AUC: 0.9054
Epoch [10/30] Train Loss: 0.2313, Test Loss: 0.5347, F1: 0.6784, AUC: 0.8818
Epoch [20/30] Train Loss: 0.0665, Test Loss: 0.9878, F1: 0.6619, AUC: 0.8623
Mejores resultados en la época:  6
f1-score 0.7248870351060132
AUC según el mejor F1-score 0.9056012930882281
Confusion Matrix:
 [[14288  2177]
 [  989  4171]]
/home/marcelino.maita/.venv/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  warnings.warn(
/home/marcelino.maita/.venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [03:36:50] WARNING: /workspace/src/learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8536
Precision:  0.6571
Recall:     0.8083
F1-score:   0.7249

--- Iteración 3 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4108, Test Loss: 0.4425, F1: 0.6960, AUC: 0.9049
Epoch [10/30] Train Loss: 0.2231, Test Loss: 0.4984, F1: 0.6993, AUC: 0.8806
Epoch [20/30] Train Loss: 0.0668, Test Loss: 0.8654, F1: 0.6836, AUC: 0.8686
Mejores resultados en la época:  4
f1-score 0.7284406898979233
AUC según el mejor F1-score 0.9070362549641358
Confusion Matrix:
 [[14400  2065]
 [ 1021  4139]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8573
Precision:  0.6672
Recall:     0.8021
F1-score:   0.7284

--- Iteración 4 de 4 para la red 5 ---
Epoch [0/30] Train Loss: 0.4113, Test Loss: 0.4328, F1: 0.6873, AUC: 0.9045
Epoch [10/30] Train Loss: 0.2193, Test Loss: 0.5774, F1: 0.6631, AUC: 0.8821
Epoch [20/30] Train Loss: 0.0609, Test Loss: 1.1001, F1: 0.6738, AUC: 0.8664
Mejores resultados en la época:  4
f1-score 0.724135028201027
AUC según el mejor F1-score 0.9081704261094122
Confusion Matrix:
 [[14047  2418]
 [  859  4301]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_959489.png
Accuracy:   0.8485
Precision:  0.6401
Recall:     0.8335
F1-score:   0.7241
Tiempo total para red 5: 264.37 segundos

Entrenando red 6 con capas [1536, 1024, 512, 256, 128, 64, 32, 1]

--- Iteración 1 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4177, Test Loss: 0.3944, F1: 0.7101, AUC: 0.9053
Epoch [10/30] Train Loss: 0.2044, Test Loss: 0.5463, F1: 0.6697, AUC: 0.8689
Epoch [20/30] Train Loss: 0.0541, Test Loss: 1.0840, F1: 0.6833, AUC: 0.8709
Mejores resultados en la época:  3
f1-score 0.7253583436559901
AUC según el mejor F1-score 0.9099485577817168
Confusion Matrix:
 [[14422  2043]
 [ 1061  4099]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8565
Precision:  0.6674
Recall:     0.7944
F1-score:   0.7254

--- Iteración 2 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4137, Test Loss: 0.4923, F1: 0.6746, AUC: 0.9035
Epoch [10/30] Train Loss: 0.2021, Test Loss: 0.5981, F1: 0.6724, AUC: 0.8727
Epoch [20/30] Train Loss: 0.0523, Test Loss: 1.3591, F1: 0.6849, AUC: 0.8749
Mejores resultados en la época:  2
f1-score 0.7213606803401701
AUC según el mejor F1-score 0.9088594081408295
Confusion Matrix:
 [[13957  2508]
 [  834  4326]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8455
Precision:  0.6330
Recall:     0.8384
F1-score:   0.7214

--- Iteración 3 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4139, Test Loss: 0.4747, F1: 0.6786, AUC: 0.9047
Epoch [10/30] Train Loss: 0.2047, Test Loss: 0.5423, F1: 0.6952, AUC: 0.8778
Epoch [20/30] Train Loss: 0.0497, Test Loss: 1.0374, F1: 0.6748, AUC: 0.8698
Mejores resultados en la época:  3
f1-score 0.7238900634249471
AUC según el mejor F1-score 0.9096886689406941
Confusion Matrix:
 [[14080  2385]
 [  880  4280]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8490
Precision:  0.6422
Recall:     0.8295
F1-score:   0.7239

--- Iteración 4 de 4 para la red 6 ---
Epoch [0/30] Train Loss: 0.4128, Test Loss: 0.3691, F1: 0.7096, AUC: 0.9050
Epoch [10/30] Train Loss: 0.2104, Test Loss: 0.7816, F1: 0.6151, AUC: 0.8648
Epoch [20/30] Train Loss: 0.0532, Test Loss: 1.1593, F1: 0.6676, AUC: 0.8659
Mejores resultados en la época:  3
f1-score 0.7196988296914641
AUC según el mejor F1-score 0.9090774358105166
Confusion Matrix:
 [[13803  2662]
 [  763  4397]]
Matriz de confusión guardada en: outputs_only_text/0/gpt/confusion_matrix_param_2273281.png
Accuracy:   0.8416
Precision:  0.6229
Recall:     0.8521
F1-score:   0.7197
Tiempo total para red 6: 296.92 segundos
Saved on: outputs_only_text/0/gpt

==============================
Model: Logistic Regression
Accuracy:  0.8428
Precision: 0.6301
Recall:    0.8258
F1-score:  0.7148
              precision    recall  f1-score   support

           0       0.94      0.85      0.89     16465
           1       0.63      0.83      0.71      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.84      0.80     21625
weighted avg       0.87      0.84      0.85     21625

[[13964  2501]
 [  899  4261]]
Hiperparámetros: {'C': 1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_logistic_regression.png
Modelo guardado como: outputs_only_text/0/gpt/logistic_regression_model.pkl

==============================
Model: SVM
Accuracy:  0.5736
Precision: 0.3249
Recall:    0.7302
F1-score:  0.4497
              precision    recall  f1-score   support

           0       0.86      0.52      0.65     16465
           1       0.32      0.73      0.45      5160

    accuracy                           0.57     21625
   macro avg       0.59      0.63      0.55     21625
weighted avg       0.73      0.57      0.60     21625

[[8637 7828]
 [1392 3768]]
Hiperparámetros: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_svm.png
Modelo guardado como: outputs_only_text/0/gpt/svm_model.pkl

==============================
Model: Decision Tree
Accuracy:  0.7598
Precision: 0.4977
Recall:    0.7477
F1-score:  0.5976
              precision    recall  f1-score   support

           0       0.91      0.76      0.83     16465
           1       0.50      0.75      0.60      5160

    accuracy                           0.76     21625
   macro avg       0.70      0.76      0.71     21625
weighted avg       0.81      0.76      0.77     21625

[[12572  3893]
 [ 1302  3858]]
Hiperparámetros: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 42, 'splitter': 'best'}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_decision_tree.png
Modelo guardado como: outputs_only_text/0/gpt/decision_tree_model.pkl

==============================
Model: Random Forest
Accuracy:  0.8427
Precision: 0.6450
Recall:    0.7583
F1-score:  0.6971
              precision    recall  f1-score   support

           0       0.92      0.87      0.89     16465
           1       0.64      0.76      0.70      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.81      0.80     21625
weighted avg       0.85      0.84      0.85     21625

[[14311  2154]
 [ 1247  3913]]
Hiperparámetros: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_random_forest.png
Modelo guardado como: outputs_only_text/0/gpt/random_forest_model.pkl

==============================
Model: XGBoost
Accuracy:  0.8379
Precision: 0.6240
Recall:    0.8068
F1-score:  0.7037
              precision    recall  f1-score   support

           0       0.93      0.85      0.89     16465
           1       0.62      0.81      0.70      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.83      0.80     21625
weighted avg       0.86      0.84      0.84     21625

[[13957  2508]
 [  997  4163]]
Hiperparámetros: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.8, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 8, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'use_label_encoder': False}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_xgboost.png
Modelo guardado como: outputs_only_text/0/gpt/xgboost_model.pkl

==============================
Model: Naive Bayes
Accuracy:  0.8404
Precision: 0.6511
Recall:    0.7132
F1-score:  0.6807
              precision    recall  f1-score   support

           0       0.91      0.88      0.89     16465
           1       0.65      0.71      0.68      5160

    accuracy                           0.84     21625
   macro avg       0.78      0.80      0.79     21625
weighted avg       0.85      0.84      0.84     21625

[[14493  1972]
 [ 1480  3680]]
Hiperparámetros: {'priors': None, 'var_smoothing': 1e-09}
Confusion matrix saved as: outputs_only_text/0/gpt/conf_matrix_naive_bayes.png
Modelo guardado como: outputs_only_text/0/gpt/naive_bayes_model.pkl


Resumen de métricas:
Logistic Regression: {'accuracy': 0.8428, 'precision': 0.6301, 'recall': 0.8258, 'f1_score': 0.7148}
XGBoost: {'accuracy': 0.8379, 'precision': 0.624, 'recall': 0.8068, 'f1_score': 0.7037}
Random Forest: {'accuracy': 0.8427, 'precision': 0.645, 'recall': 0.7583, 'f1_score': 0.6971}
Naive Bayes: {'accuracy': 0.8404, 'precision': 0.6511, 'recall': 0.7132, 'f1_score': 0.6807}
Decision Tree: {'accuracy': 0.7598, 'precision': 0.4977, 'recall': 0.7477, 'f1_score': 0.5976}
SVM: {'accuracy': 0.5736, 'precision': 0.3249, 'recall': 0.7302, 'f1_score': 0.4497}


Resumen GLOBAL de métricas:


EMBEDDINGS TYPE: TFIDF
XGBoost: {'accuracy': 0.865, 'precision': 0.6762, 'recall': 0.8329, 'f1_score': 0.7464}
Decision Tree: {'accuracy': 0.8617, 'precision': 0.6782, 'recall': 0.8004, 'f1_score': 0.7342}
Random Forest: {'accuracy': 0.8535, 'precision': 0.674, 'recall': 0.7473, 'f1_score': 0.7088}
MLP_5820417: {'accuracy': 0.8492947976878613, 'precision': 0.662784723411543, 'recall': 0.75, 'f1_score': 0.7037003363942177, 'f1_score_avg': 0.6921270014984262}
MLP_650497: {'accuracy': 0.8340809248554913, 'precision': 0.6205891377723228, 'recall': 0.7839147286821705, 'f1_score': 0.7020436803789142, 'f1_score_avg': 0.6949385342123928}
MLP_1323521: {'accuracy': 0.83278612716763, 'precision': 0.6148467717941088, 'recall': 0.8009689922480621, 'f1_score': 0.7020436803789142, 'f1_score_avg': 0.6936623914121872}
MLP_2733057: {'accuracy': 0.8349132947976878, 'precision': 0.6225339087546239, 'recall': 0.7827519379844962, 'f1_score': 0.7020436803789142, 'f1_score_avg': 0.6928976961276091}
MLP_322177: {'accuracy': 0.8393526011560694, 'precision': 0.6354869816779171, 'recall': 0.7662790697674419, 'f1_score': 0.6976744186046512, 'f1_score_avg': 0.6942523700844809}
MLP_160065: {'accuracy': 0.8328323699421966, 'precision': 0.6171697254664038, 'recall': 0.7885658914728683, 'f1_score': 0.6930556750709922, 'f1_score_avg': 0.6924026512352446}
Logistic Regression: {'accuracy': 0.8198, 'precision': 0.5973, 'recall': 0.7514, 'f1_score': 0.6655}
Naive Bayes: {'accuracy': 0.8204, 'precision': 0.6063, 'recall': 0.7052, 'f1_score': 0.652}
SVM: {'accuracy': 0.625, 'precision': 0.3522, 'recall': 0.6812, 'f1_score': 0.4643}


EMBEDDINGS TYPE: LYRICS_BERT
MLP_326657: {'accuracy': 0.8333410404624277, 'precision': 0.6348821081830791, 'recall': 0.7096899224806201, 'f1_score': 0.6709525534226729, 'f1_score_avg': 0.6683554551780765}
MLP_1007617: {'accuracy': 0.830242774566474, 'precision': 0.6264221429784343, 'recall': 0.7149224806201551, 'f1_score': 0.6709525534226729, 'f1_score_avg': 0.6634113618639492}
MLP_48897: {'accuracy': 0.8289479768786128, 'precision': 0.6222185042663544, 'recall': 0.7207364341085272, 'f1_score': 0.6706809528026211, 'f1_score_avg': 0.6682427123985352}
MLP_120321: {'accuracy': 0.8143815028901734, 'precision': 0.5872677429180627, 'recall': 0.7472868217054264, 'f1_score': 0.6706809528026211, 'f1_score_avg': 0.6653025021318372}
MLP_9665: {'accuracy': 0.8186358381502891, 'precision': 0.5947642375995101, 'recall': 0.752906976744186, 'f1_score': 0.669817563096701, 'f1_score_avg': 0.6654344636861136}
MLP_21377: {'accuracy': 0.8322312138728324, 'precision': 0.6346221441124781, 'recall': 0.6998062015503876, 'f1_score': 0.669817563096701, 'f1_score_avg': 0.6655329925655225}
Logistic Regression: {'accuracy': 0.7997, 'precision': 0.5589, 'recall': 0.7609, 'f1_score': 0.6445}
XGBoost: {'accuracy': 0.8007, 'precision': 0.5633, 'recall': 0.7328, 'f1_score': 0.637}
Random Forest: {'accuracy': 0.7723, 'precision': 0.5167, 'recall': 0.7064, 'f1_score': 0.5968}
Naive Bayes: {'accuracy': 0.6441, 'precision': 0.3788, 'recall': 0.7682, 'f1_score': 0.5074}
Decision Tree: {'accuracy': 0.6364, 'precision': 0.3664, 'recall': 0.7178, 'f1_score': 0.4851}
SVM: {'accuracy': 0.312, 'precision': 0.2525, 'recall': 0.9609, 'f1_score': 0.4}


EMBEDDINGS TYPE: GPT
MLP_49217: {'accuracy': 0.8535028901734104, 'precision': 0.6533723437018787, 'recall': 0.8222868217054263, 'f1_score': 0.7303955586398334, 'f1_score_avg': 0.7277969942239872}
MLP_100481: {'accuracy': 0.8508670520231214, 'precision': 0.6480942905250268, 'recall': 0.8205426356589147, 'f1_score': 0.7303955586398334, 'f1_score_avg': 0.7244261756663184}
MLP_207105: {'accuracy': 0.8539190751445087, 'precision': 0.6559139784946236, 'recall': 0.8156976744186046, 'f1_score': 0.7303955586398334, 'f1_score_avg': 0.7281188762799576}
MLP_436737: {'accuracy': 0.8554450867052024, 'precision': 0.6599559609940233, 'recall': 0.8131782945736434, 'f1_score': 0.7303955586398334, 'f1_score_avg': 0.7270796042333618}
MLP_959489: {'accuracy': 0.8484624277456647, 'precision': 0.6401250186039589, 'recall': 0.8335271317829457, 'f1_score': 0.7303955586398334, 'f1_score_avg': 0.725659186858362}
MLP_2273281: {'accuracy': 0.8416184971098266, 'precision': 0.622892761014308, 'recall': 0.8521317829457364, 'f1_score': 0.7303955586398334, 'f1_score_avg': 0.7225769792781428}
Logistic Regression: {'accuracy': 0.8428, 'precision': 0.6301, 'recall': 0.8258, 'f1_score': 0.7148}
XGBoost: {'accuracy': 0.8379, 'precision': 0.624, 'recall': 0.8068, 'f1_score': 0.7037}
Random Forest: {'accuracy': 0.8427, 'precision': 0.645, 'recall': 0.7583, 'f1_score': 0.6971}
Naive Bayes: {'accuracy': 0.8404, 'precision': 0.6511, 'recall': 0.7132, 'f1_score': 0.6807}
Decision Tree: {'accuracy': 0.7598, 'precision': 0.4977, 'recall': 0.7477, 'f1_score': 0.5976}
SVM: {'accuracy': 0.5736, 'precision': 0.3249, 'recall': 0.7302, 'f1_score': 0.4497}

========== CONFIGURATIONS ==========
TESTING: False
UNDERSAMPLING: True
USE_SMOTE: False
SCALED: True
STEAMING: False
REMOVE_STOPWORDS: True
NUMERIC_COLS: False
MAX_FEATURES_TFIDF: 5000
TF-IDF Columns: ['text']
Numeric Columns: Not used
====================================

